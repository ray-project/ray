atari-100k:
  # Run with --env ALE/[gym ID], e.g. ALE/Pong-v5.

  # [2]: "We follow the evaluation protocol of Machado et al. (2018) with 200M
  # environment steps, action repeat of 4, a time limit of 108,000 steps per
  # episode that correspond to 30 minutes of game play, no access to life
  # information, full action space, and sticky actions. Because the world model
  # integrates information over time, DreamerV2 does not use frame stacking.
  # The experiments use a single-task setup where a separate agent is trained
  # for each game. Moreover, each agent uses only a single environment instance.
  repeat_action_probability: 0.0  # "sticky actions" but not according to Danijar's 100k configs.
  full_action_space: false  # "full action space" but not according to Danijar's 100k configs.
  frameskip: 1  # already done by MaxAndSkip wrapper: "action repeat" == 4

  # See Appendix A.
  model_dimension: S
  training_ratio: 1024

  # Run config.
  summary_frequency_train_steps: 100
  evaluation_frequency_main_iters: 1000
  evaluation_num_episodes: 2
