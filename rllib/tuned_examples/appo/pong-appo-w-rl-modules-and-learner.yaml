# This can reach 18.0 reward in ~10 minutes on 4x M60 GPUs
# with 30 rollout workers, 4 learning workers, and 8 envs per rollout worker.
pong-appo:
    env: ALE/Pong-v5
    run: APPO
    stop:
        episode_reward_mean: 18.0
        timesteps_total: 20000000
    config:
        # Works for both torch and tf.
        framework: torch
        # Make analogous to old v4 + NoFrameskip.
        env_config:
            frameskip: 1  # no frameskip
            full_action_space: false
            repeat_action_probability: 0.0  # deterministic
        vtrace: true
        #vtrace_drop_last_ts: false
        use_kl_loss: false
        rollout_fragment_length: 50
        train_batch_size: 4000
        lr: 0.0005
        # On a 32 CPU machine (g3.2xlarge), we use 30 CPUs for the rollout workers
        # and 2 for the learner workers.
        num_workers: 30
        broadcast_interval: 1
        max_sample_requests_in_flight_per_worker: 1
        num_envs_per_worker: 8
        num_sgd_iter: 2
        vf_loss_coeff: 1.0
        clip_param: 0.3
        num_gpus: 0

        model:
            dim: 42
            conv_filters: [[16, 4, 2], [32, 4, 2], [256, 11, 1, "valid"]]
            conv_activation: relu
            conv_add_final_dense: false
            conv_flattened_dim: 256
            use_cnn_heads: true

        # Run with Learner API.
        _enable_learner_api: true
        grad_clip_by_global_norm: 10.0
        # Use N Learner worker on the GPU.
        num_learner_workers: 4
        num_gpus_per_learner_worker: 1
        # Since we are using learner workers, the driver process does not need
        # a CPU in particular.
        num_cpus_for_local_worker: 1

        # Run with RLModule API.
        _enable_rl_module_api: true
        # Need to unset this b/c we are using the RLModule API, which
        # provides exploration control via the RLModule's `forward_exploration` method.
        exploration_config: {}
