from abc import abstractmethod, ABCMeta
import logging
from typing import Dict, Optional

from ray.rllib.evaluation.episode import MultiAgentEpisode
from ray.rllib.models.modelv2 import ModelV2
from ray.rllib.policy.sample_batch import MultiAgentBatch
from ray.rllib.utils.types import AgentID, EpisodeID, EnvID, PolicyID, \
    TensorType

logger = logging.getLogger(__name__)


class _SampleCollector(metaclass=ABCMeta):
    """Collects samples for all policies and agents from a multi-agent env.

    Note: This is an experimental class only used when
    `config._use_trajectory_view_api` = True.
    Once `_use_trajectory_view_api` becomes the default in configs:
    This class will deprecate the `SampleBatchBuilder` and
    `MultiAgentBatchBuilder` classes.

    This API is controlled by RolloutWorker objects to store all data
    generated by Environments and Policies/Models during rollout and
    postprocessing. Also provides a method to generate input_dicts for
    action calculation and train batches from the collected data.
    """

    @abstractmethod
    def add_init_obs(self, episode_id: EpisodeID, agent_id: AgentID,
                     policy_id: PolicyID, init_obs: TensorType) -> None:
        """Adds an initial obs (after reset) to this collector.

        Args:
            episode_id (EpisodeID): Unique id for the episode we are adding
                values for.
            agent_id (AgentID): Unique id for the agent we are adding
                values for.
            policy_id (PolicyID): Unique id for policy controlling the agent.
            init_obs (TensorType): Initial observation (after env.reset()).
        """
        raise NotImplementedError

    @abstractmethod
    def add_action_reward_next_obs(
            self,
            episode_id: EpisodeID,
            agent_id: AgentID,
            policy_id: PolicyID,
            values: Dict[str, TensorType]) -> None:
        """Add the given dictionary (row) of values to this collector.

        The incoming data (`values`) must include action, reward, done, and
        next_obs information and may include any other information.

        Args:
            env_id (EpisodeID): Unique id for the episode we are adding values
                for.
            agent_id (AgentID): Unique id for the agent we are adding
                values for.
            policy_id (PolicyID): Unique id for policy controlling the agent.
            values (Dict[str,TensorType]): Row of values to add for this agent.
                This row must contain the keys SampleBatch.ACTION, REWARD,
                NEW_OBS, and DONE.
        """
        raise NotImplementedError

    @abstractmethod
    def total_env_steps(self) -> int:
        """Returns total number of steps taken in the env (sum of all agents).

        Returns:
            int: The number of steps taken in total in the environment over all
                agents.
        """
        raise NotImplementedError

    @abstractmethod
    def add_to_forward_pass(self,
                            episode_id: EpisodeID,
                            agent_id: AgentID,
                            env_id: EnvID) -> None:
        """Registers given agent/episode combination for the next forward pass.

        Args:
            episode_id (EpisodeID): Unique id for the episode we are
                registering for the upcoming forward pass.
            agent_id (AgentID): Unique id for the agent we are registering
                for the upcoming forward pass.
            env_id (EnvID): Unique id for the (sub)-environment.
        """
        raise NotImplementedError

    @abstractmethod
    def reset_forward_pass(self) -> None:
        """Resets all forward pass information (new items can be registered).
        """
        raise NotImplementedError

    @abstractmethod
    def get_input_dict(
            self,
            model: ModelV2,
            is_training: bool = False,
            is_postprocessing: bool = False) -> Dict[str, TensorType]:
        """Returns an input_dict for a Model's forward pass given our data.

        The input_dict can be used for either action computation or training.

        Args:
            model (ModelV2): The ModelV2 object for which to generate the view
                (input_dict) from `data`.
            is_training (bool): Whether the view should be generated for
                training purposes or inference (default).

        Returns:
            Dict[str, TensorType]: The input_dict to be passed into the ModelV2
                for inference/training.
        """
        raise NotImplementedError

    @abstractmethod
    def has_non_postprocessed_data(self) -> bool:
        """Returns whether there is pending, unprocessed data.

        Returns:
            bool: True if there is at least some data that has not been
                postprocessed yet.
        """
        raise NotImplementedError

    @abstractmethod
    def postprocess_trajectories_so_far(
            self, episode: Optional[MultiAgentEpisode] = None) -> None:
        """Apply postprocessing to unprocessed data (in one or all episodes).

        Generates (single-trajectory) SampleBatches for all Policies/Agents and
        calls Policy.postprocess_trajectory on each of these. Postprocessing
        may happens in-place, meaning any changes to the viewed data columns
        are directly reflected inside this collector's buffers.
        Also makes sure that additional (newly created) data columns are
        correctly added to the buffers.

        Args:
            episode (Optional[MultiAgentEpisode]): The Episode object for which
                to post-process data. If not provided, postprocess data for all
                episodes.
        """
        raise NotImplementedError

    @abstractmethod
    def get_multi_agent_batch_and_reset(self):
        """Returns the accumulated sample batches for each policy.

        Any unprocessed rows will be first postprocessed with a policy
        postprocessor. The internal state of this builder will be reset.

        Args:
            episode (Optional[MultiAgentEpisode]): The Episode object that
                holds this MultiAgentBatchBuilder object or None.

        Returns:
            MultiAgentBatch: Returns the accumulated sample batches for each
                policy inside one MultiAgentBatch object.
        """
        raise NotImplementedError

    @abstractmethod
    def check_missing_dones(self, episode_id: EpisodeID) -> None:
        raise NotImplementedError
