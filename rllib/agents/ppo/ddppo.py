from ray.rllib.algorithms.ddppo import (  # noqa
    DDPPO as DDPPOTrainer,
    DEFAULT_CONFIG,
)
