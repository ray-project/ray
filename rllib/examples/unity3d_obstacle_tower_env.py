"""
Example of running an RLlib Trainer against Unity3D's Obstacle Tower
environment.

Basically a modified version of:
https://github.com/Unity-Technologies/obstacle-tower-env/blob/master/examples/gcp_training.md

But with a slightly different /etc/X11/xorg.conf file!!

Instructions on how to run this on a server:
# nvidia-xconfig: X configuration file generated by nvidia-xconfig
# nvidia-xconfig:  version 418.87.01

#!!Comment out these sections: ServerLayout AND Screen!!
#Section "ServerLayout"
#    Identifier     "Layout0"
#    Screen      0  "Screen0" 0 0
#    InputDevice    "Keyboard0" "CoreKeyboard"
#    InputDevice    "Mouse0" "CorePointer"
#EndSection

Section "Files"
EndSection

Section "InputDevice"

    # generated from default
    Identifier     "Mouse0"
    Driver         "mouse"
    Option         "Protocol" "auto"
    Option         "Device" "/dev/psaux"
    Option         "Emulate3Buttons" "no"
    Option         "ZAxisMapping" "4 5"
EndSection

Section "InputDevice"

    # generated from default
    Identifier     "Keyboard0"
    Driver         "kbd"
EndSection

Section "Monitor"
    Identifier     "Monitor0"
    VendorName     "Unknown"
    ModelName      "Unknown"
    HorizSync       28.0 - 33.0
    VertRefresh     43.0 - 72.0
    Option         "IgnoreEDID"
EndSection

Section "Device"
    Identifier     "Device0"
    Driver         "nvidia"
    VendorName     "NVIDIA Corporation"
    BoardName      "Tesla M60"
    BusID          "PCI:0:30:0"
    Option         "AllowEmptyInitialConfiguration"
EndSection

#Section "Screen"
#    Identifier     "Screen0"
#    Device         "Device0"
#    Monitor        "Monitor0"
#    DefaultDepth    24
#    Option         "UseDisplayDevice" "None"
#    SubSection     "Display"
#        Virtual     1280 1024
#        Depth       24
#    EndSubSection
#EndSection


"""
#TODO: documentation above

import argparse
from gym import ObservationWrapper
from gym.spaces import Tuple
import numpy as np
from obstacle_tower_env import ObstacleTowerEnv
import os

import ray
from ray import tune
from ray.rllib.examples.models.cnn_plus_fc_concat_model import \
    CNNPlusFCConcatModel, TorchCNNPlusFCConcatModel
from ray.rllib.utils.test_utils import check_learning_achieved


class ObstacleTowerObservationWrapper(ObservationWrapper):
    def __init__(self, env):
        super().__init__(env)
        # Override observation space.
        self.observation_space = Tuple(self.observation_space.spaces[:3])

    def observation(self, observation):
        # 0=image; 1=number of keys held; 2=time remaining; 3=?
        # Filter out last component and make sure that time remaining is a
        # Box((1,)), not Box(()) as returned by the env.
        return tuple([
            observation[0], observation[1],
            np.array([observation[2]], dtype=np.float32)
        ])


parser = argparse.ArgumentParser()
parser.add_argument(
    "--from-checkpoint",
    type=str,
    default=None,
    help="Full path to a checkpoint file for restoring a previously saved "
    "Trainer state.")
parser.add_argument(
    "--file-name",
    type=str,
    default=None,
    help="The Unity3d ObstacleTower binary (compiled) game, e.g. "
    "'/home/ubuntu/ObstacleTower.x86_64'. Use `None` for finding this "
    "automatically.")
parser.add_argument("--num-workers", type=int, default=0)
parser.add_argument("--as-test", action="store_true")
parser.add_argument("--stop-iters", type=int, default=9999)
parser.add_argument("--stop-reward", type=float, default=9999.0)
parser.add_argument("--stop-timesteps", type=int, default=10000000)
parser.add_argument("--torch", action="store_true")
parser.add_argument("--num-gpus", type=int, default=0)

if __name__ == "__main__":
    ray.init()

    args = parser.parse_args()

    tune.register_env(
        "obstacle_tower",
        lambda c: ObstacleTowerObservationWrapper(
            ObstacleTowerEnv(environment_filename=args.file_name,
                             worker_id=c.worker_index,
                             **c)),
    )

    config = {
        "env": "obstacle_tower",
        # See here for more env options.
        # https://github.com/Unity-Technologies/obstacle-tower-env/
        # blob/master/reset-parameters.md
        "env_config": {
            "retro": False,
            "realtime_mode": False,
        },
        # For running in editor, force to use just one Worker (we only have
        # one Unity running)!
        "num_workers": args.num_workers,
        # Other settings.
        "lr": 0.0003,
        "lambda": 0.95,
        "gamma": 0.99,
        "sgd_minibatch_size": 500,
        "train_batch_size": 5000,
        # Use GPUs iff `RLLIB_NUM_GPUS` env var set to > 0.
        "num_gpus": max(args.num_gpus,
                        int(os.environ.get("RLLIB_NUM_GPUS", "0"))),
        "num_sgd_iter": 20,
        "rollout_fragment_length": 200,
        "clip_param": 0.1,
        "model": {
            "custom_model": TorchCNNPlusFCConcatModel if args.torch else CNNPlusFCConcatModel,
            "custom_model_config": {},
            "max_seq_len": 100,
            "use_lstm": True,
            "lstm_use_prev_action": True,
            "lstm_use_prev_reward": True,
            "lstm_cell_size": 256,
        },
        "framework": "torch" if args.torch else "tf",
    }

    stop = {
        "training_iteration": args.stop_iters,
        "timesteps_total": args.stop_timesteps,
        "episode_reward_mean": args.stop_reward,
    }

    # Run the experiment.
    results = tune.run("PPO",
                       config=config,
                       stop=stop,
                       verbose=2,
                       checkpoint_freq=20,
                       restore=args.from_checkpoint)

    # And check the results.
    if args.as_test:
        check_learning_achieved(results, args.stop_reward)

    ray.shutdown()
