{{- if not .Values.operatorOnly }}
apiVersion: cluster.ray.io/v1
kind: RayCluster
metadata:
  name: {{ .Release.Name }}
spec:
  # The maximum number of workers nodes to launch in addition to the head node.
  maxWorkers: {{ include "ray.clusterMaxWorkers" . }}
  # The autoscaler will scale up the cluster faster with higher upscaling speed.
  # E.g., if the task requires adding more nodes then autoscaler will gradually
  # scale up the cluster in chunks of upscaling_speed*currently_running_nodes.
  # This number should be > 0.
  upscalingSpeed: 1.0
  # If a node is idle for this many minutes, it will be removed.
  idleTimeoutMinutes: 5
  # Specify the pod type for the ray head node (as configured below).
  headPodType: {{ .Values.headPodType }}
  # Specify the allowed pod types for this ray cluster and the resources they provide.
  podTypes:
    {{- range $key, $val := .Values.podTypes }}
    - name: {{ $key }}
      minWorkers: {{ $val.minWorkers }}
      maxWorkers: {{ $val.maxWorkers }}
      {{- if $val.rayResources }}
      rayResources:
        {{- toYaml $val.rayResources | nindent 8 }}
      {{- end }}
      podConfig:
        apiVersion: v1
        kind: Pod
        metadata:
          generateName: {{ kebabcase $key }}-
        spec:
          restartPolicy: Never
          # This volume allocates shared memory for Ray to use for its plasma
          # object store. If you do not provide this, Ray will fall back to
          # /tmp which cause slowdowns if is not a shared memory volume.
          volumes:
          - name: dshm
            emptyDir:
              medium: Memory
          containers:
          - name: ray-node
            imagePullPolicy: Always
            image: {{ $.Values.image }}
            # Do not change this command - it keeps the pod alive until it is
            # explicitly killed.
            command: ["/bin/bash", "-c", "--"]
            args: ['trap : TERM INT; sleep infinity & wait;']
            ports:
            - containerPort: 6379  # Redis port
            - containerPort: 10001  # Used by Ray Client
            - containerPort: 8265  # Used by Ray Dashboard
            - containerPort: 8000 # Used by Ray Serve

            # This volume allocates shared memory for Ray to use for its plasma
            # object store. If you do not provide this, Ray will fall back to
            # /tmp which cause slowdowns if is not a shared memory volume.
            volumeMounts:
            - mountPath: /dev/shm
              name: dshm
            resources:
              requests:
                cpu: {{ .numCPU }}
                memory: {{ .memory }}
              limits:
                cpu: {{ .numCPU }}
                # The maximum memory that this pod is allowed to use. The
                # limit will be detected by ray and split to use 10% for
                # redis, 30% for the shared memory object store, and the
                # rest for application memory. If this limit is not set and
                # the object store size is not set manually, ray will
                # allocate a very large object store in each pod that may
                # cause problems for other pods.
                memory: {{ .memory }}
                {{- if .numGPU }}
                nvidia.com/gpu: {{ .numGPU }}
                {{- end }}
          {{- if .nodeSelector }}
          nodeSelector:
              {{- toYaml .nodeSelector | nindent 12 }}
          {{- end }}
    {{- end }}
  # Commands to start Ray on the head node. You don't need to change this.
  # Note dashboard-host is set to 0.0.0.0 so that Kubernetes can port forward.
  headStartRayCommands:
    - ray stop
    - ulimit -n 65536; ray start --head --no-monitor --dashboard-host 0.0.0.0
  # Commands to start Ray on worker nodes. You don't need to change this.
  workerStartRayCommands:
    - ray stop
    - ulimit -n 65536; ray start --address=$RAY_HEAD_IP:6379
{{- end }}
