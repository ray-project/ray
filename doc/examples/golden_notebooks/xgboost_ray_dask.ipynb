{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost-Ray with Dask\n",
    "\n",
    "This notebook includes an example workflow using [XGBoost-Ray](https://docs.ray.io/en/latest/xgboost-ray.html) and Dask for distributed model training, hyperparameter optimization and prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "\n",
    "import dask\n",
    "import dask.dataframe as dd\n",
    "\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.util.dask import ray_dask_get\n",
    "\n",
    "from xgboost_ray import RayDMatrix, RayParams, train, predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anyscale Connect\n",
    "\n",
    "The cell below connects the notebook to an Anyscale cluster. XGBoost-Ray will automatically do computation remotely. Make sure to replace the `CLUSTER_NAME`, `CLUSTER_ENV` and `CLUSTER_COMPUTE` to match your settings.\n",
    "\n",
    "It is not necessary to use Anyscale - this workflow will work just as well with a Ray cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLUSTER_NAME = \"cluster\"\n",
    "CLUSTER_ENV = \"cluster_env\"\n",
    "CLUSTER_COMPUTE = \"CLUSTER_COMPUTE\"\n",
    "#ray.init(f\"anyscale://{CLUSTER_NAME}\", cluster_env=CLUSTER_ENV, cluster_compute=CLUSTER_COMPUTE)\n",
    "ray.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation\n",
    "\n",
    "We will use the [HIGGS dataset from the UCI Machine Learning dataset repository](https://archive.ics.uci.edu/ml/datasets/HIGGS). The HIGGS dataset consists of 11,000,000 samples and 28 attributes, which is a large enough size to show the benefits of distributed computation.\n",
    "\n",
    "We set the Dask scheduler to `ray_dask_get` to use [Dask on Ray](https://docs.ray.io/en/latest/data/dask-on-ray.html) backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL_COLUMN = \"label\"\n",
    "FILE_NAME = \"HIGGS.csv.gz\"\n",
    "\n",
    "print(\"Loading HIGGS data.\")\n",
    "\n",
    "dask.config.set(scheduler=ray_dask_get)\n",
    "\n",
    "def download_higgs(target_file):\n",
    "    url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/\" \\\n",
    "          \"00280/HIGGS.csv.gz\"\n",
    "\n",
    "    try:\n",
    "        import urllib.request\n",
    "    except ImportError as e:\n",
    "        raise ValueError(\n",
    "            f\"Automatic downloading of the HIGGS dataset requires `urllib`.\"\n",
    "            f\"\\nFIX THIS by running `pip install urllib` or manually \"\n",
    "            f\"downloading the dataset from {url}.\") from e\n",
    "\n",
    "    print(f\"Downloading HIGGS dataset to {target_file}\")\n",
    "    urllib.request.urlretrieve(url, target_file)\n",
    "    return os.path.exists(target_file)\n",
    "\n",
    "download_higgs(FILE_NAME)\n",
    "\n",
    "colnames = [LABEL_COLUMN] + [\"feature-%02d\" % i for i in range(1, 29)]\n",
    "data = dd.read_csv(FILE_NAME, names=colnames)\n",
    "data = data[sorted(colnames)]\n",
    "data = data.repartition(npartitions=100)\n",
    "\n",
    "print(\"Loaded HIGGS data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will split the data into a training set and a evaluation set using a 75-25 proportion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, eval_df = data.random_split([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed training\n",
    "\n",
    "The `train_xgboost` function contains all of the logic necessary for training using XGBoost-Ray.\n",
    "\n",
    "Distributed training can not only speed up the process, but also allow you to use datasets that are to large to fit in memory of a single node. With distributed training, the dataset is sharded across different actors running on separate nodes. Those actors communicate with each other to create the final model.\n",
    "\n",
    "First, the dataframes are wrapped in `RayDMatrix` objects, which handle data sharding across the cluster. Then, the `train` function is called. The evaluation scores will be saved to `evals_result` dictionary. The function returns a tuple of the trained model (booster) and the evaluation scores.\n",
    "\n",
    "The `ray_params` variable expects a `RayParams` object that contains Ray-specific settings, such as the number of workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_xgboost(config, train_df, test_df, target_column, ray_params, num_boost_round=100):\n",
    "    train_set = RayDMatrix(train_df, target_column)\n",
    "    test_set = RayDMatrix(test_df, target_column)\n",
    "\n",
    "    evals_result = {}\n",
    "\n",
    "    start_time = time.time()\n",
    "    # Train the classifier\n",
    "    bst = train(\n",
    "        params=config,\n",
    "        dtrain=train_set,\n",
    "        evals=[(test_set, \"eval\")],\n",
    "        evals_result=evals_result,\n",
    "        verbose_eval=True,\n",
    "        num_boost_round=num_boost_round,\n",
    "        ray_params=ray_params)\n",
    "    print(f\"Total time taken: {time.time()-start_time}\")\n",
    "\n",
    "    model_path = \"model.xgb\"\n",
    "    bst.save_model(model_path)\n",
    "    print(\"Final validation error: {:.4f}\".format(\n",
    "        evals_result[\"eval\"][\"error\"][-1]))\n",
    "\n",
    "    return bst, evals_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now pass our Dask dataframes and run the function. We will use `RayParams` to specify that our model is to be trained on 4 actors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard XGBoost config for classification\n",
    "config = {\n",
    "    \"tree_method\": \"approx\",\n",
    "    \"objective\": \"binary:logistic\",\n",
    "    \"eval_metric\": [\"logloss\", \"error\"],\n",
    "}\n",
    "\n",
    "bst, evals_result = train_xgboost(config, train_df, eval_df, LABEL_COLUMN, RayParams(num_actors=4))\n",
    "evals_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter optimization\n",
    "\n",
    "If we are not content with the results obtained with default XGBoost parameters, we can use [Ray Tune](https://docs.ray.io/en/latest/tune/index.html) for cutting-edge distributed hyperparameter tuning. XGBoost-Ray automatically integrates with Ray Tune, meaning we can use the same training function as before.\n",
    "\n",
    "In this workflow, we will tune three hyperparameters - `eta`, `subsample` and `max_depth`. We are using [Tune's samplers to define the search space](https://docs.ray.io/en/latest/tune/user-guide.html#search-space-grid-random).\n",
    "\n",
    "The experiment configuration is done through `tune.run`. We set the amount of resources each trial (hyperparameter combination) requires by using the `get_tune_resources` method of `RayParams`. The `num_samples` argument controls how many trials will be ran in total. In the end, the best combination of hyperparameters evaluated during the experiment will be returned.\n",
    "\n",
    "By default, Tune will use simple random search. However, Tune also provides various [search algorithms](https://docs.ray.io/en/latest/tune/api_docs/suggestion.html) and [schedulers](https://docs.ray.io/en/latest/tune/api_docs/schedulers.html) to further improve the optimization process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_xgboost(train_df, test_df, target_column):\n",
    "    # Set XGBoost config.\n",
    "    config = {\n",
    "        \"tree_method\": \"approx\",\n",
    "        \"objective\": \"binary:logistic\",\n",
    "        \"eval_metric\": [\"logloss\", \"error\"],\n",
    "        \"eta\": tune.loguniform(1e-4, 1e-1),\n",
    "        \"subsample\": tune.uniform(0.5, 1.0),\n",
    "        \"max_depth\": tune.randint(1, 9)\n",
    "    }\n",
    "\n",
    "    ray_params = RayParams(\n",
    "        max_actor_restarts=1, gpus_per_actor=0, cpus_per_actor=8, num_actors=4)\n",
    "\n",
    "    analysis = tune.run(\n",
    "        tune.with_parameters(\n",
    "            train_xgboost,\n",
    "            train_df=train_df,\n",
    "            test_df=test_df,\n",
    "            target_column=target_column,\n",
    "            ray_params=ray_params),\n",
    "        # Use the `get_tune_resources` helper function to set the resources.\n",
    "        resources_per_trial=ray_params.get_tune_resources(),\n",
    "        config=config,\n",
    "        num_samples=10,\n",
    "        metric=\"eval-error\",\n",
    "        mode=\"min\",\n",
    "        verbose=1)\n",
    "\n",
    "    accuracy = 1. - analysis.best_result[\"eval-error\"]\n",
    "    print(f\"Best model parameters: {analysis.best_config}\")\n",
    "    print(f\"Best model total accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    return analysis.best_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter optimization may take some time to complete."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "best_hyperparameters = tune_xgboost(train_df, eval_df, LABEL_COLUMN)\n",
    "print(\"Best hyperparameters:\")\n",
    "best_hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction\n",
    "\n",
    "With the model trained, we can now predict on unseen data. For the purposes of this example, we will use the same dataset for prediction as for training.\n",
    "\n",
    "Since prediction is naively parallelizable, distributing it over multiple actors can measurably reduce the amount of time needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_df = RayDMatrix(data, ignore=[LABEL_COLUMN, \"partition\"])\n",
    "results = predict(\n",
    "    bst,\n",
    "    inference_df,\n",
    "    ray_params=RayParams(cpus_per_actor=2, num_actors=16))\n",
    "\n",
    "results"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bdd20961b5562c5cbae8d59b7d0ac4c4eb2a0ee9f55c1921a8c955b571304b48"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('venv': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}