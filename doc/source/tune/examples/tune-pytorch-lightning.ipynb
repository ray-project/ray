{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using PyTorch Lightning with Tune\n",
    "\n",
    "(tune-pytorch-lightning-ref)=\n",
    "\n",
    "PyTorch Lightning is a framework which brings structure into training PyTorch models. It\n",
    "aims to avoid boilerplate code, so you don't have to write the same training\n",
    "loops all over again when building a new model.\n",
    "\n",
    "```{image} /images/pytorch_lightning_full.png\n",
    ":align: center\n",
    "```\n",
    "\n",
    "The main abstraction of PyTorch Lightning is the `LightningModule` class, which\n",
    "should be extended by your application. There is [a great post on how to transfer your models from vanilla PyTorch to Lightning](https://towardsdatascience.com/from-pytorch-to-pytorch-lightning-a-gentle-introduction-b371b7caaf09).\n",
    "\n",
    "The class structure of PyTorch Lightning makes it very easy to define and tune model\n",
    "parameters. This tutorial will show you how to use Tune with AIR {class}`LightningTrainer <ray.train.lightning.LightningTrainer>` to find the best set of\n",
    "parameters for your application on the example of training a MNIST classifier. Notably,\n",
    "the `LightningModule` does not have to be altered at all for this - so you can\n",
    "use it plug and play for your existing models, assuming their parameters are configurable!\n",
    "\n",
    ":::{note}\n",
    "If you don't want to use AIR {class}`LightningTrainer <ray.train.lightning.LightningTrainer>`, please refer to this document: {ref}`Using vanilla Pytorch Lightning with Tune <tune-vanilla-pytorch-lightning-ref>`.\n",
    "\n",
    ":::\n",
    "\n",
    ":::{note}\n",
    "To run this example, you will need to install the following:\n",
    "\n",
    "```bash\n",
    "$ pip install \"ray[tune]\" torch torchvision pytorch-lightning\n",
    "```\n",
    ":::\n",
    "\n",
    "```{contents}\n",
    ":backlinks: none\n",
    ":local: true\n",
    "```\n",
    "\n",
    "## PyTorch Lightning classifier for MNIST\n",
    "\n",
    "Let's first start with the basic PyTorch Lightning implementation of an MNIST classifier.\n",
    "This classifier does not include any tuning code at this point.\n",
    "\n",
    "First, we run some imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "import torch.nn.functional as F\n",
    "from filelock import FileLock\n",
    "from torchmetrics import Accuracy\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "from pytorch_lightning.loggers.csv_logs import CSVLogger\n",
    "\n",
    "import ray\n",
    "import ray.tune as tune\n",
    "from ray.air.config import CheckpointConfig, ScalingConfig\n",
    "from ray.train.lightning import LightningTrainer, LightningConfigBuilder\n",
    "from ray.tune.schedulers import PopulationBasedTraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our example builds on the MNIST example from the [blog post](https://towardsdatascience.com/from-pytorch-to-pytorch-lightning-a-gentle-introduction-b371b7caaf09) we mentioned before. We adapted the original model and dataset definitions into `MNISTClassifier` and `MNISTDataModule`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTClassifier(pl.LightningModule):\n",
    "    def __init__(self, config):\n",
    "        super(MNISTClassifier, self).__init__()\n",
    "        self.accuracy = Accuracy()\n",
    "        self.layer_1_size = config[\"layer_1_size\"]\n",
    "        self.layer_2_size = config[\"layer_2_size\"]\n",
    "        self.lr = config[\"lr\"]\n",
    "\n",
    "        # mnist images are (1, 28, 28) (channels, width, height)\n",
    "        self.layer_1 = torch.nn.Linear(28 * 28, self.layer_1_size)\n",
    "        self.layer_2 = torch.nn.Linear(self.layer_1_size, self.layer_2_size)\n",
    "        self.layer_3 = torch.nn.Linear(self.layer_2_size, 10)\n",
    "\n",
    "    def cross_entropy_loss(self, logits, labels):\n",
    "        return F.nll_loss(logits, labels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, channels, width, height = x.size()\n",
    "        x = x.view(batch_size, -1)\n",
    "\n",
    "        x = self.layer_1(x)\n",
    "        x = torch.relu(x)\n",
    "\n",
    "        x = self.layer_2(x)\n",
    "        x = torch.relu(x)\n",
    "\n",
    "        x = self.layer_3(x)\n",
    "        x = torch.log_softmax(x, dim=1)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def training_step(self, train_batch, batch_idx):\n",
    "        x, y = train_batch\n",
    "        logits = self.forward(x)\n",
    "        loss = self.cross_entropy_loss(logits, y)\n",
    "        accuracy = self.accuracy(logits, y)\n",
    "\n",
    "        self.log(\"ptl/train_loss\", loss)\n",
    "        self.log(\"ptl/train_accuracy\", accuracy)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, val_batch, batch_idx):\n",
    "        x, y = val_batch\n",
    "        logits = self.forward(x)\n",
    "        loss = self.cross_entropy_loss(logits, y)\n",
    "        accuracy = self.accuracy(logits, y)\n",
    "        return {\"val_loss\": loss, \"val_accuracy\": accuracy}\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([x[\"val_loss\"] for x in outputs]).mean()\n",
    "        avg_acc = torch.stack([x[\"val_accuracy\"] for x in outputs]).mean()\n",
    "        self.log(\"ptl/val_loss\", avg_loss)\n",
    "        self.log(\"ptl/val_accuracy\", avg_acc)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "        return optimizer\n",
    "\n",
    "\n",
    "class MNISTDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, batch_size=128):\n",
    "        super().__init__()\n",
    "        self.data_dir = os.getcwd()\n",
    "        self.batch_size = batch_size\n",
    "        self.transform = transforms.Compose(\n",
    "            [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n",
    "        )\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        with FileLock(f\"{self.data_dir}.lock\"):\n",
    "            mnist = MNIST(\n",
    "                self.data_dir, train=True, download=True, transform=self.transform\n",
    "            )\n",
    "            self.mnist_train, self.mnist_val = random_split(mnist, [55000, 5000])\n",
    "\n",
    "            self.mnist_test = MNIST(\n",
    "                self.data_dir, train=False, download=True, transform=self.transform\n",
    "            )\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.mnist_train, batch_size=self.batch_size, num_workers=4)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.mnist_val, batch_size=self.batch_size, num_workers=4)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.mnist_test, batch_size=self.batch_size, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_config = {\n",
    "    \"layer_1_size\": 128,\n",
    "    \"layer_2_size\": 256,\n",
    "    \"lr\": 1e-3,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning the model parameters\n",
    "\n",
    "The parameters above should give you a good accuracy of over 90% already. However, we might improve on this simply by changing some of the hyperparameters. For instance, maybe we get an even higher accuracy if we used a smaller learning rate and larger middle layer size.\n",
    "\n",
    "Instead of manually loop through all the parameter combinitions, let's use Tune to systematically try out parameter combinations and find the best performing set.\n",
    "\n",
    "First, we need some additional imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from ray import air, tune\n",
    "from ray.air import session\n",
    "from ray.air.config import RunConfig, ScalingConfig, CheckpointConfig\n",
    "from ray.tune import CLIReporter\n",
    "from ray.tune.schedulers import ASHAScheduler, PopulationBasedTraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Configuring the search space\n",
    "\n",
    "Now we configure the parameter search space using {class}`LightningConfigBuilder <ray.train.lightning.LightningConfigBuilder>`. We would like to choose between three different layer and batch sizes. The learning rate should be sampled uniformly between `0.0001` and `0.1`. The `tune.loguniform()` function is syntactic sugar to make sampling between these different orders of magnitude easier, specifically we are able to also sample small values.\n",
    "\n",
    ":::{note}\n",
    "In `LightningTrainer`, the frequency of metric reporting is the same as the frequency of checkpointing specified in `LightningConfigBuilder`. For example, if you set `builder.checkpointing(..., every_n_epochs=2)`, then for every 2 epochs, all the latest metrics together will be reported to the tune session together with a checkpoint.\n",
    "\n",
    ":::\n",
    "\n",
    "\n",
    ":::{note}\n",
    "`LightningConfigBuilder.checkpointing()` specifies the monitor metric and checkpoint frequency in Lightning ModelCheckpoint manner. You should also provide a AIR `CheckpointConfig` to properly save top-k checkpoints under the trial folder. Otherwise, LightningTrainer copies and saves all checkpoints by default.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_epochs = 10\n",
    "dm = MNISTDataModule(batch_size=128)\n",
    "\n",
    "config = {\n",
    "    \"layer_1_size\": tune.choice([32, 64, 128]),\n",
    "    \"layer_2_size\": tune.choice([64, 128, 256]),\n",
    "    \"lr\": tune.loguniform(1e-4, 1e-1),\n",
    "}\n",
    "\n",
    "lightning_config = (\n",
    "    LightningConfigBuilder()\n",
    "    .module(cls=MNISTClassifier, config=config)\n",
    "    .trainer(max_epochs=num_epochs, accelerator=\"cpu\")\n",
    "    .fit_params(datamodule=dm)\n",
    "    .checkpointing(monitor=\"ptl/val_accuracy\", save_top_k=2, mode=\"max\")\n",
    "    .build()\n",
    ")\n",
    "\n",
    "# Make sure to also define AIR CheckpointConfig here in order to save \n",
    "run_config=RunConfig(\n",
    "    checkpoint_config=CheckpointConfig(\n",
    "        num_to_keep=2,\n",
    "        checkpoint_score_attribute=\"ptl/val_accuracy\",\n",
    "        checkpoint_score_order=\"max\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Selecting a scheduler\n",
    "\n",
    "In this example, we use an [Asynchronous Hyperband](https://blog.ml.cmu.edu/2018/12/12/massively-parallel-hyperparameter-optimization/)\n",
    "scheduler. This scheduler decides at each iteration which trials are likely to perform\n",
    "badly, and stops these trials. This way we don't waste any resources on bad hyperparameter\n",
    "configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = ASHAScheduler(\n",
    "    max_t=num_epochs,\n",
    "    grace_period=1,\n",
    "    reduction_factor=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Changing the CLI output\n",
    "\n",
    "We instantiate a `CLIReporter` to specify which metrics we would like to see in our\n",
    "output tables in the command line. This is optional, but can be used to make sure our\n",
    "output tables only include information we would like to see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "reporter = CLIReporter(\n",
    "    parameter_columns=[\"layer_1_size\", \"layer_2_size\", \"lr\", \"batch_size\"],\n",
    "    metric_columns=[\"ptl/val_loss\", \"ptl/val_accuracy\", \"training_iteration\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Training with GPUs\n",
    "\n",
    "We can specify the number of resources, including GPUs, that Tune should request for each trial.\n",
    "\n",
    "`LightningTrainer` takes care of environment setup for Distributed Data Parallel training, the model and data will automatically get distributed across GPUs. You only need to set the number of GPUs per worker in `ScalingConfig` and also set `accelerator=\"gpu\"` in LightningTrainerConfigBuilder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaling_config = ScalingConfig(\n",
    "    num_workers=3, use_gpu=True, resources_per_worker={\"CPU\": 1, \"GPU\": 1}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaling_config = ScalingConfig(\n",
    "    num_workers=3, use_gpu=False, resources_per_worker={\"CPU\": 1}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a base LightningTrainer without hyper-parameters for Tuner\n",
    "lightning_trainer = LightningTrainer(\n",
    "    scaling_config=scaling_config,\n",
    "    run_config=run_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it together\n",
    "\n",
    "Lastly, we need to create a `Tuner()` object and start Ray Tune with `tuner.fit()`.\n",
    "\n",
    "The full code looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2023-03-28 18:09:10</td></tr>\n",
       "<tr><td>Running for: </td><td>00:00:16.87        </td></tr>\n",
       "<tr><td>Memory:      </td><td>3.9/30.9 GiB       </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using FIFO scheduling algorithm.<br>Logical resource usage: 0/8 CPUs, 0/0 GPUs\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "<div class=\"messages\">\n",
       "  <h3>Messages</h3>\n",
       "  \n",
       "  \n",
       "  Number of errored trials: 1<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                                                                                       </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>LightningTrainer_45adb_00000</td><td style=\"text-align: right;\">           1</td><td>/home/ray/ray_results/LightningTrainer_2023-03-28_18-08-53/LightningTrainer_45adb_00000_0_layer_1_size=32,layer_2_size=64,lr=0.0014_2023-03-28_18-08-53/error.txt</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".messages {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  padding-left: 1em;\n",
       "  overflow-y: auto;\n",
       "}\n",
       ".messages h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n",
       "\n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">   ...odule_init_config\n",
       "/config/layer_1_size</th><th style=\"text-align: right;\">   ...odule_init_config\n",
       "/config/layer_2_size</th><th style=\"text-align: right;\">           ..._config/_module_i\n",
       "nit_config/config/lr</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>LightningTrainer_45adb_00000</td><td>ERROR   </td><td>10.0.57.221:201387</td><td style=\"text-align: right;\">32</td><td style=\"text-align: right;\">64</td><td style=\"text-align: right;\">0.00140243</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(pid=201387) /home/ray/anaconda3/lib/python3.8/site-packages/xgboost/compat.py:31: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead. [repeated 3x across cluster]\n",
      "(pid=201387)   from pandas import MultiIndex, Int64Index [repeated 3x across cluster]\n",
      "(RayTrainWorker pid=200311) Missing logger folder: /home/ray/ray_results/LightningTrainer_2023-03-28_18-07-48/LightningTrainer_1f42f_00000_0_layer_1_size=128,layer_2_size=64,lr=0.0005_2023-03-28_18-07-48/rank_2/lightning_logs [repeated 2x across cluster]\n",
      "(RayTrainWorker pid=201612) 2023-03-28 18:09:03,257\tINFO config.py:86 -- Setting up process group for: env:// [rank=0, world_size=3]\n",
      "(RayTrainWorker pid=201612)   from pandas import MultiIndex, Int64Index\n",
      "(RayTrainWorker pid=201612)   from pandas import MultiIndex, Int64Index\n",
      "(RayTrainWorker pid=201614) /home/ray/anaconda3/lib/python3.8/site-packages/xgboost/compat.py:31: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "(RayTrainWorker pid=201614)   from pandas import MultiIndex, Int64Index\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=201614) Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "(RayTrainWorker pid=201614) Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to /home/ray/prj-yx-lightning/doc/source/tune/examples/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=201612) GPU available: False, used: False\n",
      "(RayTrainWorker pid=201612) TPU available: False, using: 0 TPU cores\n",
      "(RayTrainWorker pid=201612) IPU available: False, using: 0 IPUs\n",
      "(RayTrainWorker pid=201612) HPU available: False, using: 0 HPUs\n",
      "(RayTrainWorker pid=201612) Missing logger folder: /home/ray/ray_results/LightningTrainer_2023-03-28_18-08-53/LightningTrainer_45adb_00000_0_layer_1_size=32,layer_2_size=64,lr=0.0014_2023-03-28_18-08-53/rank_0/lightning_logs\n",
      "  0%|          | 0/9912422 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=201614) Extracting /home/ray/prj-yx-lightning/doc/source/tune/examples/MNIST/raw/train-images-idx3-ubyte.gz to /home/ray/prj-yx-lightning/doc/source/tune/examples/MNIST/raw\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9912422/9912422 [00:00<00:00, 93508624.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=201614) \n",
      "(RayTrainWorker pid=201614) Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "(RayTrainWorker pid=201614) Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to /home/ray/prj-yx-lightning/doc/source/tune/examples/MNIST/raw/train-labels-idx1-ubyte.gz\n",
      "(RayTrainWorker pid=201614) Extracting /home/ray/prj-yx-lightning/doc/source/tune/examples/MNIST/raw/train-labels-idx1-ubyte.gz to /home/ray/prj-yx-lightning/doc/source/tune/examples/MNIST/raw\n",
      "(RayTrainWorker pid=201614) \n",
      "(RayTrainWorker pid=201614) Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "(RayTrainWorker pid=201614) Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to /home/ray/prj-yx-lightning/doc/source/tune/examples/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28881/28881 [00:00<00:00, 32285632.68it/s]\n",
      "  0%|          | 0/1648877 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=201614) Extracting /home/ray/prj-yx-lightning/doc/source/tune/examples/MNIST/raw/t10k-images-idx3-ubyte.gz to /home/ray/prj-yx-lightning/doc/source/tune/examples/MNIST/raw\n",
      "(RayTrainWorker pid=201614) \n",
      "(RayTrainWorker pid=201614) Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1648877/1648877 [00:00<00:00, 25374017.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=201614) Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to /home/ray/prj-yx-lightning/doc/source/tune/examples/MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
      "(RayTrainWorker pid=201614) Extracting /home/ray/prj-yx-lightning/doc/source/tune/examples/MNIST/raw/t10k-labels-idx1-ubyte.gz to /home/ray/prj-yx-lightning/doc/source/tune/examples/MNIST/raw\n",
      "(RayTrainWorker pid=201614) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4542/4542 [00:00<00:00, 33305120.22it/s]\n",
      "(RayTrainWorker pid=201612)   | Name     | Type     | Params\n",
      "(RayTrainWorker pid=201612) --------------------------------------\n",
      "(RayTrainWorker pid=201612) 0 | accuracy | Accuracy | 0     \n",
      "(RayTrainWorker pid=201612) 1 | layer_1  | Linear   | 25.1 K\n",
      "(RayTrainWorker pid=201612) 2 | layer_2  | Linear   | 2.1 K \n",
      "(RayTrainWorker pid=201612) 3 | layer_3  | Linear   | 650   \n",
      "(RayTrainWorker pid=201612) --------------------------------------\n",
      "(RayTrainWorker pid=201612) 27.9 K    Trainable params\n",
      "(RayTrainWorker pid=201612) 0         Non-trainable params\n",
      "(RayTrainWorker pid=201612) 27.9 K    Total params\n",
      "(RayTrainWorker pid=201612) 0.112     Total estimated model params size (MB)\n",
      "2023-03-28 18:09:10,291\tERROR trial_runner.py:1450 -- Trial LightningTrainer_45adb_00000: Error happened when processing _ExecutorEventType.TRAINING_RESULT.\n",
      "ray.exceptions.RayTaskError(AttributeError): ray::_Inner.train() (pid=201387, ip=10.0.57.221, repr=LightningTrainer)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/tune/trainable/trainable.py\", line 384, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/train/_internal/utils.py\", line 54, in check_for_failure\n",
      "    ray.get(object_ref)\n",
      "ray.exceptions.RayTaskError(AttributeError): ray::_RayTrainWorker__execute.get_next() (pid=201612, ip=10.0.57.221, repr=<ray.train._internal.worker_group.RayTrainWorker object at 0x7fd9bb2e6ee0>)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/train/_internal/worker_group.py\", line 31, in __execute\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/train/_internal/utils.py\", line 129, in discard_return_wrapper\n",
      "    train_func(*args, **kwargs)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/train/lightning/lightning_trainer.py\", line 499, in _lightning_train_loop_per_worker\n",
      "    trainer.fit(lightning_module, **trainer_fit_params)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 770, in fit\n",
      "    self._call_and_handle_interrupt(\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 723, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 811, in _fit_impl\n",
      "    results = self._run(model, ckpt_path=self.ckpt_path)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1236, in _run\n",
      "    results = self._run_stage()\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1323, in _run_stage\n",
      "    return self._run_train()\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1345, in _run_train\n",
      "    self._run_sanity_check()\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1413, in _run_sanity_check\n",
      "    val_loop.run()\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/pytorch_lightning/loops/base.py\", line 204, in run\n",
      "    self.advance(*args, **kwargs)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py\", line 155, in advance\n",
      "    dl_outputs = self.epoch_loop.run(self._data_fetcher, dl_max_batches, kwargs)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/pytorch_lightning/loops/base.py\", line 204, in run\n",
      "    self.advance(*args, **kwargs)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py\", line 128, in advance\n",
      "    output = self._evaluation_step(**kwargs)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py\", line 226, in _evaluation_step\n",
      "    output = self.trainer._call_strategy_hook(\"validation_step\", *kwargs.values())\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1765, in _call_strategy_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/pytorch_lightning/strategies/ddp.py\", line 355, in validation_step\n",
      "    return self.model(*args, **kwargs)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1190, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/torch/nn/parallel/distributed.py\", line 1040, in forward\n",
      "    output = self._run_ddp_forward(*inputs, **kwargs)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/torch/nn/parallel/distributed.py\", line 1003, in _run_ddp_forward\n",
      "    return module_to_run(*inputs, **kwargs)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1190, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/pytorch_lightning/overrides/base.py\", line 93, in forward\n",
      "    return self.module.validation_step(*inputs, **kwargs)\n",
      "  File \"/tmp/ipykernel_119797/1553556965.py\", line 42, in validation_step\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1265, in __getattr__\n",
      "    raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n",
      "AttributeError: 'MNISTClassifier' object has no attribute 'cross_entropy_loss'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"trialProgress\">\n",
       "  <h3>Trial Progress</h3>\n",
       "  <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>date               </th><th>hostname      </th><th>node_ip    </th><th style=\"text-align: right;\">   pid</th><th style=\"text-align: right;\">  timestamp</th><th>trial_id   </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>LightningTrainer_45adb_00000</td><td>2023-03-28_18-09-00</td><td>ip-10-0-57-221</td><td>10.0.57.221</td><td style=\"text-align: right;\">201387</td><td style=\"text-align: right;\"> 1680052140</td><td>45adb_00000</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".trialProgress {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".trialProgress h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".trialProgress td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-28 18:09:10,313\tERROR tune.py:941 -- Trials did not complete: [LightningTrainer_45adb_00000]\n",
      "2023-03-28 18:09:10,314\tINFO tune.py:945 -- Total run time: 16.89 seconds (16.86 seconds for the tuning loop).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Result(\n",
       "  error='RayTaskError(AttributeError)',\n",
       "  metrics={'trial_id': '45adb_00000'},\n",
       "  path='/home/ray/ray_results/LightningTrainer_2023-03-28_18-08-53/LightningTrainer_45adb_00000_0_layer_1_size=32,layer_2_size=64,lr=0.0014_2023-03-28_18-08-53',\n",
       "  checkpoint=None\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuner = tune.Tuner(\n",
    "    lightning_trainer,\n",
    "    param_space={\"lightning_config\": lightning_config},\n",
    "    tune_config=tune.TuneConfig(\n",
    "        metric=\"ptl/val_accuracy\",\n",
    "        mode=\"max\",\n",
    "        num_samples=1,\n",
    "    ),\n",
    ")\n",
    "results = tuner.fit()\n",
    "best_result = results.get_best_result(metric=\"ptl/val_accuracy\", mode=\"max\")\n",
    "best_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
