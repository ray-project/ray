{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using PyTorch Lightning with Tune\n",
    "\n",
    "(tune-pytorch-lightning-ref)=\n",
    "\n",
    "PyTorch Lightning is a framework which brings structure into training PyTorch models. It\n",
    "aims to avoid boilerplate code, so you don't have to write the same training\n",
    "loops all over again when building a new model.\n",
    "\n",
    "```{image} /images/pytorch_lightning_full.png\n",
    ":align: center\n",
    "```\n",
    "\n",
    "The main abstraction of PyTorch Lightning is the `LightningModule` class, which\n",
    "should be extended by your application. There is [a great post on how to transfer your models from vanilla PyTorch to Lightning](https://towardsdatascience.com/from-pytorch-to-pytorch-lightning-a-gentle-introduction-b371b7caaf09).\n",
    "\n",
    "The class structure of PyTorch Lightning makes it very easy to define and tune model\n",
    "parameters. This tutorial will show you how to use Tune with AIR {class}`LightningTrainer <ray.train.lightning.LightningTrainer>` to find the best set of\n",
    "parameters for your application on the example of training a MNIST classifier. Notably,\n",
    "the `LightningModule` does not have to be altered at all for this - so you can\n",
    "use it plug and play for your existing models, assuming their parameters are configurable!\n",
    "\n",
    ":::{note}\n",
    "If you don't want to use AIR {class}`LightningTrainer <ray.train.lightning.LightningTrainer>` and prefer using vanilla lightning trainer with function trainable, please refer to this document: {ref}`Using vanilla Pytorch Lightning with Tune <tune-vanilla-pytorch-lightning-ref>`.\n",
    "\n",
    ":::\n",
    "\n",
    ":::{note}\n",
    "To run this example, you will need to install the following:\n",
    "\n",
    "```bash\n",
    "$ pip install \"ray[tune]\" torch torchvision pytorch-lightning\n",
    "```\n",
    ":::\n",
    "\n",
    "```{contents}\n",
    ":backlinks: none\n",
    ":local: true\n",
    "```\n",
    "\n",
    "## PyTorch Lightning classifier for MNIST\n",
    "\n",
    "Let's first start with the basic PyTorch Lightning implementation of an MNIST classifier.\n",
    "This classifier does not include any tuning code at this point.\n",
    "\n",
    "First, we run some imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ray/anaconda3/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "import torch.nn.functional as F\n",
    "from filelock import FileLock\n",
    "from torchmetrics import Accuracy\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "\n",
    "import ray\n",
    "import ray.tune as tune\n",
    "from ray.air.config import CheckpointConfig, ScalingConfig\n",
    "from ray.train.lightning import LightningTrainer, LightningConfigBuilder\n",
    "from ray.tune.schedulers import PopulationBasedTraining\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# If you want to run full test, please set SMOKE_TEST to False\n",
    "SMOKE_TEST = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our example builds on the MNIST example from the [blog post](https://towardsdatascience.com/from-pytorch-to-pytorch-lightning-a-gentle-introduction-b371b7caaf09) we mentioned before. We adapted the original model and dataset definitions into `MNISTClassifier` and `MNISTDataModule`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTClassifier(pl.LightningModule):\n",
    "    def __init__(self, config):\n",
    "        super(MNISTClassifier, self).__init__()\n",
    "        self.accuracy = Accuracy()\n",
    "        self.layer_1_size = config[\"layer_1_size\"]\n",
    "        self.layer_2_size = config[\"layer_2_size\"]\n",
    "        self.lr = config[\"lr\"]\n",
    "\n",
    "        # mnist images are (1, 28, 28) (channels, width, height)\n",
    "        self.layer_1 = torch.nn.Linear(28 * 28, self.layer_1_size)\n",
    "        self.layer_2 = torch.nn.Linear(self.layer_1_size, self.layer_2_size)\n",
    "        self.layer_3 = torch.nn.Linear(self.layer_2_size, 10)\n",
    "\n",
    "    def cross_entropy_loss(self, logits, labels):\n",
    "        return F.nll_loss(logits, labels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, channels, width, height = x.size()\n",
    "        x = x.view(batch_size, -1)\n",
    "\n",
    "        x = self.layer_1(x)\n",
    "        x = torch.relu(x)\n",
    "\n",
    "        x = self.layer_2(x)\n",
    "        x = torch.relu(x)\n",
    "\n",
    "        x = self.layer_3(x)\n",
    "        x = torch.log_softmax(x, dim=1)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def training_step(self, train_batch, batch_idx):\n",
    "        x, y = train_batch\n",
    "        logits = self.forward(x)\n",
    "        loss = self.cross_entropy_loss(logits, y)\n",
    "        accuracy = self.accuracy(logits, y)\n",
    "\n",
    "        self.log(\"ptl/train_loss\", loss)\n",
    "        self.log(\"ptl/train_accuracy\", accuracy)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, val_batch, batch_idx):\n",
    "        x, y = val_batch\n",
    "        logits = self.forward(x)\n",
    "        loss = self.cross_entropy_loss(logits, y)\n",
    "        accuracy = self.accuracy(logits, y)\n",
    "        return {\"val_loss\": loss, \"val_accuracy\": accuracy}\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([x[\"val_loss\"] for x in outputs]).mean()\n",
    "        avg_acc = torch.stack([x[\"val_accuracy\"] for x in outputs]).mean()\n",
    "        self.log(\"ptl/val_loss\", avg_loss)\n",
    "        self.log(\"ptl/val_accuracy\", avg_acc)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "        return optimizer\n",
    "\n",
    "\n",
    "class MNISTDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, batch_size=128):\n",
    "        super().__init__()\n",
    "        self.data_dir = os.getcwd()\n",
    "        self.batch_size = batch_size\n",
    "        self.transform = transforms.Compose(\n",
    "            [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n",
    "        )\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        with FileLock(f\"{self.data_dir}.lock\"):\n",
    "            mnist = MNIST(\n",
    "                self.data_dir, train=True, download=True, transform=self.transform\n",
    "            )\n",
    "            self.mnist_train, self.mnist_val = random_split(mnist, [55000, 5000])\n",
    "\n",
    "            self.mnist_test = MNIST(\n",
    "                self.data_dir, train=False, download=True, transform=self.transform\n",
    "            )\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.mnist_train, batch_size=self.batch_size, num_workers=4)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.mnist_val, batch_size=self.batch_size, num_workers=4)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.mnist_test, batch_size=self.batch_size, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_config = {\n",
    "    \"layer_1_size\": 128,\n",
    "    \"layer_2_size\": 256,\n",
    "    \"lr\": 1e-3,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning the model parameters\n",
    "\n",
    "The parameters above should give you a good accuracy of over 90% already. However, we might improve on this simply by changing some of the hyperparameters. For instance, maybe we get an even higher accuracy if we used a smaller learning rate and larger middle layer size.\n",
    "\n",
    "Instead of manually loop through all the parameter combinitions, let's use Tune to systematically try out parameter combinations and find the best performing set.\n",
    "\n",
    "First, we need some additional imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from ray import air, tune\n",
    "from ray.air import session\n",
    "from ray.air.config import RunConfig, ScalingConfig, CheckpointConfig\n",
    "from ray.tune import CLIReporter\n",
    "from ray.tune.schedulers import ASHAScheduler, PopulationBasedTraining\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuring the search space\n",
    "\n",
    "Now we configure the parameter search space using {class}`LightningConfigBuilder <ray.train.lightning.LightningConfigBuilder>`. We would like to choose between three different layer and batch sizes. The learning rate should be sampled uniformly between `0.0001` and `0.1`. The `tune.loguniform()` function is syntactic sugar to make sampling between these different orders of magnitude easier, specifically we are able to also sample small values.\n",
    "\n",
    ":::{note}\n",
    "In `LightningTrainer`, the frequency of metric reporting is the same as the frequency of checkpointing. For example, if you set `builder.checkpointing(..., every_n_epochs=2)`, then for every 2 epochs, all the latest metrics will be reported to the Ray Tune session along with the latest checkpoint. Please make sure the target metrics(e.g. metrics specified in `TuneConfig`, schedulers, and searchers) are logged before saving a checkpoint.\n",
    "\n",
    ":::\n",
    "\n",
    "\n",
    ":::{note}\n",
    "Use `LightningConfigBuilder.checkpointing()` to specify the monitor metric and checkpoint frequency for the Lightning ModelCheckpoint callback. To properly save AIR checkpoints, you must also provide an AIR {class}`CheckpointConfig <ray.air.config.CheckpointConfig>`. Otherwise, LightningTrainer will create a default CheckpointConfig, which saves all the reported checkpoints by default.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The maximum training epochs\n",
    "num_epochs = 5\n",
    "\n",
    "# Number of sampls from parameter space\n",
    "num_samples = 10\n",
    "\n",
    "accelerator = \"gpu\"\n",
    "\n",
    "config = {\n",
    "    \"layer_1_size\": tune.choice([32, 64, 128]),\n",
    "    \"layer_2_size\": tune.choice([64, 128, 256]),\n",
    "    \"lr\": tune.loguniform(1e-4, 1e-1),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SMOKE_TEST:\n",
    "    num_epochs = 10\n",
    "    num_samples = 10\n",
    "    accelerator = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = MNISTDataModule(batch_size=128)\n",
    "\n",
    "lightning_config = (\n",
    "    LightningConfigBuilder()\n",
    "    .module(cls=MNISTClassifier, config=config)\n",
    "    .trainer(max_epochs=num_epochs, accelerator=accelerator)\n",
    "    .fit_params(datamodule=dm)\n",
    "    .checkpointing(monitor=\"ptl/val_accuracy\", save_top_k=2, mode=\"max\")\n",
    "    .build()\n",
    ")\n",
    "\n",
    "# Make sure to also define an AIR CheckpointConfig here\n",
    "# to properly save checkpoints in AIR format.\n",
    "run_config = RunConfig(\n",
    "    checkpoint_config=CheckpointConfig(\n",
    "        num_to_keep=2,\n",
    "        checkpoint_score_attribute=\"ptl/val_accuracy\",\n",
    "        checkpoint_score_order=\"max\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting a scheduler\n",
    "\n",
    "In this example, we use an [Asynchronous Hyperband](https://blog.ml.cmu.edu/2018/12/12/massively-parallel-hyperparameter-optimization/)\n",
    "scheduler. This scheduler decides at each iteration which trials are likely to perform\n",
    "badly, and stops these trials. This way we don't waste any resources on bad hyperparameter\n",
    "configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = ASHAScheduler(max_t=num_epochs, grace_period=1, reduction_factor=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with GPUs\n",
    "\n",
    "We can specify the number of resources, including GPUs, that Tune should request for each trial.\n",
    "\n",
    "`LightningTrainer` takes care of environment setup for Distributed Data Parallel training, the model and data will automatically get distributed across GPUs. You only need to set the number of GPUs per worker in `ScalingConfig` and also set `accelerator=\"gpu\"` in LightningTrainerConfigBuilder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaling_config = ScalingConfig(\n",
    "    num_workers=3, use_gpu=True, resources_per_worker={\"CPU\": 1, \"GPU\": 1}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SMOKE_TEST:\n",
    "    scaling_config = ScalingConfig(\n",
    "        num_workers=3, use_gpu=False, resources_per_worker={\"CPU\": 1}\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a base LightningTrainer without hyper-parameters for Tuner\n",
    "lightning_trainer = LightningTrainer(\n",
    "    scaling_config=scaling_config,\n",
    "    run_config=run_config,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it together\n",
    "\n",
    "Lastly, we need to create a `Tuner()` object and start Ray Tune with `tuner.fit()`.\n",
    "\n",
    "The full code looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_mnist_asha(num_samples=10):\n",
    "    scheduler = ASHAScheduler(max_t=num_epochs, grace_period=1, reduction_factor=2)\n",
    "\n",
    "    tuner = tune.Tuner(\n",
    "        lightning_trainer,\n",
    "        param_space={\"lightning_config\": lightning_config},\n",
    "        tune_config=tune.TuneConfig(\n",
    "            metric=\"ptl/val_accuracy\",\n",
    "            mode=\"max\",\n",
    "            num_samples=num_samples,\n",
    "            scheduler=scheduler,\n",
    "        ),\n",
    "        run_config=air.RunConfig(\n",
    "            name=\"tune_mnist_asha\",\n",
    "        ),\n",
    "    )\n",
    "    results = tuner.fit()\n",
    "    best_result = results.get_best_result(metric=\"ptl/val_accuracy\", mode=\"max\")\n",
    "    best_result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example above, Tune runs 10 trials with different hyperparameter configurations.\n",
    "An example output could look like so:\n",
    "\n",
    "```{code-block} bash\n",
    ":emphasize-lines: 12\n",
    "\n",
    "  +------------------------------+------------+-------------------+----------------+----------------+-------------+----------+-----------------+----------------------+\n",
    "  | Trial name                   | status     | loc               |   layer_1_size |   layer_2_size |          lr |     loss |   mean_accuracy |   training_iteration |\n",
    "  |------------------------------+------------+-------------------+----------------+----------------+-------------+----------+-----------------+----------------------|\n",
    "  | LightningTrainer_9532b_00001 | TERMINATED |  10.0.37.7:448989 |            32  |            64  | 0.00025324  | 0.58146  |       0.866667  |                   1  |\n",
    "  | LightningTrainer_9532b_00002 | TERMINATED |  10.0.37.7:449722 |            128 |            128 | 0.000166782 | 0.29038  |       0.933333  |                   2  |\n",
    "  | LightningTrainer_9532b_00003 | TERMINATED |  10.0.37.7:453404 |            64  |            128 | 0.0004948\t  | 0.15375  |       0.9       |                   4  |\n",
    "  | LightningTrainer_9532b_00004 | TERMINATED |  10.0.37.7:457981 |            128 |            128 | 0.000304361 | 0.17622  |       0.966667  |                   4  |\n",
    "  | LightningTrainer_9532b_00005 | TERMINATED |  10.0.37.7:467478 |            128 |            64  | 0.0344561\t  | 0.34665  |       0.866667  |                   1  |\n",
    "  | LightningTrainer_9532b_00006 | TERMINATED |  10.0.37.7:484401 |            128 |            256 | 0.0262851\t  | 0.34981  |       0.866667  |                   1  |\n",
    "  | LightningTrainer_9532b_00007 | TERMINATED |  10.0.37.7:490670 |            32  |            128 | 0.0550712\t  | 0.62575  |       0.766667  |                   1  |\n",
    "  | LightningTrainer_9532b_00008 | TERMINATED |  10.0.37.7:491159 |            32  |            64  | 0.000489046 | 0.27384  |       0.966667  |                   2  |\n",
    "  | LightningTrainer_9532b_00009 | TERMINATED |  10.0.37.7:491494 |            64  |            256 | 0.000395127 | 0.09642  |       0.933333  |                   8  |\n",
    "  +------------------------------+------------+-------------------+----------------+----------------+-------------+----------+-----------------+----------------------+\n",
    "```\n",
    "\n",
    "As you can see in the `training_iteration` column, trials with a high loss\n",
    "(and low accuracy) have been terminated early. The best performing trial used\n",
    "`layer_1_size=32`, `layer_2_size=64`, and `lr=0.000489046`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Population Based Training to find the best parameters\n",
    "\n",
    "The `ASHAScheduler` terminates those trials early that show bad performance.\n",
    "Sometimes, this stops trials that would get better after more training steps,\n",
    "and which might eventually even show better performance than other configurations.\n",
    "\n",
    "Another popular method for hyperparameter tuning, called\n",
    "[Population Based Training](https://deepmind.com/blog/article/population-based-training-neural-networks),\n",
    "instead perturbs hyperparameters during the training run. Tune implements PBT, and\n",
    "we only need to make some slight adjustments to our code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_mnist_pbt(num_samples=10):\n",
    "    # The range of hyperparameter perturbation.\n",
    "    mutations_config = (\n",
    "        LightningConfigBuilder()\n",
    "        .module(\n",
    "            config = {\n",
    "                \"lr\": tune.loguniform(1e-4, 1e-1),\n",
    "            }\n",
    "        ).build()\n",
    "    )\n",
    "\n",
    "    # Create a PBT scheduler\n",
    "    scheduler = PopulationBasedTraining(\n",
    "        perturbation_interval=1,\n",
    "        time_attr=\"training_iteration\",\n",
    "        hyperparam_mutations={\n",
    "            \"lightning_config\": mutations_config\n",
    "        }\n",
    "    )\n",
    "\n",
    "    tuner = tune.Tuner(\n",
    "        lightning_trainer,\n",
    "        param_space={\"lightning_config\": lightning_config},\n",
    "        tune_config=tune.TuneConfig(\n",
    "            metric=\"ptl/val_accuracy\",\n",
    "            mode=\"max\",\n",
    "            num_samples=num_samples,\n",
    "            scheduler=scheduler,\n",
    "        ),\n",
    "        run_config=air.RunConfig(\n",
    "            name=\"tune_mnist_pbt\",\n",
    "        ),\n",
    "    )\n",
    "    results = tuner.fit()\n",
    "    best_result = results.get_best_result(metric=\"ptl/val_accuracy\", mode=\"max\")\n",
    "    best_result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "find: ‘.git’: No such file or directory\n",
      "2023-03-29 19:41:33,900\tINFO worker.py:1415 -- Connecting to existing Ray cluster at address: 10.0.37.7:6379...\n",
      "2023-03-29 19:41:33,910\tINFO worker.py:1609 -- Connected to Ray cluster. View the dashboard at https://console.anyscale-staging.com/api/v2/sessions/ses_fxlstnzcjvmcl8zzdgutle9j7v/services?redirect_to=dashboard \n",
      "2023-03-29 19:41:34,099\tINFO packaging.py:346 -- Pushing file package 'gcs://_ray_pkg_71294a5f136ff637d61fe85afddc1f65.zip' (60.69MiB) to Ray cluster...\n",
      "2023-03-29 19:41:35,164\tINFO packaging.py:359 -- Successfully pushed file package 'gcs://_ray_pkg_71294a5f136ff637d61fe85afddc1f65.zip'.\n",
      "2023-03-29 19:41:35,213\tINFO tune.py:219 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `Tuner(...)`.\n",
      "/home/ray/anaconda3/lib/python3.7/site-packages/ray/tune/tune.py:656: UserWarning: Consider boosting PBT performance by enabling `reuse_actors` as well as implementing `reset_config` for Trainable.\n",
      "  \"Consider boosting PBT performance by enabling `reuse_actors` as \"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2023-03-29 19:47:38</td></tr>\n",
       "<tr><td>Running for: </td><td>00:06:03.46        </td></tr>\n",
       "<tr><td>Memory:      </td><td>16.1/62.0 GiB      </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      PopulationBasedTraining: 16 checkpoints, 7 perturbs<br>Logical resource usage: 16.0/16 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    ...odule_init_config\n",
       "/config/layer_1_size</th><th style=\"text-align: right;\">    ...odule_init_config\n",
       "/config/layer_2_size</th><th style=\"text-align: right;\">            ..._config/_module_i\n",
       "nit_config/config/lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ptl/train_loss</th><th style=\"text-align: right;\">  ptl/train_accuracy</th><th style=\"text-align: right;\">  ptl/val_loss</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>LightningTrainer_6331d_00002</td><td>RUNNING </td><td>10.0.37.7:1249072</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">0.00115867 </td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         75.1855</td><td style=\"text-align: right;\">      0.073703  </td><td style=\"text-align: right;\">            0.966667</td><td style=\"text-align: right;\">     0.0885324</td></tr>\n",
       "<tr><td>LightningTrainer_6331d_00003</td><td>RUNNING </td><td>10.0.37.7:1263175</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">0.00226503 </td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         71.8001</td><td style=\"text-align: right;\">      0.150485  </td><td style=\"text-align: right;\">            0.933333</td><td style=\"text-align: right;\">     0.0605363</td></tr>\n",
       "<tr><td>LightningTrainer_6331d_00004</td><td>RUNNING </td><td>10.0.37.7:1250292</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">0.00712747 </td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         71.3569</td><td style=\"text-align: right;\">      0.12994   </td><td style=\"text-align: right;\">            0.966667</td><td style=\"text-align: right;\">     0.083519 </td></tr>\n",
       "<tr><td>LightningTrainer_6331d_00005</td><td>RUNNING </td><td>10.0.37.7:1264205</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">0.00712747 </td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         74.3851</td><td style=\"text-align: right;\">      0.0211624 </td><td style=\"text-align: right;\">            1       </td><td style=\"text-align: right;\">     0.0618065</td></tr>\n",
       "<tr><td>LightningTrainer_6331d_00000</td><td>PAUSED  </td><td>10.0.37.7:1279466</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">0.00593956 </td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         91.3289</td><td style=\"text-align: right;\">      0.317815  </td><td style=\"text-align: right;\">            0.966667</td><td style=\"text-align: right;\">     0.0472228</td></tr>\n",
       "<tr><td>LightningTrainer_6331d_00001</td><td>PAUSED  </td><td>10.0.37.7:1280989</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">0.000367715</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         94.0547</td><td style=\"text-align: right;\">      0.14838   </td><td style=\"text-align: right;\">            0.966667</td><td style=\"text-align: right;\">     0.0590947</td></tr>\n",
       "<tr><td>LightningTrainer_6331d_00006</td><td>PAUSED  </td><td>10.0.37.7:1264873</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">0.00475165 </td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         72.0527</td><td style=\"text-align: right;\">      0.00528573</td><td style=\"text-align: right;\">            1       </td><td style=\"text-align: right;\">     0.0938069</td></tr>\n",
       "<tr><td>LightningTrainer_6331d_00007</td><td>PAUSED  </td><td>10.0.37.7:1266534</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">0.00712747 </td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         73.5926</td><td style=\"text-align: right;\">      0.0962502 </td><td style=\"text-align: right;\">            0.933333</td><td style=\"text-align: right;\">     0.0701004</td></tr>\n",
       "<tr><td>LightningTrainer_6331d_00008</td><td>PAUSED  </td><td>10.0.37.7:1279249</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">0.00188753 </td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         72.5808</td><td style=\"text-align: right;\">      0.131113  </td><td style=\"text-align: right;\">            0.933333</td><td style=\"text-align: right;\">     0.0690694</td></tr>\n",
       "<tr><td>LightningTrainer_6331d_00009</td><td>PAUSED  </td><td>10.0.37.7:1279305</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">256</td><td style=\"text-align: right;\">0.00343816 </td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         78.8291</td><td style=\"text-align: right;\">      0.209262  </td><td style=\"text-align: right;\">            0.933333</td><td style=\"text-align: right;\">     0.0791726</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-29 19:41:35,305\tINFO data_parallel_trainer.py:358 -- GPUs are detected in your Ray cluster, but GPU training is not enabled for this trainer. To enable GPU training, make sure to set `use_gpu` to True in your scaling config.\n",
      "2023-03-29 19:41:35,309\tINFO data_parallel_trainer.py:358 -- GPUs are detected in your Ray cluster, but GPU training is not enabled for this trainer. To enable GPU training, make sure to set `use_gpu` to True in your scaling config.\n",
      "2023-03-29 19:41:35,313\tINFO data_parallel_trainer.py:358 -- GPUs are detected in your Ray cluster, but GPU training is not enabled for this trainer. To enable GPU training, make sure to set `use_gpu` to True in your scaling config.\n",
      "2023-03-29 19:41:35,317\tINFO data_parallel_trainer.py:358 -- GPUs are detected in your Ray cluster, but GPU training is not enabled for this trainer. To enable GPU training, make sure to set `use_gpu` to True in your scaling config.\n",
      "2023-03-29 19:41:35,321\tINFO data_parallel_trainer.py:358 -- GPUs are detected in your Ray cluster, but GPU training is not enabled for this trainer. To enable GPU training, make sure to set `use_gpu` to True in your scaling config.\n",
      "2023-03-29 19:41:35,325\tINFO data_parallel_trainer.py:358 -- GPUs are detected in your Ray cluster, but GPU training is not enabled for this trainer. To enable GPU training, make sure to set `use_gpu` to True in your scaling config.\n",
      "2023-03-29 19:41:35,329\tINFO data_parallel_trainer.py:358 -- GPUs are detected in your Ray cluster, but GPU training is not enabled for this trainer. To enable GPU training, make sure to set `use_gpu` to True in your scaling config.\n",
      "2023-03-29 19:41:35,333\tINFO data_parallel_trainer.py:358 -- GPUs are detected in your Ray cluster, but GPU training is not enabled for this trainer. To enable GPU training, make sure to set `use_gpu` to True in your scaling config.\n",
      "2023-03-29 19:41:35,337\tINFO data_parallel_trainer.py:358 -- GPUs are detected in your Ray cluster, but GPU training is not enabled for this trainer. To enable GPU training, make sure to set `use_gpu` to True in your scaling config.\n",
      "2023-03-29 19:41:35,341\tINFO data_parallel_trainer.py:358 -- GPUs are detected in your Ray cluster, but GPU training is not enabled for this trainer. To enable GPU training, make sure to set `use_gpu` to True in your scaling config.\n",
      "(TrainTrainable pid=1132137) 2023-03-29 19:41:44,313\tINFO data_parallel_trainer.py:358 -- GPUs are detected in your Ray cluster, but GPU training is not enabled for this trainer. To enable GPU training, make sure to set `use_gpu` to True in your scaling config.\n",
      "(LightningTrainer pid=1132137) 2023-03-29 19:41:44,329\tINFO data_parallel_trainer.py:358 -- GPUs are detected in your Ray cluster, but GPU training is not enabled for this trainer. To enable GPU training, make sure to set `use_gpu` to True in your scaling config.\n",
      "(RayTrainWorker pid=1132464) 2023-03-29 19:41:47,513\tINFO config.py:87 -- Setting up process group for: env:// [rank=0, world_size=3]\n",
      "(TrainTrainable pid=1132467) 2023-03-29 19:41:51,654\tINFO data_parallel_trainer.py:358 -- GPUs are detected in your Ray cluster, but GPU training is not enabled for this trainer. To enable GPU training, make sure to set `use_gpu` to True in your scaling config.\n",
      "(LightningTrainer pid=1132467) 2023-03-29 19:41:51,671\tINFO data_parallel_trainer.py:358 -- GPUs are detected in your Ray cluster, but GPU training is not enabled for this trainer. To enable GPU training, make sure to set `use_gpu` to True in your scaling config.\n",
      "(RayTrainWorker pid=1132464) GPU available: False, used: False\n",
      "(RayTrainWorker pid=1132464) TPU available: False, using: 0 TPU cores\n",
      "(RayTrainWorker pid=1132464) IPU available: False, using: 0 IPUs\n",
      "(RayTrainWorker pid=1132464) HPU available: False, using: 0 HPUs\n",
      "(RayTrainWorker pid=1132464) Missing logger folder: /home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00000_0_layer_1_size=128,layer_2_size=128,lr=0.0059_2023-03-29_19-41-37/rank_0/lightning_logs\n",
      "(RayTrainWorker pid=1132464) \n",
      "(RayTrainWorker pid=1132464)   | Name     | Type     | Params\n",
      "(RayTrainWorker pid=1132464) --------------------------------------\n",
      "(RayTrainWorker pid=1132464) 0 | accuracy | Accuracy | 0     \n",
      "(RayTrainWorker pid=1132464) 1 | layer_1  | Linear   | 100 K \n",
      "(RayTrainWorker pid=1132464) 2 | layer_2  | Linear   | 16.5 K\n",
      "(RayTrainWorker pid=1132464) 3 | layer_3  | Linear   | 1.3 K \n",
      "(RayTrainWorker pid=1132464) --------------------------------------\n",
      "(RayTrainWorker pid=1132464) 118 K     Trainable params\n",
      "(RayTrainWorker pid=1132464) 0         Non-trainable params\n",
      "(RayTrainWorker pid=1132464) 118 K     Total params\n",
      "(RayTrainWorker pid=1132464) 0.473     Total estimated model params size (MB)\n",
      "(RayTrainWorker pid=1132466) [W reducer.cpp:1298] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "(RayTrainWorker pid=1132464) 2023-03-29 19:41:54.994147: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "(RayTrainWorker pid=1132464) To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "(RayTrainWorker pid=1132464) 2023-03-29 19:41:55.159831: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "(RayTrainWorker pid=1133164) 2023-03-29 19:41:55,700\tINFO config.py:87 -- Setting up process group for: env:// [rank=0, world_size=3]\n",
      "(RayTrainWorker pid=1132464) 2023-03-29 19:41:56.066711: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "(RayTrainWorker pid=1132464) 2023-03-29 19:41:56.066793: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "(RayTrainWorker pid=1132464) 2023-03-29 19:41:56.066800: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "(TrainTrainable pid=1133167) 2023-03-29 19:42:00,911\tINFO data_parallel_trainer.py:358 -- GPUs are detected in your Ray cluster, but GPU training is not enabled for this trainer. To enable GPU training, make sure to set `use_gpu` to True in your scaling config.\n",
      "(RayTrainWorker pid=1132465) Missing logger folder: /home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00000_0_layer_1_size=128,layer_2_size=128,lr=0.0059_2023-03-29_19-41-37/rank_1/lightning_logs [repeated 2x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication)\n",
      "(RayTrainWorker pid=1132464) [W reducer.cpp:1298] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator()) [repeated 2x across cluster]\n",
      "(LightningTrainer pid=1133167) 2023-03-29 19:42:00,941\tINFO data_parallel_trainer.py:358 -- GPUs are detected in your Ray cluster, but GPU training is not enabled for this trainer. To enable GPU training, make sure to set `use_gpu` to True in your scaling config.\n",
      "(RayTrainWorker pid=1133164) GPU available: False, used: False\n",
      "(RayTrainWorker pid=1133164) TPU available: False, using: 0 TPU cores\n",
      "(RayTrainWorker pid=1133164) IPU available: False, using: 0 IPUs\n",
      "(RayTrainWorker pid=1133164) HPU available: False, using: 0 HPUs\n",
      "(RayTrainWorker pid=1133164) \n",
      "(RayTrainWorker pid=1133164)   | Name     | Type     | Params\n",
      "(RayTrainWorker pid=1133164) --------------------------------------\n",
      "(RayTrainWorker pid=1133164) 0 | accuracy | Accuracy | 0     \n",
      "(RayTrainWorker pid=1133164) 1 | layer_1  | Linear   | 25.1 K\n",
      "(RayTrainWorker pid=1133164) 2 | layer_2  | Linear   | 2.1 K \n",
      "(RayTrainWorker pid=1133164) 3 | layer_3  | Linear   | 650   \n",
      "(RayTrainWorker pid=1133164) --------------------------------------\n",
      "(RayTrainWorker pid=1133164) 27.9 K    Trainable params\n",
      "(RayTrainWorker pid=1133164) 0         Non-trainable params\n",
      "(RayTrainWorker pid=1133164) 27.9 K    Total params\n",
      "(RayTrainWorker pid=1133164) 0.112     Total estimated model params size (MB)\n",
      "(RayTrainWorker pid=1133164) 2023-03-29 19:42:03.798847: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "(RayTrainWorker pid=1133164) To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "(RayTrainWorker pid=1133164) 2023-03-29 19:42:03.974534: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "(RayTrainWorker pid=1133164) 2023-03-29 19:42:04.856792: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "(RayTrainWorker pid=1133164) 2023-03-29 19:42:04.857051: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "(RayTrainWorker pid=1133164) 2023-03-29 19:42:04.857065: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "(RayTrainWorker pid=1137080) 2023-03-29 19:42:05,140\tINFO config.py:87 -- Setting up process group for: env:// [rank=0, world_size=3]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"trialProgress\">\n",
       "  <h3>Trial Progress</h3>\n",
       "  <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>_report_on     </th><th>date               </th><th>done  </th><th style=\"text-align: right;\">  epoch</th><th>hostname    </th><th style=\"text-align: right;\">  iterations_since_restore</th><th>node_ip  </th><th style=\"text-align: right;\">    pid</th><th style=\"text-align: right;\">  ptl/train_accuracy</th><th style=\"text-align: right;\">  ptl/train_loss</th><th style=\"text-align: right;\">  ptl/val_accuracy</th><th style=\"text-align: right;\">  ptl/val_loss</th><th>should_checkpoint  </th><th style=\"text-align: right;\">  step</th><th style=\"text-align: right;\">  time_since_restore</th><th style=\"text-align: right;\">  time_this_iter_s</th><th style=\"text-align: right;\">  time_total_s</th><th style=\"text-align: right;\">  timestamp</th><th style=\"text-align: right;\">  training_iteration</th><th>trial_id   </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>LightningTrainer_6331d_00000</td><td>train_epoch_end</td><td>2023-03-29_19-47-29</td><td>False </td><td style=\"text-align: right;\">      4</td><td>ip-10-0-37-7</td><td style=\"text-align: right;\">                         1</td><td>10.0.37.7</td><td style=\"text-align: right;\">1279466</td><td style=\"text-align: right;\">            0.966667</td><td style=\"text-align: right;\">      0.317815  </td><td style=\"text-align: right;\">          0.985119</td><td style=\"text-align: right;\">     0.0472228</td><td>True               </td><td style=\"text-align: right;\">   720</td><td style=\"text-align: right;\">             20.5497</td><td style=\"text-align: right;\">           20.5497</td><td style=\"text-align: right;\">       91.3289</td><td style=\"text-align: right;\"> 1680144448</td><td style=\"text-align: right;\">                   5</td><td>6331d_00000</td></tr>\n",
       "<tr><td>LightningTrainer_6331d_00001</td><td>train_epoch_end</td><td>2023-03-29_19-47-32</td><td>False </td><td style=\"text-align: right;\">      4</td><td>ip-10-0-37-7</td><td style=\"text-align: right;\">                         1</td><td>10.0.37.7</td><td style=\"text-align: right;\">1280989</td><td style=\"text-align: right;\">            0.966667</td><td style=\"text-align: right;\">      0.14838   </td><td style=\"text-align: right;\">          0.982143</td><td style=\"text-align: right;\">     0.0590947</td><td>True               </td><td style=\"text-align: right;\">   720</td><td style=\"text-align: right;\">             21.0163</td><td style=\"text-align: right;\">           21.0163</td><td style=\"text-align: right;\">       94.0547</td><td style=\"text-align: right;\"> 1680144451</td><td style=\"text-align: right;\">                   5</td><td>6331d_00001</td></tr>\n",
       "<tr><td>LightningTrainer_6331d_00002</td><td>train_epoch_end</td><td>2023-03-29_19-46-22</td><td>False </td><td style=\"text-align: right;\">      3</td><td>ip-10-0-37-7</td><td style=\"text-align: right;\">                         1</td><td>10.0.37.7</td><td style=\"text-align: right;\">1249072</td><td style=\"text-align: right;\">            0.966667</td><td style=\"text-align: right;\">      0.073703  </td><td style=\"text-align: right;\">          0.973958</td><td style=\"text-align: right;\">     0.0885324</td><td>True               </td><td style=\"text-align: right;\">   576</td><td style=\"text-align: right;\">             18.6628</td><td style=\"text-align: right;\">           18.6628</td><td style=\"text-align: right;\">       75.1855</td><td style=\"text-align: right;\"> 1680144382</td><td style=\"text-align: right;\">                   4</td><td>6331d_00002</td></tr>\n",
       "<tr><td>LightningTrainer_6331d_00003</td><td>train_epoch_end</td><td>2023-03-29_19-46-53</td><td>False </td><td style=\"text-align: right;\">      3</td><td>ip-10-0-37-7</td><td style=\"text-align: right;\">                         1</td><td>10.0.37.7</td><td style=\"text-align: right;\">1263175</td><td style=\"text-align: right;\">            0.933333</td><td style=\"text-align: right;\">      0.150485  </td><td style=\"text-align: right;\">          0.981027</td><td style=\"text-align: right;\">     0.0605363</td><td>True               </td><td style=\"text-align: right;\">   576</td><td style=\"text-align: right;\">             19.236 </td><td style=\"text-align: right;\">           19.236 </td><td style=\"text-align: right;\">       71.8001</td><td style=\"text-align: right;\"> 1680144413</td><td style=\"text-align: right;\">                   4</td><td>6331d_00003</td></tr>\n",
       "<tr><td>LightningTrainer_6331d_00004</td><td>train_epoch_end</td><td>2023-03-29_19-46-26</td><td>False </td><td style=\"text-align: right;\">      3</td><td>ip-10-0-37-7</td><td style=\"text-align: right;\">                         1</td><td>10.0.37.7</td><td style=\"text-align: right;\">1250292</td><td style=\"text-align: right;\">            0.966667</td><td style=\"text-align: right;\">      0.12994   </td><td style=\"text-align: right;\">          0.975446</td><td style=\"text-align: right;\">     0.083519 </td><td>True               </td><td style=\"text-align: right;\">   576</td><td style=\"text-align: right;\">             21.2308</td><td style=\"text-align: right;\">           21.2308</td><td style=\"text-align: right;\">       71.3569</td><td style=\"text-align: right;\"> 1680144386</td><td style=\"text-align: right;\">                   4</td><td>6331d_00004</td></tr>\n",
       "<tr><td>LightningTrainer_6331d_00005</td><td>train_epoch_end</td><td>2023-03-29_19-46-56</td><td>False </td><td style=\"text-align: right;\">      3</td><td>ip-10-0-37-7</td><td style=\"text-align: right;\">                         1</td><td>10.0.37.7</td><td style=\"text-align: right;\">1264205</td><td style=\"text-align: right;\">            1       </td><td style=\"text-align: right;\">      0.0211624 </td><td style=\"text-align: right;\">          0.980097</td><td style=\"text-align: right;\">     0.0618065</td><td>True               </td><td style=\"text-align: right;\">   576</td><td style=\"text-align: right;\">             21.1813</td><td style=\"text-align: right;\">           21.1813</td><td style=\"text-align: right;\">       74.3851</td><td style=\"text-align: right;\"> 1680144416</td><td style=\"text-align: right;\">                   4</td><td>6331d_00005</td></tr>\n",
       "<tr><td>LightningTrainer_6331d_00006</td><td>train_epoch_end</td><td>2023-03-29_19-46-57</td><td>False </td><td style=\"text-align: right;\">      3</td><td>ip-10-0-37-7</td><td style=\"text-align: right;\">                         1</td><td>10.0.37.7</td><td style=\"text-align: right;\">1264873</td><td style=\"text-align: right;\">            1       </td><td style=\"text-align: right;\">      0.00528573</td><td style=\"text-align: right;\">          0.975508</td><td style=\"text-align: right;\">     0.0938069</td><td>True               </td><td style=\"text-align: right;\">   576</td><td style=\"text-align: right;\">             20.9611</td><td style=\"text-align: right;\">           20.9611</td><td style=\"text-align: right;\">       72.0527</td><td style=\"text-align: right;\"> 1680144417</td><td style=\"text-align: right;\">                   4</td><td>6331d_00006</td></tr>\n",
       "<tr><td>LightningTrainer_6331d_00007</td><td>train_epoch_end</td><td>2023-03-29_19-47-01</td><td>False </td><td style=\"text-align: right;\">      3</td><td>ip-10-0-37-7</td><td style=\"text-align: right;\">                         1</td><td>10.0.37.7</td><td style=\"text-align: right;\">1266534</td><td style=\"text-align: right;\">            0.933333</td><td style=\"text-align: right;\">      0.0962502 </td><td style=\"text-align: right;\">          0.977679</td><td style=\"text-align: right;\">     0.0701004</td><td>True               </td><td style=\"text-align: right;\">   576</td><td style=\"text-align: right;\">             20.5557</td><td style=\"text-align: right;\">           20.5557</td><td style=\"text-align: right;\">       73.5926</td><td style=\"text-align: right;\"> 1680144421</td><td style=\"text-align: right;\">                   4</td><td>6331d_00007</td></tr>\n",
       "<tr><td>LightningTrainer_6331d_00008</td><td>train_epoch_end</td><td>2023-03-29_19-47-26</td><td>False </td><td style=\"text-align: right;\">      3</td><td>ip-10-0-37-7</td><td style=\"text-align: right;\">                         1</td><td>10.0.37.7</td><td style=\"text-align: right;\">1279249</td><td style=\"text-align: right;\">            0.933333</td><td style=\"text-align: right;\">      0.131113  </td><td style=\"text-align: right;\">          0.977679</td><td style=\"text-align: right;\">     0.0690694</td><td>True               </td><td style=\"text-align: right;\">   576</td><td style=\"text-align: right;\">             20.0167</td><td style=\"text-align: right;\">           20.0167</td><td style=\"text-align: right;\">       72.5808</td><td style=\"text-align: right;\"> 1680144446</td><td style=\"text-align: right;\">                   4</td><td>6331d_00008</td></tr>\n",
       "<tr><td>LightningTrainer_6331d_00009</td><td>train_epoch_end</td><td>2023-03-29_19-47-28</td><td>False </td><td style=\"text-align: right;\">      3</td><td>ip-10-0-37-7</td><td style=\"text-align: right;\">                         1</td><td>10.0.37.7</td><td style=\"text-align: right;\">1279305</td><td style=\"text-align: right;\">            0.933333</td><td style=\"text-align: right;\">      0.209262  </td><td style=\"text-align: right;\">          0.976191</td><td style=\"text-align: right;\">     0.0791726</td><td>True               </td><td style=\"text-align: right;\">   576</td><td style=\"text-align: right;\">             21.1968</td><td style=\"text-align: right;\">           21.1968</td><td style=\"text-align: right;\">       78.8291</td><td style=\"text-align: right;\"> 1680144448</td><td style=\"text-align: right;\">                   4</td><td>6331d_00009</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".trialProgress {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".trialProgress h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".trialProgress td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=1133165) Missing logger folder: /home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00001_1_layer_1_size=32,layer_2_size=64,lr=0.0012_2023-03-29_19-41-44/rank_1/lightning_logs [repeated 3x across cluster]\n",
      "(RayTrainWorker pid=1133165) [W reducer.cpp:1298] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator()) [repeated 3x across cluster]\n",
      "(TrainTrainable pid=1137088) 2023-03-29 19:42:10,849\tINFO data_parallel_trainer.py:358 -- GPUs are detected in your Ray cluster, but GPU training is not enabled for this trainer. To enable GPU training, make sure to set `use_gpu` to True in your scaling config.\n",
      "(LightningTrainer pid=1137088) 2023-03-29 19:42:10,866\tINFO data_parallel_trainer.py:358 -- GPUs are detected in your Ray cluster, but GPU training is not enabled for this trainer. To enable GPU training, make sure to set `use_gpu` to True in your scaling config.\n",
      "(RayTrainWorker pid=1137080) GPU available: False, used: False\n",
      "(RayTrainWorker pid=1137080) TPU available: False, using: 0 TPU cores\n",
      "(RayTrainWorker pid=1137080) IPU available: False, using: 0 IPUs\n",
      "(RayTrainWorker pid=1137080) HPU available: False, using: 0 HPUs\n",
      "(RayTrainWorker pid=1137080) \n",
      "(RayTrainWorker pid=1137080)   | Name     | Type     | Params\n",
      "(RayTrainWorker pid=1137080) --------------------------------------\n",
      "(RayTrainWorker pid=1137080) 0 | accuracy | Accuracy | 0     \n",
      "(RayTrainWorker pid=1137080) 1 | layer_1  | Linear   | 100 K \n",
      "(RayTrainWorker pid=1137080) 2 | layer_2  | Linear   | 8.3 K \n",
      "(RayTrainWorker pid=1137080) 3 | layer_3  | Linear   | 650   \n",
      "(RayTrainWorker pid=1137080) --------------------------------------\n",
      "(RayTrainWorker pid=1137080) 109 K     Trainable params\n",
      "(RayTrainWorker pid=1137080) 0         Non-trainable params\n",
      "(RayTrainWorker pid=1137080) 109 K     Total params\n",
      "(RayTrainWorker pid=1137080) 0.438     Total estimated model params size (MB)\n",
      "(RayTrainWorker pid=1137080) 2023-03-29 19:42:14.188933: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "(RayTrainWorker pid=1137080) To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "(RayTrainWorker pid=1137080) 2023-03-29 19:42:14.346718: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "(RayTrainWorker pid=1141721) 2023-03-29 19:42:15,093\tINFO config.py:87 -- Setting up process group for: env:// [rank=0, world_size=3]\n",
      "(RayTrainWorker pid=1137080) 2023-03-29 19:42:15.255660: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "(RayTrainWorker pid=1137080) 2023-03-29 19:42:15.255744: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "(RayTrainWorker pid=1137080) 2023-03-29 19:42:15.255752: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "(RayTrainWorker pid=1141721) Missing logger folder: /home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00003_3_layer_1_size=32,layer_2_size=256,lr=0.0012_2023-03-29_19-42-00/rank_0/lightning_logs [repeated 4x across cluster]\n",
      "(RayTrainWorker pid=1137082) [W reducer.cpp:1298] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator()) [repeated 3x across cluster]\n",
      "(RayTrainWorker pid=1141721) Missing logger folder: /home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00003_3_layer_1_size=32,layer_2_size=256,lr=0.0012_2023-03-29_19-42-00/rank_0/lightning_logs\n",
      "(RayTrainWorker pid=1141721) Missing logger folder: /home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00003_3_layer_1_size=32,layer_2_size=256,lr=0.0012_2023-03-29_19-42-00/rank_0/lightning_logs\n",
      "(RayTrainWorker pid=1141721) Missing logger folder: /home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00003_3_layer_1_size=32,layer_2_size=256,lr=0.0012_2023-03-29_19-42-00/rank_0/lightning_logs\n",
      "(RayTrainWorker pid=1141721) Missing logger folder: /home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00003_3_layer_1_size=32,layer_2_size=256,lr=0.0012_2023-03-29_19-42-00/rank_0/lightning_logs\n",
      "(RayTrainWorker pid=1141721) \n",
      "(RayTrainWorker pid=1141721)   | Name     | Type     | Params\n",
      "(RayTrainWorker pid=1141721) --------------------------------------\n",
      "(RayTrainWorker pid=1141721) 0 | accuracy | Accuracy | 0     \n",
      "(RayTrainWorker pid=1141721) 1 | layer_1  | Linear   | 25.1 K\n",
      "(RayTrainWorker pid=1141721) 2 | layer_2  | Linear   | 8.4 K \n",
      "(RayTrainWorker pid=1141721) 3 | layer_3  | Linear   | 2.6 K \n",
      "(RayTrainWorker pid=1141721) --------------------------------------\n",
      "(RayTrainWorker pid=1141721) 36.1 K    Trainable params\n",
      "(RayTrainWorker pid=1141721) 0         Non-trainable params\n",
      "(RayTrainWorker pid=1141721) 36.1 K    Total params\n",
      "(RayTrainWorker pid=1141721) 0.145     Total estimated model params size (MB)\n",
      "(TrainTrainable pid=1142456) 2023-03-29 19:42:22,478\tINFO data_parallel_trainer.py:358 -- GPUs are detected in your Ray cluster, but GPU training is not enabled for this trainer. To enable GPU training, make sure to set `use_gpu` to True in your scaling config.\n",
      "(LightningTrainer pid=1142456) 2023-03-29 19:42:22,501\tINFO data_parallel_trainer.py:358 -- GPUs are detected in your Ray cluster, but GPU training is not enabled for this trainer. To enable GPU training, make sure to set `use_gpu` to True in your scaling config.\n",
      "(RayTrainWorker pid=1141721) 2023-03-29 19:42:23.268073: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "(RayTrainWorker pid=1141721) To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "(RayTrainWorker pid=1141721) 2023-03-29 19:42:23.429875: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "(RayTrainWorker pid=1141721) 2023-03-29 19:42:25.048255: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "(RayTrainWorker pid=1141721) 2023-03-29 19:42:25.048332: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "(RayTrainWorker pid=1141721) 2023-03-29 19:42:25.048340: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "(RayTrainWorker pid=1147827) 2023-03-29 19:42:27,219\tINFO config.py:87 -- Setting up process group for: env:// [rank=0, world_size=3]\n",
      "(RayTrainWorker pid=1141792) Missing logger folder: /home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00003_3_layer_1_size=32,layer_2_size=256,lr=0.0012_2023-03-29_19-42-00/rank_1/lightning_logs [repeated 2x across cluster]\n",
      "(RayTrainWorker pid=1141792) [W reducer.cpp:1298] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator()) [repeated 3x across cluster]\n",
      "(TrainTrainable pid=1147957) 2023-03-29 19:42:33,294\tINFO data_parallel_trainer.py:358 -- GPUs are detected in your Ray cluster, but GPU training is not enabled for this trainer. To enable GPU training, make sure to set `use_gpu` to True in your scaling config. [repeated 3x across cluster]\n",
      "(RayTrainWorker pid=1147846) 2023-03-29 19:42:33,294\tINFO data_parallel_trainer.py:358 -- GPUs are detected in your Ray cluster, but GPU training is not enabled for this trainer. To enable GPU training, make sure to set `use_gpu` to True in your scaling config.\n",
      "(RayTrainWorker pid=1147848) Missing logger folder: /home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00004_4_layer_1_size=32,layer_2_size=64,lr=0.0010_2023-03-29_19-42-12/rank_2/lightning_logs\n",
      "(RayTrainWorker pid=1147846) GPU available: False, used: False\n",
      "(RayTrainWorker pid=1147846) TPU available: False, using: 0 TPU cores\n",
      "(RayTrainWorker pid=1147846) IPU available: False, using: 0 IPUs\n",
      "(RayTrainWorker pid=1147846) HPU available: False, using: 0 HPUs\n",
      "(RayTrainWorker pid=1147846) \n",
      "(RayTrainWorker pid=1147846)   | Name     | Type     | Params\n",
      "(RayTrainWorker pid=1147846) --------------------------------------\n",
      "(RayTrainWorker pid=1147846) 0 | accuracy | Accuracy | 0     \n",
      "(RayTrainWorker pid=1147846) 1 | layer_1  | Linear   | 25.1 K\n",
      "(RayTrainWorker pid=1147846) 2 | layer_2  | Linear   | 2.1 K \n",
      "(RayTrainWorker pid=1147846) 3 | layer_3  | Linear   | 650   \n",
      "(RayTrainWorker pid=1147846) --------------------------------------\n",
      "(RayTrainWorker pid=1147846) 27.9 K    Trainable params\n",
      "(RayTrainWorker pid=1147846) 0         Non-trainable params\n",
      "(RayTrainWorker pid=1147846) 27.9 K    Total params\n",
      "(RayTrainWorker pid=1147846) 0.112     Total estimated model params size (MB)\n",
      "(RayTrainWorker pid=1147846) [W reducer.cpp:1298] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "(RayTrainWorker pid=1147846) 2023-03-29 19:42:37.210122: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "(RayTrainWorker pid=1147846) To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "(RayTrainWorker pid=1147846) 2023-03-29 19:42:37.423072: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "(RayTrainWorker pid=1151461) 2023-03-29 19:42:38,176\tINFO config.py:87 -- Setting up process group for: env:// [rank=0, world_size=3]\n",
      "(RayTrainWorker pid=1147846) 2023-03-29 19:42:38.358953: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "(RayTrainWorker pid=1147846) 2023-03-29 19:42:38.359034: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "(RayTrainWorker pid=1147846) 2023-03-29 19:42:38.359041: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "(LightningTrainer pid=1147957) 2023-03-29 19:42:38.524677: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2023-03-29 19:42:45,895\tINFO pbt.py:817 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial 6331d_00000 (score = 0.967076) into trial 6331d_00005 (score = 0.293651)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "MUTATION:  {'lr': <ray.tune.search.sample.Float object at 0x7f901753aed0>}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ORIGINAL:  {'layer_1_size': 128, 'layer_2_size': 128, 'lr': 0.005939557174932708}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-29 19:42:45,910\tINFO pbt.py:838 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of trial6331d_00005:\n",
      "lightning_config : \n",
      "    _module_init_config : \n",
      "        config : \n",
      "            lr : 0.005939557174932708 --- (* 1.2) --> 0.007127468609919249\n",
      "    _trainer_init_config : \n",
      "    _trainer_fit_params : \n",
      "    _ddp_strategy_config : \n",
      "    _model_checkpoint_config : \n",
      "\n",
      "2023-03-29 19:42:45,912\tWARNING trial_runner.py:1544 -- You are trying to access pause_trial interface of TrialRunner in TrialScheduler, which is being restricted. If you believe it is reasonable for your scheduler to access this TrialRunner API, please reach out to Ray team on GitHub. A more strict API access pattern would be enforced starting 1.12s.0\n",
      "(TrainTrainable pid=1151588)2023-03-29 19:42:46,443\tWARNING util.py:244 -- The `scheduler.on_trial_result` operation took 0.548 s, which may be a performance bottleneck.\n",
      " 2023-03-29 19:42:45,851\tINFO data_parallel_trainer.py:358 -- GPUs are detected in your Ray cluster, but GPU training is not enabled for this trainer. To enable GPU training, make sure to set `use_gpu` to True in your scaling config.\n",
      "(RayTrainWorker pid=1147829) Missing logger folder: /home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00005_5_layer_1_size=64,layer_2_size=128,lr=0.0746_2023-03-29_19-42-12/rank_2/lightning_logs [repeated 5x across cluster]\n",
      "(RayTrainWorker pid=1147827) 2023-03-29 19:42:45,851\tINFO data_parallel_trainer.py:358 -- GPUs are detected in your Ray cluster, but GPU training is not enabled for this trainer. To enable GPU training, make sure to set `use_gpu` to True in your scaling config.\n",
      "(RayTrainWorker pid=1147827) 2023-03-29 19:42:45,851\tINFO data_parallel_trainer.py:358 -- GPUs are detected in your Ray cluster, but GPU training is not enabled for this trainer. To enable GPU training, make sure to set `use_gpu` to True in your scaling config.\n",
      "2023-03-29 19:42:46,460\tWARNING util.py:244 -- The `process_trial_result` operation took 0.565 s, which may be a performance bottleneck.\n",
      "(RayTrainWorker pid=1147827) 2023-03-29 19:42:45,851\tINFO data_parallel_trainer.py:358 -- GPUs are detected in your Ray cluster, but GPU training is not enabled for this trainer. To enable GPU training, make sure to set `use_gpu` to True in your scaling config.2023-03-29 19:42:46,463\tWARNING util.py:244 -- Processing trial results took 0.568 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "\n",
      "(RayTrainWorker pid=1147827) 2023-03-29 19:42:45,851\tINFO data_parallel_trainer.py:358 -- GPUs are detected in your Ray cluster, but GPU training is not enabled for this trainer. To enable GPU training, make sure to set `use_gpu` to True in your scaling config.\n",
      "(RayTrainWorker pid=1147827) 2023-03-29 19:42:45,851\tINFO data_parallel_trainer.py:358 -- GPUs are detected in your Ray cluster, but GPU training is not enabled for this trainer. To enable GPU training, make sure to set `use_gpu` to True in your scaling config.2023-03-29 19:42:46,467\tWARNING util.py:244 -- The `process_trial_result` operation took 0.572 s, which may be a performance bottleneck.\n",
      "\n",
      "(RayTrainWorker pid=1147827) 2023-03-29 19:42:45,851\tINFO data_parallel_trainer.py:358 -- GPUs are detected in your Ray cluster, but GPU training is not enabled for this trainer. To enable GPU training, make sure to set `use_gpu` to True in your scaling config.\n",
      "(RayTrainWorker pid=1147827) -------------------------------------- [repeated 2x across cluster]\n",
      "(RayTrainWorker pid=1147827) 2023-03-29 19:42:45,851\tINFO data_parallel_trainer.py:358 -- GPUs are detected in your Ray cluster, but GPU training is not enabled for this trainer. To enable GPU training, make sure to set `use_gpu` to True in your scaling config.\n",
      "(RayTrainWorker pid=1147827) 3 | layer_3  | Linear   | 1.3 K 2023-03-29 19:42:46,546\tINFO pbt.py:651 -- [pbt]: no checkpoint for trial. Skip exploit for Trial LightningTrainer_6331d_00004\n",
      " [repeated 3x across cluster]\n",
      "(RayTrainWorker pid=1147827) 2023-03-29 19:42:45,851\tINFO data_parallel_trainer.py:358 -- GPUs are detected in your Ray cluster, but GPU training is not enabled for this trainer. To enable GPU training, make sure to set `use_gpu` to True in your scaling config.\n",
      "(RayTrainWorker pid=1147827) 2023-03-29 19:42:45,851\tINFO data_parallel_trainer.py:358 -- GPUs are detected in your Ray cluster, but GPU training is not enabled for this trainer. To enable GPU training, make sure to set `use_gpu` to True in your scaling config.\n",
      "(RayTrainWorker pid=1147827) 2023-03-29 19:42:45,851\tINFO data_parallel_trainer.py:358 -- GPUs are detected in your Ray cluster, but GPU training is not enabled for this trainer. To enable GPU training, make sure to set `use_gpu` to True in your scaling config.\n",
      "(RayTrainWorker pid=1147827) 2023-03-29 19:42:45,851\tINFO data_parallel_trainer.py:358 -- GPUs are detected in your Ray cluster, but GPU training is not enabled for this trainer. To enable GPU training, make sure to set `use_gpu` to True in your scaling config.\n",
      "(RayTrainWorker pid=1147829) [W reducer.cpp:1298] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator()) [repeated 5x across cluster]\n",
      "(RayTrainWorker pid=1147827) 2023-03-29 19:42:45,851\tINFO data_parallel_trainer.py:358 -- GPUs are detected in your Ray cluster, but GPU training is not enabled for this trainer. To enable GPU training, make sure to set `use_gpu` to True in your scaling config.\n",
      "(RayTrainWorker pid=1147827) 2023-03-29 19:42:45,851\tINFO data_parallel_trainer.py:358 -- GPUs are detected in your Ray cluster, but GPU training is not enabled for this trainer. To enable GPU training, make sure to set `use_gpu` to True in your scaling config.\n",
      "(RayTrainWorker pid=1147827) 2023-03-29 19:42:45,851\tINFO data_parallel_trainer.py:358 -- GPUs are detected in your Ray cluster, but GPU training is not enabled for this trainer. To enable GPU training, make sure to set `use_gpu` to True in your scaling config.\n",
      "(RayTrainWorker pid=1147827) 2023-03-29 19:42:38.524669: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64 [repeated 2x across cluster]\n",
      "(RayTrainWorker pid=1147827) 2023-03-29 19:42:45,851\tINFO data_parallel_trainer.py:358 -- GPUs are detected in your Ray cluster, but GPU training is not enabled for this trainer. To enable GPU training, make sure to set `use_gpu` to True in your scaling config.\n",
      "(LightningTrainer pid=1151588) 2023-03-29 19:42:45,897\tINFO data_parallel_trainer.py:358 -- GPUs are detected in your Ray cluster, but GPU training is not enabled for this trainer. To enable GPU training, make sure to set `use_gpu` to True in your scaling config.\n",
      "(RayTrainWorker pid=1151461) GPU available: False, used: False\n",
      "(RayTrainWorker pid=1151461) TPU available: False, using: 0 TPU cores\n",
      "(RayTrainWorker pid=1151461) IPU available: False, using: 0 IPUs\n",
      "(RayTrainWorker pid=1151461) HPU available: False, using: 0 HPUs\n",
      "(RayTrainWorker pid=1151461) \n",
      "(RayTrainWorker pid=1151461)   | Name     | Type     | Params\n",
      "(RayTrainWorker pid=1151461) 0 | accuracy | Accuracy | 0     \n",
      "(RayTrainWorker pid=1151461) 59.9 K    Trainable params\n",
      "(RayTrainWorker pid=1151461) 0         Non-trainable params\n",
      "(RayTrainWorker pid=1151461) 59.9 K    Total params\n",
      "(RayTrainWorker pid=1151461) 0.239     Total estimated model params size (MB)\n",
      "(RayTrainWorker pid=1151461) 2023-03-29 19:42:49.283680: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "(RayTrainWorker pid=1151461) To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "(RayTrainWorker pid=1151461) 2023-03-29 19:42:49.487062: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "(RayTrainWorker pid=1151461) 2023-03-29 19:42:50.375165: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "(RayTrainWorker pid=1157715) 2023-03-29 19:42:50,357\tINFO config.py:87 -- Setting up process group for: env:// [rank=0, world_size=3]\n",
      "(TrainTrainable pid=1158431) 2023-03-29 19:42:56,721\tINFO data_parallel_trainer.py:358 -- GPUs are detected in your Ray cluster, but GPU training is not enabled for this trainer. To enable GPU training, make sure to set `use_gpu` to True in your scaling config.\n",
      "(RayTrainWorker pid=1151463) Missing logger folder: /home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00006_6_layer_1_size=64,layer_2_size=128,lr=0.0003_2023-03-29_19-42-23/rank_2/lightning_logs [repeated 3x across cluster]\n",
      "(RayTrainWorker pid=1151461) -------------------------------------- [repeated 2x across cluster]\n",
      "(RayTrainWorker pid=1151461) 3 | layer_3  | Linear   | 1.3 K  [repeated 3x across cluster]\n",
      "(RayTrainWorker pid=1151461) [W reducer.cpp:1298] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator()) [repeated 3x across cluster]\n",
      "(RayTrainWorker pid=1151461) 2023-03-29 19:42:50.375157: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64 [repeated 2x across cluster]\n",
      "(LightningTrainer pid=1158431) 2023-03-29 19:42:56,738\tINFO data_parallel_trainer.py:358 -- GPUs are detected in your Ray cluster, but GPU training is not enabled for this trainer. To enable GPU training, make sure to set `use_gpu` to True in your scaling config.\n",
      "(RayTrainWorker pid=1157715) GPU available: False, used: False\n",
      "(RayTrainWorker pid=1157715) TPU available: False, using: 0 TPU cores\n",
      "(RayTrainWorker pid=1157715) IPU available: False, using: 0 IPUs\n",
      "(RayTrainWorker pid=1157715) HPU available: False, using: 0 HPUs\n",
      "(RayTrainWorker pid=1157715) \n",
      "(RayTrainWorker pid=1157715)   | Name     | Type     | Params\n",
      "(RayTrainWorker pid=1157715) 0 | accuracy | Accuracy | 0     \n",
      "(RayTrainWorker pid=1157715) 30.6 K    Trainable params\n",
      "(RayTrainWorker pid=1157715) 0         Non-trainable params\n",
      "(RayTrainWorker pid=1157715) 30.6 K    Total params\n",
      "(RayTrainWorker pid=1157715) 0.123     Total estimated model params size (MB)\n",
      "(RayTrainWorker pid=1157715) 2023-03-29 19:43:00.300305: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "(RayTrainWorker pid=1157715) To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "(RayTrainWorker pid=1157715) 2023-03-29 19:43:00.468381: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "(RayTrainWorker pid=1162883) 2023-03-29 19:43:01,375\tINFO config.py:87 -- Setting up process group for: env:// [rank=0, world_size=3]\n",
      "(RayTrainWorker pid=1157715) 2023-03-29 19:43:01.515947: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "(RayTrainWorker pid=1157718) Missing logger folder: /home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00007_7_layer_1_size=32,layer_2_size=128,lr=0.0001_2023-03-29_19-42-34/rank_2/lightning_logs [repeated 3x across cluster]\n",
      "(RayTrainWorker pid=1157715) -------------------------------------- [repeated 2x across cluster]\n",
      "(RayTrainWorker pid=1157715) 3 | layer_3  | Linear   | 1.3 K  [repeated 3x across cluster]\n",
      "(RayTrainWorker pid=1157715) [W reducer.cpp:1298] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator()) [repeated 3x across cluster]\n",
      "(RayTrainWorker pid=1157715) 2023-03-29 19:43:01.515936: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64 [repeated 2x across cluster]\n",
      "(LightningTrainer pid=1158522) 2023-03-29 19:42:57,332\tINFO data_parallel_trainer.py:358 -- GPUs are detected in your Ray cluster, but GPU training is not enabled for this trainer. To enable GPU training, make sure to set `use_gpu` to True in your scaling config. [repeated 2x across cluster]\n",
      "2023-03-29 19:43:04,720\tINFO pbt.py:817 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial 6331d_00000 (score = 0.967076) into trial 6331d_00007 (score = 0.820995)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "MUTATION:  {'lr': <ray.tune.search.sample.Float object at 0x7f901753aed0>}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ORIGINAL:  {'layer_1_size': 128, 'layer_2_size': 128, 'lr': 0.005939557174932708}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-29 19:43:04,726\tINFO pbt.py:838 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of trial6331d_00007:\n",
      "lightning_config : \n",
      "    _module_init_config : \n",
      "        config : \n",
      "            lr : 0.005939557174932708 --- (* 1.2) --> 0.007127468609919249\n",
      "    _trainer_init_config : \n",
      "    _trainer_fit_params : \n",
      "    _ddp_strategy_config : \n",
      "    _model_checkpoint_config : \n",
      "\n",
      "(RayTrainWorker pid=1162952) 2023-03-29 19:43:07,797\tINFO data_parallel_trainer.py:358 -- GPUs are detected in your Ray cluster, but GPU training is not enabled for this trainer. To enable GPU training, make sure to set `use_gpu` to True in your scaling config.\n",
      "(TrainTrainable pid=1163350) 2023-03-29 19:43:07,797\tINFO data_parallel_trainer.py:358 -- GPUs are detected in your Ray cluster, but GPU training is not enabled for this trainer. To enable GPU training, make sure to set `use_gpu` to True in your scaling config.\n",
      "(LightningTrainer pid=1163350) 2023-03-29 19:43:07,830\tINFO trainable.py:914 -- Restored on 10.0.37.7 from checkpoint: /home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00000_0_layer_1_size=128,layer_2_size=128,lr=0.0059_2023-03-29_19-41-37/checkpoint_tmpace687\n",
      "(LightningTrainer pid=1163350) 2023-03-29 19:43:07,830\tINFO trainable.py:922 -- Current state after restoring: {'_iteration': 1, '_timesteps_total': None, '_time_total': 14.624611139297485, '_episodes_total': None}\n",
      "(LightningTrainer pid=1163350) 2023-03-29 19:43:07,834\tINFO data_parallel_trainer.py:358 -- GPUs are detected in your Ray cluster, but GPU training is not enabled for this trainer. To enable GPU training, make sure to set `use_gpu` to True in your scaling config.\n",
      "(RayTrainWorker pid=1162883) GPU available: False, used: False\n",
      "(RayTrainWorker pid=1162883) TPU available: False, using: 0 TPU cores\n",
      "(RayTrainWorker pid=1162883) IPU available: False, using: 0 IPUs\n",
      "(RayTrainWorker pid=1162883) HPU available: False, using: 0 HPUs\n",
      "(RayTrainWorker pid=1162883) Missing logger folder: /home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00008_8_layer_1_size=32,layer_2_size=128,lr=0.0004_2023-03-29_19-42-46/rank_0/lightning_logs\n",
      "(RayTrainWorker pid=1162883) \n",
      "(RayTrainWorker pid=1162883)   | Name     | Type     | Params\n",
      "(RayTrainWorker pid=1162883) --------------------------------------\n",
      "(RayTrainWorker pid=1162883) 0 | accuracy | Accuracy | 0     \n",
      "(RayTrainWorker pid=1162883) 1 | layer_1  | Linear   | 25.1 K\n",
      "(RayTrainWorker pid=1162883) 2 | layer_2  | Linear   | 4.2 K \n",
      "(RayTrainWorker pid=1162883) 3 | layer_3  | Linear   | 1.3 K \n",
      "(RayTrainWorker pid=1162883) --------------------------------------\n",
      "(RayTrainWorker pid=1162883) 30.6 K    Trainable params\n",
      "(RayTrainWorker pid=1162883) 0         Non-trainable params\n",
      "(RayTrainWorker pid=1162883) 30.6 K    Total params\n",
      "(RayTrainWorker pid=1162883) 0.123     Total estimated model params size (MB)\n",
      "(RayTrainWorker pid=1162885) [W reducer.cpp:1298] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "(RayTrainWorker pid=1162883) 2023-03-29 19:43:11.098526: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "(RayTrainWorker pid=1162883) To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "(RayTrainWorker pid=1162883) 2023-03-29 19:43:11.317659: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "(RayTrainWorker pid=1162883) 2023-03-29 19:43:12.276373: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "(RayTrainWorker pid=1162883) 2023-03-29 19:43:12.276448: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "(RayTrainWorker pid=1162883) 2023-03-29 19:43:12.276455: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "(RayTrainWorker pid=1167263) 2023-03-29 19:43:12,275\tINFO config.py:87 -- Setting up process group for: env:// [rank=0, world_size=3]\n",
      "(TrainTrainable pid=1166411) 2023-03-29 19:43:15,472\tINFO data_parallel_trainer.py:358 -- GPUs are detected in your Ray cluster, but GPU training is not enabled for this trainer. To enable GPU training, make sure to set `use_gpu` to True in your scaling config.\n",
      "(RayTrainWorker pid=1162952) 2023-03-29 19:43:15,472\tINFO data_parallel_trainer.py:358 -- GPUs are detected in your Ray cluster, but GPU training is not enabled for this trainer. To enable GPU training, make sure to set `use_gpu` to True in your scaling config.\n",
      "(RayTrainWorker pid=1162952) 2023-03-29 19:43:15,472\tINFO data_parallel_trainer.py:358 -- GPUs are detected in your Ray cluster, but GPU training is not enabled for this trainer. To enable GPU training, make sure to set `use_gpu` to True in your scaling config.\n",
      "(RayTrainWorker pid=1162952) 2023-03-29 19:43:15,472\tINFO data_parallel_trainer.py:358 -- GPUs are detected in your Ray cluster, but GPU training is not enabled for this trainer. To enable GPU training, make sure to set `use_gpu` to True in your scaling config.\n",
      "(RayTrainWorker pid=1162952) 2023-03-29 19:43:15,472\tINFO data_parallel_trainer.py:358 -- GPUs are detected in your Ray cluster, but GPU training is not enabled for this trainer. To enable GPU training, make sure to set `use_gpu` to True in your scaling config.\n",
      "(RayTrainWorker pid=1162952) Missing logger folder: /home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00009_9_layer_1_size=32,layer_2_size=256,lr=0.0034_2023-03-29_19-42-47/rank_0/lightning_logs [repeated 5x across cluster]\n",
      "(RayTrainWorker pid=1162952) 2023-03-29 19:43:15,472\tINFO data_parallel_trainer.py:358 -- GPUs are detected in your Ray cluster, but GPU training is not enabled for this trainer. To enable GPU training, make sure to set `use_gpu` to True in your scaling config.\n",
      "(RayTrainWorker pid=1162952) 2023-03-29 19:43:15,472\tINFO data_parallel_trainer.py:358 -- GPUs are detected in your Ray cluster, but GPU training is not enabled for this trainer. To enable GPU training, make sure to set `use_gpu` to True in your scaling config.\n",
      "(RayTrainWorker pid=1162952) -------------------------------------- [repeated 2x across cluster]\n",
      "(RayTrainWorker pid=1162952) 2023-03-29 19:43:15,472\tINFO data_parallel_trainer.py:358 -- GPUs are detected in your Ray cluster, but GPU training is not enabled for this trainer. To enable GPU training, make sure to set `use_gpu` to True in your scaling config.\n",
      "(RayTrainWorker pid=1162952) 3 | layer_3  | Linear   | 2.6 K  [repeated 3x across cluster]\n",
      "(RayTrainWorker pid=1162952) 2023-03-29 19:43:15,472\tINFO data_parallel_trainer.py:358 -- GPUs are detected in your Ray cluster, but GPU training is not enabled for this trainer. To enable GPU training, make sure to set `use_gpu` to True in your scaling config.\n",
      "(RayTrainWorker pid=1162952) 2023-03-29 19:43:15,472\tINFO data_parallel_trainer.py:358 -- GPUs are detected in your Ray cluster, but GPU training is not enabled for this trainer. To enable GPU training, make sure to set `use_gpu` to True in your scaling config.\n",
      "(RayTrainWorker pid=1162952) 2023-03-29 19:43:15,472\tINFO data_parallel_trainer.py:358 -- GPUs are detected in your Ray cluster, but GPU training is not enabled for this trainer. To enable GPU training, make sure to set `use_gpu` to True in your scaling config.\n",
      "(RayTrainWorker pid=1162952) 2023-03-29 19:43:15,472\tINFO data_parallel_trainer.py:358 -- GPUs are detected in your Ray cluster, but GPU training is not enabled for this trainer. To enable GPU training, make sure to set `use_gpu` to True in your scaling config.\n",
      "(RayTrainWorker pid=1162952) [W reducer.cpp:1298] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator()) [repeated 5x across cluster]\n",
      "(LightningTrainer pid=1166411) 2023-03-29 19:43:15,507\tINFO trainable.py:914 -- Restored on 10.0.37.7 from checkpoint: /home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00001_1_layer_1_size=32,layer_2_size=64,lr=0.0012_2023-03-29_19-41-44/checkpoint_tmpdbeb5a\n",
      "(LightningTrainer pid=1166411) 2023-03-29 19:43:15,507\tINFO trainable.py:922 -- Current state after restoring: {'_iteration': 1, '_timesteps_total': None, '_time_total': 15.85108494758606, '_episodes_total': None}\n",
      "(LightningTrainer pid=1166411) 2023-03-29 19:43:15,515\tINFO data_parallel_trainer.py:358 -- GPUs are detected in your Ray cluster, but GPU training is not enabled for this trainer. To enable GPU training, make sure to set `use_gpu` to True in your scaling config.\n",
      "(RayTrainWorker pid=1162952) Missing logger folder: /home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00000_0_layer_1_size=128,layer_2_size=128,lr=0.0059_2023-03-29_19-41-37/rank_1/lightning_logs\n",
      "(RayTrainWorker pid=1162952) Missing logger folder: /home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00000_0_layer_1_size=128,layer_2_size=128,lr=0.0059_2023-03-29_19-41-37/rank_1/lightning_logs\n",
      "(RayTrainWorker pid=1162952) Missing logger folder: /home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00000_0_layer_1_size=128,layer_2_size=128,lr=0.0059_2023-03-29_19-41-37/rank_1/lightning_logs\n",
      "(RayTrainWorker pid=1162952) 2023-03-29 19:43:12.473918: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64 [repeated 2x across cluster]\n",
      "(RayTrainWorker pid=1162952) Missing logger folder: /home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00000_0_layer_1_size=128,layer_2_size=128,lr=0.0059_2023-03-29_19-41-37/rank_1/lightning_logs\n",
      "(RayTrainWorker pid=1167263) GPU available: False, used: False\n",
      "(RayTrainWorker pid=1167263) TPU available: False, using: 0 TPU cores\n",
      "(RayTrainWorker pid=1167263) IPU available: False, using: 0 IPUs\n",
      "(RayTrainWorker pid=1167263) HPU available: False, using: 0 HPUs\n",
      "(RayTrainWorker pid=1167266) /home/ray/anaconda3/lib/python3.7/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:343: UserWarning: The dirpath has changed from '/home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00000_0_layer_1_size=128,layer_2_size=128,lr=0.0059_2023-03-29_19-41-37/rank_0/lightning_logs/version_0/checkpoints' to '/home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00000_0_layer_1_size=128,layer_2_size=128,lr=0.0059_2023-03-29_19-41-37/rank_0/lightning_logs/version_1/checkpoints', therefore `best_model_score`, `kth_best_model_path`, `kth_value`, `last_model_path` and `best_k_models` won't be reloaded. Only `best_model_path` will be reloaded.\n",
      "(RayTrainWorker pid=1167266)   f\"The dirpath has changed from {dirpath_from_ckpt!r} to {self.dirpath!r},\"\n",
      "(RayTrainWorker pid=1167263) Restoring states from the checkpoint path at /tmp/checkpoint_tmp_a05f6bfa36fd4cae8e8e4ef9028fcf33/model\n",
      "(RayTrainWorker pid=1167263) \n",
      "(RayTrainWorker pid=1167263)   | Name     | Type     | Params\n",
      "(RayTrainWorker pid=1167263) 0 | accuracy | Accuracy | 0     \n",
      "(RayTrainWorker pid=1167263) 118 K     Trainable params\n",
      "(RayTrainWorker pid=1167263) 0         Non-trainable params\n",
      "(RayTrainWorker pid=1167263) 118 K     Total params\n",
      "(RayTrainWorker pid=1167263) 0.473     Total estimated model params size (MB)\n",
      "(RayTrainWorker pid=1167263) Restored all states from the checkpoint file at /tmp/checkpoint_tmp_a05f6bfa36fd4cae8e8e4ef9028fcf33/model\n",
      "(RayTrainWorker pid=1171022) 2023-03-29 19:43:20,038\tINFO config.py:87 -- Setting up process group for: env:// [rank=0, world_size=3]\n",
      "(RayTrainWorker pid=1167263) 2023-03-29 19:43:20.197863: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "(RayTrainWorker pid=1167263) To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "(RayTrainWorker pid=1167263) 2023-03-29 19:43:20.345728: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "(RayTrainWorker pid=1167263) 2023-03-29 19:43:21.261482: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "(RayTrainWorker pid=1167268) Missing logger folder: /home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00000_0_layer_1_size=128,layer_2_size=128,lr=0.0059_2023-03-29_19-41-37/rank_2/lightning_logs [repeated 2x across cluster]\n",
      "(RayTrainWorker pid=1167263) -------------------------------------- [repeated 2x across cluster]\n",
      "(RayTrainWorker pid=1167263) 3 | layer_3  | Linear   | 1.3 K  [repeated 3x across cluster]\n",
      "(RayTrainWorker pid=1167263) [W reducer.cpp:1298] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator()) [repeated 3x across cluster]\n",
      "(RayTrainWorker pid=1167263) 2023-03-29 19:43:21.261452: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64 [repeated 2x across cluster]\n",
      "(RayTrainWorker pid=1171022) HPU available: False, using: 0 HPUs\n",
      "(RayTrainWorker pid=1171022) HPU available: False, using: 0 HPUs\n",
      "(RayTrainWorker pid=1171022) HPU available: False, using: 0 HPUs\n",
      "(RayTrainWorker pid=1171022) HPU available: False, using: 0 HPUs\n",
      "(RayTrainWorker pid=1167263) /home/ray/anaconda3/lib/python3.7/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:343: UserWarning: The dirpath has changed from '/home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00000_0_layer_1_size=128,layer_2_size=128,lr=0.0059_2023-03-29_19-41-37/rank_0/lightning_logs/version_0/checkpoints' to '/home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00000_0_layer_1_size=128,layer_2_size=128,lr=0.0059_2023-03-29_19-41-37/rank_0/lightning_logs/version_1/checkpoints', therefore `best_model_score`, `kth_best_model_path`, `kth_value`, `last_model_path` and `best_k_models` won't be reloaded. Only `best_model_path` will be reloaded. [repeated 2x across cluster]\n",
      "(RayTrainWorker pid=1167263)   f\"The dirpath has changed from {dirpath_from_ckpt!r} to {self.dirpath!r},\" [repeated 2x across cluster]\n",
      "(RayTrainWorker pid=1171023) Missing logger folder: /home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00001_1_layer_1_size=32,layer_2_size=64,lr=0.0012_2023-03-29_19-41-44/rank_1/lightning_logs\n",
      "(TrainTrainable pid=1173155) 2023-03-29 19:43:26,738\tINFO data_parallel_trainer.py:358 -- GPUs are detected in your Ray cluster, but GPU training is not enabled for this trainer. To enable GPU training, make sure to set `use_gpu` to True in your scaling config.\n",
      "(LightningTrainer pid=1173155) 2023-03-29 19:43:26,761\tINFO trainable.py:914 -- Restored on 10.0.37.7 from checkpoint: /home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00002_2_layer_1_size=128,layer_2_size=64,lr=0.0012_2023-03-29_19-41-51/checkpoint_tmpd78deb\n",
      "(LightningTrainer pid=1173155) 2023-03-29 19:43:26,761\tINFO trainable.py:922 -- Current state after restoring: {'_iteration': 1, '_timesteps_total': None, '_time_total': 17.38657021522522, '_episodes_total': None}\n",
      "(LightningTrainer pid=1173155) 2023-03-29 19:43:26,763\tINFO data_parallel_trainer.py:358 -- GPUs are detected in your Ray cluster, but GPU training is not enabled for this trainer. To enable GPU training, make sure to set `use_gpu` to True in your scaling config.\n",
      "(RayTrainWorker pid=1171022) Restoring states from the checkpoint path at /tmp/checkpoint_tmp_c5e465eb087c437e83b84dc606299952/model\n",
      "(RayTrainWorker pid=1171022) \n",
      "(RayTrainWorker pid=1171022)   | Name     | Type     | Params\n",
      "(RayTrainWorker pid=1171022) --------------------------------------\n",
      "(RayTrainWorker pid=1171022) 0 | accuracy | Accuracy | 0     \n",
      "(RayTrainWorker pid=1171022) 1 | layer_1  | Linear   | 25.1 K\n",
      "(RayTrainWorker pid=1171022) 2 | layer_2  | Linear   | 2.1 K \n",
      "(RayTrainWorker pid=1171022) 3 | layer_3  | Linear   | 650   \n",
      "(RayTrainWorker pid=1171022) --------------------------------------\n",
      "(RayTrainWorker pid=1171022) 27.9 K    Trainable params\n",
      "(RayTrainWorker pid=1171022) 0         Non-trainable params\n",
      "(RayTrainWorker pid=1171022) 27.9 K    Total params\n",
      "(RayTrainWorker pid=1171022) 0.112     Total estimated model params size (MB)\n",
      "(RayTrainWorker pid=1171022) Restored all states from the checkpoint file at /tmp/checkpoint_tmp_c5e465eb087c437e83b84dc606299952/model\n",
      "(RayTrainWorker pid=1171022) [W reducer.cpp:1298] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "(RayTrainWorker pid=1171022) 2023-03-29 19:43:27.963302: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "(RayTrainWorker pid=1171022) To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "(RayTrainWorker pid=1171022) 2023-03-29 19:43:28.247909: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "(RayTrainWorker pid=1171022) 2023-03-29 19:43:29.649278: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "(RayTrainWorker pid=1177227) 2023-03-29 19:43:31,294\tINFO config.py:87 -- Setting up process group for: env:// [rank=0, world_size=3]\n",
      "(RayTrainWorker pid=1171022) 2023-03-29 19:43:29.649270: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64 [repeated 2x across cluster]\n",
      "(RayTrainWorker pid=1171024) /home/ray/anaconda3/lib/python3.7/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:343: UserWarning: The dirpath has changed from '/home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00001_1_layer_1_size=32,layer_2_size=64,lr=0.0012_2023-03-29_19-41-44/rank_0/lightning_logs/version_0/checkpoints' to '/home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00001_1_layer_1_size=32,layer_2_size=64,lr=0.0012_2023-03-29_19-41-44/rank_0/lightning_logs/version_1/checkpoints', therefore `best_model_score`, `kth_best_model_path`, `kth_value`, `last_model_path` and `best_k_models` won't be reloaded. Only `best_model_path` will be reloaded. [repeated 3x across cluster]\n",
      "(RayTrainWorker pid=1171024)   f\"The dirpath has changed from {dirpath_from_ckpt!r} to {self.dirpath!r},\" [repeated 3x across cluster]\n",
      "(RayTrainWorker pid=1171024) 2023-03-29 19:43:32,605\tINFO config.py:87 -- Setting up process group for: env:// [rank=0, world_size=3]\n",
      "(LightningTrainer pid=1173651) 2023-03-29 19:43:28,031\tINFO data_parallel_trainer.py:358 -- GPUs are detected in your Ray cluster, but GPU training is not enabled for this trainer. To enable GPU training, make sure to set `use_gpu` to True in your scaling config. [repeated 2x across cluster]\n",
      "(LightningTrainer pid=1173651) 2023-03-29 19:43:32,605\tINFO config.py:87 -- Setting up process group for: env:// [rank=0, world_size=3]\n",
      "(LightningTrainer pid=1173651) 2023-03-29 19:43:32,605\tINFO config.py:87 -- Setting up process group for: env:// [rank=0, world_size=3]\n",
      "(RayTrainWorker pid=1171024) [W reducer.cpp:1298] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator()) [repeated 2x across cluster]\n",
      "(RayTrainWorker pid=1178610) 2023-03-29 19:43:36,563\tINFO data_parallel_trainer.py:358 -- GPUs are detected in your Ray cluster, but GPU training is not enabled for this trainer. To enable GPU training, make sure to set `use_gpu` to True in your scaling config.\n",
      "(LightningTrainer pid=1177090) 2023-03-29 19:43:36,593\tINFO trainable.py:914 -- Restored on 10.0.37.7 from checkpoint: /home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00004_4_layer_1_size=32,layer_2_size=64,lr=0.0010_2023-03-29_19-42-12/checkpoint_tmpbbb615\n",
      "(LightningTrainer pid=1177090) 2023-03-29 19:43:36,594\tINFO trainable.py:922 -- Current state after restoring: {'_iteration': 1, '_timesteps_total': None, '_time_total': 19.604857683181763, '_episodes_total': None}\n",
      "(RayTrainWorker pid=1178612) Missing logger folder: /home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00003_3_layer_1_size=32,layer_2_size=256,lr=0.0012_2023-03-29_19-42-00/rank_2/lightning_logs\n",
      "(LightningTrainer pid=1177090) 2023-03-29 19:43:36,597\tINFO data_parallel_trainer.py:358 -- GPUs are detected in your Ray cluster, but GPU training is not enabled for this trainer. To enable GPU training, make sure to set `use_gpu` to True in your scaling config. [repeated 2x across cluster]\n",
      "(RayTrainWorker pid=1178610) GPU available: False, used: False\n",
      "(RayTrainWorker pid=1178610) TPU available: False, using: 0 TPU cores\n",
      "(RayTrainWorker pid=1178610) IPU available: False, using: 0 IPUs\n",
      "(RayTrainWorker pid=1178610) HPU available: False, using: 0 HPUs\n",
      "(RayTrainWorker pid=1178612) /home/ray/anaconda3/lib/python3.7/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:343: UserWarning: The dirpath has changed from '/home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00003_3_layer_1_size=32,layer_2_size=256,lr=0.0012_2023-03-29_19-42-00/rank_0/lightning_logs/version_0/checkpoints' to '/home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00003_3_layer_1_size=32,layer_2_size=256,lr=0.0012_2023-03-29_19-42-00/rank_0/lightning_logs/version_1/checkpoints', therefore `best_model_score`, `kth_best_model_path`, `kth_value`, `last_model_path` and `best_k_models` won't be reloaded. Only `best_model_path` will be reloaded.\n",
      "(RayTrainWorker pid=1178612)   f\"The dirpath has changed from {dirpath_from_ckpt!r} to {self.dirpath!r},\"\n",
      "(RayTrainWorker pid=1178610) Restoring states from the checkpoint path at /tmp/checkpoint_tmp_151068e7913d455586f622d6e61733ba/model\n",
      "(RayTrainWorker pid=1178610) \n",
      "(RayTrainWorker pid=1178610)   | Name     | Type     | Params\n",
      "(RayTrainWorker pid=1178610) --------------------------------------\n",
      "(RayTrainWorker pid=1178610) 0 | accuracy | Accuracy | 0     \n",
      "(RayTrainWorker pid=1178610) 1 | layer_1  | Linear   | 25.1 K\n",
      "(RayTrainWorker pid=1178610) 2 | layer_2  | Linear   | 8.4 K \n",
      "(RayTrainWorker pid=1178610) 3 | layer_3  | Linear   | 2.6 K \n",
      "(RayTrainWorker pid=1178610) --------------------------------------\n",
      "(RayTrainWorker pid=1178610) 36.1 K    Trainable params\n",
      "(RayTrainWorker pid=1178610) 0         Non-trainable params\n",
      "(RayTrainWorker pid=1178610) 36.1 K    Total params\n",
      "(RayTrainWorker pid=1178610) 0.145     Total estimated model params size (MB)\n",
      "(RayTrainWorker pid=1178610) Restored all states from the checkpoint file at /tmp/checkpoint_tmp_151068e7913d455586f622d6e61733ba/model\n",
      "(RayTrainWorker pid=1177227) 3 | layer_3  | Linear   | 650   \n",
      "(RayTrainWorker pid=1178610) 2023-03-29 19:43:39.691503: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "(RayTrainWorker pid=1178610) To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "(RayTrainWorker pid=1178610) 2023-03-29 19:43:39.986194: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "(RayTrainWorker pid=1178610) 2023-03-29 19:43:40.983133: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "(RayTrainWorker pid=1178610) 2023-03-29 19:43:40.983207: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "(RayTrainWorker pid=1178610) 2023-03-29 19:43:40.983215: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "(RayTrainWorker pid=1180965) 2023-03-29 19:43:40,992\tINFO config.py:87 -- Setting up process group for: env:// [rank=0, world_size=3]\n",
      "(RayTrainWorker pid=1177227) [W reducer.cpp:1298] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator()) [repeated 6x across cluster]\n",
      "(RayTrainWorker pid=1177229) Missing logger folder: /home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00002_2_layer_1_size=128,layer_2_size=64,lr=0.0012_2023-03-29_19-41-51/rank_2/lightning_logs [repeated 3x across cluster]\n",
      "(TrainTrainable pid=1180738) 2023-03-29 19:43:44,606\tINFO data_parallel_trainer.py:358 -- GPUs are detected in your Ray cluster, but GPU training is not enabled for this trainer. To enable GPU training, make sure to set `use_gpu` to True in your scaling config.\n",
      "(RayTrainWorker pid=1177227) 2023-03-29 19:43:44,606\tINFO data_parallel_trainer.py:358 -- GPUs are detected in your Ray cluster, but GPU training is not enabled for this trainer. To enable GPU training, make sure to set `use_gpu` to True in your scaling config.\n",
      "(RayTrainWorker pid=1177227) 2023-03-29 19:43:44,606\tINFO data_parallel_trainer.py:358 -- GPUs are detected in your Ray cluster, but GPU training is not enabled for this trainer. To enable GPU training, make sure to set `use_gpu` to True in your scaling config.\n",
      "(RayTrainWorker pid=1177227) 2023-03-29 19:43:44,606\tINFO data_parallel_trainer.py:358 -- GPUs are detected in your Ray cluster, but GPU training is not enabled for this trainer. To enable GPU training, make sure to set `use_gpu` to True in your scaling config.\n",
      "(RayTrainWorker pid=1177227) 2023-03-29 19:43:44,606\tINFO data_parallel_trainer.py:358 -- GPUs are detected in your Ray cluster, but GPU training is not enabled for this trainer. To enable GPU training, make sure to set `use_gpu` to True in your scaling config.\n",
      "(RayTrainWorker pid=1177227) /home/ray/anaconda3/lib/python3.7/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:343: UserWarning: The dirpath has changed from '/home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00002_2_layer_1_size=128,layer_2_size=64,lr=0.0012_2023-03-29_19-41-51/rank_0/lightning_logs/version_0/checkpoints' to '/home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00002_2_layer_1_size=128,layer_2_size=64,lr=0.0012_2023-03-29_19-41-51/rank_0/lightning_logs/version_1/checkpoints', therefore `best_model_score`, `kth_best_model_path`, `kth_value`, `last_model_path` and `best_k_models` won't be reloaded. Only `best_model_path` will be reloaded. [repeated 5x across cluster]\n",
      "(RayTrainWorker pid=1177227)   f\"The dirpath has changed from {dirpath_from_ckpt!r} to {self.dirpath!r},\" [repeated 5x across cluster]\n",
      "(RayTrainWorker pid=1177227) 2023-03-29 19:43:44,606\tINFO data_parallel_trainer.py:358 -- GPUs are detected in your Ray cluster, but GPU training is not enabled for this trainer. To enable GPU training, make sure to set `use_gpu` to True in your scaling config.\n",
      "(RayTrainWorker pid=1177227) 2023-03-29 19:43:44,606\tINFO data_parallel_trainer.py:358 -- GPUs are detected in your Ray cluster, but GPU training is not enabled for this trainer. To enable GPU training, make sure to set `use_gpu` to True in your scaling config.\n",
      "(RayTrainWorker pid=1177227) 2023-03-29 19:43:44,606\tINFO data_parallel_trainer.py:358 -- GPUs are detected in your Ray cluster, but GPU training is not enabled for this trainer. To enable GPU training, make sure to set `use_gpu` to True in your scaling config.\n",
      "(RayTrainWorker pid=1177227) -------------------------------------- [repeated 2x across cluster]\n",
      "(RayTrainWorker pid=1177227) 2023-03-29 19:43:44,606\tINFO data_parallel_trainer.py:358 -- GPUs are detected in your Ray cluster, but GPU training is not enabled for this trainer. To enable GPU training, make sure to set `use_gpu` to True in your scaling config.\n",
      "(RayTrainWorker pid=1177227) 2 | layer_2  | Linear   | 8.3 K  [repeated 2x across cluster]\n",
      "(RayTrainWorker pid=1177227) 2023-03-29 19:43:44,606\tINFO data_parallel_trainer.py:358 -- GPUs are detected in your Ray cluster, but GPU training is not enabled for this trainer. To enable GPU training, make sure to set `use_gpu` to True in your scaling config.\n",
      "(RayTrainWorker pid=1177227) 2023-03-29 19:43:44,606\tINFO data_parallel_trainer.py:358 -- GPUs are detected in your Ray cluster, but GPU training is not enabled for this trainer. To enable GPU training, make sure to set `use_gpu` to True in your scaling config.\n",
      "(RayTrainWorker pid=1177227) 2023-03-29 19:43:44,606\tINFO data_parallel_trainer.py:358 -- GPUs are detected in your Ray cluster, but GPU training is not enabled for this trainer. To enable GPU training, make sure to set `use_gpu` to True in your scaling config.\n",
      "(RayTrainWorker pid=1177227) 2023-03-29 19:43:44,606\tINFO data_parallel_trainer.py:358 -- GPUs are detected in your Ray cluster, but GPU training is not enabled for this trainer. To enable GPU training, make sure to set `use_gpu` to True in your scaling config.\n",
      "(RayTrainWorker pid=1177227) 2023-03-29 19:43:44,606\tINFO data_parallel_trainer.py:358 -- GPUs are detected in your Ray cluster, but GPU training is not enabled for this trainer. To enable GPU training, make sure to set `use_gpu` to True in your scaling config.\n",
      "(LightningTrainer pid=1180738) 2023-03-29 19:43:44,640\tINFO trainable.py:914 -- Restored on 10.0.37.7 from checkpoint: /home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00005_5_layer_1_size=64,layer_2_size=128,lr=0.0746_2023-03-29_19-42-12/checkpoint_tmpd38ba0\n",
      "(LightningTrainer pid=1180738) 2023-03-29 19:43:44,640\tINFO trainable.py:922 -- Current state after restoring: {'_iteration': 1, '_timesteps_total': None, '_time_total': 14.624611139297485, '_episodes_total': None}\n",
      "(LightningTrainer pid=1180738) 2023-03-29 19:43:44,669\tINFO data_parallel_trainer.py:358 -- GPUs are detected in your Ray cluster, but GPU training is not enabled for this trainer. To enable GPU training, make sure to set `use_gpu` to True in your scaling config.\n",
      "(RayTrainWorker pid=1177227) 2023-03-29 19:43:44,669\tINFO data_parallel_trainer.py:358 -- GPUs are detected in your Ray cluster, but GPU training is not enabled for this trainer. To enable GPU training, make sure to set `use_gpu` to True in your scaling config.\n",
      "(RayTrainWorker pid=1177227) 2023-03-29 19:43:44,669\tINFO data_parallel_trainer.py:358 -- GPUs are detected in your Ray cluster, but GPU training is not enabled for this trainer. To enable GPU training, make sure to set `use_gpu` to True in your scaling config.\n",
      "(RayTrainWorker pid=1177227) 2023-03-29 19:43:49,585\tINFO config.py:87 -- Setting up process group for: env:// [rank=0, world_size=3]\n",
      "(RayTrainWorker pid=1177227) 2023-03-29 19:43:41.262891: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64 [repeated 2x across cluster]\n",
      "(RayTrainWorker pid=1177227) 2023-03-29 19:43:49,585\tINFO config.py:87 -- Setting up process group for: env:// [rank=0, world_size=3]\n",
      "(RayTrainWorker pid=1184808) 2023-03-29 19:43:49,585\tINFO config.py:87 -- Setting up process group for: env:// [rank=0, world_size=3]\n",
      "(RayTrainWorker pid=1180965) GPU available: False, used: False\n",
      "(RayTrainWorker pid=1180965) TPU available: False, using: 0 TPU cores\n",
      "(RayTrainWorker pid=1180965) IPU available: False, using: 0 IPUs\n",
      "(RayTrainWorker pid=1180965) HPU available: False, using: 0 HPUs\n",
      "(RayTrainWorker pid=1180966) Missing logger folder: /home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00004_4_layer_1_size=32,layer_2_size=64,lr=0.0010_2023-03-29_19-42-12/rank_1/lightning_logs\n",
      "(RayTrainWorker pid=1180965) Restoring states from the checkpoint path at /tmp/checkpoint_tmp_7ee1081a578c4e3fb929cf28e9839a9e/model\n",
      "(RayTrainWorker pid=1180965) /home/ray/anaconda3/lib/python3.7/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:343: UserWarning: The dirpath has changed from '/home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00004_4_layer_1_size=32,layer_2_size=64,lr=0.0010_2023-03-29_19-42-12/rank_0/lightning_logs/version_0/checkpoints' to '/home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00004_4_layer_1_size=32,layer_2_size=64,lr=0.0010_2023-03-29_19-42-12/rank_0/lightning_logs/version_1/checkpoints', therefore `best_model_score`, `kth_best_model_path`, `kth_value`, `last_model_path` and `best_k_models` won't be reloaded. Only `best_model_path` will be reloaded.\n",
      "(RayTrainWorker pid=1180965)   f\"The dirpath has changed from {dirpath_from_ckpt!r} to {self.dirpath!r},\"\n",
      "(RayTrainWorker pid=1180965) \n",
      "(RayTrainWorker pid=1180965)   | Name     | Type     | Params\n",
      "(RayTrainWorker pid=1180965) --------------------------------------\n",
      "(RayTrainWorker pid=1180965) 0 | accuracy | Accuracy | 0     \n",
      "(RayTrainWorker pid=1180965) 1 | layer_1  | Linear   | 25.1 K\n",
      "(RayTrainWorker pid=1180965) 2 | layer_2  | Linear   | 2.1 K \n",
      "(RayTrainWorker pid=1180965) 3 | layer_3  | Linear   | 650   \n",
      "(RayTrainWorker pid=1180965) --------------------------------------\n",
      "(RayTrainWorker pid=1180965) 27.9 K    Trainable params\n",
      "(RayTrainWorker pid=1180965) 0         Non-trainable params\n",
      "(RayTrainWorker pid=1180965) 27.9 K    Total params\n",
      "(RayTrainWorker pid=1180965) 0.112     Total estimated model params size (MB)\n",
      "(RayTrainWorker pid=1180965) Restored all states from the checkpoint file at /tmp/checkpoint_tmp_7ee1081a578c4e3fb929cf28e9839a9e/model\n",
      "(RayTrainWorker pid=1180965) [W reducer.cpp:1298] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "(RayTrainWorker pid=1180965) 2023-03-29 19:43:50.933119: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "(RayTrainWorker pid=1180965) To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "(RayTrainWorker pid=1180965) 2023-03-29 19:43:51.098834: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "(RayTrainWorker pid=1180965) 2023-03-29 19:43:52.031082: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "(RayTrainWorker pid=1180965) 2023-03-29 19:43:52.031074: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64 [repeated 2x across cluster]\n",
      "(RayTrainWorker pid=1184810) Missing logger folder: /home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00005_5_layer_1_size=64,layer_2_size=128,lr=0.0746_2023-03-29_19-42-12/rank_2/lightning_logs [repeated 2x across cluster]\n",
      "(RayTrainWorker pid=1180967) /home/ray/anaconda3/lib/python3.7/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:343: UserWarning: The dirpath has changed from '/home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00004_4_layer_1_size=32,layer_2_size=64,lr=0.0010_2023-03-29_19-42-12/rank_0/lightning_logs/version_0/checkpoints' to '/home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00004_4_layer_1_size=32,layer_2_size=64,lr=0.0010_2023-03-29_19-42-12/rank_0/lightning_logs/version_1/checkpoints', therefore `best_model_score`, `kth_best_model_path`, `kth_value`, `last_model_path` and `best_k_models` won't be reloaded. Only `best_model_path` will be reloaded. [repeated 2x across cluster]\n",
      "(RayTrainWorker pid=1180967)   f\"The dirpath has changed from {dirpath_from_ckpt!r} to {self.dirpath!r},\" [repeated 2x across cluster]\n",
      "(RayTrainWorker pid=1180966) [W reducer.cpp:1298] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator()) [repeated 2x across cluster]\n",
      "(RayTrainWorker pid=1184808) GPU available: False, used: False\n",
      "(RayTrainWorker pid=1184808) TPU available: False, using: 0 TPU cores\n",
      "(RayTrainWorker pid=1184808) IPU available: False, using: 0 IPUs\n",
      "(RayTrainWorker pid=1184808) HPU available: False, using: 0 HPUs\n",
      "(RayTrainWorker pid=1184808) Restoring states from the checkpoint path at /tmp/checkpoint_tmp_7fbed08e777e49fcade4d2cf00c6c4f5/model\n",
      "(RayTrainWorker pid=1184808) \n",
      "(RayTrainWorker pid=1184808)   | Name     | Type     | Params\n",
      "(RayTrainWorker pid=1184808) --------------------------------------\n",
      "(RayTrainWorker pid=1184808) 0 | accuracy | Accuracy | 0     \n",
      "(RayTrainWorker pid=1184808) 1 | layer_1  | Linear   | 100 K \n",
      "(RayTrainWorker pid=1184808) 2 | layer_2  | Linear   | 16.5 K\n",
      "(RayTrainWorker pid=1184808) 3 | layer_3  | Linear   | 1.3 K \n",
      "(RayTrainWorker pid=1184808) --------------------------------------\n",
      "(RayTrainWorker pid=1184808) 118 K     Trainable params\n",
      "(RayTrainWorker pid=1184808) 0         Non-trainable params\n",
      "(RayTrainWorker pid=1184808) 118 K     Total params\n",
      "(RayTrainWorker pid=1184808) 0.473     Total estimated model params size (MB)\n",
      "(RayTrainWorker pid=1184808) Restored all states from the checkpoint file at /tmp/checkpoint_tmp_7fbed08e777e49fcade4d2cf00c6c4f5/model\n",
      "(RayTrainWorker pid=1184808) 2023-03-29 19:43:57.272506: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "(RayTrainWorker pid=1184808) To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "(RayTrainWorker pid=1184808) 2023-03-29 19:43:57.423204: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "(RayTrainWorker pid=1184808) 2023-03-29 19:43:58.352966: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "(TrainTrainable pid=1188137) 2023-03-29 19:43:58,574\tINFO data_parallel_trainer.py:358 -- GPUs are detected in your Ray cluster, but GPU training is not enabled for this trainer. To enable GPU training, make sure to set `use_gpu` to True in your scaling config.\n",
      "(LightningTrainer pid=1188137) 2023-03-29 19:43:58,594\tINFO trainable.py:914 -- Restored on 10.0.37.7 from checkpoint: /home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00006_6_layer_1_size=64,layer_2_size=128,lr=0.0003_2023-03-29_19-42-23/checkpoint_tmp9c64d9\n",
      "(LightningTrainer pid=1188137) 2023-03-29 19:43:58,595\tINFO trainable.py:922 -- Current state after restoring: {'_iteration': 1, '_timesteps_total': None, '_time_total': 20.06472682952881, '_episodes_total': None}\n",
      "(LightningTrainer pid=1188137) 2023-03-29 19:43:58,597\tINFO data_parallel_trainer.py:358 -- GPUs are detected in your Ray cluster, but GPU training is not enabled for this trainer. To enable GPU training, make sure to set `use_gpu` to True in your scaling config.\n",
      "(RayTrainWorker pid=1193170) 2023-03-29 19:44:03,770\tINFO config.py:87 -- Setting up process group for: env:// [rank=0, world_size=3]\n",
      "(RayTrainWorker pid=1184808) 2023-03-29 19:43:58.352958: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64 [repeated 2x across cluster]\n",
      "(RayTrainWorker pid=1184809) 2023-03-29 19:44:03,770\tINFO config.py:87 -- Setting up process group for: env:// [rank=0, world_size=3]\n",
      "(RayTrainWorker pid=1184808) /home/ray/anaconda3/lib/python3.7/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:343: UserWarning: The dirpath has changed from '/home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00000_0_layer_1_size=128,layer_2_size=128,lr=0.0059_2023-03-29_19-41-37/rank_0/lightning_logs/version_0/checkpoints' to '/home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00005_5_layer_1_size=64,layer_2_size=128,lr=0.0746_2023-03-29_19-42-12/rank_0/lightning_logs/version_1/checkpoints', therefore `best_model_score`, `kth_best_model_path`, `kth_value`, `last_model_path` and `best_k_models` won't be reloaded. Only `best_model_path` will be reloaded. [repeated 3x across cluster]\n",
      "(RayTrainWorker pid=1184808)   f\"The dirpath has changed from {dirpath_from_ckpt!r} to {self.dirpath!r},\" [repeated 3x across cluster]\n",
      "(RayTrainWorker pid=1184808) [W reducer.cpp:1298] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator()) [repeated 3x across cluster]\n",
      "(LightningTrainer pid=1188858) 2023-03-29 19:43:59,645\tINFO data_parallel_trainer.py:358 -- GPUs are detected in your Ray cluster, but GPU training is not enabled for this trainer. To enable GPU training, make sure to set `use_gpu` to True in your scaling config. [repeated 2x across cluster]\n",
      "(LightningTrainer pid=1188858) 2023-03-29 19:44:03,770\tINFO config.py:87 -- Setting up process group for: env:// [rank=0, world_size=3]\n",
      "(LightningTrainer pid=1188858) 2023-03-29 19:44:03,770\tINFO config.py:87 -- Setting up process group for: env:// [rank=0, world_size=3]\n",
      "(LightningTrainer pid=1191545) 2023-03-29 19:44:06,217\tINFO trainable.py:914 -- Restored on 10.0.37.7 from checkpoint: /home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00008_8_layer_1_size=32,layer_2_size=128,lr=0.0004_2023-03-29_19-42-46/checkpoint_tmp42aac7\n",
      "(LightningTrainer pid=1191545) 2023-03-29 19:44:06,217\tINFO trainable.py:922 -- Current state after restoring: {'_iteration': 1, '_timesteps_total': None, '_time_total': 19.68397569656372, '_episodes_total': None}\n",
      "(RayTrainWorker pid=1193170) GPU available: False, used: False\n",
      "(RayTrainWorker pid=1193170) TPU available: False, using: 0 TPU cores\n",
      "(RayTrainWorker pid=1193170) IPU available: False, using: 0 IPUs\n",
      "(RayTrainWorker pid=1193170) HPU available: False, using: 0 HPUs\n",
      "(RayTrainWorker pid=1193244) HPU available: False, using: 0 HPUs\n",
      "(LightningTrainer pid=1191545) 2023-03-29 19:44:06,220\tINFO data_parallel_trainer.py:358 -- GPUs are detected in your Ray cluster, but GPU training is not enabled for this trainer. To enable GPU training, make sure to set `use_gpu` to True in your scaling config. [repeated 2x across cluster]\n",
      "(RayTrainWorker pid=1193171) Missing logger folder: /home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00006_6_layer_1_size=64,layer_2_size=128,lr=0.0003_2023-03-29_19-42-23/rank_1/lightning_logs\n",
      "(RayTrainWorker pid=1193170) Restoring states from the checkpoint path at /tmp/checkpoint_tmp_7cef3fb97dc54a2b97c1ad1d6d5547dd/model\n",
      "(RayTrainWorker pid=1193170) /home/ray/anaconda3/lib/python3.7/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:343: UserWarning: The dirpath has changed from '/home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00006_6_layer_1_size=64,layer_2_size=128,lr=0.0003_2023-03-29_19-42-23/rank_0/lightning_logs/version_0/checkpoints' to '/home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00006_6_layer_1_size=64,layer_2_size=128,lr=0.0003_2023-03-29_19-42-23/rank_0/lightning_logs/version_1/checkpoints', therefore `best_model_score`, `kth_best_model_path`, `kth_value`, `last_model_path` and `best_k_models` won't be reloaded. Only `best_model_path` will be reloaded.\n",
      "(RayTrainWorker pid=1193170)   f\"The dirpath has changed from {dirpath_from_ckpt!r} to {self.dirpath!r},\"\n",
      "(RayTrainWorker pid=1193170) \n",
      "(RayTrainWorker pid=1193170)   | Name     | Type     | Params\n",
      "(RayTrainWorker pid=1193170) --------------------------------------\n",
      "(RayTrainWorker pid=1193170) 0 | accuracy | Accuracy | 0     \n",
      "(RayTrainWorker pid=1193170) 1 | layer_1  | Linear   | 50.2 K\n",
      "(RayTrainWorker pid=1193170) 2 | layer_2  | Linear   | 8.3 K \n",
      "(RayTrainWorker pid=1193170) 3 | layer_3  | Linear   | 1.3 K \n",
      "(RayTrainWorker pid=1193170) --------------------------------------\n",
      "(RayTrainWorker pid=1193170) 59.9 K    Trainable params\n",
      "(RayTrainWorker pid=1193170) 0         Non-trainable params\n",
      "(RayTrainWorker pid=1193170) 59.9 K    Total params\n",
      "(RayTrainWorker pid=1193170) 0.239     Total estimated model params size (MB)\n",
      "(RayTrainWorker pid=1193170) Restored all states from the checkpoint file at /tmp/checkpoint_tmp_7cef3fb97dc54a2b97c1ad1d6d5547dd/model\n",
      "(RayTrainWorker pid=1193171) [W reducer.cpp:1298] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "(RayTrainWorker pid=1193170) 2023-03-29 19:44:10.418325: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "(RayTrainWorker pid=1193170) To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "(RayTrainWorker pid=1193170) 2023-03-29 19:44:10.582494: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "(RayTrainWorker pid=1195537) 2023-03-29 19:44:10,637\tINFO config.py:87 -- Setting up process group for: env:// [rank=0, world_size=3]\n",
      "(RayTrainWorker pid=1193170) 2023-03-29 19:44:11.481729: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "(RayTrainWorker pid=1193170) 2023-03-29 19:44:11.481811: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "(RayTrainWorker pid=1193170) 2023-03-29 19:44:11.481819: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "(LightningTrainer pid=1195212) 2023-03-29 19:44:13,877\tINFO trainable.py:914 -- Restored on 10.0.37.7 from checkpoint: /home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00009_9_layer_1_size=32,layer_2_size=256,lr=0.0034_2023-03-29_19-42-47/checkpoint_tmpcdaae4\n",
      "(LightningTrainer pid=1195212) 2023-03-29 19:44:13,877\tINFO trainable.py:922 -- Current state after restoring: {'_iteration': 1, '_timesteps_total': None, '_time_total': 19.59176254272461, '_episodes_total': None}\n",
      "2023-03-29 19:44:16,362\tINFO pbt.py:817 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial 6331d_00000 (score = 0.966146) into trial 6331d_00006 (score = 0.910776)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "MUTATION:  {'lr': <ray.tune.search.sample.Float object at 0x7f901753aed0>}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ORIGINAL:  {'layer_1_size': 128, 'layer_2_size': 128, 'lr': 0.005939557174932708}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-29 19:44:16,372\tINFO pbt.py:838 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of trial6331d_00006:\n",
      "lightning_config : \n",
      "    _module_init_config : \n",
      "        config : \n",
      "            lr : 0.005939557174932708 --- (* 0.8) --> 0.0047516457399461665\n",
      "    _trainer_init_config : \n",
      "    _trainer_fit_params : \n",
      "    _ddp_strategy_config : \n",
      "    _model_checkpoint_config : \n",
      "\n",
      "(RayTrainWorker pid=1193244) Missing logger folder: /home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00008_8_layer_1_size=32,layer_2_size=128,lr=0.0004_2023-03-29_19-42-46/rank_2/lightning_logs\n",
      "(RayTrainWorker pid=1193244) Missing logger folder: /home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00008_8_layer_1_size=32,layer_2_size=128,lr=0.0004_2023-03-29_19-42-46/rank_2/lightning_logs\n",
      "(RayTrainWorker pid=1193244) Missing logger folder: /home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00008_8_layer_1_size=32,layer_2_size=128,lr=0.0004_2023-03-29_19-42-46/rank_2/lightning_logs\n",
      "(RayTrainWorker pid=1193244) Missing logger folder: /home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00008_8_layer_1_size=32,layer_2_size=128,lr=0.0004_2023-03-29_19-42-46/rank_2/lightning_logs\n",
      "(LightningTrainer pid=1195212) 2023-03-29 19:44:13,883\tINFO data_parallel_trainer.py:358 -- GPUs are detected in your Ray cluster, but GPU training is not enabled for this trainer. To enable GPU training, make sure to set `use_gpu` to True in your scaling config. [repeated 2x across cluster]\n",
      "(RayTrainWorker pid=1195539) Missing logger folder: /home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00008_8_layer_1_size=32,layer_2_size=128,lr=0.0004_2023-03-29_19-42-46/rank_2/lightning_logs [repeated 4x across cluster]\n",
      "(RayTrainWorker pid=1193244) Missing logger folder: /home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00008_8_layer_1_size=32,layer_2_size=128,lr=0.0004_2023-03-29_19-42-46/rank_2/lightning_logs\n",
      "(RayTrainWorker pid=1193246) /home/ray/anaconda3/lib/python3.7/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:343: UserWarning: The dirpath has changed from '/home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00000_0_layer_1_size=128,layer_2_size=128,lr=0.0059_2023-03-29_19-41-37/rank_0/lightning_logs/version_0/checkpoints' to '/home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00007_7_layer_1_size=32,layer_2_size=128,lr=0.0001_2023-03-29_19-42-34/rank_0/lightning_logs/version_1/checkpoints', therefore `best_model_score`, `kth_best_model_path`, `kth_value`, `last_model_path` and `best_k_models` won't be reloaded. Only `best_model_path` will be reloaded. [repeated 5x across cluster]\n",
      "(RayTrainWorker pid=1193246)   f\"The dirpath has changed from {dirpath_from_ckpt!r} to {self.dirpath!r},\" [repeated 5x across cluster]\n",
      "(RayTrainWorker pid=1193244) Missing logger folder: /home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00008_8_layer_1_size=32,layer_2_size=128,lr=0.0004_2023-03-29_19-42-46/rank_2/lightning_logs\n",
      "(RayTrainWorker pid=1193244) Missing logger folder: /home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00008_8_layer_1_size=32,layer_2_size=128,lr=0.0004_2023-03-29_19-42-46/rank_2/lightning_logs\n",
      "(RayTrainWorker pid=1193244) -------------------------------------- [repeated 2x across cluster]\n",
      "(RayTrainWorker pid=1193244) Missing logger folder: /home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00008_8_layer_1_size=32,layer_2_size=128,lr=0.0004_2023-03-29_19-42-46/rank_2/lightning_logs\n",
      "(RayTrainWorker pid=1193244) 3 | layer_3  | Linear   | 1.3 K  [repeated 3x across cluster]\n",
      "(RayTrainWorker pid=1193244) Missing logger folder: /home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00008_8_layer_1_size=32,layer_2_size=128,lr=0.0004_2023-03-29_19-42-46/rank_2/lightning_logs\n",
      "(RayTrainWorker pid=1193244) Missing logger folder: /home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00008_8_layer_1_size=32,layer_2_size=128,lr=0.0004_2023-03-29_19-42-46/rank_2/lightning_logs\n",
      "(RayTrainWorker pid=1193244) Missing logger folder: /home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00008_8_layer_1_size=32,layer_2_size=128,lr=0.0004_2023-03-29_19-42-46/rank_2/lightning_logs\n",
      "(RayTrainWorker pid=1193244) Missing logger folder: /home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00008_8_layer_1_size=32,layer_2_size=128,lr=0.0004_2023-03-29_19-42-46/rank_2/lightning_logs\n",
      "(RayTrainWorker pid=1193244) Missing logger folder: /home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00008_8_layer_1_size=32,layer_2_size=128,lr=0.0004_2023-03-29_19-42-46/rank_2/lightning_logs\n",
      "(RayTrainWorker pid=1193244) [W reducer.cpp:1298] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator()) [repeated 5x across cluster]\n",
      "(RayTrainWorker pid=1193244) Missing logger folder: /home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00008_8_layer_1_size=32,layer_2_size=128,lr=0.0004_2023-03-29_19-42-46/rank_2/lightning_logs\n",
      "(RayTrainWorker pid=1193244) Missing logger folder: /home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00008_8_layer_1_size=32,layer_2_size=128,lr=0.0004_2023-03-29_19-42-46/rank_2/lightning_logs\n",
      "(RayTrainWorker pid=1193244) Missing logger folder: /home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00008_8_layer_1_size=32,layer_2_size=128,lr=0.0004_2023-03-29_19-42-46/rank_2/lightning_logs\n",
      "(RayTrainWorker pid=1193244) 2023-03-29 19:44:13.712005: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64 [repeated 2x across cluster]\n",
      "(RayTrainWorker pid=1193244) Missing logger folder: /home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00008_8_layer_1_size=32,layer_2_size=128,lr=0.0004_2023-03-29_19-42-46/rank_2/lightning_logs\n",
      "(RayTrainWorker pid=1195537) GPU available: False, used: False\n",
      "(RayTrainWorker pid=1195537) TPU available: False, using: 0 TPU cores\n",
      "(RayTrainWorker pid=1195537) IPU available: False, using: 0 IPUs\n",
      "(RayTrainWorker pid=1195537) HPU available: False, using: 0 HPUs\n",
      "(RayTrainWorker pid=1199367) 2023-03-29 19:44:19,273\tINFO config.py:87 -- Setting up process group for: env:// [rank=0, world_size=3]\n",
      "(RayTrainWorker pid=1195537) Restoring states from the checkpoint path at /tmp/checkpoint_tmp_0f740499c29b4ca886122311fe0ad312/model\n",
      "(RayTrainWorker pid=1195537) \n",
      "(RayTrainWorker pid=1195537)   | Name     | Type     | Params\n",
      "(RayTrainWorker pid=1195537) 0 | accuracy | Accuracy | 0     \n",
      "(RayTrainWorker pid=1195537) 30.6 K    Trainable params\n",
      "(RayTrainWorker pid=1195537) 0         Non-trainable params\n",
      "(RayTrainWorker pid=1195537) 30.6 K    Total params\n",
      "(RayTrainWorker pid=1195537) 0.123     Total estimated model params size (MB)\n",
      "(RayTrainWorker pid=1195537) Restored all states from the checkpoint file at /tmp/checkpoint_tmp_0f740499c29b4ca886122311fe0ad312/model\n",
      "(RayTrainWorker pid=1195537) 2023-03-29 19:44:20.722621: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "(RayTrainWorker pid=1195537) To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "(RayTrainWorker pid=1195537) 2023-03-29 19:44:21.024298: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "(RayTrainWorker pid=1195537) 2023-03-29 19:44:22.068156: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "(RayTrainWorker pid=1199368) Missing logger folder: /home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00009_9_layer_1_size=32,layer_2_size=256,lr=0.0034_2023-03-29_19-42-47/rank_1/lightning_logs [repeated 2x across cluster]\n",
      "(RayTrainWorker pid=1195537) /home/ray/anaconda3/lib/python3.7/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:343: UserWarning: The dirpath has changed from '/home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00008_8_layer_1_size=32,layer_2_size=128,lr=0.0004_2023-03-29_19-42-46/rank_0/lightning_logs/version_0/checkpoints' to '/home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00008_8_layer_1_size=32,layer_2_size=128,lr=0.0004_2023-03-29_19-42-46/rank_0/lightning_logs/version_1/checkpoints', therefore `best_model_score`, `kth_best_model_path`, `kth_value`, `last_model_path` and `best_k_models` won't be reloaded. Only `best_model_path` will be reloaded. [repeated 3x across cluster]\n",
      "(RayTrainWorker pid=1195537)   f\"The dirpath has changed from {dirpath_from_ckpt!r} to {self.dirpath!r},\" [repeated 3x across cluster]\n",
      "(RayTrainWorker pid=1195537) -------------------------------------- [repeated 2x across cluster]\n",
      "(RayTrainWorker pid=1195537) 3 | layer_3  | Linear   | 1.3 K  [repeated 3x across cluster]\n",
      "2023-03-29 19:44:26,074\tINFO pbt.py:817 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial 6331d_00000 (score = 0.966146) into trial 6331d_00008 (score = 0.915427)\n",
      "\n",
      "(RayTrainWorker pid=1195537)"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "MUTATION: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " [W reducer.cpp:1298] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " [repeated 3x across cluster]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lr': <ray.tune.search.sample.Float object at 0x7f901753aed0>}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=1195537)"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "ORIGINAL: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 2023-03-29 19:44:22.068149: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " [repeated 2x across cluster]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'layer_1_size': 128, 'layer_2_size': 128, 'lr': 0.005939557174932708}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-29 19:44:26,083\tINFO pbt.py:838 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of trial6331d_00008:\n",
      "lightning_config : \n",
      "    _module_init_config : \n",
      "        config : \n",
      "            lr : 0.005939557174932708 --- (resample) --> 0.0018875272358204525\n",
      "    _trainer_init_config : \n",
      "    _trainer_fit_params : \n",
      "    _ddp_strategy_config : \n",
      "    _model_checkpoint_config : \n",
      "\n",
      "(RayTrainWorker pid=1199367) GPU available: False, used: False\n",
      "(RayTrainWorker pid=1199367) TPU available: False, using: 0 TPU cores\n",
      "(RayTrainWorker pid=1199367) IPU available: False, using: 0 IPUs\n",
      "(RayTrainWorker pid=1199367) HPU available: False, using: 0 HPUs\n",
      "(RayTrainWorker pid=1195538) Traceback (most recent call last):\n",
      "(RayTrainWorker pid=1195538)   File \"/home/ray/anaconda3/lib/python3.7/multiprocessing/resource_sharer.py\", line 142, in _serve\n",
      "(RayTrainWorker pid=1195538)     with self._listener.accept() as conn:\n",
      "(RayTrainWorker pid=1195538)   File \"/home/ray/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 455, in accept\n",
      "(RayTrainWorker pid=1195538)     deliver_challenge(c, self._authkey)\n",
      "(RayTrainWorker pid=1195538)   File \"/home/ray/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 730, in deliver_challenge\n",
      "(RayTrainWorker pid=1195538)     response = connection.recv_bytes(256)        # reject large message\n",
      "(RayTrainWorker pid=1195538)   File \"/home/ray/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "(RayTrainWorker pid=1195538)     buf = self._recv_bytes(maxlength)\n",
      "(RayTrainWorker pid=1195538)   File \"/home/ray/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "(RayTrainWorker pid=1195538)     buf = self._recv(4)\n",
      "(RayTrainWorker pid=1195538)   File \"/home/ray/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 379, in _recv\n",
      "(RayTrainWorker pid=1195538)     chunk = read(handle, remaining)\n",
      "(RayTrainWorker pid=1195538) ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "(RayTrainWorker pid=1199367) Restoring states from the checkpoint path at /tmp/checkpoint_tmp_78e2b2460ef94d519bbe29a6cb58a17a/model\n",
      "(RayTrainWorker pid=1199367) \n",
      "(RayTrainWorker pid=1199367)   | Name     | Type     | Params\n",
      "(RayTrainWorker pid=1199367) 0 | accuracy | Accuracy | 0     \n",
      "(RayTrainWorker pid=1199367) 36.1 K    Trainable params\n",
      "(RayTrainWorker pid=1199367) 0         Non-trainable params\n",
      "(RayTrainWorker pid=1199367) 36.1 K    Total params\n",
      "(RayTrainWorker pid=1199367) 0.145     Total estimated model params size (MB)\n",
      "(RayTrainWorker pid=1199367) Restored all states from the checkpoint file at /tmp/checkpoint_tmp_78e2b2460ef94d519bbe29a6cb58a17a/model\n",
      "(RayTrainWorker pid=1199367) 2023-03-29 19:44:27.110034: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "(RayTrainWorker pid=1199367) To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "(RayTrainWorker pid=1199367) 2023-03-29 19:44:27.281157: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "(RayTrainWorker pid=1199367) 2023-03-29 19:44:28.218806: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "(TrainTrainable pid=1201881) 2023-03-29 19:44:29,264\tINFO data_parallel_trainer.py:358 -- GPUs are detected in your Ray cluster, but GPU training is not enabled for this trainer. To enable GPU training, make sure to set `use_gpu` to True in your scaling config.\n",
      "(LightningTrainer pid=1201881) 2023-03-29 19:44:29,293\tINFO trainable.py:914 -- Restored on 10.0.37.7 from checkpoint: /home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00000_0_layer_1_size=128,layer_2_size=128,lr=0.0059_2023-03-29_19-41-37/checkpoint_tmpe148e7\n",
      "(LightningTrainer pid=1201881) 2023-03-29 19:44:29,293\tINFO trainable.py:922 -- Current state after restoring: {'_iteration': 2, '_timesteps_total': None, '_time_total': 31.914966821670532, '_episodes_total': None}\n",
      "(LightningTrainer pid=1201881) 2023-03-29 19:44:29,300\tINFO data_parallel_trainer.py:358 -- GPUs are detected in your Ray cluster, but GPU training is not enabled for this trainer. To enable GPU training, make sure to set `use_gpu` to True in your scaling config.\n",
      "(RayTrainWorker pid=1199370)     answer_challenge(c, self._authkey)\n",
      "(RayTrainWorker pid=1199370)   File \"/home/ray/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 747, in answer_challenge\n",
      "(RayTrainWorker pid=1199370) ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "(RayTrainWorker pid=1199367) /home/ray/anaconda3/lib/python3.7/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:343: UserWarning: The dirpath has changed from '/home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00009_9_layer_1_size=32,layer_2_size=256,lr=0.0034_2023-03-29_19-42-47/rank_0/lightning_logs/version_0/checkpoints' to '/home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00009_9_layer_1_size=32,layer_2_size=256,lr=0.0034_2023-03-29_19-42-47/rank_0/lightning_logs/version_1/checkpoints', therefore `best_model_score`, `kth_best_model_path`, `kth_value`, `last_model_path` and `best_k_models` won't be reloaded. Only `best_model_path` will be reloaded. [repeated 3x across cluster]\n",
      "(RayTrainWorker pid=1199367)   f\"The dirpath has changed from {dirpath_from_ckpt!r} to {self.dirpath!r},\" [repeated 3x across cluster]\n",
      "(RayTrainWorker pid=1199367) -------------------------------------- [repeated 2x across cluster]\n",
      "(RayTrainWorker pid=1199367) 3 | layer_3  | Linear   | 2.6 K  [repeated 3x across cluster]\n",
      "(RayTrainWorker pid=1199370) [W reducer.cpp:1298] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator()) [repeated 3x across cluster]\n",
      "(RayTrainWorker pid=1199367) 2023-03-29 19:44:28.218798: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64 [repeated 2x across cluster]\n",
      "(RayTrainWorker pid=1199370) ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "(RayTrainWorker pid=1199370) ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "(RayTrainWorker pid=1199370) ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "(RayTrainWorker pid=1199370) ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "(RayTrainWorker pid=1199370) ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "(RayTrainWorker pid=1199370) ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "(RayTrainWorker pid=1199370) ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "(RayTrainWorker pid=1199370) ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "(RayTrainWorker pid=1199370) ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "(RayTrainWorker pid=1199370) ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "(RayTrainWorker pid=1199370) ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "(RayTrainWorker pid=1199370) ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "(RayTrainWorker pid=1207521) 2023-03-29 19:44:34,204\tINFO config.py:87 -- Setting up process group for: env:// [rank=0, world_size=3]\n",
      "(LightningTrainer pid=1203788) 2023-03-29 19:44:29,857\tINFO data_parallel_trainer.py:358 -- GPUs are detected in your Ray cluster, but GPU training is not enabled for this trainer. To enable GPU training, make sure to set `use_gpu` to True in your scaling config. [repeated 2x across cluster]\n",
      "(LightningTrainer pid=1203788) 2023-03-29 19:44:34,616\tINFO config.py:87 -- Setting up process group for: env:// [rank=0, world_size=3]\n",
      "(LightningTrainer pid=1203788) 2023-03-29 19:44:34,616\tINFO config.py:87 -- Setting up process group for: env:// [rank=0, world_size=3]\n",
      "(LightningTrainer pid=1207197) 2023-03-29 19:44:36,899\tINFO trainable.py:914 -- Restored on 10.0.37.7 from checkpoint: /home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00002_2_layer_1_size=128,layer_2_size=64,lr=0.0012_2023-03-29_19-41-51/checkpoint_tmp74e33e\n",
      "(LightningTrainer pid=1207197) 2023-03-29 19:44:36,900\tINFO trainable.py:922 -- Current state after restoring: {'_iteration': 2, '_timesteps_total': None, '_time_total': 38.183873414993286, '_episodes_total': None}\n",
      "(RayTrainWorker pid=1207522) Missing logger folder: /home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00000_0_layer_1_size=128,layer_2_size=128,lr=0.0059_2023-03-29_19-41-37/rank_1/lightning_logs\n",
      "(RayTrainWorker pid=1207579) Missing logger folder: /home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00000_0_layer_1_size=128,layer_2_size=128,lr=0.0059_2023-03-29_19-41-37/rank_1/lightning_logs\n",
      "(LightningTrainer pid=1207197) 2023-03-29 19:44:36,902\tINFO data_parallel_trainer.py:358 -- GPUs are detected in your Ray cluster, but GPU training is not enabled for this trainer. To enable GPU training, make sure to set `use_gpu` to True in your scaling config. [repeated 2x across cluster]\n",
      "(RayTrainWorker pid=1207521) GPU available: False, used: False\n",
      "(RayTrainWorker pid=1207521) TPU available: False, using: 0 TPU cores\n",
      "(RayTrainWorker pid=1207521) IPU available: False, using: 0 IPUs\n",
      "(RayTrainWorker pid=1207521) HPU available: False, using: 0 HPUs\n",
      "(RayTrainWorker pid=1207522) /home/ray/anaconda3/lib/python3.7/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:343: UserWarning: The dirpath has changed from '/home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00000_0_layer_1_size=128,layer_2_size=128,lr=0.0059_2023-03-29_19-41-37/rank_0/lightning_logs/version_1/checkpoints' to '/home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00000_0_layer_1_size=128,layer_2_size=128,lr=0.0059_2023-03-29_19-41-37/rank_0/lightning_logs/version_2/checkpoints', therefore `best_model_score`, `kth_best_model_path`, `kth_value`, `last_model_path` and `best_k_models` won't be reloaded. Only `best_model_path` will be reloaded.\n",
      "(RayTrainWorker pid=1207522)   f\"The dirpath has changed from {dirpath_from_ckpt!r} to {self.dirpath!r},\"\n",
      "(RayTrainWorker pid=1207521) Restoring states from the checkpoint path at /tmp/checkpoint_tmp_55cf52715dd44673bb60e0bbbe56126f/model\n",
      "(RayTrainWorker pid=1207521) \n",
      "(RayTrainWorker pid=1207521)   | Name     | Type     | Params\n",
      "(RayTrainWorker pid=1207521) --------------------------------------\n",
      "(RayTrainWorker pid=1207521) 0 | accuracy | Accuracy | 0     \n",
      "(RayTrainWorker pid=1207521) 1 | layer_1  | Linear   | 100 K \n",
      "(RayTrainWorker pid=1207521) 2 | layer_2  | Linear   | 16.5 K\n",
      "(RayTrainWorker pid=1207521) 3 | layer_3  | Linear   | 1.3 K \n",
      "(RayTrainWorker pid=1207521) --------------------------------------\n",
      "(RayTrainWorker pid=1207521) 118 K     Trainable params\n",
      "(RayTrainWorker pid=1207521) 0         Non-trainable params\n",
      "(RayTrainWorker pid=1207521) 118 K     Total params\n",
      "(RayTrainWorker pid=1207521) 0.473     Total estimated model params size (MB)\n",
      "(RayTrainWorker pid=1207521) Restored all states from the checkpoint file at /tmp/checkpoint_tmp_55cf52715dd44673bb60e0bbbe56126f/model\n",
      "(RayTrainWorker pid=1207522) [W reducer.cpp:1298] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "(RayTrainWorker pid=1207521) 2023-03-29 19:44:40.864565: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "(RayTrainWorker pid=1207521) To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "(RayTrainWorker pid=1207521) 2023-03-29 19:44:41.031997: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "(RayTrainWorker pid=1209877) 2023-03-29 19:44:41,255\tINFO config.py:87 -- Setting up process group for: env:// [rank=0, world_size=3]\n",
      "(RayTrainWorker pid=1207579) 3 | layer_3  | Linear   | 650   \n",
      "(RayTrainWorker pid=1207521) 2023-03-29 19:44:42.145487: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "(RayTrainWorker pid=1207521) 2023-03-29 19:44:42.145634: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "(RayTrainWorker pid=1207521) 2023-03-29 19:44:42.145648: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "(LightningTrainer pid=1209430) 2023-03-29 19:44:43,908\tINFO trainable.py:914 -- Restored on 10.0.37.7 from checkpoint: /home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00003_3_layer_1_size=32,layer_2_size=256,lr=0.0012_2023-03-29_19-42-00/checkpoint_tmp9dd9a6\n",
      "(LightningTrainer pid=1209430) 2023-03-29 19:44:43,908\tINFO trainable.py:922 -- Current state after restoring: {'_iteration': 2, '_timesteps_total': None, '_time_total': 35.85082221031189, '_episodes_total': None}\n",
      "(RayTrainWorker pid=1209879) Missing logger folder: /home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00002_2_layer_1_size=128,layer_2_size=64,lr=0.0012_2023-03-29_19-41-51/rank_2/lightning_logs [repeated 4x across cluster]\n",
      "(LightningTrainer pid=1209430) 2023-03-29 19:44:43,926\tINFO data_parallel_trainer.py:358 -- GPUs are detected in your Ray cluster, but GPU training is not enabled for this trainer. To enable GPU training, make sure to set `use_gpu` to True in your scaling config. [repeated 2x across cluster]\n",
      "(RayTrainWorker pid=1207579) Missing logger folder: /home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00002_2_layer_1_size=128,layer_2_size=64,lr=0.0012_2023-03-29_19-41-51/rank_2/lightning_logs\n",
      "(RayTrainWorker pid=1207579) Missing logger folder: /home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00002_2_layer_1_size=128,layer_2_size=64,lr=0.0012_2023-03-29_19-41-51/rank_2/lightning_logs\n",
      "(RayTrainWorker pid=1207579) Missing logger folder: /home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00002_2_layer_1_size=128,layer_2_size=64,lr=0.0012_2023-03-29_19-41-51/rank_2/lightning_logs\n",
      "(RayTrainWorker pid=1207579) Missing logger folder: /home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00002_2_layer_1_size=128,layer_2_size=64,lr=0.0012_2023-03-29_19-41-51/rank_2/lightning_logs\n",
      "(RayTrainWorker pid=1207579) /home/ray/anaconda3/lib/python3.7/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:343: UserWarning: The dirpath has changed from '/home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00001_1_layer_1_size=32,layer_2_size=64,lr=0.0012_2023-03-29_19-41-44/rank_0/lightning_logs/version_1/checkpoints' to '/home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00001_1_layer_1_size=32,layer_2_size=64,lr=0.0012_2023-03-29_19-41-44/rank_0/lightning_logs/version_2/checkpoints', therefore `best_model_score`, `kth_best_model_path`, `kth_value`, `last_model_path` and `best_k_models` won't be reloaded. Only `best_model_path` will be reloaded. [repeated 5x across cluster]\n",
      "(RayTrainWorker pid=1207579)   f\"The dirpath has changed from {dirpath_from_ckpt!r} to {self.dirpath!r},\" [repeated 5x across cluster]\n",
      "(RayTrainWorker pid=1207579) Missing logger folder: /home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00002_2_layer_1_size=128,layer_2_size=64,lr=0.0012_2023-03-29_19-41-51/rank_2/lightning_logs\n",
      "(RayTrainWorker pid=1207579) Missing logger folder: /home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00002_2_layer_1_size=128,layer_2_size=64,lr=0.0012_2023-03-29_19-41-51/rank_2/lightning_logs\n",
      "(RayTrainWorker pid=1207579) Missing logger folder: /home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00002_2_layer_1_size=128,layer_2_size=64,lr=0.0012_2023-03-29_19-41-51/rank_2/lightning_logs\n",
      "(RayTrainWorker pid=1207579) -------------------------------------- [repeated 2x across cluster]\n",
      "(RayTrainWorker pid=1207579) Missing logger folder: /home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00002_2_layer_1_size=128,layer_2_size=64,lr=0.0012_2023-03-29_19-41-51/rank_2/lightning_logs\n",
      "(RayTrainWorker pid=1207579) 2 | layer_2  | Linear   | 2.1 K  [repeated 2x across cluster]\n",
      "(RayTrainWorker pid=1207579) Missing logger folder: /home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00002_2_layer_1_size=128,layer_2_size=64,lr=0.0012_2023-03-29_19-41-51/rank_2/lightning_logs\n",
      "(RayTrainWorker pid=1207579) Missing logger folder: /home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00002_2_layer_1_size=128,layer_2_size=64,lr=0.0012_2023-03-29_19-41-51/rank_2/lightning_logs\n",
      "(RayTrainWorker pid=1207579) Missing logger folder: /home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00002_2_layer_1_size=128,layer_2_size=64,lr=0.0012_2023-03-29_19-41-51/rank_2/lightning_logs\n",
      "(RayTrainWorker pid=1207579) Missing logger folder: /home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00002_2_layer_1_size=128,layer_2_size=64,lr=0.0012_2023-03-29_19-41-51/rank_2/lightning_logs\n",
      "(RayTrainWorker pid=1207579) Missing logger folder: /home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00002_2_layer_1_size=128,layer_2_size=64,lr=0.0012_2023-03-29_19-41-51/rank_2/lightning_logs\n",
      "(RayTrainWorker pid=1207579) [W reducer.cpp:1298] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator()) [repeated 5x across cluster]\n",
      "(RayTrainWorker pid=1207579) Missing logger folder: /home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00002_2_layer_1_size=128,layer_2_size=64,lr=0.0012_2023-03-29_19-41-51/rank_2/lightning_logs\n",
      "(RayTrainWorker pid=1207579) Missing logger folder: /home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00002_2_layer_1_size=128,layer_2_size=64,lr=0.0012_2023-03-29_19-41-51/rank_2/lightning_logs\n",
      "(RayTrainWorker pid=1207579) Missing logger folder: /home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00002_2_layer_1_size=128,layer_2_size=64,lr=0.0012_2023-03-29_19-41-51/rank_2/lightning_logs\n",
      "(RayTrainWorker pid=1207579) 2023-03-29 19:44:44.010684: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64 [repeated 2x across cluster]\n",
      "(RayTrainWorker pid=1207579) Missing logger folder: /home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00002_2_layer_1_size=128,layer_2_size=64,lr=0.0012_2023-03-29_19-41-51/rank_2/lightning_logs\n",
      "(RayTrainWorker pid=1209877) GPU available: False, used: False\n",
      "(RayTrainWorker pid=1209877) TPU available: False, using: 0 TPU cores\n",
      "(RayTrainWorker pid=1209877) IPU available: False, using: 0 IPUs\n",
      "(RayTrainWorker pid=1209877) HPU available: False, using: 0 HPUs\n",
      "(RayTrainWorker pid=1209877) Restoring states from the checkpoint path at /tmp/checkpoint_tmp_04f02bf4bf784d739a5377ebab140939/model\n",
      "(RayTrainWorker pid=1209877) \n",
      "(RayTrainWorker pid=1209877)   | Name     | Type     | Params\n",
      "(RayTrainWorker pid=1209877) 0 | accuracy | Accuracy | 0     \n",
      "(RayTrainWorker pid=1209877) 3 | layer_3  | Linear   | 650   \n",
      "(RayTrainWorker pid=1209877) 109 K     Trainable params\n",
      "(RayTrainWorker pid=1209877) 0         Non-trainable params\n",
      "(RayTrainWorker pid=1209877) 109 K     Total params\n",
      "(RayTrainWorker pid=1209877) 0.438     Total estimated model params size (MB)\n",
      "(RayTrainWorker pid=1209877) Restored all states from the checkpoint file at /tmp/checkpoint_tmp_04f02bf4bf784d739a5377ebab140939/model\n",
      "(RayTrainWorker pid=1213701) 2023-03-29 19:44:49,899\tINFO config.py:87 -- Setting up process group for: env:// [rank=0, world_size=3]\n",
      "(RayTrainWorker pid=1209877) 2023-03-29 19:44:49.948514: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "(RayTrainWorker pid=1209877) To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "(RayTrainWorker pid=1209877) 2023-03-29 19:44:50.103552: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "(RayTrainWorker pid=1209877) 2023-03-29 19:44:51.185151: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "(RayTrainWorker pid=1209877) Traceback (most recent call last):\n",
      "(RayTrainWorker pid=1209877)   File \"/home/ray/anaconda3/lib/python3.7/multiprocessing/resource_sharer.py\", line 142, in _serve\n",
      "(RayTrainWorker pid=1209877)     with self._listener.accept() as conn:\n",
      "(RayTrainWorker pid=1209877)   File \"/home/ray/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 455, in accept\n",
      "(RayTrainWorker pid=1209877)     deliver_challenge(c, self._authkey)\n",
      "(RayTrainWorker pid=1209877)   File \"/home/ray/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 730, in deliver_challenge\n",
      "(RayTrainWorker pid=1209877)     response = connection.recv_bytes(256)        # reject large message\n",
      "(RayTrainWorker pid=1209877)   File \"/home/ray/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "(RayTrainWorker pid=1209877)     buf = self._recv_bytes(maxlength)\n",
      "(RayTrainWorker pid=1209877)   File \"/home/ray/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "(RayTrainWorker pid=1209877)     buf = self._recv(4)\n",
      "(RayTrainWorker pid=1209877)   File \"/home/ray/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 379, in _recv\n",
      "(RayTrainWorker pid=1209877)     chunk = read(handle, remaining)\n",
      "(RayTrainWorker pid=1209877) ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "(RayTrainWorker pid=1209878) ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "(RayTrainWorker pid=1209877) /home/ray/anaconda3/lib/python3.7/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:343: UserWarning: The dirpath has changed from '/home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00002_2_layer_1_size=128,layer_2_size=64,lr=0.0012_2023-03-29_19-41-51/rank_0/lightning_logs/version_1/checkpoints' to '/home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00002_2_layer_1_size=128,layer_2_size=64,lr=0.0012_2023-03-29_19-41-51/rank_0/lightning_logs/version_2/checkpoints', therefore `best_model_score`, `kth_best_model_path`, `kth_value`, `last_model_path` and `best_k_models` won't be reloaded. Only `best_model_path` will be reloaded. [repeated 3x across cluster]\n",
      "(RayTrainWorker pid=1209877)   f\"The dirpath has changed from {dirpath_from_ckpt!r} to {self.dirpath!r},\" [repeated 3x across cluster]\n",
      "(RayTrainWorker pid=1209877) -------------------------------------- [repeated 2x across cluster]\n",
      "(RayTrainWorker pid=1209877) 2 | layer_2  | Linear   | 8.3 K  [repeated 2x across cluster]\n",
      "(RayTrainWorker pid=1209877) [W reducer.cpp:1298] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator()) [repeated 3x across cluster]\n",
      "(RayTrainWorker pid=1209877) 2023-03-29 19:44:51.185141: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64 [repeated 2x across cluster]\n",
      "(RayTrainWorker pid=1213701) GPU available: False, used: False\n",
      "(RayTrainWorker pid=1213701) TPU available: False, using: 0 TPU cores\n",
      "(RayTrainWorker pid=1213701) IPU available: False, using: 0 IPUs\n",
      "(RayTrainWorker pid=1213701) HPU available: False, using: 0 HPUs\n",
      "(RayTrainWorker pid=1213704) Missing logger folder: /home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00003_3_layer_1_size=32,layer_2_size=256,lr=0.0012_2023-03-29_19-42-00/rank_2/lightning_logs\n",
      "(RayTrainWorker pid=1213701) Restoring states from the checkpoint path at /tmp/checkpoint_tmp_856743c9cb2645d1a34c7294d6adcd64/model\n",
      "(RayTrainWorker pid=1213701) \n",
      "(RayTrainWorker pid=1213701)   | Name     | Type     | Params\n",
      "(RayTrainWorker pid=1213701) 0 | accuracy | Accuracy | 0     \n",
      "(RayTrainWorker pid=1213701) 36.1 K    Trainable params\n",
      "(RayTrainWorker pid=1213701) 0         Non-trainable params\n",
      "(RayTrainWorker pid=1213701) 36.1 K    Total params\n",
      "(RayTrainWorker pid=1213701) 0.145     Total estimated model params size (MB)\n",
      "(RayTrainWorker pid=1213701) Restored all states from the checkpoint file at /tmp/checkpoint_tmp_856743c9cb2645d1a34c7294d6adcd64/model\n",
      "(RayTrainWorker pid=1213701) 2023-03-29 19:44:57.916568: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "(RayTrainWorker pid=1213701) To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "(RayTrainWorker pid=1213701) 2023-03-29 19:44:58.067981: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "(RayTrainWorker pid=1213701) 2023-03-29 19:44:58.955160: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "(TrainTrainable pid=1218304) 2023-03-29 19:44:59,862\tINFO data_parallel_trainer.py:358 -- GPUs are detected in your Ray cluster, but GPU training is not enabled for this trainer. To enable GPU training, make sure to set `use_gpu` to True in your scaling config.\n",
      "(LightningTrainer pid=1218304) 2023-03-29 19:44:59,892\tINFO trainable.py:914 -- Restored on 10.0.37.7 from checkpoint: /home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00004_4_layer_1_size=32,layer_2_size=64,lr=0.0010_2023-03-29_19-42-12/checkpoint_tmpd05ad0\n",
      "(LightningTrainer pid=1218304) 2023-03-29 19:44:59,893\tINFO trainable.py:922 -- Current state after restoring: {'_iteration': 2, '_timesteps_total': None, '_time_total': 38.7507643699646, '_episodes_total': None}\n",
      "(LightningTrainer pid=1218304) 2023-03-29 19:44:59,896\tINFO data_parallel_trainer.py:358 -- GPUs are detected in your Ray cluster, but GPU training is not enabled for this trainer. To enable GPU training, make sure to set `use_gpu` to True in your scaling config.\n",
      "(RayTrainWorker pid=1213703) /home/ray/anaconda3/lib/python3.7/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:343: UserWarning: The dirpath has changed from '/home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00003_3_layer_1_size=32,layer_2_size=256,lr=0.0012_2023-03-29_19-42-00/rank_0/lightning_logs/version_1/checkpoints' to '/home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00003_3_layer_1_size=32,layer_2_size=256,lr=0.0012_2023-03-29_19-42-00/rank_0/lightning_logs/version_2/checkpoints', therefore `best_model_score`, `kth_best_model_path`, `kth_value`, `last_model_path` and `best_k_models` won't be reloaded. Only `best_model_path` will be reloaded. [repeated 3x across cluster]\n",
      "(RayTrainWorker pid=1213703)   f\"The dirpath has changed from {dirpath_from_ckpt!r} to {self.dirpath!r},\" [repeated 3x across cluster]\n",
      "(RayTrainWorker pid=1213701) -------------------------------------- [repeated 2x across cluster]\n",
      "(RayTrainWorker pid=1213701) 3 | layer_3  | Linear   | 2.6 K  [repeated 3x across cluster]\n",
      "(RayTrainWorker pid=1213703) [W reducer.cpp:1298] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator()) [repeated 3x across cluster]\n",
      "(RayTrainWorker pid=1213701) 2023-03-29 19:44:58.955152: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64 [repeated 2x across cluster]\n",
      "(RayTrainWorker pid=1222007) 2023-03-29 19:45:04,873\tINFO config.py:87 -- Setting up process group for: env:// [rank=0, world_size=3]\n",
      "(RayTrainWorker pid=1213703) 2023-03-29 19:45:04,873\tINFO config.py:87 -- Setting up process group for: env:// [rank=0, world_size=3]\n",
      "(LightningTrainer pid=1218452) 2023-03-29 19:45:00,861\tINFO data_parallel_trainer.py:358 -- GPUs are detected in your Ray cluster, but GPU training is not enabled for this trainer. To enable GPU training, make sure to set `use_gpu` to True in your scaling config. [repeated 2x across cluster]\n",
      "(LightningTrainer pid=1218452) 2023-03-29 19:45:05,458\tINFO config.py:87 -- Setting up process group for: env:// [rank=0, world_size=3]\n",
      "(LightningTrainer pid=1218452) 2023-03-29 19:45:05,458\tINFO config.py:87 -- Setting up process group for: env:// [rank=0, world_size=3]\n",
      "(LightningTrainer pid=1220327) 2023-03-29 19:45:05,801\tINFO trainable.py:914 -- Restored on 10.0.37.7 from checkpoint: /home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00006_6_layer_1_size=64,layer_2_size=128,lr=0.0003_2023-03-29_19-42-23/checkpoint_tmp42fa6d\n",
      "(LightningTrainer pid=1220327) 2023-03-29 19:45:05,801\tINFO trainable.py:922 -- Current state after restoring: {'_iteration': 2, '_timesteps_total': None, '_time_total': 31.914966821670532, '_episodes_total': None}\n",
      "(RayTrainWorker pid=1224014) 2023-03-29 19:45:10,000\tINFO config.py:87 -- Setting up process group for: env:// [rank=0, world_size=3] [repeated 2x across cluster]\n",
      "(RayTrainWorker pid=1222007) GPU available: False, used: False\n",
      "(RayTrainWorker pid=1222007) TPU available: False, using: 0 TPU cores\n",
      "(RayTrainWorker pid=1222007) IPU available: False, using: 0 IPUs\n",
      "(RayTrainWorker pid=1222007) HPU available: False, using: 0 HPUs\n",
      "(LightningTrainer pid=1220327) 2023-03-29 19:45:05,803\tINFO data_parallel_trainer.py:358 -- GPUs are detected in your Ray cluster, but GPU training is not enabled for this trainer. To enable GPU training, make sure to set `use_gpu` to True in your scaling config. [repeated 2x across cluster]\n",
      "(RayTrainWorker pid=1222008) Missing logger folder: /home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00004_4_layer_1_size=32,layer_2_size=64,lr=0.0010_2023-03-29_19-42-12/rank_1/lightning_logs\n",
      "(RayTrainWorker pid=1222007) Restoring states from the checkpoint path at /tmp/checkpoint_tmp_aaa562ee33294b0ba1d0bee61aa0f263/model\n",
      "(RayTrainWorker pid=1222007) /home/ray/anaconda3/lib/python3.7/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:343: UserWarning: The dirpath has changed from '/home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00004_4_layer_1_size=32,layer_2_size=64,lr=0.0010_2023-03-29_19-42-12/rank_0/lightning_logs/version_1/checkpoints' to '/home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00004_4_layer_1_size=32,layer_2_size=64,lr=0.0010_2023-03-29_19-42-12/rank_0/lightning_logs/version_2/checkpoints', therefore `best_model_score`, `kth_best_model_path`, `kth_value`, `last_model_path` and `best_k_models` won't be reloaded. Only `best_model_path` will be reloaded.\n",
      "(RayTrainWorker pid=1222007)   f\"The dirpath has changed from {dirpath_from_ckpt!r} to {self.dirpath!r},\"\n",
      "(RayTrainWorker pid=1222007) \n",
      "(RayTrainWorker pid=1222007)   | Name     | Type     | Params\n",
      "(RayTrainWorker pid=1222007) --------------------------------------\n",
      "(RayTrainWorker pid=1222007) 0 | accuracy | Accuracy | 0     \n",
      "(RayTrainWorker pid=1222007) 1 | layer_1  | Linear   | 25.1 K\n",
      "(RayTrainWorker pid=1222007) 2 | layer_2  | Linear   | 2.1 K \n",
      "(RayTrainWorker pid=1222007) 3 | layer_3  | Linear   | 650   \n",
      "(RayTrainWorker pid=1222007) --------------------------------------\n",
      "(RayTrainWorker pid=1222007) 27.9 K    Trainable params\n",
      "(RayTrainWorker pid=1222007) 0         Non-trainable params\n",
      "(RayTrainWorker pid=1222007) 27.9 K    Total params\n",
      "(RayTrainWorker pid=1222007) 0.112     Total estimated model params size (MB)\n",
      "(RayTrainWorker pid=1222007) Restored all states from the checkpoint file at /tmp/checkpoint_tmp_aaa562ee33294b0ba1d0bee61aa0f263/model\n",
      "(RayTrainWorker pid=1222007) [W reducer.cpp:1298] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "(RayTrainWorker pid=1222007) 2023-03-29 19:45:12.641281: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "(RayTrainWorker pid=1222007) To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "(RayTrainWorker pid=1222007) 2023-03-29 19:45:12.832800: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "(LightningTrainer pid=1223845) 2023-03-29 19:45:12,964\tINFO trainable.py:914 -- Restored on 10.0.37.7 from checkpoint: /home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00007_7_layer_1_size=32,layer_2_size=128,lr=0.0001_2023-03-29_19-42-34/checkpoint_tmp3ec70f\n",
      "(LightningTrainer pid=1223845) 2023-03-29 19:45:12,964\tINFO trainable.py:922 -- Current state after restoring: {'_iteration': 2, '_timesteps_total': None, '_time_total': 34.508357763290405, '_episodes_total': None}\n",
      "(RayTrainWorker pid=1222007) 2023-03-29 19:45:14.392475: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "(RayTrainWorker pid=1222007) 2023-03-29 19:45:14.392613: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "(RayTrainWorker pid=1222007) 2023-03-29 19:45:14.392625: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "(RayTrainWorker pid=1222080) Missing logger folder: /home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00006_6_layer_1_size=64,layer_2_size=128,lr=0.0003_2023-03-29_19-42-23/rank_1/lightning_logs\n",
      "(RayTrainWorker pid=1222080) Missing logger folder: /home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00006_6_layer_1_size=64,layer_2_size=128,lr=0.0003_2023-03-29_19-42-23/rank_1/lightning_logs\n",
      "(RayTrainWorker pid=1222080) Missing logger folder: /home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00006_6_layer_1_size=64,layer_2_size=128,lr=0.0003_2023-03-29_19-42-23/rank_1/lightning_logs\n",
      "(RayTrainWorker pid=1222080) Missing logger folder: /home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00006_6_layer_1_size=64,layer_2_size=128,lr=0.0003_2023-03-29_19-42-23/rank_1/lightning_logs\n",
      "(LightningTrainer pid=1223845) 2023-03-29 19:45:12,984\tINFO data_parallel_trainer.py:358 -- GPUs are detected in your Ray cluster, but GPU training is not enabled for this trainer. To enable GPU training, make sure to set `use_gpu` to True in your scaling config. [repeated 2x across cluster]\n",
      "(RayTrainWorker pid=1224015) Missing logger folder: /home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00006_6_layer_1_size=64,layer_2_size=128,lr=0.0003_2023-03-29_19-42-23/rank_1/lightning_logs [repeated 4x across cluster]\n",
      "(RayTrainWorker pid=1222080) Missing logger folder: /home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00006_6_layer_1_size=64,layer_2_size=128,lr=0.0003_2023-03-29_19-42-23/rank_1/lightning_logs\n",
      "(RayTrainWorker pid=1222082) /home/ray/anaconda3/lib/python3.7/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:343: UserWarning: The dirpath has changed from '/home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00005_5_layer_1_size=64,layer_2_size=128,lr=0.0746_2023-03-29_19-42-12/rank_0/lightning_logs/version_1/checkpoints' to '/home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00005_5_layer_1_size=64,layer_2_size=128,lr=0.0746_2023-03-29_19-42-12/rank_0/lightning_logs/version_2/checkpoints', therefore `best_model_score`, `kth_best_model_path`, `kth_value`, `last_model_path` and `best_k_models` won't be reloaded. Only `best_model_path` will be reloaded. [repeated 5x across cluster]\n",
      "(RayTrainWorker pid=1222082)   f\"The dirpath has changed from {dirpath_from_ckpt!r} to {self.dirpath!r},\" [repeated 5x across cluster]\n",
      "(RayTrainWorker pid=1222080) Missing logger folder: /home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00006_6_layer_1_size=64,layer_2_size=128,lr=0.0003_2023-03-29_19-42-23/rank_1/lightning_logs\n",
      "(RayTrainWorker pid=1222080) Missing logger folder: /home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00006_6_layer_1_size=64,layer_2_size=128,lr=0.0003_2023-03-29_19-42-23/rank_1/lightning_logs\n",
      "(RayTrainWorker pid=1222080) -------------------------------------- [repeated 2x across cluster]\n",
      "(RayTrainWorker pid=1222080) Missing logger folder: /home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00006_6_layer_1_size=64,layer_2_size=128,lr=0.0003_2023-03-29_19-42-23/rank_1/lightning_logs\n",
      "(RayTrainWorker pid=1222080) 3 | layer_3  | Linear   | 1.3 K  [repeated 3x across cluster]\n",
      "(RayTrainWorker pid=1222080) Missing logger folder: /home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00006_6_layer_1_size=64,layer_2_size=128,lr=0.0003_2023-03-29_19-42-23/rank_1/lightning_logs\n",
      "(RayTrainWorker pid=1222080) Missing logger folder: /home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00006_6_layer_1_size=64,layer_2_size=128,lr=0.0003_2023-03-29_19-42-23/rank_1/lightning_logs\n",
      "(RayTrainWorker pid=1222080) Missing logger folder: /home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00006_6_layer_1_size=64,layer_2_size=128,lr=0.0003_2023-03-29_19-42-23/rank_1/lightning_logs\n",
      "(RayTrainWorker pid=1222080) Missing logger folder: /home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00006_6_layer_1_size=64,layer_2_size=128,lr=0.0003_2023-03-29_19-42-23/rank_1/lightning_logs\n",
      "(RayTrainWorker pid=1222080) Missing logger folder: /home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00006_6_layer_1_size=64,layer_2_size=128,lr=0.0003_2023-03-29_19-42-23/rank_1/lightning_logs\n",
      "(RayTrainWorker pid=1224014) GPU available: False, used: False\n",
      "(RayTrainWorker pid=1224014) TPU available: False, using: 0 TPU cores\n",
      "(RayTrainWorker pid=1224014) IPU available: False, using: 0 IPUs\n",
      "(RayTrainWorker pid=1224014) HPU available: False, using: 0 HPUs\n",
      "(RayTrainWorker pid=1222082) [W reducer.cpp:1298] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator()) [repeated 5x across cluster]\n",
      "(RayTrainWorker pid=1224014) Restoring states from the checkpoint path at /tmp/checkpoint_tmp_15049295c04341e8bbe4eb806ba85196/model\n",
      "(RayTrainWorker pid=1224014) \n",
      "(RayTrainWorker pid=1224014)   | Name     | Type     | Params\n",
      "(RayTrainWorker pid=1224014) 0 | accuracy | Accuracy | 0     \n",
      "(RayTrainWorker pid=1224014) 118 K     Trainable params\n",
      "(RayTrainWorker pid=1224014) 0         Non-trainable params\n",
      "(RayTrainWorker pid=1224014) 118 K     Total params\n",
      "(RayTrainWorker pid=1224014) 0.473     Total estimated model params size (MB)\n",
      "(RayTrainWorker pid=1224014) Restored all states from the checkpoint file at /tmp/checkpoint_tmp_15049295c04341e8bbe4eb806ba85196/model\n",
      "(RayTrainWorker pid=1227485) 2023-03-29 19:45:17,892\tINFO config.py:87 -- Setting up process group for: env:// [rank=0, world_size=3]\n",
      "(RayTrainWorker pid=1222080) 2023-03-29 19:45:17,892\tINFO config.py:87 -- Setting up process group for: env:// [rank=0, world_size=3]\n",
      "(RayTrainWorker pid=1222080) 2023-03-29 19:45:17,892\tINFO config.py:87 -- Setting up process group for: env:// [rank=0, world_size=3]\n",
      "(RayTrainWorker pid=1222080) [W reducer.cpp:1298] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "(RayTrainWorker pid=1224014) 2023-03-29 19:45:18.770810: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "(RayTrainWorker pid=1224014) To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "(RayTrainWorker pid=1224014) 2023-03-29 19:45:19.096223: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-03-29 19:45:19,423\tINFO pbt.py:817 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial 6331d_00000 (score = 0.982887) into trial 6331d_00004 (score = 0.950893)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "MUTATION:  {'lr': <ray.tune.search.sample.Float object at 0x7f901753aed0>}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ORIGINAL:  {'layer_1_size': 128, 'layer_2_size': 128, 'lr': 0.005939557174932708}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-29 19:45:19,432\tINFO pbt.py:838 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of trial6331d_00004:\n",
      "lightning_config : \n",
      "    _module_init_config : \n",
      "        config : \n",
      "            lr : 0.005939557174932708 --- (* 1.2) --> 0.007127468609919249\n",
      "    _trainer_init_config : \n",
      "    _trainer_fit_params : \n",
      "    _ddp_strategy_config : \n",
      "    _model_checkpoint_config : \n",
      "\n",
      "(RayTrainWorker pid=1224014) 2023-03-29 19:45:20.522669: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64 [repeated 4x across cluster]\n",
      "(RayTrainWorker pid=1224014) 2023-03-29 19:45:20.522680: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. [repeated 2x across cluster]\n",
      "(RayTrainWorker pid=1224016) HPU available: False, using: 0 HPUs\n",
      "(RayTrainWorker pid=1224014) /home/ray/anaconda3/lib/python3.7/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:343: UserWarning: The dirpath has changed from '/home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00000_0_layer_1_size=128,layer_2_size=128,lr=0.0059_2023-03-29_19-41-37/rank_0/lightning_logs/version_1/checkpoints' to '/home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00006_6_layer_1_size=64,layer_2_size=128,lr=0.0003_2023-03-29_19-42-23/rank_0/lightning_logs/version_2/checkpoints', therefore `best_model_score`, `kth_best_model_path`, `kth_value`, `last_model_path` and `best_k_models` won't be reloaded. Only `best_model_path` will be reloaded. [repeated 3x across cluster]\n",
      "(RayTrainWorker pid=1224014)   f\"The dirpath has changed from {dirpath_from_ckpt!r} to {self.dirpath!r},\" [repeated 3x across cluster]\n",
      "(RayTrainWorker pid=1224014) -------------------------------------- [repeated 2x across cluster]\n",
      "(RayTrainWorker pid=1224014) 3 | layer_3  | Linear   | 1.3 K  [repeated 3x across cluster]\n",
      "(RayTrainWorker pid=1227485) HPU available: False, using: 0 HPUs\n",
      "(RayTrainWorker pid=1227485) HPU available: False, using: 0 HPUs\n",
      "(RayTrainWorker pid=1227485) HPU available: False, using: 0 HPUs\n",
      "(RayTrainWorker pid=1227485) HPU available: False, using: 0 HPUs\n",
      "(RayTrainWorker pid=1224014) [W reducer.cpp:1298] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator()) [repeated 3x across cluster]\n",
      "(RayTrainWorker pid=1227490) Missing logger folder: /home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00007_7_layer_1_size=32,layer_2_size=128,lr=0.0001_2023-03-29_19-42-34/rank_1/lightning_logs\n",
      "(RayTrainWorker pid=1227485) Restoring states from the checkpoint path at /tmp/checkpoint_tmp_82a2368c7caa4d36ab9b27c013b77df6/model\n",
      "(RayTrainWorker pid=1227485) \n",
      "(RayTrainWorker pid=1227485)   | Name     | Type     | Params\n",
      "(RayTrainWorker pid=1227485) 0 | accuracy | Accuracy | 0     \n",
      "(RayTrainWorker pid=1227485) 118 K     Trainable params\n",
      "(RayTrainWorker pid=1227485) 0         Non-trainable params\n",
      "(RayTrainWorker pid=1227485) 118 K     Total params\n",
      "(RayTrainWorker pid=1227485) 0.473     Total estimated model params size (MB)\n",
      "(RayTrainWorker pid=1227485) Restored all states from the checkpoint file at /tmp/checkpoint_tmp_82a2368c7caa4d36ab9b27c013b77df6/model\n",
      "(RayTrainWorker pid=1227485) 2023-03-29 19:45:27.111773: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "(RayTrainWorker pid=1227485) To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "(RayTrainWorker pid=1227485) 2023-03-29 19:45:27.267361: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "(RayTrainWorker pid=1227485) 2023-03-29 19:45:28.151261: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "(RayTrainWorker pid=1227485) 2023-03-29 19:45:28.151341: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "(RayTrainWorker pid=1227485) 2023-03-29 19:45:28.151348: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "(TrainTrainable pid=1232846) 2023-03-29 19:45:32,206\tINFO data_parallel_trainer.py:358 -- GPUs are detected in your Ray cluster, but GPU training is not enabled for this trainer. To enable GPU training, make sure to set `use_gpu` to True in your scaling config.\n",
      "(RayTrainWorker pid=1227491) /home/ray/anaconda3/lib/python3.7/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:343: UserWarning: The dirpath has changed from '/home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00007_7_layer_1_size=32,layer_2_size=128,lr=0.0001_2023-03-29_19-42-34/rank_0/lightning_logs/version_1/checkpoints' to '/home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00007_7_layer_1_size=32,layer_2_size=128,lr=0.0001_2023-03-29_19-42-34/rank_0/lightning_logs/version_2/checkpoints', therefore `best_model_score`, `kth_best_model_path`, `kth_value`, `last_model_path` and `best_k_models` won't be reloaded. Only `best_model_path` will be reloaded. [repeated 3x across cluster]\n",
      "(RayTrainWorker pid=1227491)   f\"The dirpath has changed from {dirpath_from_ckpt!r} to {self.dirpath!r},\" [repeated 3x across cluster]\n",
      "(RayTrainWorker pid=1227485) -------------------------------------- [repeated 2x across cluster]\n",
      "(RayTrainWorker pid=1227485) 3 | layer_3  | Linear   | 1.3 K  [repeated 3x across cluster]\n",
      "(RayTrainWorker pid=1227491) [W reducer.cpp:1298] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator()) [repeated 3x across cluster]\n",
      "(RayTrainWorker pid=1227491) 2023-03-29 19:45:32,206\tINFO data_parallel_trainer.py:358 -- GPUs are detected in your Ray cluster, but GPU training is not enabled for this trainer. To enable GPU training, make sure to set `use_gpu` to True in your scaling config.\n",
      "(LightningTrainer pid=1232846) 2023-03-29 19:45:32,227\tINFO trainable.py:914 -- Restored on 10.0.37.7 from checkpoint: /home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00008_8_layer_1_size=32,layer_2_size=128,lr=0.0004_2023-03-29_19-42-46/checkpoint_tmp2762cf\n",
      "(LightningTrainer pid=1232846) 2023-03-29 19:45:32,227\tINFO trainable.py:922 -- Current state after restoring: {'_iteration': 2, '_timesteps_total': None, '_time_total': 31.914966821670532, '_episodes_total': None}\n",
      "(LightningTrainer pid=1232846) 2023-03-29 19:45:32,230\tINFO data_parallel_trainer.py:358 -- GPUs are detected in your Ray cluster, but GPU training is not enabled for this trainer. To enable GPU training, make sure to set `use_gpu` to True in your scaling config.\n",
      "(RayTrainWorker pid=1237700) 2023-03-29 19:45:36,835\tINFO config.py:87 -- Setting up process group for: env:// [rank=0, world_size=3]\n",
      "(LightningTrainer pid=1234660) 2023-03-29 19:45:34,634\tINFO data_parallel_trainer.py:358 -- GPUs are detected in your Ray cluster, but GPU training is not enabled for this trainer. To enable GPU training, make sure to set `use_gpu` to True in your scaling config. [repeated 4x across cluster]\n",
      "(LightningTrainer pid=1234660) 2023-03-29 19:45:34,631\tINFO trainable.py:914 -- Restored on 10.0.37.7 from checkpoint: /home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00000_0_layer_1_size=128,layer_2_size=128,lr=0.0059_2023-03-29_19-41-37/checkpoint_tmpe04d16 [repeated 2x across cluster]\n",
      "(LightningTrainer pid=1234660) 2023-03-29 19:45:34,631\tINFO trainable.py:922 -- Current state after restoring: {'_iteration': 3, '_timesteps_total': None, '_time_total': 50.12615728378296, '_episodes_total': None} [repeated 2x across cluster]\n",
      "(RayTrainWorker pid=1237702) Missing logger folder: /home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00008_8_layer_1_size=32,layer_2_size=128,lr=0.0004_2023-03-29_19-42-46/rank_2/lightning_logs\n",
      "(RayTrainWorker pid=1238325) 2023-03-29 19:45:38,945\tINFO config.py:87 -- Setting up process group for: env:// [rank=0, world_size=3] [repeated 2x across cluster]\n",
      "(LightningTrainer pid=1237716) 2023-03-29 19:45:41,567\tINFO data_parallel_trainer.py:358 -- GPUs are detected in your Ray cluster, but GPU training is not enabled for this trainer. To enable GPU training, make sure to set `use_gpu` to True in your scaling config. [repeated 2x across cluster]\n",
      "(LightningTrainer pid=1237716) Missing logger folder: /home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00008_8_layer_1_size=32,layer_2_size=128,lr=0.0004_2023-03-29_19-42-46/rank_2/lightning_logs\n",
      "(LightningTrainer pid=1237716) Missing logger folder: /home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00008_8_layer_1_size=32,layer_2_size=128,lr=0.0004_2023-03-29_19-42-46/rank_2/lightning_logs\n",
      "(RayTrainWorker pid=1237700) GPU available: False, used: False\n",
      "(RayTrainWorker pid=1237700) TPU available: False, using: 0 TPU cores\n",
      "(RayTrainWorker pid=1237700) IPU available: False, using: 0 IPUs\n",
      "(RayTrainWorker pid=1237700) HPU available: False, using: 0 HPUs\n",
      "(RayTrainWorker pid=1237702) /home/ray/anaconda3/lib/python3.7/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:343: UserWarning: The dirpath has changed from '/home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00000_0_layer_1_size=128,layer_2_size=128,lr=0.0059_2023-03-29_19-41-37/rank_0/lightning_logs/version_1/checkpoints' to '/home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00008_8_layer_1_size=32,layer_2_size=128,lr=0.0004_2023-03-29_19-42-46/rank_0/lightning_logs/version_2/checkpoints', therefore `best_model_score`, `kth_best_model_path`, `kth_value`, `last_model_path` and `best_k_models` won't be reloaded. Only `best_model_path` will be reloaded.\n",
      "(RayTrainWorker pid=1237702)   f\"The dirpath has changed from {dirpath_from_ckpt!r} to {self.dirpath!r},\"\n",
      "(RayTrainWorker pid=1237700) Restoring states from the checkpoint path at /tmp/checkpoint_tmp_a015b1ac0fd744de99cbf1badcdbd6d7/model\n",
      "(RayTrainWorker pid=1237700) \n",
      "(RayTrainWorker pid=1237700)   | Name     | Type     | Params\n",
      "(RayTrainWorker pid=1237700) --------------------------------------\n",
      "(RayTrainWorker pid=1237700) 0 | accuracy | Accuracy | 0     \n",
      "(RayTrainWorker pid=1237700) 1 | layer_1  | Linear   | 100 K \n",
      "(RayTrainWorker pid=1237700) 2 | layer_2  | Linear   | 16.5 K\n",
      "(RayTrainWorker pid=1237700) 3 | layer_3  | Linear   | 1.3 K \n",
      "(RayTrainWorker pid=1237700) --------------------------------------\n",
      "(RayTrainWorker pid=1237700) 118 K     Trainable params\n",
      "(RayTrainWorker pid=1237700) 0         Non-trainable params\n",
      "(RayTrainWorker pid=1237700) 118 K     Total params\n",
      "(RayTrainWorker pid=1237700) 0.473     Total estimated model params size (MB)\n",
      "(RayTrainWorker pid=1237700) Restored all states from the checkpoint file at /tmp/checkpoint_tmp_a015b1ac0fd744de99cbf1badcdbd6d7/model\n",
      "(RayTrainWorker pid=1237702) [W reducer.cpp:1298] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "(RayTrainWorker pid=1237700) 2023-03-29 19:45:45.475263: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "(RayTrainWorker pid=1237700) To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "(RayTrainWorker pid=1237700) 2023-03-29 19:45:45.691857: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "(RayTrainWorker pid=1237700) 2023-03-29 19:45:46.659336: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "(RayTrainWorker pid=1237700) 2023-03-29 19:45:46.659418: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "(RayTrainWorker pid=1237700) 2023-03-29 19:45:46.659425: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "(RayTrainWorker pid=1238327) Missing logger folder: /home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00000_0_layer_1_size=128,layer_2_size=128,lr=0.0059_2023-03-29_19-41-37/rank_2/lightning_logs [repeated 5x across cluster]\n",
      "(RayTrainWorker pid=1239424) 2023-03-29 19:45:49.419750: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "(RayTrainWorker pid=1238325) GPU available: False, used: False [repeated 2x across cluster]\n",
      "(RayTrainWorker pid=1238325) TPU available: False, using: 0 TPU cores [repeated 2x across cluster]\n",
      "(RayTrainWorker pid=1238325) IPU available: False, using: 0 IPUs [repeated 2x across cluster]\n",
      "(RayTrainWorker pid=1238325) HPU available: False, using: 0 HPUs [repeated 2x across cluster]\n",
      "(RayTrainWorker pid=1238325) /home/ray/anaconda3/lib/python3.7/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:343: UserWarning: The dirpath has changed from '/home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00000_0_layer_1_size=128,layer_2_size=128,lr=0.0059_2023-03-29_19-41-37/rank_0/lightning_logs/version_2/checkpoints' to '/home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00000_0_layer_1_size=128,layer_2_size=128,lr=0.0059_2023-03-29_19-41-37/rank_0/lightning_logs/version_3/checkpoints', therefore `best_model_score`, `kth_best_model_path`, `kth_value`, `last_model_path` and `best_k_models` won't be reloaded. Only `best_model_path` will be reloaded. [repeated 8x across cluster]\n",
      "(RayTrainWorker pid=1238325)   f\"The dirpath has changed from {dirpath_from_ckpt!r} to {self.dirpath!r},\" [repeated 8x across cluster]\n",
      "(RayTrainWorker pid=1238325) Restoring states from the checkpoint path at /tmp/checkpoint_tmp_d5b7a046122245818e32ca6ddfe0d233/model [repeated 2x across cluster]\n",
      "(RayTrainWorker pid=1238325)  [repeated 2x across cluster]\n",
      "(RayTrainWorker pid=1238325)   | Name     | Type     | Params [repeated 2x across cluster]\n",
      "(RayTrainWorker pid=1238325) -------------------------------------- [repeated 4x across cluster]\n",
      "(RayTrainWorker pid=1238325) 0 | accuracy | Accuracy | 0      [repeated 2x across cluster]\n",
      "(RayTrainWorker pid=1238325) 3 | layer_3  | Linear   | 1.3 K  [repeated 6x across cluster]\n",
      "(RayTrainWorker pid=1238325) 118 K     Trainable params [repeated 2x across cluster]\n",
      "(RayTrainWorker pid=1238325) 0         Non-trainable params [repeated 2x across cluster]\n",
      "(RayTrainWorker pid=1238325) 118 K     Total params [repeated 2x across cluster]\n",
      "(RayTrainWorker pid=1238325) 0.473     Total estimated model params size (MB) [repeated 2x across cluster]\n",
      "(RayTrainWorker pid=1238325) Restored all states from the checkpoint file at /tmp/checkpoint_tmp_d5b7a046122245818e32ca6ddfe0d233/model [repeated 2x across cluster]\n",
      "(RayTrainWorker pid=1238327) [W reducer.cpp:1298] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator()) [repeated 8x across cluster]\n",
      "(RayTrainWorker pid=1238325) 2023-03-29 19:45:47.881634: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA [repeated 2x across cluster]\n",
      "(RayTrainWorker pid=1238325) To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. [repeated 2x across cluster]\n",
      "(RayTrainWorker pid=1238325) 2023-03-29 19:45:48.079004: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`. [repeated 2x across cluster]\n",
      "(RayTrainWorker pid=1238325) 2023-03-29 19:45:49.419736: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64 [repeated 4x across cluster]\n",
      "(RayTrainWorker pid=1238325) 2023-03-29 19:45:49.419750: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. [repeated 2x across cluster]\n",
      "(RayTrainWorker pid=1239424) HPU available: False, using: 0 HPUs\n",
      "(RayTrainWorker pid=1239424) HPU available: False, using: 0 HPUs\n",
      "(RayTrainWorker pid=1239424) HPU available: False, using: 0 HPUs\n",
      "(RayTrainWorker pid=1239424) HPU available: False, using: 0 HPUs\n",
      "(RayTrainWorker pid=1239426) Missing logger folder: /home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00001_1_layer_1_size=32,layer_2_size=64,lr=0.0012_2023-03-29_19-41-44/rank_2/lightning_logs\n",
      "(RayTrainWorker pid=1239424) Restoring states from the checkpoint path at /tmp/checkpoint_tmp_93ae157e3b384d18a30ff85c1a3637c8/model\n",
      "(RayTrainWorker pid=1239424) /home/ray/anaconda3/lib/python3.7/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:343: UserWarning: The dirpath has changed from '/home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00001_1_layer_1_size=32,layer_2_size=64,lr=0.0012_2023-03-29_19-41-44/rank_0/lightning_logs/version_2/checkpoints' to '/home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00001_1_layer_1_size=32,layer_2_size=64,lr=0.0012_2023-03-29_19-41-44/rank_0/lightning_logs/version_3/checkpoints', therefore `best_model_score`, `kth_best_model_path`, `kth_value`, `last_model_path` and `best_k_models` won't be reloaded. Only `best_model_path` will be reloaded.\n",
      "(RayTrainWorker pid=1239424)   f\"The dirpath has changed from {dirpath_from_ckpt!r} to {self.dirpath!r},\"\n",
      "(RayTrainWorker pid=1239424) \n",
      "(RayTrainWorker pid=1239424)   | Name     | Type     | Params\n",
      "(RayTrainWorker pid=1239424) --------------------------------------\n",
      "(RayTrainWorker pid=1239424) 0 | accuracy | Accuracy | 0     \n",
      "(RayTrainWorker pid=1239424) 1 | layer_1  | Linear   | 25.1 K\n",
      "(RayTrainWorker pid=1239424) 2 | layer_2  | Linear   | 2.1 K \n",
      "(RayTrainWorker pid=1239424) 3 | layer_3  | Linear   | 650   \n",
      "(RayTrainWorker pid=1239424) --------------------------------------\n",
      "(RayTrainWorker pid=1239424) 27.9 K    Trainable params\n",
      "(RayTrainWorker pid=1239424) 0         Non-trainable params\n",
      "(RayTrainWorker pid=1239424) 27.9 K    Total params\n",
      "(RayTrainWorker pid=1239424) 0.112     Total estimated model params size (MB)\n",
      "(RayTrainWorker pid=1239424) Restored all states from the checkpoint file at /tmp/checkpoint_tmp_93ae157e3b384d18a30ff85c1a3637c8/model\n",
      "2023-03-29 19:46:01,358\tINFO pbt.py:817 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial 6331d_00007 (score = 0.979539) into trial 6331d_00001 (score = 0.948909)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "MUTATION:  {'lr': <ray.tune.search.sample.Float object at 0x7f901753aed0>}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ORIGINAL:  {'layer_1_size': 128, 'layer_2_size': 128, 'lr': 0.007127468609919249}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-29 19:46:01,366\tINFO pbt.py:838 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of trial6331d_00001:\n",
      "lightning_config : \n",
      "    _module_init_config : \n",
      "        config : \n",
      "            lr : 0.007127468609919249 --- (resample) --> 0.0003677149260722046\n",
      "    _trainer_init_config : \n",
      "    _trainer_fit_params : \n",
      "    _ddp_strategy_config : \n",
      "    _model_checkpoint_config : \n",
      "\n",
      "(TrainTrainable pid=1249072) 2023-03-29 19:46:04,162\tINFO data_parallel_trainer.py:358 -- GPUs are detected in your Ray cluster, but GPU training is not enabled for this trainer. To enable GPU training, make sure to set `use_gpu` to True in your scaling config.\n",
      "(RayTrainWorker pid=1239425) [W reducer.cpp:1298] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator()) [repeated 3x across cluster]\n",
      "(RayTrainWorker pid=1239424) 2023-03-29 19:46:04,162\tINFO data_parallel_trainer.py:358 -- GPUs are detected in your Ray cluster, but GPU training is not enabled for this trainer. To enable GPU training, make sure to set `use_gpu` to True in your scaling config.\n",
      "(RayTrainWorker pid=1239424) 2023-03-29 19:46:04,162\tINFO data_parallel_trainer.py:358 -- GPUs are detected in your Ray cluster, but GPU training is not enabled for this trainer. To enable GPU training, make sure to set `use_gpu` to True in your scaling config.\n",
      "(RayTrainWorker pid=1239424) 2023-03-29 19:46:04,162\tINFO data_parallel_trainer.py:358 -- GPUs are detected in your Ray cluster, but GPU training is not enabled for this trainer. To enable GPU training, make sure to set `use_gpu` to True in your scaling config.\n",
      "(RayTrainWorker pid=1239424) 2023-03-29 19:45:58.214274: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64 [repeated 2x across cluster]\n",
      "(RayTrainWorker pid=1239424) 2023-03-29 19:46:04,162\tINFO data_parallel_trainer.py:358 -- GPUs are detected in your Ray cluster, but GPU training is not enabled for this trainer. To enable GPU training, make sure to set `use_gpu` to True in your scaling config.\n",
      "(RayTrainWorker pid=1239425) 2023-03-29 19:46:04,162\tINFO data_parallel_trainer.py:358 -- GPUs are detected in your Ray cluster, but GPU training is not enabled for this trainer. To enable GPU training, make sure to set `use_gpu` to True in your scaling config.\n",
      "(RayTrainWorker pid=1239425) /home/ray/anaconda3/lib/python3.7/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:343: UserWarning: The dirpath has changed from '/home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00001_1_layer_1_size=32,layer_2_size=64,lr=0.0012_2023-03-29_19-41-44/rank_0/lightning_logs/version_2/checkpoints' to '/home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00001_1_layer_1_size=32,layer_2_size=64,lr=0.0012_2023-03-29_19-41-44/rank_0/lightning_logs/version_3/checkpoints', therefore `best_model_score`, `kth_best_model_path`, `kth_value`, `last_model_path` and `best_k_models` won't be reloaded. Only `best_model_path` will be reloaded. [repeated 2x across cluster]\n",
      "(RayTrainWorker pid=1239425)   f\"The dirpath has changed from {dirpath_from_ckpt!r} to {self.dirpath!r},\" [repeated 2x across cluster]\n",
      "(LightningTrainer pid=1249072) 2023-03-29 19:46:04,183\tINFO trainable.py:914 -- Restored on 10.0.37.7 from checkpoint: /home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00002_2_layer_1_size=128,layer_2_size=64,lr=0.0012_2023-03-29_19-41-51/checkpoint_tmp544ba2\n",
      "(LightningTrainer pid=1249072) 2023-03-29 19:46:04,183\tINFO trainable.py:922 -- Current state after restoring: {'_iteration': 3, '_timesteps_total': None, '_time_total': 56.52263689041138, '_episodes_total': None}\n",
      "(LightningTrainer pid=1249072) 2023-03-29 19:46:04,186\tINFO data_parallel_trainer.py:358 -- GPUs are detected in your Ray cluster, but GPU training is not enabled for this trainer. To enable GPU training, make sure to set `use_gpu` to True in your scaling config.\n",
      "(RayTrainWorker pid=1252553) 2023-03-29 19:46:08,698\tINFO config.py:87 -- Setting up process group for: env:// [rank=0, world_size=3]\n",
      "(LightningTrainer pid=1250292) 2023-03-29 19:46:05,115\tINFO data_parallel_trainer.py:358 -- GPUs are detected in your Ray cluster, but GPU training is not enabled for this trainer. To enable GPU training, make sure to set `use_gpu` to True in your scaling config. [repeated 4x across cluster]\n",
      "(LightningTrainer pid=1250292) 2023-03-29 19:46:05,112\tINFO trainable.py:914 -- Restored on 10.0.37.7 from checkpoint: /home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00004_4_layer_1_size=32,layer_2_size=64,lr=0.0010_2023-03-29_19-42-12/checkpoint_tmpb19c78 [repeated 2x across cluster]\n",
      "(LightningTrainer pid=1250292) 2023-03-29 19:46:05,112\tINFO trainable.py:922 -- Current state after restoring: {'_iteration': 3, '_timesteps_total': None, '_time_total': 50.12615728378296, '_episodes_total': None} [repeated 2x across cluster]\n",
      "(RayTrainWorker pid=1252555) Missing logger folder: /home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00002_2_layer_1_size=128,layer_2_size=64,lr=0.0012_2023-03-29_19-41-51/rank_2/lightning_logs\n",
      "(RayTrainWorker pid=1252686) 2023-03-29 19:46:09,928\tINFO config.py:87 -- Setting up process group for: env:// [rank=0, world_size=3] [repeated 2x across cluster]\n",
      "(LightningTrainer pid=1252130) 2023-03-29 19:46:11,494\tINFO data_parallel_trainer.py:358 -- GPUs are detected in your Ray cluster, but GPU training is not enabled for this trainer. To enable GPU training, make sure to set `use_gpu` to True in your scaling config. [repeated 2x across cluster]\n",
      "(LightningTrainer pid=1252130) Missing logger folder: /home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00002_2_layer_1_size=128,layer_2_size=64,lr=0.0012_2023-03-29_19-41-51/rank_2/lightning_logs\n",
      "(LightningTrainer pid=1252130) Missing logger folder: /home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00002_2_layer_1_size=128,layer_2_size=64,lr=0.0012_2023-03-29_19-41-51/rank_2/lightning_logs\n",
      "(RayTrainWorker pid=1252553) GPU available: False, used: False\n",
      "(RayTrainWorker pid=1252553) TPU available: False, using: 0 TPU cores\n",
      "(RayTrainWorker pid=1252553) IPU available: False, using: 0 IPUs\n",
      "(RayTrainWorker pid=1252553) HPU available: False, using: 0 HPUs\n",
      "(RayTrainWorker pid=1252555) /home/ray/anaconda3/lib/python3.7/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:343: UserWarning: The dirpath has changed from '/home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00002_2_layer_1_size=128,layer_2_size=64,lr=0.0012_2023-03-29_19-41-51/rank_0/lightning_logs/version_2/checkpoints' to '/home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00002_2_layer_1_size=128,layer_2_size=64,lr=0.0012_2023-03-29_19-41-51/rank_0/lightning_logs/version_3/checkpoints', therefore `best_model_score`, `kth_best_model_path`, `kth_value`, `last_model_path` and `best_k_models` won't be reloaded. Only `best_model_path` will be reloaded.\n",
      "(RayTrainWorker pid=1252555)   f\"The dirpath has changed from {dirpath_from_ckpt!r} to {self.dirpath!r},\"\n",
      "(RayTrainWorker pid=1252553) Restoring states from the checkpoint path at /tmp/checkpoint_tmp_19819c4e955e45bbaaaa98052f5f5402/model\n",
      "(RayTrainWorker pid=1252553) \n",
      "(RayTrainWorker pid=1252553)   | Name     | Type     | Params\n",
      "(RayTrainWorker pid=1252553) --------------------------------------\n",
      "(RayTrainWorker pid=1252553) 0 | accuracy | Accuracy | 0     \n",
      "(RayTrainWorker pid=1252553) 1 | layer_1  | Linear   | 100 K \n",
      "(RayTrainWorker pid=1252553) 2 | layer_2  | Linear   | 8.3 K \n",
      "(RayTrainWorker pid=1252553) 3 | layer_3  | Linear   | 650   \n",
      "(RayTrainWorker pid=1252553) --------------------------------------\n",
      "(RayTrainWorker pid=1252553) 109 K     Trainable params\n",
      "(RayTrainWorker pid=1252553) 0         Non-trainable params\n",
      "(RayTrainWorker pid=1252553) 109 K     Total params\n",
      "(RayTrainWorker pid=1252553) 0.438     Total estimated model params size (MB)\n",
      "(RayTrainWorker pid=1252553) Restored all states from the checkpoint file at /tmp/checkpoint_tmp_19819c4e955e45bbaaaa98052f5f5402/model\n",
      "(RayTrainWorker pid=1252555) [W reducer.cpp:1298] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "(RayTrainWorker pid=1252553) 2023-03-29 19:46:16.214378: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "(RayTrainWorker pid=1252553) To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "(RayTrainWorker pid=1252553) 2023-03-29 19:46:16.382156: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "(RayTrainWorker pid=1252553) 2023-03-29 19:46:17.673661: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "(RayTrainWorker pid=1252553) 2023-03-29 19:46:17.673780: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "(RayTrainWorker pid=1252553) 2023-03-29 19:46:17.673793: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "(RayTrainWorker pid=1252688) Missing logger folder: /home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00004_4_layer_1_size=32,layer_2_size=64,lr=0.0010_2023-03-29_19-42-12/rank_2/lightning_logs [repeated 5x across cluster]\n",
      "(RayTrainWorker pid=1253916) 2023-03-29 19:46:20.174157: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "(RayTrainWorker pid=1252686) GPU available: False, used: False [repeated 2x across cluster]\n",
      "(RayTrainWorker pid=1252686) TPU available: False, using: 0 TPU cores [repeated 2x across cluster]\n",
      "(RayTrainWorker pid=1252686) IPU available: False, using: 0 IPUs [repeated 2x across cluster]\n",
      "(RayTrainWorker pid=1252686) HPU available: False, using: 0 HPUs [repeated 2x across cluster]\n",
      "(RayTrainWorker pid=1252686) /home/ray/anaconda3/lib/python3.7/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:343: UserWarning: The dirpath has changed from '/home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00000_0_layer_1_size=128,layer_2_size=128,lr=0.0059_2023-03-29_19-41-37/rank_0/lightning_logs/version_2/checkpoints' to '/home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00004_4_layer_1_size=32,layer_2_size=64,lr=0.0010_2023-03-29_19-42-12/rank_0/lightning_logs/version_3/checkpoints', therefore `best_model_score`, `kth_best_model_path`, `kth_value`, `last_model_path` and `best_k_models` won't be reloaded. Only `best_model_path` will be reloaded. [repeated 8x across cluster]\n",
      "(RayTrainWorker pid=1252686)   f\"The dirpath has changed from {dirpath_from_ckpt!r} to {self.dirpath!r},\" [repeated 8x across cluster]\n",
      "(RayTrainWorker pid=1252686) Restoring states from the checkpoint path at /tmp/checkpoint_tmp_c777665be63a4bc1877e49df96318841/model [repeated 2x across cluster]\n",
      "(RayTrainWorker pid=1252686)  [repeated 2x across cluster]\n",
      "(RayTrainWorker pid=1252686)   | Name     | Type     | Params [repeated 2x across cluster]\n",
      "(RayTrainWorker pid=1252686) -------------------------------------- [repeated 4x across cluster]\n",
      "(RayTrainWorker pid=1252686) 0 | accuracy | Accuracy | 0      [repeated 2x across cluster]\n",
      "(RayTrainWorker pid=1252686) 3 | layer_3  | Linear   | 1.3 K  [repeated 6x across cluster]\n",
      "(RayTrainWorker pid=1252686) 118 K     Trainable params [repeated 2x across cluster]\n",
      "(RayTrainWorker pid=1252686) 0         Non-trainable params [repeated 2x across cluster]\n",
      "(RayTrainWorker pid=1252686) 118 K     Total params [repeated 2x across cluster]\n",
      "(RayTrainWorker pid=1252686) 0.473     Total estimated model params size (MB) [repeated 2x across cluster]\n",
      "(RayTrainWorker pid=1252686) Restored all states from the checkpoint file at /tmp/checkpoint_tmp_c777665be63a4bc1877e49df96318841/model [repeated 2x across cluster]\n",
      "2023-03-29 19:46:24,824\tINFO pbt.py:817 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial 6331d_00008 (score = 0.982887) into trial 6331d_00003 (score = 0.940476)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "MUTATION:  {'lr': <ray.tune.search.sample.Float object at 0x7f901753aed0>}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ORIGINAL:  {'layer_1_size': 128, 'layer_2_size': 128, 'lr': 0.0018875272358204525}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-29 19:46:24,845\tINFO pbt.py:838 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of trial6331d_00003:\n",
      "lightning_config : \n",
      "    _module_init_config : \n",
      "        config : \n",
      "            lr : 0.0018875272358204525 --- (* 1.2) --> 0.0022650326829845428\n",
      "    _trainer_init_config : \n",
      "    _trainer_fit_params : \n",
      "    _ddp_strategy_config : \n",
      "    _model_checkpoint_config : \n",
      "\n",
      "(RayTrainWorker pid=1252686) [W reducer.cpp:1298] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator()) [repeated 8x across cluster]\n",
      "(RayTrainWorker pid=1252686) 2023-03-29 19:46:18.723303: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA [repeated 2x across cluster]\n",
      "(RayTrainWorker pid=1252686) To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. [repeated 2x across cluster]\n",
      "(RayTrainWorker pid=1252686) 2023-03-29 19:46:18.874336: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`. [repeated 2x across cluster]\n",
      "(RayTrainWorker pid=1252686) 2023-03-29 19:46:20.174140: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64 [repeated 4x across cluster]\n",
      "(RayTrainWorker pid=1252686) 2023-03-29 19:46:20.174157: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. [repeated 2x across cluster]\n",
      "(RayTrainWorker pid=1253918) Missing logger folder: /home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00001_1_layer_1_size=32,layer_2_size=64,lr=0.0012_2023-03-29_19-41-44/rank_2/lightning_logs\n",
      "(RayTrainWorker pid=1253916) GPU available: False, used: False\n",
      "(RayTrainWorker pid=1253916) TPU available: False, using: 0 TPU cores\n",
      "(RayTrainWorker pid=1253916) IPU available: False, using: 0 IPUs\n",
      "(RayTrainWorker pid=1253916) HPU available: False, using: 0 HPUs\n",
      "(RayTrainWorker pid=1253917) Missing logger folder: /home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00001_1_layer_1_size=32,layer_2_size=64,lr=0.0012_2023-03-29_19-41-44/rank_1/lightning_logs\n",
      "(RayTrainWorker pid=1253916) Restoring states from the checkpoint path at /tmp/checkpoint_tmp_2b5f125d91e44ee7bbd30e3f0e758688/model\n",
      "(RayTrainWorker pid=1253916) /home/ray/anaconda3/lib/python3.7/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:343: UserWarning: The dirpath has changed from '/home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00007_7_layer_1_size=32,layer_2_size=128,lr=0.0001_2023-03-29_19-42-34/rank_0/lightning_logs/version_2/checkpoints' to '/home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00001_1_layer_1_size=32,layer_2_size=64,lr=0.0012_2023-03-29_19-41-44/rank_0/lightning_logs/version_4/checkpoints', therefore `best_model_score`, `kth_best_model_path`, `kth_value`, `last_model_path` and `best_k_models` won't be reloaded. Only `best_model_path` will be reloaded.\n",
      "(RayTrainWorker pid=1253916)   f\"The dirpath has changed from {dirpath_from_ckpt!r} to {self.dirpath!r},\"\n",
      "(RayTrainWorker pid=1253916) \n",
      "(RayTrainWorker pid=1253916)   | Name     | Type     | Params\n",
      "(RayTrainWorker pid=1253916) --------------------------------------\n",
      "(RayTrainWorker pid=1253916) 0 | accuracy | Accuracy | 0     \n",
      "(RayTrainWorker pid=1253916) 1 | layer_1  | Linear   | 100 K \n",
      "(RayTrainWorker pid=1253916) 2 | layer_2  | Linear   | 16.5 K\n",
      "(RayTrainWorker pid=1253916) 3 | layer_3  | Linear   | 1.3 K \n",
      "(RayTrainWorker pid=1253916) --------------------------------------\n",
      "(RayTrainWorker pid=1253916) 118 K     Trainable params\n",
      "(RayTrainWorker pid=1253916) 0         Non-trainable params\n",
      "(RayTrainWorker pid=1253916) 118 K     Total params\n",
      "(RayTrainWorker pid=1253916) 0.473     Total estimated model params size (MB)\n",
      "(RayTrainWorker pid=1253916) Restored all states from the checkpoint file at /tmp/checkpoint_tmp_2b5f125d91e44ee7bbd30e3f0e758688/model\n",
      "(TrainTrainable pid=1263175) 2023-03-29 19:46:34,660\tINFO data_parallel_trainer.py:358 -- GPUs are detected in your Ray cluster, but GPU training is not enabled for this trainer. To enable GPU training, make sure to set `use_gpu` to True in your scaling config.\n",
      "(RayTrainWorker pid=1253917) [W reducer.cpp:1298] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator()) [repeated 3x across cluster]\n",
      "(RayTrainWorker pid=1253916) 2023-03-29 19:46:34,660\tINFO data_parallel_trainer.py:358 -- GPUs are detected in your Ray cluster, but GPU training is not enabled for this trainer. To enable GPU training, make sure to set `use_gpu` to True in your scaling config.\n",
      "(RayTrainWorker pid=1253916) 2023-03-29 19:46:34,660\tINFO data_parallel_trainer.py:358 -- GPUs are detected in your Ray cluster, but GPU training is not enabled for this trainer. To enable GPU training, make sure to set `use_gpu` to True in your scaling config.\n",
      "(RayTrainWorker pid=1253916) 2023-03-29 19:46:34,660\tINFO data_parallel_trainer.py:358 -- GPUs are detected in your Ray cluster, but GPU training is not enabled for this trainer. To enable GPU training, make sure to set `use_gpu` to True in your scaling config.\n",
      "(RayTrainWorker pid=1253916) 2023-03-29 19:46:28.137307: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64 [repeated 2x across cluster]\n",
      "(RayTrainWorker pid=1253916) 2023-03-29 19:46:34,660\tINFO data_parallel_trainer.py:358 -- GPUs are detected in your Ray cluster, but GPU training is not enabled for this trainer. To enable GPU training, make sure to set `use_gpu` to True in your scaling config.\n",
      "(RayTrainWorker pid=1253918) /home/ray/anaconda3/lib/python3.7/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:343: UserWarning: The dirpath has changed from '/home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00007_7_layer_1_size=32,layer_2_size=128,lr=0.0001_2023-03-29_19-42-34/rank_0/lightning_logs/version_2/checkpoints' to '/home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00001_1_layer_1_size=32,layer_2_size=64,lr=0.0012_2023-03-29_19-41-44/rank_0/lightning_logs/version_4/checkpoints', therefore `best_model_score`, `kth_best_model_path`, `kth_value`, `last_model_path` and `best_k_models` won't be reloaded. Only `best_model_path` will be reloaded. [repeated 2x across cluster]\n",
      "(RayTrainWorker pid=1253918)   f\"The dirpath has changed from {dirpath_from_ckpt!r} to {self.dirpath!r},\" [repeated 2x across cluster]\n",
      "(LightningTrainer pid=1263175) 2023-03-29 19:46:34,681\tINFO trainable.py:914 -- Restored on 10.0.37.7 from checkpoint: /home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00003_3_layer_1_size=32,layer_2_size=256,lr=0.0012_2023-03-29_19-42-00/checkpoint_tmpbe4da2\n",
      "(LightningTrainer pid=1263175) 2023-03-29 19:46:34,682\tINFO trainable.py:922 -- Current state after restoring: {'_iteration': 3, '_timesteps_total': None, '_time_total': 52.564085721969604, '_episodes_total': None}\n",
      "(LightningTrainer pid=1263175) 2023-03-29 19:46:34,684\tINFO data_parallel_trainer.py:358 -- GPUs are detected in your Ray cluster, but GPU training is not enabled for this trainer. To enable GPU training, make sure to set `use_gpu` to True in your scaling config.\n",
      "(RayTrainWorker pid=1266870) 2023-03-29 19:46:39,010\tINFO config.py:87 -- Setting up process group for: env:// [rank=0, world_size=3]\n",
      "(LightningTrainer pid=1264873) 2023-03-29 19:46:36,621\tINFO data_parallel_trainer.py:358 -- GPUs are detected in your Ray cluster, but GPU training is not enabled for this trainer. To enable GPU training, make sure to set `use_gpu` to True in your scaling config. [repeated 4x across cluster]\n",
      "(LightningTrainer pid=1264873) 2023-03-29 19:46:36,450\tINFO trainable.py:914 -- Restored on 10.0.37.7 from checkpoint: /home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00006_6_layer_1_size=64,layer_2_size=128,lr=0.0003_2023-03-29_19-42-23/checkpoint_tmp153bb4 [repeated 2x across cluster]\n",
      "(LightningTrainer pid=1264873) 2023-03-29 19:46:36,450\tINFO trainable.py:922 -- Current state after restoring: {'_iteration': 3, '_timesteps_total': None, '_time_total': 51.091646671295166, '_episodes_total': None} [repeated 2x across cluster]\n",
      "(RayTrainWorker pid=1268206) 2023-03-29 19:46:45,562\tINFO config.py:87 -- Setting up process group for: env:// [rank=0, world_size=3] [repeated 3x across cluster]\n",
      "(LightningTrainer pid=1266534) 2023-03-29 19:46:40,979\tINFO data_parallel_trainer.py:358 -- GPUs are detected in your Ray cluster, but GPU training is not enabled for this trainer. To enable GPU training, make sure to set `use_gpu` to True in your scaling config. [repeated 2x across cluster]\n",
      "(RayTrainWorker pid=1266870) GPU available: False, used: False\n",
      "(RayTrainWorker pid=1266870) TPU available: False, using: 0 TPU cores\n",
      "(RayTrainWorker pid=1266870) IPU available: False, using: 0 IPUs\n",
      "(RayTrainWorker pid=1266870) HPU available: False, using: 0 HPUs\n",
      "(LightningTrainer pid=1266534) HPU available: False, using: 0 HPUs\n",
      "(LightningTrainer pid=1266534) HPU available: False, using: 0 HPUs\n",
      "(RayTrainWorker pid=1266872) Missing logger folder: /home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00003_3_layer_1_size=32,layer_2_size=256,lr=0.0012_2023-03-29_19-42-00/rank_2/lightning_logs\n",
      "(RayTrainWorker pid=1266872) /home/ray/anaconda3/lib/python3.7/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:343: UserWarning: The dirpath has changed from '/home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00008_8_layer_1_size=32,layer_2_size=128,lr=0.0004_2023-03-29_19-42-46/rank_0/lightning_logs/version_2/checkpoints' to '/home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00003_3_layer_1_size=32,layer_2_size=256,lr=0.0012_2023-03-29_19-42-00/rank_0/lightning_logs/version_4/checkpoints', therefore `best_model_score`, `kth_best_model_path`, `kth_value`, `last_model_path` and `best_k_models` won't be reloaded. Only `best_model_path` will be reloaded.\n",
      "(RayTrainWorker pid=1266872)   f\"The dirpath has changed from {dirpath_from_ckpt!r} to {self.dirpath!r},\"\n",
      "(RayTrainWorker pid=1266870) Restoring states from the checkpoint path at /tmp/checkpoint_tmp_f5016f67919c4ef8933f222919db6882/model\n",
      "(RayTrainWorker pid=1266870) \n",
      "(RayTrainWorker pid=1266870)   | Name     | Type     | Params\n",
      "(RayTrainWorker pid=1266870) --------------------------------------\n",
      "(RayTrainWorker pid=1266870) 0 | accuracy | Accuracy | 0     \n",
      "(RayTrainWorker pid=1266870) 1 | layer_1  | Linear   | 100 K \n",
      "(RayTrainWorker pid=1266870) 2 | layer_2  | Linear   | 16.5 K\n",
      "(RayTrainWorker pid=1266870) 3 | layer_3  | Linear   | 1.3 K \n",
      "(RayTrainWorker pid=1266870) --------------------------------------\n",
      "(RayTrainWorker pid=1266870) 118 K     Trainable params\n",
      "(RayTrainWorker pid=1266870) 0         Non-trainable params\n",
      "(RayTrainWorker pid=1266870) 118 K     Total params\n",
      "(RayTrainWorker pid=1266870) 0.473     Total estimated model params size (MB)\n",
      "(RayTrainWorker pid=1266870) Restored all states from the checkpoint file at /tmp/checkpoint_tmp_f5016f67919c4ef8933f222919db6882/model\n",
      "(RayTrainWorker pid=1266871) [W reducer.cpp:1298] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "(RayTrainWorker pid=1266870) 2023-03-29 19:46:47.258226: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "(RayTrainWorker pid=1266870) To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "(RayTrainWorker pid=1266870) 2023-03-29 19:46:47.423179: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "(RayTrainWorker pid=1266870) 2023-03-29 19:46:48.783825: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "(RayTrainWorker pid=1266870) 2023-03-29 19:46:48.783918: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "(RayTrainWorker pid=1266870) 2023-03-29 19:46:48.783926: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "(RayTrainWorker pid=1267392) GPU available: False, used: False [repeated 2x across cluster]\n",
      "(RayTrainWorker pid=1267392) TPU available: False, using: 0 TPU cores [repeated 2x across cluster]\n",
      "(RayTrainWorker pid=1267392) IPU available: False, using: 0 IPUs [repeated 2x across cluster]\n",
      "(RayTrainWorker pid=1267392) HPU available: False, using: 0 HPUs [repeated 2x across cluster]\n",
      "(RayTrainWorker pid=1268208) Missing logger folder: /home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00007_7_layer_1_size=32,layer_2_size=128,lr=0.0001_2023-03-29_19-42-34/rank_2/lightning_logs [repeated 6x across cluster]\n",
      "(RayTrainWorker pid=1267392) /home/ray/anaconda3/lib/python3.7/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:343: UserWarning: The dirpath has changed from '/home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00006_6_layer_1_size=64,layer_2_size=128,lr=0.0003_2023-03-29_19-42-23/rank_0/lightning_logs/version_2/checkpoints' to '/home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00006_6_layer_1_size=64,layer_2_size=128,lr=0.0003_2023-03-29_19-42-23/rank_0/lightning_logs/version_3/checkpoints', therefore `best_model_score`, `kth_best_model_path`, `kth_value`, `last_model_path` and `best_k_models` won't be reloaded. Only `best_model_path` will be reloaded. [repeated 8x across cluster]\n",
      "(RayTrainWorker pid=1267392)   f\"The dirpath has changed from {dirpath_from_ckpt!r} to {self.dirpath!r},\" [repeated 8x across cluster]\n",
      "(RayTrainWorker pid=1267392) Restoring states from the checkpoint path at /tmp/checkpoint_tmp_0bf3f4a85d59431f819f8163e047edcf/model [repeated 2x across cluster]\n",
      "(RayTrainWorker pid=1267392)  [repeated 2x across cluster]\n",
      "(RayTrainWorker pid=1267392)   | Name     | Type     | Params [repeated 2x across cluster]\n",
      "(RayTrainWorker pid=1267392) -------------------------------------- [repeated 4x across cluster]\n",
      "(RayTrainWorker pid=1267392) 0 | accuracy | Accuracy | 0      [repeated 2x across cluster]\n",
      "(RayTrainWorker pid=1267392) 3 | layer_3  | Linear   | 1.3 K  [repeated 6x across cluster]\n",
      "(RayTrainWorker pid=1267392) 118 K     Trainable params [repeated 2x across cluster]\n",
      "(RayTrainWorker pid=1267392) 0         Non-trainable params [repeated 2x across cluster]\n",
      "(RayTrainWorker pid=1267392) 118 K     Total params [repeated 2x across cluster]\n",
      "(RayTrainWorker pid=1267392) 0.473     Total estimated model params size (MB) [repeated 2x across cluster]\n",
      "(RayTrainWorker pid=1267392) Restored all states from the checkpoint file at /tmp/checkpoint_tmp_0bf3f4a85d59431f819f8163e047edcf/model [repeated 2x across cluster]\n",
      "(RayTrainWorker pid=1267392) [W reducer.cpp:1298] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator()) [repeated 8x across cluster]\n",
      "(RayTrainWorker pid=1267392) 2023-03-29 19:46:49.587560: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA [repeated 2x across cluster]\n",
      "(RayTrainWorker pid=1267392) To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. [repeated 2x across cluster]\n",
      "(RayTrainWorker pid=1267392) 2023-03-29 19:46:49.755020: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`. [repeated 2x across cluster]\n",
      "(RayTrainWorker pid=1267392) 2023-03-29 19:46:50.973605: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64 [repeated 4x across cluster]\n",
      "(RayTrainWorker pid=1267392) 2023-03-29 19:46:50.973617: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. [repeated 2x across cluster]\n",
      "(RayTrainWorker pid=1266872) Traceback (most recent call last):\n",
      "(RayTrainWorker pid=1266872)   File \"/home/ray/anaconda3/lib/python3.7/multiprocessing/resource_sharer.py\", line 142, in _serve\n",
      "(RayTrainWorker pid=1266872)     with self._listener.accept() as conn:\n",
      "(RayTrainWorker pid=1266872)   File \"/home/ray/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 456, in accept\n",
      "(RayTrainWorker pid=1266872)     answer_challenge(c, self._authkey)\n",
      "(RayTrainWorker pid=1266872)   File \"/home/ray/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 747, in answer_challenge\n",
      "(RayTrainWorker pid=1266872)     response = connection.recv_bytes(256)        # reject large message\n",
      "(RayTrainWorker pid=1266872)   File \"/home/ray/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "(RayTrainWorker pid=1266872)     buf = self._recv_bytes(maxlength)\n",
      "(RayTrainWorker pid=1266872)   File \"/home/ray/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "(RayTrainWorker pid=1266872)     buf = self._recv(4)\n",
      "(RayTrainWorker pid=1266872)   File \"/home/ray/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 379, in _recv\n",
      "(RayTrainWorker pid=1266872)     chunk = read(handle, remaining)\n",
      "(RayTrainWorker pid=1266872) ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "(RayTrainWorker pid=1268206) 2023-03-29 19:46:56.071100: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "(RayTrainWorker pid=1268206) 2023-03-29 19:46:56.071100: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "(RayTrainWorker pid=1268206) 2023-03-29 19:46:56.071100: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "(RayTrainWorker pid=1268206) 2023-03-29 19:46:56.071100: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "(RayTrainWorker pid=1268207) ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "(RayTrainWorker pid=1268206) /home/ray/anaconda3/lib/python3.7/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:343: UserWarning: The dirpath has changed from '/home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00007_7_layer_1_size=32,layer_2_size=128,lr=0.0001_2023-03-29_19-42-34/rank_0/lightning_logs/version_2/checkpoints' to '/home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00007_7_layer_1_size=32,layer_2_size=128,lr=0.0001_2023-03-29_19-42-34/rank_0/lightning_logs/version_3/checkpoints', therefore `best_model_score`, `kth_best_model_path`, `kth_value`, `last_model_path` and `best_k_models` won't be reloaded. Only `best_model_path` will be reloaded. [repeated 3x across cluster]\n",
      "(RayTrainWorker pid=1268206)   f\"The dirpath has changed from {dirpath_from_ckpt!r} to {self.dirpath!r},\" [repeated 3x across cluster]\n",
      "(RayTrainWorker pid=1268206) ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "(RayTrainWorker pid=1268206) ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "(RayTrainWorker pid=1268206) ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "(RayTrainWorker pid=1268206) -------------------------------------- [repeated 2x across cluster]\n",
      "(RayTrainWorker pid=1268206) ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "(RayTrainWorker pid=1268206) 3 | layer_3  | Linear   | 1.3 K  [repeated 3x across cluster]\n",
      "(RayTrainWorker pid=1268206) ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "(RayTrainWorker pid=1268206) ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "(RayTrainWorker pid=1268206) ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "(RayTrainWorker pid=1268206) ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "(RayTrainWorker pid=1268206) ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "(RayTrainWorker pid=1268207) [W reducer.cpp:1298] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator()) [repeated 3x across cluster]\n",
      "(RayTrainWorker pid=1268206) ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "(RayTrainWorker pid=1268206) ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "(RayTrainWorker pid=1268206) ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "(RayTrainWorker pid=1268206) 2023-03-29 19:46:57.485747: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64 [repeated 2x across cluster]\n",
      "(RayTrainWorker pid=1268206) ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "(RayTrainWorker pid=1268208) ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "(RayTrainWorker pid=1268208) ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "(RayTrainWorker pid=1268208) ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "(RayTrainWorker pid=1268208) ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "(RayTrainWorker pid=1268208) ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "(RayTrainWorker pid=1268208) ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "(RayTrainWorker pid=1268208) ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "(RayTrainWorker pid=1268208) ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "(RayTrainWorker pid=1268208) ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "(RayTrainWorker pid=1268208) ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "(RayTrainWorker pid=1268208) ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "(RayTrainWorker pid=1268208) ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "(RayTrainWorker pid=1268208) ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "(RayTrainWorker pid=1268208) ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "(TrainTrainable pid=1279249) 2023-03-29 19:47:06,294\tINFO data_parallel_trainer.py:358 -- GPUs are detected in your Ray cluster, but GPU training is not enabled for this trainer. To enable GPU training, make sure to set `use_gpu` to True in your scaling config.\n",
      "(LightningTrainer pid=1279249) 2023-03-29 19:47:06,317\tINFO trainable.py:914 -- Restored on 10.0.37.7 from checkpoint: /home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00008_8_layer_1_size=32,layer_2_size=128,lr=0.0004_2023-03-29_19-42-46/checkpoint_tmpc2b8ed\n",
      "(LightningTrainer pid=1279249) 2023-03-29 19:47:06,317\tINFO trainable.py:922 -- Current state after restoring: {'_iteration': 3, '_timesteps_total': None, '_time_total': 52.564085721969604, '_episodes_total': None}\n",
      "(LightningTrainer pid=1279249) 2023-03-29 19:47:06,320\tINFO data_parallel_trainer.py:358 -- GPUs are detected in your Ray cluster, but GPU training is not enabled for this trainer. To enable GPU training, make sure to set `use_gpu` to True in your scaling config.\n",
      "(RayTrainWorker pid=1281442) 2023-03-29 19:47:10,992\tINFO config.py:87 -- Setting up process group for: env:// [rank=0, world_size=3]\n",
      "(LightningTrainer pid=1280989) 2023-03-29 19:47:11,061\tINFO data_parallel_trainer.py:358 -- GPUs are detected in your Ray cluster, but GPU training is not enabled for this trainer. To enable GPU training, make sure to set `use_gpu` to True in your scaling config. [repeated 6x across cluster]\n",
      "(LightningTrainer pid=1280989) 2023-03-29 19:47:11,059\tINFO trainable.py:914 -- Restored on 10.0.37.7 from checkpoint: /home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00001_1_layer_1_size=32,layer_2_size=64,lr=0.0012_2023-03-29_19-41-44/checkpoint_tmpd08502 [repeated 3x across cluster]\n",
      "(LightningTrainer pid=1280989) 2023-03-29 19:47:11,059\tINFO trainable.py:922 -- Current state after restoring: {'_iteration': 4, '_timesteps_total': None, '_time_total': 73.03845596313477, '_episodes_total': None} [repeated 3x across cluster]\n",
      "(RayTrainWorker pid=1281444) Missing logger folder: /home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00008_8_layer_1_size=32,layer_2_size=128,lr=0.0004_2023-03-29_19-42-46/rank_1/lightning_logs\n",
      "(RayTrainWorker pid=1282467) 2023-03-29 19:47:15,364\tINFO config.py:87 -- Setting up process group for: env:// [rank=0, world_size=3] [repeated 3x across cluster]\n",
      "(RayTrainWorker pid=1281442) GPU available: False, used: False\n",
      "(RayTrainWorker pid=1281442) TPU available: False, using: 0 TPU cores\n",
      "(RayTrainWorker pid=1281442) IPU available: False, using: 0 IPUs\n",
      "(RayTrainWorker pid=1281442) HPU available: False, using: 0 HPUs\n",
      "(RayTrainWorker pid=1281444) /home/ray/anaconda3/lib/python3.7/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:343: UserWarning: The dirpath has changed from '/home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00008_8_layer_1_size=32,layer_2_size=128,lr=0.0004_2023-03-29_19-42-46/rank_0/lightning_logs/version_2/checkpoints' to '/home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00008_8_layer_1_size=32,layer_2_size=128,lr=0.0004_2023-03-29_19-42-46/rank_0/lightning_logs/version_3/checkpoints', therefore `best_model_score`, `kth_best_model_path`, `kth_value`, `last_model_path` and `best_k_models` won't be reloaded. Only `best_model_path` will be reloaded.\n",
      "(RayTrainWorker pid=1281444)   f\"The dirpath has changed from {dirpath_from_ckpt!r} to {self.dirpath!r},\"\n",
      "(RayTrainWorker pid=1281442) Restoring states from the checkpoint path at /tmp/checkpoint_tmp_dc39d796d085450e92022946eed02671/model\n",
      "(RayTrainWorker pid=1281442) \n",
      "(RayTrainWorker pid=1281442)   | Name     | Type     | Params\n",
      "(RayTrainWorker pid=1281442) --------------------------------------\n",
      "(RayTrainWorker pid=1281442) 0 | accuracy | Accuracy | 0     \n",
      "(RayTrainWorker pid=1281442) 1 | layer_1  | Linear   | 100 K \n",
      "(RayTrainWorker pid=1281442) 2 | layer_2  | Linear   | 16.5 K\n",
      "(RayTrainWorker pid=1281442) 3 | layer_3  | Linear   | 1.3 K \n",
      "(RayTrainWorker pid=1281442) --------------------------------------\n",
      "(RayTrainWorker pid=1281442) 118 K     Trainable params\n",
      "(RayTrainWorker pid=1281442) 0         Non-trainable params\n",
      "(RayTrainWorker pid=1281442) 118 K     Total params\n",
      "(RayTrainWorker pid=1281442) 0.473     Total estimated model params size (MB)\n",
      "(RayTrainWorker pid=1281442) Restored all states from the checkpoint file at /tmp/checkpoint_tmp_dc39d796d085450e92022946eed02671/model\n",
      "(RayTrainWorker pid=1281442) [W reducer.cpp:1298] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "(RayTrainWorker pid=1281442) 2023-03-29 19:47:19.247673: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "(RayTrainWorker pid=1281442) To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "(RayTrainWorker pid=1281442) 2023-03-29 19:47:19.428489: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "(RayTrainWorker pid=1281442) 2023-03-29 19:47:20.801480: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "(RayTrainWorker pid=1281442) 2023-03-29 19:47:20.801572: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "(RayTrainWorker pid=1281442) 2023-03-29 19:47:20.801580: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "(RayTrainWorker pid=1282468) Missing logger folder: /home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00001_1_layer_1_size=32,layer_2_size=64,lr=0.0012_2023-03-29_19-41-44/rank_1/lightning_logs [repeated 7x across cluster]\n",
      "(RayTrainWorker pid=1282467) GPU available: False, used: False [repeated 3x across cluster]\n",
      "(RayTrainWorker pid=1282467) TPU available: False, using: 0 TPU cores [repeated 3x across cluster]\n",
      "(RayTrainWorker pid=1282467) IPU available: False, using: 0 IPUs [repeated 3x across cluster]\n",
      "(RayTrainWorker pid=1282467) HPU available: False, using: 0 HPUs [repeated 3x across cluster]\n",
      "(RayTrainWorker pid=1282468) /home/ray/anaconda3/lib/python3.7/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:343: UserWarning: The dirpath has changed from '/home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00001_1_layer_1_size=32,layer_2_size=64,lr=0.0012_2023-03-29_19-41-44/rank_0/lightning_logs/version_4/checkpoints' to '/home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00001_1_layer_1_size=32,layer_2_size=64,lr=0.0012_2023-03-29_19-41-44/rank_0/lightning_logs/version_5/checkpoints', therefore `best_model_score`, `kth_best_model_path`, `kth_value`, `last_model_path` and `best_k_models` won't be reloaded. Only `best_model_path` will be reloaded. [repeated 11x across cluster]\n",
      "(RayTrainWorker pid=1282468)   f\"The dirpath has changed from {dirpath_from_ckpt!r} to {self.dirpath!r},\" [repeated 11x across cluster]\n",
      "(RayTrainWorker pid=1282467) Restoring states from the checkpoint path at /tmp/checkpoint_tmp_0b2365e1faa9448d910ee65e7787b322/model [repeated 3x across cluster]\n",
      "(RayTrainWorker pid=1282467)  [repeated 3x across cluster]\n",
      "(RayTrainWorker pid=1282467)   | Name     | Type     | Params [repeated 3x across cluster]\n",
      "(RayTrainWorker pid=1282467) -------------------------------------- [repeated 6x across cluster]\n",
      "(RayTrainWorker pid=1282467) 0 | accuracy | Accuracy | 0      [repeated 3x across cluster]\n",
      "(RayTrainWorker pid=1282467) 3 | layer_3  | Linear   | 1.3 K  [repeated 9x across cluster]\n",
      "(RayTrainWorker pid=1282467) 118 K     Trainable params [repeated 3x across cluster]\n",
      "(RayTrainWorker pid=1282467) 0         Non-trainable params [repeated 3x across cluster]\n",
      "(RayTrainWorker pid=1282467) 118 K     Total params [repeated 3x across cluster]\n",
      "(RayTrainWorker pid=1282467) 0.473     Total estimated model params size (MB) [repeated 3x across cluster]\n",
      "(RayTrainWorker pid=1282467) Restored all states from the checkpoint file at /tmp/checkpoint_tmp_0b2365e1faa9448d910ee65e7787b322/model [repeated 3x across cluster]\n",
      "(RayTrainWorker pid=1282469) [W reducer.cpp:1298] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator()) [repeated 9x across cluster]\n",
      "(RayTrainWorker pid=1282467) 2023-03-29 19:47:24.645678: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA [repeated 3x across cluster]\n",
      "(RayTrainWorker pid=1282467) To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. [repeated 3x across cluster]\n",
      "(RayTrainWorker pid=1281903) 2023-03-29 19:47:21.849202: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`. [repeated 2x across cluster]\n",
      "(RayTrainWorker pid=1281444) Traceback (most recent call last):\n",
      "(RayTrainWorker pid=1281444)   File \"/home/ray/anaconda3/lib/python3.7/multiprocessing/resource_sharer.py\", line 142, in _serve\n",
      "(RayTrainWorker pid=1281444)     with self._listener.accept() as conn:\n",
      "(RayTrainWorker pid=1281444)   File \"/home/ray/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 455, in accept\n",
      "(RayTrainWorker pid=1281444)     deliver_challenge(c, self._authkey)\n",
      "(RayTrainWorker pid=1281444)   File \"/home/ray/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 730, in deliver_challenge\n",
      "(RayTrainWorker pid=1281444)     response = connection.recv_bytes(256)        # reject large message\n",
      "(RayTrainWorker pid=1281444)   File \"/home/ray/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "(RayTrainWorker pid=1281444)     buf = self._recv_bytes(maxlength)\n",
      "(RayTrainWorker pid=1281444)   File \"/home/ray/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "(RayTrainWorker pid=1281444)     buf = self._recv(4)\n",
      "(RayTrainWorker pid=1281444)   File \"/home/ray/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 379, in _recv\n",
      "(RayTrainWorker pid=1281444)     chunk = read(handle, remaining)\n",
      "(RayTrainWorker pid=1281444) ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "(RayTrainWorker pid=1281903) 2023-03-29 19:47:23.074369: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64 [repeated 4x across cluster]\n",
      "(RayTrainWorker pid=1281903) 2023-03-29 19:47:23.074385: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly. [repeated 2x across cluster]\n",
      "2023-03-29 19:47:37,426\tWARNING tune.py:185 -- Stop signal received (e.g. via SIGINT/Ctrl+C), ending Ray Tune run. This will try to checkpoint the experiment state one last time. Press CTRL+C (or send SIGINT/SIGKILL/SIGTERM) to skip. \n",
      "(TrainTrainable pid=1293877) 2023-03-29 19:47:38,237\tINFO data_parallel_trainer.py:358 -- GPUs are detected in your Ray cluster, but GPU training is not enabled for this trainer. To enable GPU training, make sure to set `use_gpu` to True in your scaling config.\n",
      "(RayTrainWorker pid=1282468) [W reducer.cpp:1298] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator()) [repeated 2x across cluster]\n",
      "(RayTrainWorker pid=1282467) 2023-03-29 19:47:38,237\tINFO data_parallel_trainer.py:358 -- GPUs are detected in your Ray cluster, but GPU training is not enabled for this trainer. To enable GPU training, make sure to set `use_gpu` to True in your scaling config.\n",
      "(RayTrainWorker pid=1282467) 2023-03-29 19:47:26.795804: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64 [repeated 2x across cluster]\n",
      "(RayTrainWorker pid=1282467) 2023-03-29 19:47:38,237\tINFO data_parallel_trainer.py:358 -- GPUs are detected in your Ray cluster, but GPU training is not enabled for this trainer. To enable GPU training, make sure to set `use_gpu` to True in your scaling config.\n",
      "(LightningTrainer pid=1293877) 2023-03-29 19:47:38,258\tINFO trainable.py:914 -- Restored on 10.0.37.7 from checkpoint: /home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00002_2_layer_1_size=128,layer_2_size=64,lr=0.0012_2023-03-29_19-41-51/checkpoint_tmp848741\n",
      "(LightningTrainer pid=1293877) 2023-03-29 19:47:38,258\tINFO trainable.py:922 -- Current state after restoring: {'_iteration': 4, '_timesteps_total': None, '_time_total': 75.18545436859131, '_episodes_total': None}\n",
      "(LightningTrainer pid=1293877) 2023-03-29 19:47:38,261\tINFO data_parallel_trainer.py:358 -- GPUs are detected in your Ray cluster, but GPU training is not enabled for this trainer. To enable GPU training, make sure to set `use_gpu` to True in your scaling config.\n",
      "(RayTrainWorker pid=1296038) 2023-03-29 19:47:43,296\tINFO config.py:87 -- Setting up process group for: env:// [rank=0, world_size=3]\n",
      "(LightningTrainer pid=1295476) 2023-03-29 19:47:42,215\tINFO data_parallel_trainer.py:358 -- GPUs are detected in your Ray cluster, but GPU training is not enabled for this trainer. To enable GPU training, make sure to set `use_gpu` to True in your scaling config. [repeated 6x across cluster]\n",
      "(LightningTrainer pid=1295476) 2023-03-29 19:47:42,212\tINFO trainable.py:914 -- Restored on 10.0.37.7 from checkpoint: /home/ray/ray_results/tune_mnist_pbt/LightningTrainer_6331d_00005_5_layer_1_size=64,layer_2_size=128,lr=0.0746_2023-03-29_19-42-12/checkpoint_tmp1fdfb2 [repeated 3x across cluster]\n",
      "(LightningTrainer pid=1295476) 2023-03-29 19:47:42,212\tINFO trainable.py:922 -- Current state after restoring: {'_iteration': 4, '_timesteps_total': None, '_time_total': 74.38509392738342, '_episodes_total': None} [repeated 3x across cluster]\n"
     ]
    }
   ],
   "source": [
    "# tune_mnist_asha(num_samples=num_samples)\n",
    "tune_mnist_pbt(num_samples=num_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have more resources available (e.g. a GPU), you can modify the above parameters accordingly.\n",
    "\n",
    "An example output of a run could look like this:\n",
    "\n",
    "```bash\n",
    ":emphasize-lines: 12\n",
    "\n",
    " +------------------------------+------------+-------+----------------+----------------+-----------+-----------+-----------------+----------------------+\n",
    " | Trial name                   | status     | loc   |   layer_1_size |   layer_2_size |        lr |      loss |   mean_accuracy |   training_iteration |\n",
    " |------------------------------+------------+-------+----------------+----------------+-----------+-----------+-----------------+----------------------|\n",
    " | LightningTrainer_85489_00000 | TERMINATED |       |            128 |            128 | 0.001     | 0.108734  |        0.973101 |                   10 |\n",
    " | LightningTrainer_85489_00001 | TERMINATED |       |            128 |            128 | 0.001     | 0.093577  |        0.978639 |                   10 |\n",
    " | LightningTrainer_85489_00002 | TERMINATED |       |            128 |            256 | 0.0008    | 0.0922348 |        0.979299 |                   10 |\n",
    " | LightningTrainer_85489_00003 | TERMINATED |       |             64 |            256 | 0.001     | 0.124648  |        0.973892 |                   10 |\n",
    " | LightningTrainer_85489_00004 | TERMINATED |       |            128 |             64 | 0.001     | 0.101717  |        0.975079 |                   10 |\n",
    " | LightningTrainer_85489_00005 | TERMINATED |       |             64 |             64 | 0.001     | 0.121467  |        0.969146 |                   10 |\n",
    " | LightningTrainer_85489_00006 | TERMINATED |       |            128 |            256 | 0.00064   | 0.053446  |        0.987062 |                   10 |\n",
    " | LightningTrainer_85489_00007 | TERMINATED |       |            128 |            256 | 0.001     | 0.129804  |        0.973497 |                   10 |\n",
    " | LightningTrainer_85489_00008 | TERMINATED |       |             64 |            256 | 0.0285125 | 0.363236  |        0.913867 |                   10 |\n",
    " | LightningTrainer_85489_00009 | TERMINATED |       |             32 |            256 | 0.001     | 0.150946  |        0.964201 |                   10 |\n",
    " +------------------------------+------------+-------+----------------+----------------+-----------+-----------+-----------------+----------------------+\n",
    "```\n",
    "\n",
    "As you can see, each sample ran the full number of 10 iterations.\n",
    "All trials ended with quite good parameter combinations and showed relatively good performances.\n",
    "In some runs, the parameters have been perturbed. And the best configuration even reached a\n",
    "mean validation accuracy of `0.987062`!\n",
    "\n",
    "In summary, AIR LightningTrainer is easy to extend to use with Tune. It only required adding a few lines of code to integrate with Ray Tuner to get great performing parameter configurations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More PyTorch Lightning Examples\n",
    "\n",
    "- {ref}`Use LightningTrainer for Image Classification <lightning_mnist_example>`.\n",
    "- {doc}`/tune/examples/includes/mnist_ptl_mini`:\n",
    "  A minimal example of using [Pytorch Lightning](https://github.com/PyTorchLightning/pytorch-lightning)\n",
    "  to train a MNIST model. This example utilizes the Ray Tune-provided\n",
    "  {ref}`PyTorch Lightning callbacks <tune-integration-pytorch-lightning>`.\n",
    "- {doc}`/tune/examples/includes/mlflow_ptl_example`: Example for using [MLflow](https://github.com/mlflow/mlflow/)\n",
    "  and [Pytorch Lightning](https://github.com/PyTorchLightning/pytorch-lightning) with Ray Tune."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
