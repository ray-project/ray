{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79edf6b3",
   "metadata": {},
   "source": [
    "# Running Tune experiments with Optuna\n",
    "\n",
    "This example demonstrates the usage of Optuna with Ray Tune via `OptunaSearch`, including conditional search spaces and the multi-objective use case.\n",
    "\n",
    "While you may use a scheduler with `OptunaSearch`, e.g. `AsyncHyperBandScheduler`, please note that schedulers may not work correctly with multi-objective optimization, since they often expect a scalar score.\n",
    "\n",
    "Click below to see all the imports we need for this example.\n",
    "You can also launch directly into a Binder instance to run this notebook yourself.\n",
    "Just click on the rocket symbol at the top of the navigation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66036a97",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from typing import Dict, Optional, Any\n",
    "\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune.suggest import ConcurrencyLimiter\n",
    "from ray.tune.schedulers import AsyncHyperBandScheduler\n",
    "from ray.tune.suggest.optuna import OptunaSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4617eca2",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "ray.init(configure_logging=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fcaf0c",
   "metadata": {},
   "source": [
    "Let's start by defining a simple evaluation function.\n",
    "An explicit math formula is queried here for demonstration, yet in practice this is typically a black-box function-- e.g. the performance results after training an ML model.\n",
    "We artificially sleep for a bit (`0.1` seconds) to simulate a long-running ML experiment.\n",
    "This setup assumes that we're running multiple `step`s of an experiment while tuning three hyperparameters,\n",
    "namely `width`, `height`, and `activation`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6626c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(step, width, height, activation):\n",
    "    time.sleep(0.1)\n",
    "    activation_boost = 10 if activation==\"relu\" else 0\n",
    "    return (0.1 + width * step / 100) ** (-1) + height * 0.1 + activation_boost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbf06b7",
   "metadata": {},
   "source": [
    "Next, our ``objective`` function to be optimized takes a Tune ``config``, evaluates the `score` of your experiment in a training loop,\n",
    "and uses `tune.report` to report the `score` back to Tune."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23bb55a2",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "def objective(config):\n",
    "    for step in range(config[\"steps\"]):\n",
    "        score = evaluate(step, config[\"width\"], config[\"height\"], config[\"activation\"])\n",
    "        tune.report(iterations=step, mean_loss=score)\n",
    "              "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0b0171",
   "metadata": {},
   "source": [
    "Next we define a search space. The critical assumption is that the optimal hyperparamters live within this space. Yet, if the space is very large, then those hyperparamters may be difficult to find in a short amount of time.\n",
    "\n",
    "The simplest case is a search space with independent dimensions. In this case, a config dictionary will suffice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532efa23",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_space = {\n",
    "    \"steps\": 100,\n",
    "    \"width\": tune.uniform(0, 20),\n",
    "    \"height\": tune.uniform(-100, 100),\n",
    "    \"activation\": tune.choice([\"relu\", \"tanh\"]),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e7b9d6",
   "metadata": {},
   "source": [
    "While defining the search algorithm, we may choose to provide an initial set of hyperparameters that we believe are especially promising or informative, and\n",
    "pass this information as a helpful starting point for the `OptunaSearch` object.\n",
    "\n",
    "Here we define the Optuna search algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbe8b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "searcher = OptunaSearch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646344b6",
   "metadata": {},
   "source": [
    "We also constrain the the number of concurrent trials to `4` with a `ConcurrencyLimiter`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc082d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "algo = ConcurrencyLimiter(searcher, max_concurrent=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f131794",
   "metadata": {},
   "source": [
    "Lastly, we set the number of samples for this Tune run to `1000`\n",
    "(you can decrease this if it takes too long on your machine)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934f5d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8befc313",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# We override here for our smoke tests.\n",
    "num_samples = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7c54dc",
   "metadata": {},
   "source": [
    "Furthermore, we define a `scheduler` to go along with our algorithm. This is optional, and only to demonstrate that we don't need to compromise other great features of Ray Tune while using Optuna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d670c672",
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = AsyncHyperBandScheduler()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9ea081",
   "metadata": {},
   "source": [
    "Now all that's left is running the experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3108ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis = tune.run(\n",
    "    objective,\n",
    "    search_alg=algo,\n",
    "    scheduler=scheduler,\n",
    "    metric=\"mean_loss\",\n",
    "    mode=\"min\",\n",
    "    num_samples=num_samples,\n",
    "    config=search_space\n",
    ")\n",
    "\n",
    "print(\"Best hyperparameters found were: \", analysis.best_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57b8c3f",
   "metadata": {},
   "source": [
    "While defining the search algorithm, we may choose to provide an initial set of hyperparameters that we believe are especially promising or informative, and\n",
    "pass this information as a helpful starting point for the `OptunaSearch` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728070a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_params = [\n",
    "    {\"width\": 1, \"height\": 2, \"activation\": \"relu\"},\n",
    "    {\"width\": 4, \"height\": 2, \"activation\": \"relu\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9314dab6",
   "metadata": {},
   "source": [
    "Now the `search_alg` built using `OptunaSearch` takes `points_to_evaluate`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae30cbd",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "searcher = OptunaSearch(points_to_evaluate=initial_params)\n",
    "algo = ConcurrencyLimiter(searcher, max_concurrent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67c844b",
   "metadata": {},
   "source": [
    "And run the experiment with initial hyperparameter evaluations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf7f44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis = tune.run(\n",
    "    objective,\n",
    "    search_alg=algo,\n",
    "    metric=\"mean_loss\",\n",
    "    mode=\"min\",\n",
    "    num_samples=num_samples,\n",
    "    config=search_space\n",
    ")\n",
    "\n",
    "print(\"Best hyperparameters found were: \", analysis.best_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbfbca87",
   "metadata": {},
   "source": [
    "Sometimes we may want to build a more complicated search space that has conditional dependencies on other hyperparameters. In this case, we pass a define-by-run function to the `search_alg` argument in `ray.tune()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0f61bb",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "def define_by_run_func(trial) -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"Define-by-run function to create the search space.\n",
    "\n",
    "    Ensure no actual computation takes place here. That should go into\n",
    "    the trainable passed to ``tune.run`` (in this example, that's\n",
    "    ``objective``).\n",
    "\n",
    "    For more information, see https://optuna.readthedocs.io/en/stable\\\n",
    "    /tutorial/10_key_features/002_configurations.html\n",
    "\n",
    "    This function should either return None or a dict with constant values.\n",
    "    \"\"\"\n",
    "\n",
    "    activation = trial.suggest_categorical(\"activation\", [\"relu\", \"tanh\"])\n",
    "\n",
    "    # Define-by-run allows for conditional search spaces.\n",
    "    if activation == \"relu\":\n",
    "        trial.suggest_float(\"width\", 0, 20)\n",
    "        trial.suggest_float(\"height\", -100, 100)\n",
    "    else:\n",
    "        trial.suggest_float(\"width\", -1, 21)\n",
    "        trial.suggest_float(\"height\", -101, 101)\n",
    "        \n",
    "    # Return all constants in a dictionary.\n",
    "    return {\"steps\": 100}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c858e5f1",
   "metadata": {},
   "source": [
    "As before, we create the `search_alg` from `OptunaSearch` and `ConcurrencyLimiter`, this time we define the scope of search via the `space` argument and provide no initialization. We also must specific metric and mode when using `space`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88782cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "searcher = OptunaSearch(space=define_by_run_func, metric=\"mean_loss\", mode=\"min\")\n",
    "algo = ConcurrencyLimiter(searcher, max_concurrent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610fd049",
   "metadata": {},
   "source": [
    "Running the experiment with a define-by-run search space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b043045",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "analysis = tune.run(\n",
    "    objective,\n",
    "    search_alg=algo,\n",
    "    num_samples=num_samples,\n",
    ")\n",
    "\n",
    "print(\"Best hyperparameters for loss found were: \", analysis.get_best_config(\"mean_loss\", \"min\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14eab606",
   "metadata": {},
   "source": [
    "Finally, let's take a look at the multi-objective case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88f8722",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_objective(config):\n",
    "    # Hyperparameters\n",
    "    width, height = config[\"width\"], config[\"height\"]\n",
    "\n",
    "    for step in range(config[\"steps\"]):\n",
    "        # Iterative training function - can be any arbitrary training procedure\n",
    "        intermediate_score = evaluate(step, config[\"width\"], config[\"height\"], config[\"activation\"])\n",
    "        # Feed the score back back to Tune.\n",
    "        tune.report(\n",
    "           iterations=step, loss=intermediate_score, gain=intermediate_score * width\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3eb75b3",
   "metadata": {},
   "source": [
    "We define the `OptunaSearch` object this time with metric and mode as list arguments rather than passing `ray.tune` a single element for each.\n",
    "Inside `tune.run()`, we remove the scheduler because it relies on a single scalar score, rather than the two scores we use here: loss, gain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99db5a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "searcher = OptunaSearch(metric=[\"loss\", \"gain\"], mode=[\"min\", \"max\"])\n",
    "algo = ConcurrencyLimiter(searcher, max_concurrent=4)\n",
    "\n",
    "analysis = tune.run(\n",
    "    multi_objective,\n",
    "    search_alg=algo,\n",
    "    num_samples=num_samples,\n",
    "    config=search_space\n",
    ")\n",
    "\n",
    "print(\"Best hyperparameters for loss found were: \", analysis.get_best_config(\"loss\", \"min\"))\n",
    "print(\"Best hyperparameters for gain found were: \", analysis.get_best_config(\"gain\", \"max\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ada9df",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "orphan": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
