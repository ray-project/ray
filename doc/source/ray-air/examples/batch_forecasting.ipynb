{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16a0b509",
   "metadata": {},
   "source": [
    "# Parallel demand forecasting at scale using Ray Tune"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d5d891",
   "metadata": {},
   "source": [
    "**Batch training and tuning** are common tasks in machine learning use-cases. They require training simple models, on data batches, typcially corresponding to different locations, products, etc. Batch training can take less time to process all the data at once, but only if those batches can run in parallel!\n",
    "\n",
    "This notebook showcases how to conduct batch forecasting with [Prophet](https://github.com/facebook/prophet) and [ARIMA](https://github.com/Nixtla/statsforecast). **Prophet** is a popular open-source library developed by Facebook and designed for automatic forecasting of univariate time series data. **ARIMA** is an older, well-known algorithm for forecasting univariate time series at less fine-grained detail than Prophet.\n",
    "\n",
    "![Batch training diagram](../../data/examples/images/batch-training.svg)\n",
    "\n",
    "For the data, we will use the [NYC Taxi dataset](https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page). This popular tabular dataset contains historical taxi pickups by timestamp and location in NYC.\n",
    "\n",
    "For the training, we will train a separate forecasting model to predict #pickups at each location in NYC at daily level for the next 28 days. Specifically, we will use the `pickup_location_id` column in the dataset to group the dataset into data batches. Then we will conduct an experiment for each location, to find the best either Prophet or ARIMA model, per location."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c2a39a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Contents\n",
    "\n",
    "In this this tutorial, you will learn how to:\n",
    " 1. [Define how to load and prepare Parquet data](#prepare_data2)\n",
    " 2. [Define a Trainable (callable) function](#define_trainable2)\n",
    " 3. [Run batch training and inference with Ray Tune](#run_tune_search2)\n",
    " 4. [Load a model from checkpoint](#load_checkpoint2)\n",
    " 5. [Create a forecast from model restored from checkpoint](#create_prediction2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8033e35",
   "metadata": {},
   "source": [
    "# Walkthrough\n",
    "\n",
    "```{tip}\n",
    "Prerequisite for this notebook: Read the [Key Concepts](tune-60-seconds) page for Ray Tune.\n",
    "```\n",
    "\n",
    "First, let's make sure we have all Python packages we need installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6df3441",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "!pip install -q \"ray[air]\" scikit-learn prophet statsmodels statsforecast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649ba4c9",
   "metadata": {},
   "source": [
    "Next, let's import a few required libraries, including open-source [Ray](https://github.com/ray-project/ray) itself!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42669159",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "num_cpu = os.cpu_count()\n",
    "\n",
    "print(f\"Number of CPUs in this system: {num_cpu}\")\n",
    "from typing import Tuple, List, Union, Optional, Callable\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(f\"numpy: {np.__version__}\")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "import scipy\n",
    "\n",
    "print(f\"scipy: {scipy.__version__}\")\n",
    "import pyarrow\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow.dataset as pds\n",
    "\n",
    "print(f\"pyarrow: {pyarrow.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46adc58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "\n",
    "if ray.is_initialized():\n",
    "    ray.shutdown()\n",
    "ray.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130cc1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ray.cluster_resources())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89928c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import forecasting libraries.\n",
    "import prophet\n",
    "from prophet import Prophet\n",
    "\n",
    "print(f\"prophet: {prophet.__version__}\")\n",
    "\n",
    "import statsforecast\n",
    "from statsforecast import StatsForecast\n",
    "from statsforecast.models import AutoARIMA\n",
    "\n",
    "print(f\"statsforecast: {statsforecast.__version__}\")\n",
    "\n",
    "# Import ray libraries.\n",
    "from ray import air, tune, serve\n",
    "from ray.air import session, ScalingConfig\n",
    "from ray.air.checkpoint import Checkpoint\n",
    "\n",
    "RAY_IGNORE_UNHANDLED_ERRORS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7559a0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For benchmarking purposes, we can print the times of various operations.\n",
    "# In order to reduce clutter in the output, this is set to False by default.\n",
    "PRINT_TIMES = False\n",
    "\n",
    "\n",
    "def print_time(msg: str):\n",
    "    if PRINT_TIMES:\n",
    "        print(msg)\n",
    "\n",
    "\n",
    "# To speed things up, weâ€™ll only use a small subset of the full dataset consisting of two last months of 2019.\n",
    "# You can choose to use the full dataset for 2018-2019 by setting the SMOKE_TEST variable to False.\n",
    "SMOKE_TEST = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e47315",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Define how to load and prepare Parquet data <a class=\"anchor\" id=\"prepare_data2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d29ea0",
   "metadata": {},
   "source": [
    "First, we need to load some data. Since the NYC Taxi dataset is fairly large, we will filter files first into a PyArrow dataset. And then in the next cell after, we will filter the data on read into a PyArrow table and convert that to a pandas dataframe.\n",
    "\n",
    "```{tip}\n",
    "Use PyArrow dataset and table for reading or writing large parquet files, since its native multithreaded C++ adapter is faster than pandas read_parquet, even using engine=pyarrow.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb48598a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some global variables.\n",
    "TARGET = \"y\"\n",
    "FORECAST_LENGTH = 28\n",
    "MAX_DATE = datetime(2019, 6, 30)\n",
    "s3_partitions = pds.dataset(\n",
    "    \"s3://anonymous@air-example-data/ursa-labs-taxi-data/by_year/\",\n",
    "    partitioning=[\"year\", \"month\"],\n",
    ")\n",
    "s3_files = [f\"s3://anonymous@{file}\" for file in s3_partitions.files]\n",
    "\n",
    "# Obtain all location IDs\n",
    "all_location_ids = (\n",
    "    pq.read_table(s3_files[0], columns=[\"pickup_location_id\"])[\"pickup_location_id\"]\n",
    "    .unique()\n",
    "    .to_pylist()\n",
    ")\n",
    "# drop [264, 265, 199]\n",
    "all_location_ids.remove(264)\n",
    "all_location_ids.remove(265)\n",
    "all_location_ids.remove(199)\n",
    "\n",
    "# Use smoke testing or not.\n",
    "starting_idx = -2 if SMOKE_TEST else 0\n",
    "# TODO: drop location 199 to test error-handling before final git checkin\n",
    "sample_locations = [141, 229, 173] if SMOKE_TEST else all_location_ids\n",
    "\n",
    "# Display what data will be used.\n",
    "s3_files = s3_files[starting_idx:]\n",
    "print(f\"NYC Taxi using {len(s3_files)} file(s)!\")\n",
    "print(f\"s3_files: {s3_files}\")\n",
    "print(f\"Locations: {sample_locations}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736fcb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "############\n",
    "# STEP 1.  Define Python functions to\n",
    "#          a) read and prepare a segment of data, and\n",
    "############\n",
    "\n",
    "# Function to read a pyarrow.Table object using pyarrow parquet\n",
    "def read_data(file: str, sample_id: np.int32) -> pd.DataFrame:\n",
    "\n",
    "    # parse out min expected date\n",
    "    part_zero = \"s3://anonymous@air-example-data/ursa-labs-taxi-data/by_year/\"\n",
    "    split_text = file.split(part_zero)[1]\n",
    "    min_year = split_text.split(\"/\")[0]\n",
    "    min_month = split_text.split(\"/\")[1]\n",
    "    string_date = min_year + \"-\" + min_month + \"-\" + \"01\" + \" 00:00:00\"\n",
    "    min_date = datetime.strptime(string_date, \"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    df = pq.read_table(\n",
    "        file,\n",
    "        filters=[\n",
    "            (\"pickup_at\", \">\", min_date),\n",
    "            (\"pickup_at\", \"<=\", MAX_DATE),\n",
    "            (\"passenger_count\", \">\", 0),\n",
    "            (\"trip_distance\", \">\", 0),\n",
    "            (\"fare_amount\", \">\", 0),\n",
    "            (\"pickup_location_id\", \"not in\", [264, 265]),\n",
    "            (\"dropoff_location_id\", \"not in\", [264, 265]),\n",
    "            (\"pickup_location_id\", \"=\", sample_id),\n",
    "        ],\n",
    "        columns=[\n",
    "            \"pickup_at\",\n",
    "            \"dropoff_at\",\n",
    "            \"pickup_location_id\",\n",
    "            \"dropoff_location_id\",\n",
    "            \"passenger_count\",\n",
    "            \"trip_distance\",\n",
    "            \"fare_amount\",\n",
    "        ],\n",
    "    ).to_pandas()\n",
    "    return df\n",
    "\n",
    "\n",
    "# Function to transform a pandas dataframe\n",
    "def transform_df(input_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = input_df.copy()\n",
    "\n",
    "    # calculate trip_duration\n",
    "    df[\"trip_duration\"] = (df[\"dropoff_at\"] - df[\"pickup_at\"]).dt.seconds\n",
    "    # filter trip_durations > 1 minute and less than 24 hours\n",
    "    df = df[df[\"trip_duration\"] > 60]\n",
    "    df = df[df[\"trip_duration\"] < 24 * 60 * 60]\n",
    "\n",
    "    # Prophet requires timstamp is 'ds' and target_value name is 'y'\n",
    "    # Prophet requires at least 2 data points per timestamp\n",
    "    # StatsForecast requires location name is 'unique_id'\n",
    "\n",
    "    # add year_month_day and concat into a unique column to use as groupby key\n",
    "    df[\"ds\"] = df[\"pickup_at\"].dt.to_period(\"D\").dt.to_timestamp()\n",
    "    df[\"loc_year_month_day\"] = (\n",
    "        df[\"pickup_location_id\"].astype(str)\n",
    "        + \"_\"\n",
    "        + df[\"pickup_at\"].dt.year.astype(str)\n",
    "        + \"_\"\n",
    "        + df[\"pickup_at\"].dt.month.astype(str)\n",
    "        + \"_\"\n",
    "        + df[\"pickup_at\"].dt.day.astype(str)\n",
    "    )\n",
    "    # add target_value quantity for groupby count later\n",
    "    df[\"y\"] = 1\n",
    "    # rename pickup_location_id to unique_id\n",
    "    df.rename(columns={\"pickup_location_id\": \"unique_id\"}, inplace=True)\n",
    "    # keep only necessary columns\n",
    "    df = df[[\"loc_year_month_day\", \"unique_id\", \"ds\", \"y\"]].copy()\n",
    "\n",
    "    # groupby aggregregate\n",
    "    g = df.groupby(\"loc_year_month_day\").agg({\"unique_id\": min, \"ds\": min, \"y\": sum})\n",
    "    # having num rows in group > 2\n",
    "    g.dropna(inplace=True)\n",
    "    g = g[g[\"y\"] > 2].copy()\n",
    "\n",
    "    # Drop groupby variable since we do not need it anymore\n",
    "    g.reset_index(inplace=True)\n",
    "    g.drop([\"loc_year_month_day\"], axis=1, inplace=True)\n",
    "\n",
    "    return g\n",
    "\n",
    "\n",
    "def prepare_data(sample_location_id: np.int32) -> pd.DataFrame:\n",
    "\n",
    "    # Load data.\n",
    "    df_list = [read_data(f, sample_location_id) for f in s3_files]\n",
    "    df_raw = pd.concat(df_list, ignore_index=True)\n",
    "    # Abort Tune to avoid Tune Error if df has too few rows\n",
    "    if df_raw.shape[0] < FORECAST_LENGTH:\n",
    "        print_time(f\"Location {sample_location_id} has only {df_raw.shape[0]} rows\")\n",
    "        session.report(dict(error=None))\n",
    "        return None\n",
    "\n",
    "    # Transform data.\n",
    "    df = transform_df(df_raw)\n",
    "    # Abort Tune to avoid Tune Error if df has too few rows\n",
    "    if df.shape[0] < FORECAST_LENGTH:\n",
    "        print_time(f\"Location {sample_location_id} has only {df.shape[0]} rows\")\n",
    "        session.report(dict(error=None))\n",
    "        return None\n",
    "    else:\n",
    "        df.sort_values(by=\"ds\", inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2358421",
   "metadata": {},
   "source": [
    "## Define a Trainable (callable) function <a class=\"anchor\" id=\"define_trainable2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94eb28a7",
   "metadata": {},
   "source": [
    "Next, we define a trainable function, called `train_model()`, in order to train and evaluate a model on a data partition. This function will be called *in parallel for every permutation* in the Tune search space! \n",
    "\n",
    "Inside this trainable function:\n",
    "- ðŸ“– The input must include a `config` argument. \n",
    "- ðŸ“ˆ Inside the function, the tuning metric (a model's loss or error) must be calculated and reported using `session.report()`.\n",
    "- âœ”ï¸ Optionally [checkpoint](air-checkpoints-doc) (save) the model for fault tolerance and easy deployment later.\n",
    "\n",
    "```{tip}\n",
    "Ray Tune has two ways of [defining a trainable](tune_60_seconds_trainables), namely the Function API and the Class API. Both are valid ways of defining a trainable, but *the Function API is generally recommended*.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e044119a",
   "metadata": {},
   "outputs": [],
   "source": [
    "############\n",
    "# STEP 1.  Define Python functions to\n",
    "#          b) train and evaluate a model on a segment of data.\n",
    "############\n",
    "\n",
    "\n",
    "def evaluate_model_prophet(\n",
    "    model: \"prophet.forecaster.Prophet\",\n",
    ") -> Tuple[float, pd.DataFrame]:\n",
    "\n",
    "    # Inference model using FORECAST_LENGTH.\n",
    "    future_dates = model.make_future_dataframe(periods=FORECAST_LENGTH, freq=\"D\")\n",
    "    future = model.predict(future_dates)\n",
    "\n",
    "    # Calculate mean absolute forecast error.\n",
    "    temp = future.copy()\n",
    "    temp[\"forecast_error\"] = np.abs(temp[\"yhat\"] - temp[\"trend\"])\n",
    "    error = np.mean(temp[\"forecast_error\"])\n",
    "\n",
    "    return error, future\n",
    "\n",
    "\n",
    "def evaluate_model_statsforecast(\n",
    "    model: \"statsforecast.models.AutoARIMA\", test_df: pd.DataFrame\n",
    ") -> Tuple[float, pd.DataFrame]:\n",
    "\n",
    "    # Inference model using test data.\n",
    "    forecast = model.forecast(FORECAST_LENGTH + 1).reset_index()\n",
    "    forecast.set_index([\"ds\"], inplace=True)\n",
    "    test_df.set_index(\"ds\", inplace=True)\n",
    "    future = pd.concat([test_df, forecast[[\"AutoARIMA\"]]], axis=1)\n",
    "    future.dropna(inplace=True)\n",
    "    future.columns = [\"unique_id\", \"trend\", \"yhat\"]\n",
    "\n",
    "    # Calculate mean absolute forecast error.\n",
    "    temp = future.copy()\n",
    "    temp[\"forecast_error\"] = np.abs(temp[\"yhat\"] - temp[\"trend\"])\n",
    "    error = np.mean(temp[\"forecast_error\"])\n",
    "\n",
    "    return error, future\n",
    "\n",
    "\n",
    "# 2. Define a custom train function\n",
    "def train_model(config: dict) -> None:\n",
    "\n",
    "    # Get Tune parameters\n",
    "    sample_location_id = config[\"params\"][\"location\"]\n",
    "    model_type = config[\"params\"][\"algorithm\"]\n",
    "\n",
    "    # Define Prophet model with 75% confidence interval\n",
    "    if model_type == \"prophet_additive\":\n",
    "        model = Prophet(interval_width=0.75, seasonality_mode=\"additive\")\n",
    "    elif model_type == \"prophet_multiplicative\":\n",
    "        model = Prophet(interval_width=0.75, seasonality_mode=\"multiplicative\")\n",
    "\n",
    "    # Define ARIMA model with daily frequency which implies seasonality = 7\n",
    "    elif model_type == \"arima\":\n",
    "        model = [AutoARIMA(season_length=7, approximation=True)]\n",
    "\n",
    "    # Read and transform data.\n",
    "    df = prepare_data(sample_location_id)\n",
    "\n",
    "    # Train model.\n",
    "    if model_type == \"arima\":\n",
    "\n",
    "        try:\n",
    "            # split data into train, test.\n",
    "            train_end = df.ds.max() - timedelta(days=FORECAST_LENGTH + 1)\n",
    "            train_df = df.loc[(df.ds <= train_end), :].copy()\n",
    "            test_df = df.iloc[-FORECAST_LENGTH:, :].copy()\n",
    "\n",
    "            # fit AutoARIMA.\n",
    "            model = StatsForecast(df=train_df, models=model, freq=\"D\")\n",
    "\n",
    "            # Inference model and evaluate error.\n",
    "            error, future = evaluate_model_statsforecast(model, test_df)\n",
    "        except:\n",
    "            print(f\"ARIMA error processing location: {sample_location_id}\")\n",
    "\n",
    "    else:  # model type is Prophet\n",
    "        try:\n",
    "            # fit Prophet.\n",
    "            model = model.fit(df[[\"ds\", \"y\"]])\n",
    "\n",
    "            # Inference model and evaluate error.\n",
    "            error, future = evaluate_model_prophet(model)\n",
    "        except:\n",
    "            print(f\"Prophet error processing location: {sample_location_id}\")\n",
    "\n",
    "    # Define a model checkpoint using AIR API.\n",
    "    # https://docs.ray.io/en/latest/tune/tutorials/tune-checkpoints.html\n",
    "    checkpoint = ray.air.checkpoint.Checkpoint.from_dict(\n",
    "        {\n",
    "            \"model\": model,\n",
    "            \"forecast_df\": future,\n",
    "            \"location_id\": sample_location_id,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Save checkpoint and report back metrics, using ray.air.session.report()\n",
    "    # The metrics you specify here will appear in Tune summary table.\n",
    "    # They will also be recorded in Tune results under `metrics`.\n",
    "    metrics = dict(error=error)\n",
    "    session.report(metrics, checkpoint=checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139008ea",
   "metadata": {},
   "source": [
    "## Run batch training on Ray Tune <a class=\"anchor\" id=\"run_tune_search2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100f5e98",
   "metadata": {},
   "source": [
    "**Recall what we are doing, high level, is training several different models per pickup location.** We are using Ray Tune so we can *run all these trials in parallel* on a Ray cluster. At the end, we will inspect the results of the experiment and deploy only the best model per pickup location.\n",
    "\n",
    "**Step 1. Define Python functions to read and prepare a segment of data and train and evaluate one or many models per segment of data**.  We already did this, above.\n",
    "\n",
    "**Step 2. Scaling**:\n",
    "Below, we specify training resources in a `ray.air.ScalingConfig` object inside the Tune search space.  For more information about configuring resource allocations, see [A Guide To Parallelism and Resources](tune-parallelism). \n",
    "\n",
    "**Step 3. Search Space**:\n",
    "Below, we define our [Tune search space](tune-key-concepts-search-spaces), which consists of:\n",
    "- Different algorithms, either:\n",
    "  - Prophet with [multiplicative or additive](https://facebook.github.io/prophet/docs/multiplicative_seasonality.html) seasonal effects \n",
    "  - [AutoARIMA](https://github.com/Nixtla/statsforecast).\n",
    "- NYC taxi pick-up locations.\n",
    "- Scaling config\n",
    "\n",
    "**Step 4. Search Algorithm or Strategy**:\n",
    "Below, our Tune jobs will be defined using a search space and simple grid search. \n",
    "> The typical use case for Tune search spaces is for hyperparameter tuning. In our case, we are defining the Tune search space in order to run distributed tuning jobs automatically.  Each training job will use a different data partition (taxi pickup location), different algorithm, and the compute resources we defined in the Scaling config.\n",
    "\n",
    "**Step 5. Now we are ready to kick off a Ray Tune experiment!** \n",
    "- Define a `tuner` object.\n",
    "- Put the training function `train_model()` inside the `tuner` object.\n",
    "- Run the experiment using `tuner.fit()`.\n",
    "\n",
    "ðŸ’¡ After you run the cell below, right-click on it and choose \"Enable Scrolling for Outputs\"! This will make it easier to view, since tuning output can be very long!\n",
    "\n",
    "**Setting SMOKE_TEST=False, running on Anyscale: 771 models, using 18 NYC Taxi S3 files dating from 2018/01 to 2019/06 (split into partitions approx 1GiB each), were simultaneously trained on a 7-node AWS cluster of m5.4xlarges, within 40 minutes.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef7c6d6",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "############\n",
    "# STEP 2. Customize distributed compute scaling.\n",
    "############\n",
    "num_training_workers = min(num_cpu - 2, 32)\n",
    "scaling_config = ScalingConfig(\n",
    "    # Number of distributed workers.\n",
    "    num_workers=num_training_workers,\n",
    "    # Turn on/off GPU.\n",
    "    use_gpu=False,\n",
    "    # Specify resources used for trainer.\n",
    "    trainer_resources={\"CPU\": 1},\n",
    "    # Try to schedule workers on different nodes.\n",
    "    placement_strategy=\"SPREAD\",\n",
    ")\n",
    "\n",
    "############\n",
    "# STEP 3. Define a search space dict of all config parameters.\n",
    "############\n",
    "SEARCH_SPACE = {\n",
    "    \"scaling_config\": scaling_config,\n",
    "    \"params\": {\n",
    "        \"algorithm\": tune.grid_search(\n",
    "            [\"prophet_additive\", \"prophet_multiplicative\", \"arima\"]\n",
    "        ),\n",
    "        \"location\": tune.grid_search(sample_locations),\n",
    "    },\n",
    "}\n",
    "\n",
    "# Optional STEP 4. Specify the hyperparameter tuning search strategy.\n",
    "\n",
    "############\n",
    "# STEP 5. Run the experiment with Ray AIR APIs.\n",
    "# https://docs.ray.io/en/latest/ray-air/examples/huggingface_text_classification.html\n",
    "############\n",
    "start = time.time()\n",
    "\n",
    "# Define a tuner object.\n",
    "tuner = tune.Tuner(\n",
    "    train_model,\n",
    "    param_space=SEARCH_SPACE,\n",
    "    tune_config=tune.TuneConfig(\n",
    "        metric=\"error\",\n",
    "        mode=\"min\",\n",
    "    ),\n",
    "    run_config=air.RunConfig(\n",
    "        # Redirect logs to relative path instead of default ~/ray_results/.\n",
    "        storage_path=\"my_Tune_logs\",\n",
    "        # Specify name to make logs easier to find in log path.\n",
    "        name=\"ptf_nyc\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Fit the tuner object.\n",
    "results = tuner.fit()\n",
    "\n",
    "total_time_taken = time.time() - start\n",
    "print(f\"Total number of models: {len(results)}\")\n",
    "print(f\"TOTAL TIME TAKEN: {total_time_taken/60:.2f} minutes\")\n",
    "\n",
    "# Total number of models: 771\n",
    "# TOTAL TIME TAKEN: 44.64 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cafe75d",
   "metadata": {},
   "source": [
    "## Load a model from checkpoint  <a class=\"anchor\" id=\"load_checkpoint2\"></a>\n",
    "\n",
    "- After the Tune experiment has finished, we can assemble the Tune {doc}`ResultGrid </tune/examples/tune_analyze_results>` object into a pandas dataframe.\n",
    "\n",
    "- Next, we'll sort the pandas dataframe by pickuplocation and error, and keep only the best model with minimum error per pickup location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ceeb770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a list of training loss errors\n",
    "errors = [i.metrics.get(\"error\", 10000.0) for i in results]\n",
    "\n",
    "# get a list of checkpoints\n",
    "checkpoints = [i.checkpoint for i in results]\n",
    "\n",
    "# get a list of locations\n",
    "locations = [i.config[\"params\"][\"location\"] for i in results]\n",
    "\n",
    "# get a list of model params\n",
    "algorithm = [i.config[\"params\"][\"algorithm\"] for i in results]\n",
    "\n",
    "# Assemble a pandas dataframe from Tune results\n",
    "results_df = pd.DataFrame(\n",
    "    zip(locations, errors, algorithm, checkpoints),\n",
    "    columns=[\"location_id\", \"error\", \"algorithm\", \"checkpoint\"],\n",
    ")\n",
    "results_df.head(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0191ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only 1 model per location_id with minimum error\n",
    "final_df = results_df.copy()\n",
    "final_df = final_df.loc[(final_df.error > 0), :]\n",
    "final_df = final_df.loc[final_df.groupby(\"location_id\")[\"error\"].idxmin()]\n",
    "final_df.sort_values(by=[\"error\"], inplace=True)\n",
    "final_df.set_index(\"location_id\", inplace=True, drop=True)\n",
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4aa9e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df[[\"algorithm\"]].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec846be",
   "metadata": {},
   "source": [
    "## Create a forecast from model restored from checkpoint <a class=\"anchor\" id=\"create_prediction2\"></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23cf01e",
   "metadata": {},
   "source": [
    "Finally, we will restore the best and worst models from checkpoint, generate predictions, and inspect the forecasts. \n",
    "\n",
    "Prophet includes a convenient plot library which displays actual data along with backtest predictions and confidence intervals and future forecasts. With ARIMA, you have to create a prediciton manually.\n",
    "\n",
    "- We will easily obtain AIR Checkpoint objects from the Tune results. \n",
    "- We will restore a Prophet or ARIMA model directly from checkpoint, and demonstrate it can be used for prediction.\n",
    "\n",
    "```{tip}\n",
    "[Ray AIR Predictors](air-predictors) make batch inference easy since they have internal logic to parallelize the inference.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c194870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the pickup location for the best model\n",
    "if SMOKE_TEST:\n",
    "    sample_location_id = final_df.index[0]\n",
    "else:\n",
    "    sample_location_id = final_df.index[120]\n",
    "\n",
    "# Get the algorithm used\n",
    "sample_algorithm = final_df.loc[[sample_location_id]].algorithm.values[0]\n",
    "\n",
    "# Get a checkpoint directly from the pandas dataframe of Tune results\n",
    "checkpoint = final_df.checkpoint[sample_location_id]\n",
    "print(f\"checkpoint type:: {type(checkpoint)}\")\n",
    "\n",
    "# Restore a model from checkpoint\n",
    "sample_model = checkpoint.to_dict()[\"model\"]\n",
    "\n",
    "# Prophet .fit() performs inference + prediction.\n",
    "# Arima train only performs inference; prediction is an extra step.\n",
    "if sample_algorithm == \"arima\":\n",
    "    prediction = (\n",
    "        sample_model.forecast(2 * (FORECAST_LENGTH + 1)).reset_index().set_index(\"ds\")\n",
    "    )\n",
    "    prediction[\"trend\"] = None\n",
    "    prediction.rename(columns={\"AutoARIMA\": \"yhat\"}, inplace=True)\n",
    "    prediction = prediction.tail(FORECAST_LENGTH + 1)\n",
    "\n",
    "# Restore already-created predictions from model training and eval\n",
    "forecast_df = checkpoint.to_dict()[\"forecast_df\"]\n",
    "\n",
    "# Print pickup location ID, algorithm used, and model validation error.\n",
    "sample_error = final_df.loc[[sample_location_id]].error.values[0]\n",
    "print(\n",
    "    f\"location {sample_location_id}, algorithm {sample_algorithm}, best error {sample_error}\"\n",
    ")\n",
    "\n",
    "# Plot forecast prediction using best model for this pickup location ID.\n",
    "# If prophet model, use prophet built-in plot\n",
    "if sample_algorithm == \"arima\":\n",
    "    forecast_df[[\"trend\", \"yhat\"]].plot()\n",
    "else:\n",
    "    plot1 = sample_model.plot(forecast_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3638844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the pickup location for the worst model\n",
    "sample_location_id = final_df.index[len(final_df) - 2]\n",
    "\n",
    "# Get the algorithm used\n",
    "sample_algorithm = final_df.loc[[sample_location_id]].algorithm.values[0]\n",
    "\n",
    "# Get a checkpoint directly from the pandas dataframe of Tune results\n",
    "checkpoint = final_df.checkpoint[sample_location_id]\n",
    "print(f\"checkpoint type:: {type(checkpoint)}\")\n",
    "\n",
    "# Restore a model from checkpoint\n",
    "sample_model = checkpoint.to_dict()[\"model\"]\n",
    "\n",
    "# Prophet .fit() performs inference + prediction.\n",
    "# Arima train only performs inference; prediction is an extra step.\n",
    "if sample_algorithm == \"arima\":\n",
    "    prediction = (\n",
    "        sample_model.forecast(2 * (FORECAST_LENGTH + 1)).reset_index().set_index(\"ds\")\n",
    "    )\n",
    "    prediction[\"trend\"] = None\n",
    "    prediction.rename(columns={\"AutoARIMA\": \"yhat\"}, inplace=True)\n",
    "    prediction = prediction.tail(FORECAST_LENGTH + 1)\n",
    "\n",
    "# Restore already-created inferences from model training and eval\n",
    "forecast_df = checkpoint.to_dict()[\"forecast_df\"]\n",
    "\n",
    "# Append the prediction to the inferences\n",
    "forecast_df = pd.concat([forecast_df, prediction])\n",
    "\n",
    "# Print pickup location ID, algorithm used, and model validation error.\n",
    "sample_error = final_df.loc[[sample_location_id]].error.values[0]\n",
    "print(\n",
    "    f\"location {sample_location_id}, algorithm {sample_algorithm}, best error {sample_error}\"\n",
    ")\n",
    "\n",
    "# Plot forecast prediction using best model for this pickup location ID.\n",
    "if sample_algorithm == \"arima\":\n",
    "    forecast_df[[\"trend\", \"yhat\"]].plot()\n",
    "else:\n",
    "    plot1 = sample_model.plot(forecast_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ffaf781",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
