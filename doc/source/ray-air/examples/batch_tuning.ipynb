{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d44abca1",
   "metadata": {},
   "source": [
    "# Batch training & tuning on Ray Tune"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060f506a",
   "metadata": {},
   "source": [
    "**Batch training and tuning** are common tasks in simple machine learning use-cases such as time series forecasting. They require fitting of simple models on data batches corresponding to different locations, products, etc. Batch training can take less time to process all the data at once, but only if those batches can run in parallel!\n",
    "\n",
    "This notebook showcases how to conduct batch regression with algorithms from [XGBoost](https://docs.ray.io/en/latest/tune/examples/tune-xgboost.html) and [Scikit-learn](https://docs.ray.io/en/latest/ray-more-libs/joblib.html) with **[Ray Tune](https://docs.ray.io/en/latest/tune/index.html)**. **XGBoost** is a popular open-source library used for regression and classification. **Scikit-learn** is a popular open-source library with a vast assortment of well-known ML algorithms.\n",
    "\n",
    "![Batch training diagram](../../data/examples/images/batch-training.svg)\n",
    "\n",
    "For the data, we will use the [NYC Taxi dataset](https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page). This popular tabular dataset contains historical taxi pickups by timestamp and location in NYC.\n",
    "\n",
    "For the training, we will train separate regression models to predict `trip_duration`, with a different model for each dropoff location in NYC. Specifically, we will conduct an experiment for each `dropoff_location_id`, to find the best either XGBoost or Scikit-learn model, per location."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fde5937",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Contents\n",
    "\n",
    "In this this tutorial, you will learn how to:\n",
    " 1. [Define how to load and prepare Parquet data](#prepare_data)\n",
    " 2. [Define a Trainable (callable) function](#define_trainable)\n",
    " 3. [Run batch training and inference with Ray Tune](#run_tune_search)\n",
    " 4. [Load a model from checkpoint and perform batch prediction](#load_checkpoint)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3247a583",
   "metadata": {},
   "source": [
    "# Walkthrough\n",
    "\n",
    "```{tip}\n",
    "Prerequisite for this notebook: Read the [Key Concepts](https://docs.ray.io/en/latest/tune/key-concepts.html) page for Ray Tune.\n",
    "```\n",
    "\n",
    "Let us start by importing a few required libraries, including open-source [Ray](https://github.com/ray-project/ray) itself!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc7abddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of CPUs in this system: 8\n",
      "numpy: 1.21.6\n",
      "pyarrow: 10.0.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "print(f\"Number of CPUs in this system: {os.cpu_count()}\")\n",
    "from typing import Tuple, List, Union, Optional, Callable\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(f\"numpy: {np.__version__}\")\n",
    "import pyarrow\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow.dataset as pds\n",
    "\n",
    "print(f\"pyarrow: {pyarrow.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4257313e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-10 15:02:42,767\tINFO worker.py:1230 -- Using address localhost:9031 set in the environment variable RAY_ADDRESS\n",
      "2023-01-10 15:02:42,768\tINFO worker.py:1352 -- Connecting to existing Ray cluster at address: 172.31.214.212:9031...\n",
      "2023-01-10 15:02:42,803\tINFO worker.py:1529 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32mhttps://console.anyscale-staging.com/api/v2/sessions/ses_b5q8xHd42BTdukSgFqTxejLT/services?redirect_to=dashboard \u001b[39m\u001b[22m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "    <div style=\"margin-left: 50px;display: flex;flex-direction: row;align-items: center\">\n",
       "        <h3 style=\"color: var(--jp-ui-font-color0)\">Ray</h3>\n",
       "        <svg version=\"1.1\" id=\"ray\" width=\"3em\" viewBox=\"0 0 144.5 144.6\" style=\"margin-left: 3em;margin-right: 3em\">\n",
       "            <g id=\"layer-1\">\n",
       "                <path fill=\"#00a2e9\" class=\"st0\" d=\"M97.3,77.2c-3.8-1.1-6.2,0.9-8.3,5.1c-3.5,6.8-9.9,9.9-17.4,9.6S58,88.1,54.8,81.2c-1.4-3-3-4-6.3-4.1\n",
       "                    c-5.6-0.1-9.9,0.1-13.1,6.4c-3.8,7.6-13.6,10.2-21.8,7.6C5.2,88.4-0.4,80.5,0,71.7c0.1-8.4,5.7-15.8,13.8-18.2\n",
       "                    c8.4-2.6,17.5,0.7,22.3,8c1.3,1.9,1.3,5.2,3.6,5.6c3.9,0.6,8,0.2,12,0.2c1.8,0,1.9-1.6,2.4-2.8c3.5-7.8,9.7-11.8,18-11.9\n",
       "                    c8.2-0.1,14.4,3.9,17.8,11.4c1.3,2.8,2.9,3.6,5.7,3.3c1-0.1,2,0.1,3,0c2.8-0.5,6.4,1.7,8.1-2.7s-2.3-5.5-4.1-7.5\n",
       "                    c-5.1-5.7-10.9-10.8-16.1-16.3C84,38,81.9,37.1,78,38.3C66.7,42,56.2,35.7,53,24.1C50.3,14,57.3,2.8,67.7,0.5\n",
       "                    C78.4-2,89,4.7,91.5,15.3c0.1,0.3,0.1,0.5,0.2,0.8c0.7,3.4,0.7,6.9-0.8,9.8c-1.7,3.2-0.8,5,1.5,7.2c6.7,6.5,13.3,13,19.8,19.7\n",
       "                    c1.8,1.8,3,2.1,5.5,1.2c9.1-3.4,17.9-0.6,23.4,7c4.8,6.9,4.6,16.1-0.4,22.9c-5.4,7.2-14.2,9.9-23.1,6.5c-2.3-0.9-3.5-0.6-5.1,1.1\n",
       "                    c-6.7,6.9-13.6,13.7-20.5,20.4c-1.8,1.8-2.5,3.2-1.4,5.9c3.5,8.7,0.3,18.6-7.7,23.6c-7.9,5-18.2,3.8-24.8-2.9\n",
       "                    c-6.4-6.4-7.4-16.2-2.5-24.3c4.9-7.8,14.5-11,23.1-7.8c3,1.1,4.7,0.5,6.9-1.7C91.7,98.4,98,92.3,104.2,86c1.6-1.6,4.1-2.7,2.6-6.2\n",
       "                    c-1.4-3.3-3.8-2.5-6.2-2.6C99.8,77.2,98.9,77.2,97.3,77.2z M72.1,29.7c5.5,0.1,9.9-4.3,10-9.8c0-0.1,0-0.2,0-0.3\n",
       "                    C81.8,14,77,9.8,71.5,10.2c-5,0.3-9,4.2-9.3,9.2c-0.2,5.5,4,10.1,9.5,10.3C71.8,29.7,72,29.7,72.1,29.7z M72.3,62.3\n",
       "                    c-5.4-0.1-9.9,4.2-10.1,9.7c0,0.2,0,0.3,0,0.5c0.2,5.4,4.5,9.7,9.9,10c5.1,0.1,9.9-4.7,10.1-9.8c0.2-5.5-4-10-9.5-10.3\n",
       "                    C72.6,62.3,72.4,62.3,72.3,62.3z M115,72.5c0.1,5.4,4.5,9.7,9.8,9.9c5.6-0.2,10-4.8,10-10.4c-0.2-5.4-4.6-9.7-10-9.7\n",
       "                    c-5.3-0.1-9.8,4.2-9.9,9.5C115,72.1,115,72.3,115,72.5z M19.5,62.3c-5.4,0.1-9.8,4.4-10,9.8c-0.1,5.1,5.2,10.4,10.2,10.3\n",
       "                    c5.6-0.2,10-4.9,9.8-10.5c-0.1-5.4-4.5-9.7-9.9-9.6C19.6,62.3,19.5,62.3,19.5,62.3z M71.8,134.6c5.9,0.2,10.3-3.9,10.4-9.6\n",
       "                    c0.5-5.5-3.6-10.4-9.1-10.8c-5.5-0.5-10.4,3.6-10.8,9.1c0,0.5,0,0.9,0,1.4c-0.2,5.3,4,9.8,9.3,10\n",
       "                    C71.6,134.6,71.7,134.6,71.8,134.6z\"/>\n",
       "            </g>\n",
       "        </svg>\n",
       "        <table>\n",
       "            <tr>\n",
       "                <td style=\"text-align: left\"><b>Python version:</b></td>\n",
       "                <td style=\"text-align: left\"><b>3.8.13</b></td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                <td style=\"text-align: left\"><b>Ray version:</b></td>\n",
       "                <td style=\"text-align: left\"><b> 2.2.0</b></td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "    <td style=\"text-align: left\"><b>Dashboard:</b></td>\n",
       "    <td style=\"text-align: left\"><b><a href=\"http://console.anyscale-staging.com/api/v2/sessions/ses_b5q8xHd42BTdukSgFqTxejLT/services?redirect_to=dashboard\" target=\"_blank\">http://console.anyscale-staging.com/api/v2/sessions/ses_b5q8xHd42BTdukSgFqTxejLT/services?redirect_to=dashboard</a></b></td>\n",
       "</tr>\n",
       "\n",
       "        </table>\n",
       "    </div>\n",
       "</div>\n"
      ],
      "text/plain": [
       "RayContext(dashboard_url='console.anyscale-staging.com/api/v2/sessions/ses_b5q8xHd42BTdukSgFqTxejLT/services?redirect_to=dashboard', python_version='3.8.13', ray_version='2.2.0', ray_commit='b6af0887ee5f2e460202133791ad941a41f15beb', address_info={'node_ip_address': '172.31.214.212', 'raylet_ip_address': '172.31.214.212', 'redis_address': None, 'object_store_address': '/tmp/ray/session_2023-01-10_14-35-51_385880_159/sockets/plasma_store', 'raylet_socket_name': '/tmp/ray/session_2023-01-10_14-35-51_385880_159/sockets/raylet', 'webui_url': 'console.anyscale-staging.com/api/v2/sessions/ses_b5q8xHd42BTdukSgFqTxejLT/services?redirect_to=dashboard', 'session_dir': '/tmp/ray/session_2023-01-10_14-35-51_385880_159', 'metrics_export_port': 55340, 'gcs_address': '172.31.214.212:9031', 'address': '172.31.214.212:9031', 'dashboard_agent_listen_port': 52365, 'node_id': '4377f45806d84045ba64a945e4c56f2f66810f7eb9ea005d7de7de63'})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ray\n",
    "\n",
    "if ray.is_initialized():\n",
    "    ray.shutdown()\n",
    "ray.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae206af9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'CPU': 24.0, 'object_store_memory': 27594768383.0, 'node:172.31.214.212': 1.0, 'memory': 66381399655.0, 'node:172.31.244.220': 1.0}\n"
     ]
    }
   ],
   "source": [
    "print(ray.cluster_resources())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a051b16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn: 1.2.0\n",
      "xgboost: 1.3.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ray/anaconda3/lib/python3.8/site-packages/xgboost/compat.py:31: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n"
     ]
    }
   ],
   "source": [
    "# import standard sklearn libraries\n",
    "import sklearn\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "print(f\"sklearn: {sklearn.__version__}\")\n",
    "import xgboost as xgb\n",
    "\n",
    "print(f\"xgboost: {xgb.__version__}\")\n",
    "# import ray libraries\n",
    "from ray import air, tune\n",
    "from ray.air import session\n",
    "from ray.air.checkpoint import Checkpoint\n",
    "\n",
    "# set global random seed for sklearn models\n",
    "np.random.seed(415)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04609041",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For benchmarking purposes, we can print the times of various operations.\n",
    "# In order to reduce clutter in the output, this is set to False by default.\n",
    "PRINT_TIMES = False\n",
    "\n",
    "\n",
    "def print_time(msg: str):\n",
    "    if PRINT_TIMES:\n",
    "        print(msg)\n",
    "\n",
    "\n",
    "# To speed things up, weâ€™ll only use a small subset of the full dataset consisting of two last months of 2019.\n",
    "# You can choose to use the full dataset for 2018-2019 by setting the SMOKE_TEST variable to False.\n",
    "SMOKE_TEST = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b26b486",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Define how to load and prepare Parquet data <a class=\"anchor\" id=\"prepare_data\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eba0c40",
   "metadata": {},
   "source": [
    "First, we need to load some data. Since the NYC Taxi dataset is fairly large, we will filter files first into a PyArrow dataset. And then in the next cell after, we will filter the data on read into a PyArrow table and convert that to a pandas dataframe.\n",
    "\n",
    "```{tip}\n",
    "Use PyArrow dataset and table for reading or writing large parquet files, since its native multithreaded C++ adapter is faster than pandas read_parquet, even using engine=pyarrow.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57d040ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NYC Taxi using 1 file(s)!\n",
      "s3_files: ['s3://anonymous@air-example-data/ursa-labs-taxi-data/by_year/2019/06/data.parquet/ab5b9d2b8cc94be19346e260b543ec35_000000.parquet']\n",
      "Locations: [141, 229, 173]\n"
     ]
    }
   ],
   "source": [
    "# Define some global variables.\n",
    "TARGET = \"trip_duration\"\n",
    "s3_partitions = pds.dataset(\n",
    "    \"s3://anonymous@air-example-data/ursa-labs-taxi-data/by_year/\",\n",
    "    partitioning=[\"year\", \"month\"],\n",
    ")\n",
    "s3_files = [f\"s3://anonymous@{file}\" for file in s3_partitions.files]\n",
    "\n",
    "# Obtain all location IDs\n",
    "all_location_ids = (\n",
    "    pq.read_table(s3_files[0], columns=[\"dropoff_location_id\"])[\n",
    "        \"dropoff_location_id\"\n",
    "    ]\n",
    "    .unique()\n",
    "    .to_pylist()\n",
    ")\n",
    "# drop [264, 265]\n",
    "all_location_ids.remove(264)\n",
    "all_location_ids.remove(265)\n",
    "\n",
    "# Use smoke testing or not.\n",
    "starting_idx = -1 if SMOKE_TEST else 0\n",
    "# TODO: drop location 199 to test error-handling before final git checkin\n",
    "sample_locations = [141, 229, 173] if SMOKE_TEST else all_location_ids\n",
    "\n",
    "# Display what data will be used.\n",
    "s3_files = s3_files[starting_idx:]\n",
    "print(f\"NYC Taxi using {len(s3_files)} file(s)!\")\n",
    "print(f\"s3_files: {s3_files}\")\n",
    "print(f\"Locations: {sample_locations}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "182ccf10",
   "metadata": {},
   "outputs": [],
   "source": [
    "############\n",
    "# STEP 1.  Define Python functions to \n",
    "#          a) read and prepare a segment of data.\n",
    "############\n",
    "\n",
    "# Function to read a pyarrow.Table object using pyarrow parquet\n",
    "def read_data(file: str, sample_id: np.int32) -> pd.DataFrame:\n",
    "\n",
    "    df = pq.read_table(\n",
    "        file,\n",
    "        filters=[\n",
    "            (\"passenger_count\", \">\", 0),\n",
    "            (\"trip_distance\", \">\", 0),\n",
    "            (\"fare_amount\", \">\", 0),\n",
    "            (\"pickup_location_id\", \"not in\", [264, 265]),\n",
    "            (\"dropoff_location_id\", \"not in\", [264, 265]),\n",
    "            (\"dropoff_location_id\", \"=\", sample_id),\n",
    "        ],\n",
    "        columns=[\n",
    "            \"pickup_at\",\n",
    "            \"dropoff_at\",\n",
    "            \"pickup_location_id\",\n",
    "            \"dropoff_location_id\",\n",
    "            \"passenger_count\",\n",
    "            \"trip_distance\",\n",
    "            \"fare_amount\",\n",
    "        ],\n",
    "    ).to_pandas()\n",
    "\n",
    "    return df\n",
    "\n",
    "# Function to transform a pandas dataframe\n",
    "def transform_df(input_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = input_df.copy()\n",
    "\n",
    "    # calculate trip_duration\n",
    "    df[\"trip_duration\"] = (df[\"dropoff_at\"] - df[\"pickup_at\"]).dt.seconds\n",
    "    # filter trip_durations > 1 minute and less than 24 hours\n",
    "    df = df[df[\"trip_duration\"] > 60]\n",
    "    df = df[df[\"trip_duration\"] < 24 * 60 * 60]\n",
    "    # keep only necessary columns\n",
    "    df = df[['dropoff_location_id', 'passenger_count', 'trip_distance', 'trip_duration']].copy()\n",
    "    df[\"dropoff_location_id\"] = df[\"dropoff_location_id\"].fillna(-1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31aa496",
   "metadata": {},
   "source": [
    "## Define a Trainable (callable) function <a class=\"anchor\" id=\"define_trainable\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7278b69",
   "metadata": {},
   "source": [
    "Next, we define a trainable function, called `train_model()`, in order to train and evaluate a model on a data partition. This function will be called *in parallel for every permutation* in the Tune search space! \n",
    "\n",
    "Inside this trainable function:\n",
    "- ðŸ“– The input must include a config, `type dict`, argument. \n",
    "- ðŸ“ˆ Inside the function, `metrics = `, the tuning metric (a model's loss or error) must be calculated.\n",
    "- âœ”ï¸ Optionally [checkpoint](https://docs.ray.io/en/master/ray-air/key-concepts.html#checkpoints), or save, the model for easy deployment later.\n",
    "\n",
    "```{tip}\n",
    "Ray Tune has two ways of defining a trainable, namely the [Function API](https://docs.ray.io/en/latest/tune/api_docs/trainable.html#trainable-docs) and the Class API. Both are valid ways of defining a trainable, but *the Function API is generally recommended*.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2a9c0a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "############\n",
    "# STEP 1.  Define Python functions to \n",
    "#          b) train and evaluate a model on a segment of data.\n",
    "############\n",
    "def train_model(config: dict) -> None:\n",
    "\n",
    "    algorithm = config[\"algorithm\"]\n",
    "    sample_location_id = config[\"location\"]\n",
    "\n",
    "    # Load data.\n",
    "    df_list = [read_data(f, sample_location_id) for f in s3_files]\n",
    "    df_raw = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "    # Transform data.\n",
    "    df = transform_df(df_raw)\n",
    "\n",
    "    # We need at least 10 rows to create a train / test split.\n",
    "    if df.shape[0] < 10:\n",
    "        print_time(\n",
    "            f\"Location {sample_location_id} has only {df.shape[0]} rows.\"\n",
    "        )\n",
    "        session.report(dict(error=None))\n",
    "        return None\n",
    "\n",
    "    # Train/valid split.\n",
    "    train_df, valid_df = train_test_split(df, test_size=0.2, shuffle=True)\n",
    "    train_X = train_df[[\"passenger_count\", \"trip_distance\"]]\n",
    "    train_y = train_df[TARGET]\n",
    "    valid_X = valid_df[[\"passenger_count\", \"trip_distance\"]]\n",
    "    valid_y = valid_df[TARGET]\n",
    "\n",
    "    # Train model.\n",
    "    model = algorithm.fit(train_X, train_y)\n",
    "    pred_y = model.predict(valid_X)\n",
    "\n",
    "    # Evaluate.\n",
    "    error = sklearn.metrics.mean_absolute_error(valid_y, pred_y)\n",
    "\n",
    "    # Define a model checkpoint using AIR API.\n",
    "    # https://docs.ray.io/en/latest/tune/tutorials/tune-checkpoints.html\n",
    "    checkpoint = ray.air.checkpoint.Checkpoint.from_dict(\n",
    "        {\"model\": algorithm, \"location_id\": sample_location_id}\n",
    "    )\n",
    "\n",
    "    # Save checkpoint and report back metrics, using ray.air.session.report()\n",
    "    # The metrics you specify here will appear in Tune summary table.\n",
    "    # They will also be recorded in Tune results under `metrics`.\n",
    "    metrics = dict(error=error)\n",
    "    session.report(metrics, checkpoint=checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff7198f",
   "metadata": {},
   "source": [
    "## Run batch training on Ray Tune <a class=\"anchor\" id=\"run_tune_search\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31da0f0",
   "metadata": {},
   "source": [
    "**Recall what we are doing, high level, is training several different models per pickup location.** We are using Ray Tune so we can *run all these trials in parallel* on a Ray cluster. At the end, we will inspect the results of the experiment and deploy only the best model per pickup location.\n",
    "\n",
    "**Step 1. Define Python functions to read and prepare a segment of data and train and evaluate one or many models per segment of data**.  We already did this, above.\n",
    "\n",
    "**Step 2. Scaling**:\n",
    "Below, we use the default resources config which is 1 CPU core for each task. For more information about configuring resource allocations, see [A Guide To Parallelism and Resources](https://docs.ray.io/en/master/tune/tutorials/tune-resources.html#tune-parallelism). \n",
    "\n",
    "**Step 3. Search Space**:\n",
    "Below, we define our [Tune search space](https://docs.ray.io/en/latest/tune/api_docs/search_space.htmlhttps://docs.ray.io/en/latest/tune/api_docs/search_space.html) consists of:\n",
    "- Different algorithms:\n",
    "  - XGBoost\n",
    "  - Scikit-learn LinearRegression\n",
    "- Some or all NYC taxi drop-off locations. \n",
    "\n",
    "**Step 4. Search Algorithm or Strategy**:\n",
    "Below, our Tune jobs will be defined using a search space and simple grid search. \n",
    "> The typical use case for Tune search spaces is for hyperparameter tuning. In our case, we are defining the Tune search space in order to run distributed tuning jobs automatically.  Each training job will use a different data partition (taxi pickup location), different algorithm, and the compute resources we defined in the Scaling config.\n",
    "\n",
    "**Step 5. Now we are ready to kick off a Ray Tune experiment!** \n",
    "- Define a `tuner` object.\n",
    "- Put the training function `train_model()` inside the `tuner` object.\n",
    "- Run the experiment using `tuner.fit()`.\n",
    "\n",
    "ðŸ’¡ After you run the cell below, right-click on it and choose \"Enable Scrolling for Outputs\"! This will make it easier to view, since tuning output can be very long!\n",
    "\n",
    "**Setting SMOKE_TEST=False, running on Anyscale:  518 models, using 18 NYC Taxi S3 files dating from 2018/01 to 2019/06 (split into partitions approx 1GiB each), simultaneously trained on a 23-node AWS cluster of [m5.4xlarges](https://aws.amazon.com/ec2/instance-types/m5/). Total data reading and train time was 37 minutes.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2672a3cd",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2023-01-10 15:05:27</td></tr>\n",
       "<tr><td>Running for: </td><td>00:01:09.50        </td></tr>\n",
       "<tr><td>Memory:      </td><td>2.4/30.6 GiB       </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using FIFO scheduling algorithm.<br>Resources requested: 0/24 CPUs, 0/0 GPUs, 0.0/61.82 GiB heap, 0.0/25.7 GiB objects\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name             </th><th>status    </th><th>loc                </th><th>algorithm           </th><th style=\"text-align: right;\">  location</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   error</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_model_1c677_00000</td><td>TERMINATED</td><td>172.31.244.220:3031</td><td>LinearRegression()  </td><td style=\"text-align: right;\">       141</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         1.97273</td><td style=\"text-align: right;\"> 506.148</td></tr>\n",
       "<tr><td>train_model_1c677_00001</td><td>TERMINATED</td><td>172.31.244.220:3079</td><td>XGBRegressor(ba_2970</td><td style=\"text-align: right;\">       141</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        56.642  </td><td style=\"text-align: right;\"> 515.192</td></tr>\n",
       "<tr><td>train_model_1c677_00002</td><td>TERMINATED</td><td>172.31.244.220:3080</td><td>LinearRegression()  </td><td style=\"text-align: right;\">       229</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         1.90393</td><td style=\"text-align: right;\"> 572.096</td></tr>\n",
       "<tr><td>train_model_1c677_00003</td><td>TERMINATED</td><td>172.31.244.220:3081</td><td>XGBRegressor(ba_42e0</td><td style=\"text-align: right;\">       229</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        56.2942 </td><td style=\"text-align: right;\"> 561.104</td></tr>\n",
       "<tr><td>train_model_1c677_00004</td><td>TERMINATED</td><td>172.31.244.220:3082</td><td>LinearRegression()  </td><td style=\"text-align: right;\">       173</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         1.79799</td><td style=\"text-align: right;\"> 826.806</td></tr>\n",
       "<tr><td>train_model_1c677_00005</td><td>TERMINATED</td><td>172.31.244.220:3083</td><td>XGBRegressor(ba_4b50</td><td style=\"text-align: right;\">       173</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        56.4441 </td><td style=\"text-align: right;\">2143.97 </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div class=\"trialProgress\">\n",
       "  <h3>Trial Progress</h3>\n",
       "  <table>\n",
       "<thead>\n",
       "<tr><th>Trial name             </th><th style=\"text-align: right;\">   error</th><th>should_checkpoint  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_model_1c677_00000</td><td style=\"text-align: right;\"> 506.148</td><td>True               </td></tr>\n",
       "<tr><td>train_model_1c677_00001</td><td style=\"text-align: right;\"> 515.192</td><td>True               </td></tr>\n",
       "<tr><td>train_model_1c677_00002</td><td style=\"text-align: right;\"> 572.096</td><td>True               </td></tr>\n",
       "<tr><td>train_model_1c677_00003</td><td style=\"text-align: right;\"> 561.104</td><td>True               </td></tr>\n",
       "<tr><td>train_model_1c677_00004</td><td style=\"text-align: right;\"> 826.806</td><td>True               </td></tr>\n",
       "<tr><td>train_model_1c677_00005</td><td style=\"text-align: right;\">2143.97 </td><td>True               </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".trialProgress {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".trialProgress h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".trialProgress td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-10 15:04:26,526\tWARNING util.py:244 -- The `process_trial_save` operation took 3.586 s, which may be a performance bottleneck.\n",
      "2023-01-10 15:04:26,527\tWARNING trial_runner.py:1059 -- Consider turning off forced head-worker trial checkpoint syncs by setting sync_on_checkpoint=False. Note that this may result in faulty trial restoration if a failure occurs while the checkpoint is being synced from the worker to the head node.\n",
      "2023-01-10 15:04:30,093\tWARNING util.py:244 -- The `process_trial_save` operation took 3.544 s, which may be a performance bottleneck.\n",
      "2023-01-10 15:04:31,783\tWARNING util.py:244 -- The `process_trial_save` operation took 1.674 s, which may be a performance bottleneck.\n",
      "2023-01-10 15:04:31,793\tINFO tensorboardx.py:267 -- Removed the following hyperparameter values when logging to tensorboard: {'algorithm': LinearRegression()}\n",
      "2023-01-10 15:04:33,580\tINFO tensorboardx.py:267 -- Removed the following hyperparameter values when logging to tensorboard: {'algorithm': LinearRegression()}\n",
      "2023-01-10 15:04:35,329\tINFO tensorboardx.py:267 -- Removed the following hyperparameter values when logging to tensorboard: {'algorithm': LinearRegression()}\n",
      "2023-01-10 15:05:22,053\tWARNING util.py:244 -- The `process_trial_save` operation took 2.203 s, which may be a performance bottleneck.\n",
      "2023-01-10 15:05:22,098\tINFO tensorboardx.py:267 -- Removed the following hyperparameter values when logging to tensorboard: {'algorithm': XGBRegressor(base_score=None, booster=None, colsample_bylevel=None,\n",
      "             colsample_bynode=None, colsample_bytree=None, gamma=None,\n",
      "             gpu_id=None, importance_type='gain', interaction_constraints=None,\n",
      "             learning_rate=None, max_delta_step=None, max_depth=4,\n",
      "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "             n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
      "             random_state=None, reg_alpha=None, reg_lambda=None,\n",
      "             scale_pos_weight=None, subsample=None, tree_method=None,\n",
      "             validate_parameters=None, verbosity=None)}\n",
      "2023-01-10 15:05:24,360\tWARNING util.py:244 -- The `process_trial_save` operation took 1.102 s, which may be a performance bottleneck.\n",
      "2023-01-10 15:05:24,402\tINFO tensorboardx.py:267 -- Removed the following hyperparameter values when logging to tensorboard: {'algorithm': XGBRegressor(base_score=None, booster=None, colsample_bylevel=None,\n",
      "             colsample_bynode=None, colsample_bytree=None, gamma=None,\n",
      "             gpu_id=None, importance_type='gain', interaction_constraints=None,\n",
      "             learning_rate=None, max_delta_step=None, max_depth=4,\n",
      "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "             n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
      "             random_state=None, reg_alpha=None, reg_lambda=None,\n",
      "             scale_pos_weight=None, subsample=None, tree_method=None,\n",
      "             validate_parameters=None, verbosity=None)}\n",
      "2023-01-10 15:05:26,657\tWARNING util.py:244 -- The `process_trial_save` operation took 1.114 s, which may be a performance bottleneck.\n",
      "2023-01-10 15:05:26,682\tINFO tensorboardx.py:267 -- Removed the following hyperparameter values when logging to tensorboard: {'algorithm': XGBRegressor(base_score=None, booster=None, colsample_bylevel=None,\n",
      "             colsample_bynode=None, colsample_bytree=None, gamma=None,\n",
      "             gpu_id=None, importance_type='gain', interaction_constraints=None,\n",
      "             learning_rate=None, max_delta_step=None, max_depth=4,\n",
      "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "             n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
      "             random_state=None, reg_alpha=None, reg_lambda=None,\n",
      "             scale_pos_weight=None, subsample=None, tree_method=None,\n",
      "             validate_parameters=None, verbosity=None)}\n",
      "2023-01-10 15:05:28,011\tINFO tune.py:762 -- Total run time: 69.62 seconds (69.38 seconds for the tuning loop).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of models: 6\n",
      "TOTAL TIME TAKEN: 1.16 minutes\n"
     ]
    }
   ],
   "source": [
    "############\n",
    "# STEP 2. Customize distributed compute scaling.\n",
    "############\n",
    "# Use Ray AIR default resources config which is 1 CPU core for each task.\n",
    "\n",
    "############\n",
    "# STEP 3. Define a search space dict of all config parameters.\n",
    "############\n",
    "search_space = {\n",
    "    \"algorithm\": tune.grid_search(\n",
    "        [LinearRegression(fit_intercept=True), xgb.XGBRegressor(max_depth=4)]\n",
    "    ),\n",
    "    \"location\": tune.grid_search(sample_locations),\n",
    "}\n",
    "\n",
    "# Optional STEP 4. Specify the hyperparameter tuning search strategy.\n",
    "\n",
    "############\n",
    "# STEP 5. Run the experiment with Ray AIR APIs.\n",
    "# https://docs.ray.io/en/latest/tune/examples/tune-pytorch-lightning.html\n",
    "############\n",
    "start = time.time()\n",
    "\n",
    "# Define a tuner object.\n",
    "tuner = tune.Tuner(\n",
    "    train_model,\n",
    "    param_space=search_space,\n",
    "    run_config=air.RunConfig(\n",
    "        # redirect logs to relative path instead of default ~/ray_results/\n",
    "        local_dir=\"my_Tune_logs\",\n",
    "        name=\"batch_tuning\",\n",
    "        # Set Ray Tune verbosity. Print summary table only with levels 2 or 3.\n",
    "        verbose=2,\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Fit the tuner object.\n",
    "results = tuner.fit()\n",
    "\n",
    "total_time_taken = time.time() - start\n",
    "print(f\"Total number of models: {len(results)}\")\n",
    "print(f\"TOTAL TIME TAKEN: {total_time_taken/60:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0d965e",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "**After the Tune experiment has run, select the best model per dropoff location.**\n",
    "\n",
    "We can assemble the Tune results ([ResultGrid object](https://docs.ray.io/en/master/tune/examples/tune_analyze_results.html)) into a pandas dataframe, then sort by minimum error, to select the best model per dropoff location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6d1c85aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>location_id</th>\n",
       "      <th>error</th>\n",
       "      <th>algorithm</th>\n",
       "      <th>checkpoint</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>141</td>\n",
       "      <td>506.147908</td>\n",
       "      <td>LinearRegression()</td>\n",
       "      <td>Checkpoint(local_path=/home/ray/christy-air/fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>141</td>\n",
       "      <td>515.191622</td>\n",
       "      <td>XGBRegressor(base_score=0.5, booster='gbtree',...</td>\n",
       "      <td>Checkpoint(local_path=/home/ray/christy-air/fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>229</td>\n",
       "      <td>572.096299</td>\n",
       "      <td>LinearRegression()</td>\n",
       "      <td>Checkpoint(local_path=/home/ray/christy-air/fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>229</td>\n",
       "      <td>561.104089</td>\n",
       "      <td>XGBRegressor(base_score=0.5, booster='gbtree',...</td>\n",
       "      <td>Checkpoint(local_path=/home/ray/christy-air/fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>173</td>\n",
       "      <td>826.806384</td>\n",
       "      <td>LinearRegression()</td>\n",
       "      <td>Checkpoint(local_path=/home/ray/christy-air/fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>173</td>\n",
       "      <td>2143.973972</td>\n",
       "      <td>XGBRegressor(base_score=0.5, booster='gbtree',...</td>\n",
       "      <td>Checkpoint(local_path=/home/ray/christy-air/fo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   location_id        error  \\\n",
       "0          141   506.147908   \n",
       "1          141   515.191622   \n",
       "2          229   572.096299   \n",
       "3          229   561.104089   \n",
       "4          173   826.806384   \n",
       "5          173  2143.973972   \n",
       "\n",
       "                                           algorithm  \\\n",
       "0                                 LinearRegression()   \n",
       "1  XGBRegressor(base_score=0.5, booster='gbtree',...   \n",
       "2                                 LinearRegression()   \n",
       "3  XGBRegressor(base_score=0.5, booster='gbtree',...   \n",
       "4                                 LinearRegression()   \n",
       "5  XGBRegressor(base_score=0.5, booster='gbtree',...   \n",
       "\n",
       "                                          checkpoint  \n",
       "0  Checkpoint(local_path=/home/ray/christy-air/fo...  \n",
       "1  Checkpoint(local_path=/home/ray/christy-air/fo...  \n",
       "2  Checkpoint(local_path=/home/ray/christy-air/fo...  \n",
       "3  Checkpoint(local_path=/home/ray/christy-air/fo...  \n",
       "4  Checkpoint(local_path=/home/ray/christy-air/fo...  \n",
       "5  Checkpoint(local_path=/home/ray/christy-air/fo...  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get a list of training loss errors\n",
    "errors = [i.metrics.get(\"error\", 10000.0) for i in results]\n",
    "\n",
    "# get a list of checkpoints\n",
    "checkpoints = [i.checkpoint for i in results]\n",
    "\n",
    "# get a list of locations\n",
    "locations = [i.config[\"location\"] for i in results]\n",
    "\n",
    "# get a list of model params\n",
    "algorithms = [i.config[\"algorithm\"] for i in results]\n",
    "\n",
    "# Assemble a pandas dataframe from Tune results\n",
    "results_df = pd.DataFrame(\n",
    "    zip(locations, errors, algorithms, checkpoints),\n",
    "    columns=[\"location_id\", \"error\", \"algorithm\", \"checkpoint\"],\n",
    ")\n",
    "results_df.head(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "28bc1109",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>error</th>\n",
       "      <th>algorithm</th>\n",
       "      <th>checkpoint</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>location_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>506.147908</td>\n",
       "      <td>LinearRegression()</td>\n",
       "      <td>Checkpoint(local_path=/home/ray/christy-air/fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>561.104089</td>\n",
       "      <td>XGBRegressor(base_score=0.5, booster='gbtree',...</td>\n",
       "      <td>Checkpoint(local_path=/home/ray/christy-air/fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>826.806384</td>\n",
       "      <td>LinearRegression()</td>\n",
       "      <td>Checkpoint(local_path=/home/ray/christy-air/fo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  error                                          algorithm  \\\n",
       "location_id                                                                  \n",
       "141          506.147908                                 LinearRegression()   \n",
       "229          561.104089  XGBRegressor(base_score=0.5, booster='gbtree',...   \n",
       "173          826.806384                                 LinearRegression()   \n",
       "\n",
       "                                                    checkpoint  \n",
       "location_id                                                     \n",
       "141          Checkpoint(local_path=/home/ray/christy-air/fo...  \n",
       "229          Checkpoint(local_path=/home/ray/christy-air/fo...  \n",
       "173          Checkpoint(local_path=/home/ray/christy-air/fo...  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Keep only 1 model per location_id with minimum error\n",
    "final_df = results_df.copy()\n",
    "final_df = final_df.loc[(final_df.error > 0), :]\n",
    "final_df = final_df.loc[final_df.groupby(\"location_id\")[\"error\"].idxmin()]\n",
    "final_df.sort_values(by=[\"error\"], inplace=True)\n",
    "final_df.set_index(\"location_id\", inplace=True, drop=True)\n",
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "94e5f52b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "algorithm                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \n",
       "LinearRegression()                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       0.666667\n",
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\\n             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\\n             importance_type='gain', interaction_constraints='',\\n             learning_rate=0.300000012, max_delta_step=0, max_depth=4,\\n             min_child_weight=1, missing=nan, monotone_constraints='()',\\n             n_estimators=100, n_jobs=16, num_parallel_tree=1, random_state=0,\\n             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\\n             tree_method='exact', validate_parameters=1, verbosity=None)    0.333333\n",
       "dtype: float64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df[[\"algorithm\"]].astype(\"str\").value_counts(normalize=True)\n",
    "\n",
    "# 0.67 XGB\n",
    "# 0.33 Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90cfeaf4",
   "metadata": {},
   "source": [
    "## Load a model from checkpoint and perform batch prediction  <a class=\"anchor\" id=\"load_checkpoint\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c15d82",
   "metadata": {},
   "source": [
    "```{tip}\n",
    "[Ray AIR Predictors](https://docs.ray.io/en/latest/ray-air/predictors.html) make batch inference easy since they have internal logic to parallelize the inference.\n",
    "```\n",
    "\n",
    "Finally, we will restore the best and worst models from checkpoint and make predictions. \n",
    "\n",
    "- We will easily obtain AIR Checkpoint objects from the Tune results. \n",
    "- We will restore a regression model directly from checkpoint, and demonstrate it can be used for prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b93f441f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "141"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Choose a dropoff location\n",
    "sample_location_id = final_df.index[0]\n",
    "sample_location_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f7e0abe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "algorithm type:: <class 'sklearn.linear_model._base.LinearRegression'>\n",
      "checkpoint type:: <class 'ray.air.checkpoint.Checkpoint'>\n"
     ]
    }
   ],
   "source": [
    "# Get the algorithm used\n",
    "sample_algorithm = final_df.loc[[sample_location_id]].algorithm.values[0]\n",
    "print(f\"algorithm type:: {type(sample_algorithm)}\")\n",
    "\n",
    "# Get a checkpoint directly from the pandas dataframe of Tune results\n",
    "checkpoint = final_df.checkpoint[sample_location_id]\n",
    "print(f\"checkpoint type:: {type(checkpoint)}\")\n",
    "\n",
    "# Restore a model from checkpoint\n",
    "sample_model = checkpoint.to_dict()[\"model\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "98eb20d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create some test data\n",
    "df_list = [read_data(f, sample_location_id) for f in s3_files[:1]]\n",
    "df_raw = pd.concat(df_list, ignore_index=True)\n",
    "df = transform_df(df_raw)\n",
    "_, test_df = train_test_split(df, test_size=0.2, shuffle=True)\n",
    "test_X = test_df[[\"passenger_count\", \"trip_distance\"]]\n",
    "test_y = np.array(test_df.trip_duration)  # actual values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5ac4f9d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred_y</th>\n",
       "      <th>trip_duration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>857.925903</td>\n",
       "      <td>351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>899.751526</td>\n",
       "      <td>398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>913.467896</td>\n",
       "      <td>885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>655.732483</td>\n",
       "      <td>248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1185.630005</td>\n",
       "      <td>1334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>694.276978</td>\n",
       "      <td>207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>752.727905</td>\n",
       "      <td>423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>584.681519</td>\n",
       "      <td>123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>712.542847</td>\n",
       "      <td>466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>761.860840</td>\n",
       "      <td>506</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        pred_y  trip_duration\n",
       "0   857.925903            351\n",
       "1   899.751526            398\n",
       "2   913.467896            885\n",
       "3   655.732483            248\n",
       "4  1185.630005           1334\n",
       "5   694.276978            207\n",
       "6   752.727905            423\n",
       "7   584.681519            123\n",
       "8   712.542847            466\n",
       "9   761.860840            506"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perform batch prediction using restored model from checkpoint\n",
    "pred_y = sample_model.predict(test_X)\n",
    "\n",
    "# Zip together predictions and actuals to visualize\n",
    "pd.DataFrame(zip(pred_y, test_y), columns=[\"pred_y\", TARGET])[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc05d848",
   "metadata": {},
   "source": [
    "**Compare validation and test error.**\n",
    "\n",
    "During model training we reported error on \"validation\" data (random sample). Below, we will report error on a pretend \"test\" data set (a different random sample).\n",
    "\n",
    "Do a quick validation that both errors are reasonably close together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f363b84e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test error: 483.4339918473955\n"
     ]
    }
   ],
   "source": [
    "# Evaluate restored model on test data.\n",
    "error = sklearn.metrics.mean_absolute_error(test_y, pred_y)\n",
    "print(f\"Test error: {error}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fbcac5eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation error: 506.1479080318775\n"
     ]
    }
   ],
   "source": [
    "# Compare test error with training validation error\n",
    "print(f\"Validation error: {final_df.error[sample_location_id]}\")\n",
    "\n",
    "# Validation and test errors should be reasonably close together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87dc620b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "3c0d54d489a08ae47a06eae2fd00ff032d6cddb527c382959b7b2575f6a8167f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
