{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(air-serving-guide)=\n",
    "\n",
    "# Serve Ray AIR Predictor with `ModelWrapper`\n",
    "\n",
    "[Ray Serve](rayserve) is the recommended tool deploy models trained with AIR.\n",
    "\n",
    "The core concept is called `ModelWrapper`. `ModelWrapper` takes a [predictor](ray.ml.predictor.Predictor) class and a [checkpoint](ray.ml.checkpoint) and transforms them to live HTTP endpoint. \n",
    "\n",
    "We'll start with a simple quick start demo showcase where does ModelWrapper fits in Ray AIR."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first make sure Ray AIR is installed. For the quick start, we'll also use Ray AIR to train and serve a very simple XGBoost model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"ray[air]\" xgboost scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can find the preprocessor and trainer in the [key concepts walk-through](air-key-concepts)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-02 18:44:27,520\tINFO services.py:1483 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-06-02 18:44:42 (running for 00:00:11.41)<br>Memory usage on this node: 38.8/64.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/24.08 GiB heap, 0.0/2.0 GiB objects<br>Result logdir: /Users/simonmo/ray_results/XGBoostTrainer_2022-06-02_18-44-30<br>Number of trials: 1/1 (1 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status    </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  train-logloss</th><th style=\"text-align: right;\">  train-error</th><th style=\"text-align: right;\">  valid-logloss</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>XGBoostTrainer_b5f1b_00000</td><td>TERMINATED</td><td>127.0.0.1:54313</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         7.12531</td><td style=\"text-align: right;\">       0.191591</td><td style=\"text-align: right;\">     0.035176</td><td style=\"text-align: right;\">       0.220995</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[33m(raylet)\u001b[0m 2022-06-02 18:44:31,942\tINFO context.py:70 -- Exec'ing worker with command: exec /Users/simonmo/miniconda3/bin/python /Users/simonmo/Desktop/ray/ray/python/ray/workers/default_worker.py --node-ip-address=127.0.0.1 --node-manager-port=53715 --object-store-name=/tmp/ray/session_2022-06-02_18-44-24_587688_53629/sockets/plasma_store --raylet-name=/tmp/ray/session_2022-06-02_18-44-24_587688_53629/sockets/raylet --redis-address=None --storage=None --temp-dir=/tmp/ray --metrics-agent-port=64635 --logging-rotate-bytes=536870912 --logging-rotate-backup-count=5 --gcs-address=127.0.0.1:59662 --redis-password=5241590000000000 --startup-token=16 --runtime-env-hash=2087164853\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m 2022-06-02 18:44:35,216\tINFO context.py:70 -- Exec'ing worker with command: exec /Users/simonmo/miniconda3/bin/python /Users/simonmo/Desktop/ray/ray/python/ray/workers/default_worker.py --node-ip-address=127.0.0.1 --node-manager-port=53715 --object-store-name=/tmp/ray/session_2022-06-02_18-44-24_587688_53629/sockets/plasma_store --raylet-name=/tmp/ray/session_2022-06-02_18-44-24_587688_53629/sockets/raylet --redis-address=None --storage=None --temp-dir=/tmp/ray --metrics-agent-port=64635 --logging-rotate-bytes=536870912 --logging-rotate-backup-count=5 --gcs-address=127.0.0.1:59662 --redis-password=5241590000000000 --startup-token=17 --runtime-env-hash=2087164788\n",
      "\u001b[2m\u001b[36m(GBDTTrainable pid=54313)\u001b[0m UserWarning: `num_actors` in `ray_params` is smaller than 2 (1). XGBoost will NOT be distributed!\n",
      "\u001b[2m\u001b[36m(GBDTTrainable pid=54313)\u001b[0m 2022-06-02 18:44:37,017\tINFO main.py:979 -- [RayXGBoost] Created 1 new actors (1 total actors). Waiting until actors are ready for training.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m 2022-06-02 18:44:38,068\tINFO context.py:70 -- Exec'ing worker with command: exec /Users/simonmo/miniconda3/bin/python /Users/simonmo/Desktop/ray/ray/python/ray/workers/default_worker.py --node-ip-address=127.0.0.1 --node-manager-port=53715 --object-store-name=/tmp/ray/session_2022-06-02_18-44-24_587688_53629/sockets/plasma_store --raylet-name=/tmp/ray/session_2022-06-02_18-44-24_587688_53629/sockets/raylet --redis-address=None --storage=None --temp-dir=/tmp/ray --metrics-agent-port=64635 --logging-rotate-bytes=536870912 --logging-rotate-backup-count=5 --gcs-address=127.0.0.1:59662 --redis-password=5241590000000000 --startup-token=19 --runtime-env-hash=2087164853\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m 2022-06-02 18:44:38,068\tINFO context.py:70 -- Exec'ing worker with command: exec /Users/simonmo/miniconda3/bin/python /Users/simonmo/Desktop/ray/ray/python/ray/workers/default_worker.py --node-ip-address=127.0.0.1 --node-manager-port=53715 --object-store-name=/tmp/ray/session_2022-06-02_18-44-24_587688_53629/sockets/plasma_store --raylet-name=/tmp/ray/session_2022-06-02_18-44-24_587688_53629/sockets/raylet --redis-address=None --storage=None --temp-dir=/tmp/ray --metrics-agent-port=64635 --logging-rotate-bytes=536870912 --logging-rotate-backup-count=5 --gcs-address=127.0.0.1:59662 --redis-password=5241590000000000 --startup-token=18 --runtime-env-hash=2087164853\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m 2022-06-02 18:44:38,078\tINFO context.py:70 -- Exec'ing worker with command: exec /Users/simonmo/miniconda3/bin/python /Users/simonmo/Desktop/ray/ray/python/ray/workers/default_worker.py --node-ip-address=127.0.0.1 --node-manager-port=53715 --object-store-name=/tmp/ray/session_2022-06-02_18-44-24_587688_53629/sockets/plasma_store --raylet-name=/tmp/ray/session_2022-06-02_18-44-24_587688_53629/sockets/raylet --redis-address=None --storage=None --temp-dir=/tmp/ray --metrics-agent-port=64635 --logging-rotate-bytes=536870912 --logging-rotate-backup-count=5 --gcs-address=127.0.0.1:59662 --redis-password=5241590000000000 --startup-token=20 --runtime-env-hash=2087164853\n",
      "\u001b[2m\u001b[36m(GBDTTrainable pid=54313)\u001b[0m 2022-06-02 18:44:40,169\tINFO main.py:1024 -- [RayXGBoost] Starting XGBoost training.\n",
      "\u001b[2m\u001b[36m(_RemoteRayXGBoostActor pid=54326)\u001b[0m [18:44:40] task [xgboost.ray]:140413088227000 got new rank 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for XGBoostTrainer_b5f1b_00000:\n",
      "  date: 2022-06-02_18-44-41\n",
      "  done: false\n",
      "  experiment_id: 10970b9dd33c42daae667a98674efb6c\n",
      "  hostname: Simons-MacBook-Pro.local\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 127.0.0.1\n",
      "  pid: 54313\n",
      "  should_checkpoint: true\n",
      "  time_since_restore: 7.0694568157196045\n",
      "  time_this_iter_s: 7.0694568157196045\n",
      "  time_total_s: 7.0694568157196045\n",
      "  timestamp: 1654220681\n",
      "  timesteps_since_restore: 0\n",
      "  train-error: 0.050251\n",
      "  train-logloss: 0.483589\n",
      "  training_iteration: 1\n",
      "  trial_id: b5f1b_00000\n",
      "  valid-error: 0.070175\n",
      "  valid-logloss: 0.497381\n",
      "  warmup_time: 0.004261970520019531\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(GBDTTrainable pid=54313)\u001b[0m 2022-06-02 18:44:41,273\tINFO main.py:1506 -- [RayXGBoost] Finished XGBoost training on training data with total N=398 in 4.28 seconds (1.10 pure XGBoost training time).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for XGBoostTrainer_b5f1b_00000:\n",
      "  date: 2022-06-02_18-44-41\n",
      "  done: true\n",
      "  experiment_id: 10970b9dd33c42daae667a98674efb6c\n",
      "  experiment_tag: '0'\n",
      "  hostname: Simons-MacBook-Pro.local\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 127.0.0.1\n",
      "  pid: 54313\n",
      "  should_checkpoint: true\n",
      "  time_since_restore: 7.125314712524414\n",
      "  time_this_iter_s: 0.011556863784790039\n",
      "  time_total_s: 7.125314712524414\n",
      "  timestamp: 1654220681\n",
      "  timesteps_since_restore: 0\n",
      "  train-error: 0.035176\n",
      "  train-logloss: 0.191591\n",
      "  training_iteration: 5\n",
      "  trial_id: b5f1b_00000\n",
      "  valid-error: 0.05848\n",
      "  valid-logloss: 0.220995\n",
      "  warmup_time: 0.004261970520019531\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-02 18:44:42,282\tINFO tune.py:753 -- Total run time: 11.81 seconds (11.41 seconds for the tuning loop).\n"
     ]
    }
   ],
   "source": [
    "import ray\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from ray.ml.train.integrations.xgboost import XGBoostTrainer\n",
    "from ray.ml.preprocessors import StandardScaler\n",
    "\n",
    "data_raw = load_breast_cancer()\n",
    "dataset_df = pd.DataFrame(data_raw[\"data\"], columns=data_raw[\"feature_names\"])\n",
    "dataset_df[\"target\"] = data_raw[\"target\"]\n",
    "train_df, test_df = train_test_split(dataset_df, test_size=0.3)\n",
    "train_dataset = ray.data.from_pandas(train_df)\n",
    "valid_dataset = ray.data.from_pandas(test_df)\n",
    "test_dataset = ray.data.from_pandas(test_df.drop(\"target\", axis=1))\n",
    "\n",
    "# Define preprocessor\n",
    "columns_to_scale = [\"mean radius\", \"mean texture\"]\n",
    "preprocessor = StandardScaler(columns=columns_to_scale)\n",
    "\n",
    "# Define trainer\n",
    "trainer = XGBoostTrainer(\n",
    "    scaling_config={\n",
    "        \"num_workers\": 1\n",
    "    },\n",
    "    label_column=\"target\",\n",
    "    params={\n",
    "    \"tree_method\": \"approx\",\n",
    "    \"objective\": \"binary:logistic\",\n",
    "    \"eval_metric\": [\"logloss\", \"error\"],\n",
    "    \"max_depth\": 2,\n",
    "},\n",
    "    datasets={\"train\": train_dataset, \"valid\": valid_dataset},\n",
    "    preprocessor=preprocessor,\n",
    "    num_boost_round=5,\n",
    ")\n",
    "\n",
    "result = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following block serves a Ray AIR model from checkpoint, using built-in `XGBoostPredictor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(ServeController pid=54360)\u001b[0m INFO 2022-06-02 18:45:18,255 controller 54360 checkpoint_path.py:17 - Using RayInternalKVStore for controller checkpoint and recovery.\n",
      "\u001b[2m\u001b[36m(ServeController pid=54360)\u001b[0m INFO 2022-06-02 18:45:18,257 controller 54360 http_state.py:115 - Starting HTTP proxy with name 'SERVE_CONTROLLER_ACTOR:SERVE_PROXY_ACTOR-node:127.0.0.1-0' on node 'node:127.0.0.1-0' listening on '127.0.0.1:8000'\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=54369)\u001b[0m INFO:     Started server process [54369]\n",
      "\u001b[2m\u001b[36m(ServeController pid=54360)\u001b[0m INFO 2022-06-02 18:45:20,610 controller 54360 deployment_state.py:1217 - Adding 1 replicas to deployment 'XGBoostService'.\n"
     ]
    }
   ],
   "source": [
    "from ray.ml.predictors.integrations.xgboost import XGBoostPredictor\n",
    "from ray import serve\n",
    "from ray.serve.model_wrappers import ModelWrapperDeployment\n",
    "from ray.serve.http_adapters import pandas_read_json\n",
    "\n",
    "\n",
    "serve.start(detached=True)\n",
    "deployment = ModelWrapperDeployment.options(name=\"XGBoostService\")\n",
    "\n",
    "deployment.deploy(\n",
    "    XGBoostPredictor, result.checkpoint, http_adapter=pandas_read_json\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's send a request through HTTP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'predictions': 0.8807213306427002}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=54369)\u001b[0m INFO 2022-06-02 18:45:25,289 http_proxy 127.0.0.1 http_proxy.py:320 - POST /XGBoostService 307 3.9ms\n",
      "\u001b[2m\u001b[36m(XGBoostService pid=54373)\u001b[0m INFO 2022-06-02 18:45:25,288 XGBoostService XGBoostService#yYNdzY replica.py:483 - HANDLE __call__ OK 0.3ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=54369)\u001b[0m INFO 2022-06-02 18:45:25,332 http_proxy 127.0.0.1 http_proxy.py:320 - POST /XGBoostService 200 40.0ms\n",
      "\u001b[2m\u001b[36m(XGBoostService pid=54373)\u001b[0m INFO 2022-06-02 18:45:25,330 XGBoostService XGBoostService#yYNdzY replica.py:483 - HANDLE __call__ OK 36.9ms\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "sample_input = test_dataset.take(1)\n",
    "sample_input = dict(sample_input[0])\n",
    "\n",
    "output = requests.post(deployment.url, json=[sample_input]).json()\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "It works! As you can see, `ModelWrapper` is one of the core component in Ray AIR that deploy trained checkpoint as live endpoint. You can find more end-to-end examples with your specific frameworks in the [examples](air-examples-ref) page.\n",
    "\n",
    "This tutorial is gear towards in depth understanding of ModelWrappers, in particular, we'll demonstrate:\n",
    "- How to serve a predictor accepting array input.\n",
    "- How to serve a predictor accepting dataframe input.\n",
    "- How to serve a predictor accepting custom input that can be transformed to array or dataframe.\n",
    "- How to configure micro-batching to enhance performance.\n",
    "\n",
    "But before that, let's review the key concepts:\n",
    "- [`Checkpoint`](ray.ml.checkpoint) represents a trained model stored in memory, file, or remote uri.\n",
    "- [`Predictor`](ray.ml.predictor.Predictor)s understand how to perform a model inference given checkpoints and the model definition. Ray AIR comes with predictors for each supported frameworks. \n",
    "- [`Deployment`](serve-key-concepts-deployment) is a Ray Serve construct that represent an HTTP endpoint along with scalable pool of models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Predictor accepting NumPy array\n",
    "We'll use a simple predictor implementation that adds a scaler to input array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from ray.ml.predictor import Predictor\n",
    "from ray.ml.checkpoint import Checkpoint\n",
    "\n",
    "class AdderPredictor(Predictor):\n",
    "    \"\"\"Dummy predictor that increments input by a staic value.\"\"\"\n",
    "    def __init__(self, increment: int):\n",
    "        self.increment = increment\n",
    "    \n",
    "    @classmethod\n",
    "    def from_checkpoint(cls, ckpt: Checkpoint):\n",
    "        \"\"\"Create predictor from checkpoint.\n",
    "        \n",
    "        Args:\n",
    "          ckpt: The AIR checkpoint representing a single dictionary. The dictionary\n",
    "              should have key `increment` and an integer value.\n",
    "        \"\"\"\n",
    "        return cls(ckpt.to_dict()[\"increment\"])\n",
    "    \n",
    "    def predict(self, inp: np.ndarray) -> np.ndarray:\n",
    "        return inp + self.increment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first test it locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_checkpoint = Checkpoint.from_dict({\"increment\": 2})\n",
    "local_predictor = AdderPredictor.from_checkpoint(local_checkpoint)\n",
    "assert local_predictor.predict(np.array([40])) == np.array([42])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It worked! Now let's serve it behind HTTP. In Ray Serve, the core unit of HTTP service is called a [`Deployment`](serve-key-concepts-deployment). It turns a Python class into queryable HTTP endpoint. For Ray AIR, Serve provides a `ModelWrapperDeployment` make it simpler. You don't need to implement any Python classes. You just pass in your predictor and checkpoint instead.\n",
    "\n",
    "The deployment takes several arguments. It requires two arguments to start:\n",
    "- `predictor_cls (Type[Predictor] | str)`: The predictor Python class. Typically you just need to use the builtin integration from Ray AIR like `TorchPredictor`. Alternatively, you can specify the class path to import such predictor like `\"ray.ml.integrations.torch.TorchPredictor\"`.\n",
    "- `checkpoint (Checkpoint | str)`: A checkpoint instance, or uri to load checkpoint from.\n",
    "\n",
    "The following cell showcase how to create a deployment with our `AdderPredictor`\n",
    "\n",
    "For more about Ray Serve the framework, checkout [its documentation](rayserve)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-20 11:30:34,597\tINFO services.py:1483 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n",
      "\u001b[2m\u001b[36m(ServeController pid=66733)\u001b[0m INFO 2022-05-20 11:30:40,278 controller 66733 checkpoint_path.py:17 - Using RayInternalKVStore for controller checkpoint and recovery.\n",
      "\u001b[2m\u001b[36m(ServeController pid=66733)\u001b[0m INFO 2022-05-20 11:30:40,386 controller 66733 http_state.py:115 - Starting HTTP proxy with name 'SERVE_CONTROLLER_ACTOR:cjsRQe:SERVE_PROXY_ACTOR-node:127.0.0.1-0' on node 'node:127.0.0.1-0' listening on '127.0.0.1:8000'\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=66737)\u001b[0m INFO:     Started server process [66737]\n",
      "\u001b[2m\u001b[36m(ServeController pid=66733)\u001b[0m INFO 2022-05-20 11:30:42,814 controller 66733 deployment_state.py:1217 - Adding 1 replicas to deployment 'Adder'.\n"
     ]
    }
   ],
   "source": [
    "from ray import serve\n",
    "from ray.serve.model_wrappers import ModelWrapperDeployment\n",
    "\n",
    "# Create Ray Serve instance\n",
    "serve.start()\n",
    "\n",
    "# Deploy the model behind HTTP endpoint\n",
    "ModelWrapperDeployment.options(name=\"Adder\").deploy(\n",
    "    predictor_cls=AdderPredictor,\n",
    "    checkpoint=local_checkpoint\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the model has been deployed, let's send an HTTP request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[42.0]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=66737)\u001b[0m INFO 2022-05-20 11:31:05,461 http_proxy 127.0.0.1 http_proxy.py:320 - POST /Adder 200 17.0ms\n",
      "\u001b[2m\u001b[36m(Adder pid=66741)\u001b[0m INFO 2022-05-20 11:31:05,459 Adder Adder#vorDbO replica.py:483 - HANDLE __call__ OK 12.9ms\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "resp = requests.post(\"http://localhost:8000/Adder/\", json={\"array\": [40]})\n",
    "resp.raise_for_status()\n",
    "resp.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice! We sent `[40]` as our input and got `[42]` as our output in JSON format.\n",
    "\n",
    "You can also specify multi-dimensional array in the JSON payload, as well as \"dtype\" and \"shape\" field to process to array. For more information about the array input schema, see [Ndarray](serve-ndarray-schema).\n",
    " \n",
    "That's it for array! Let's take a look at tabular input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictor accepting Pandas DataFrame\n",
    "Let's now take a look at a predictor accepting dataframe input. We'll perform some simple column wise transformation on the input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "class DataFramePredictor(Predictor):\n",
    "    \"\"\"Dummy predictor that first multiplies input then increment it.\"\"\"\n",
    "    def __init__(self, increment: int):\n",
    "        self.increment = increment\n",
    "\n",
    "    @classmethod\n",
    "    def from_checkpoint(cls, ckpt: Checkpoint):\n",
    "        return cls(ckpt.to_dict()[\"increment\"])\n",
    "\n",
    "    def predict(self, inp: pd.DataFrame) -> pd.DataFrame:\n",
    "        inp[\"prediction\"] =  inp[\"base\"] * inp[\"multiplier\"] + self.increment\n",
    "        return inp\n",
    "\n",
    "local_df_predictor = DataFramePredictor.from_checkpoint(local_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like the `AdderPredictor`, we'll use the same `ModelWrapperDeployment` approach to make it queryable with HTTP. \n",
    "\n",
    "You might noticed a small addition this time. We added `http_adapter=pandas_read_json` as the keyword argument. This tells Serve how to parse incoming JSON request into a DataFrame. The `pandas_read_json` adapter accepts:\n",
    "- [Pandas-parsable JSON](https://pandas.pydata.org/docs/reference/api/pandas.read_json.html) in HTTP body\n",
    "- Optionally keyword arguments to [`pandas.read_json`](https://pandas.pydata.org/docs/reference/api/pandas.read_json.html) function via HTTP url parameters.\n",
    "\n",
    "To learn more, see [HTTP Adapters](serve-http-adapters).\n",
    "\n",
    "```{note}\n",
    "You might wonders why does the previous array predictor doesn't need to specify any http adapter. This is because Serve default to a built-in adapter called `json_to_ndarray`(ray.serve.http_adapters.json_to_ndarray)!\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(ServeController pid=66733)\u001b[0m INFO 2022-05-20 11:31:09,317 controller 66733 deployment_state.py:1217 - Adding 1 replicas to deployment 'DataFramePredictor'.\n"
     ]
    }
   ],
   "source": [
    "from ray.serve.http_adapters import pandas_read_json\n",
    "\n",
    "ModelWrapperDeployment.options(name=\"DataFramePredictor\").deploy(\n",
    "    predictor_cls=DataFramePredictor,\n",
    "    checkpoint=local_checkpoint,\n",
    "    http_adapter=pandas_read_json\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's send a request to our endpoint. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[{\"base\":1,\"multipiler\":2,\"prediction\":4},{\"base\":3,\"multipiler\":4,\"prediction\":14}]'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=66737)\u001b[0m INFO 2022-05-20 11:31:15,388 http_proxy 127.0.0.1 http_proxy.py:320 - POST /DataFramePredictor 200 28.6ms\n",
      "\u001b[2m\u001b[36m(DataFramePredictor pid=66765)\u001b[0m INFO 2022-05-20 11:31:15,387 DataFramePredictor DataFramePredictor#lcGDjS replica.py:483 - HANDLE __call__ OK 24.4ms\n"
     ]
    }
   ],
   "source": [
    "resp = requests.post(\n",
    "    \"http://localhost:8000/DataFramePredictor/\",\n",
    "    json=[{\"base\": 1, \"multiplier\": 2}, {\"base\": 3, \"multiplier\": 4}],\n",
    "    params={\"orient\": \"records\"},\n",
    ")\n",
    "resp.raise_for_status()\n",
    "resp.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! You can see that the input JSON has been converted to a dataframe, so our predictor can work with pure dataframes instead of raw HTTP requests.\n",
    "\n",
    "But what if we need to configure the HTTP request? You can do that as well.\n",
    "\n",
    "## 3. Accepting custom inputs via `http_adapter`\n",
    "\n",
    "The `http_adapter` field accept any callable function that's type annotated. You can also bring in additional types that's accepted by FastAPI's dependency injection framework. You can learn more detail [here](serve-http-adapters). In the following example, instead of using the pandas adapter Serve provides, we'll implement our own request adapter that work with just http parameters instead of JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def our_own_http_adapter(base: int, multipiler: int):\n",
    "    return pd.DataFrame([{\"base\": base, \"multipiler\": multipiler}])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's deploy it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(ServeController pid=66733)\u001b[0m INFO 2022-05-20 11:31:18,916 controller 66733 deployment_state.py:1176 - Stopping 1 replicas of deployment 'DataFramePredictor' with outdated versions.\n",
      "\u001b[2m\u001b[36m(ServeController pid=66733)\u001b[0m INFO 2022-05-20 11:31:21,084 controller 66733 deployment_state.py:1217 - Adding 1 replicas to deployment 'DataFramePredictor'.\n"
     ]
    }
   ],
   "source": [
    "from ray.serve.http_adapters import pandas_read_json\n",
    "\n",
    "ModelWrapperDeployment.options(name=\"DataFramePredictor\").deploy(\n",
    "    predictor_cls=DataFramePredictor,\n",
    "    checkpoint=local_checkpoint,\n",
    "    http_adapter=our_own_http_adapter\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now send a request, note that the new predictor accepts our specified input via HTTP parameters. \n",
    "\n",
    "The equivalent curl request would be `curl -X POST http://localhost:8000/DataFramePredictor/?base=10&multiplier=4`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[{\"base\":10,\"multipiler\":4,\"prediction\":42}]'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=66737)\u001b[0m INFO 2022-05-20 11:31:25,950 http_proxy 127.0.0.1 http_proxy.py:320 - POST /DataFramePredictor 200 24.8ms\n",
      "\u001b[2m\u001b[36m(DataFramePredictor pid=66771)\u001b[0m INFO 2022-05-20 11:31:25,949 DataFramePredictor DataFramePredictor#abtzzn replica.py:483 - HANDLE __call__ OK 20.8ms\n"
     ]
    }
   ],
   "source": [
    "resp = requests.post(\n",
    "    \"http://localhost:8000/DataFramePredictor/\",\n",
    "    params={\"base\": 10, \"multipiler\": 4}\n",
    ")\n",
    "resp.raise_for_status()\n",
    "resp.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. `ModelWrapper` performs microbatching to improve performance\n",
    "\n",
    "Common machine learning models take a batch of inputs for prediction. Common ML Frameworks are optimized with vectorized instruction to make inference on a batch requests almost as fast as single requests. \n",
    "\n",
    "In Serve's `ModelWrapperDeployment`, the incoming requests are automatically batched. \n",
    "\n",
    "When multiple clients send requests at the same time, Serve will combine the requests into a single batch (array or dataframe) and predict is run only once for multiple requests. Let's take a look at a predictor that returns each row's content, batch_size, and batch group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "class BatchSizePredictor(Predictor):\n",
    "    @classmethod\n",
    "    def from_checkpoint(cls, _: Checkpoint):\n",
    "        return cls()\n",
    "    \n",
    "    def predict(self, inp: np.ndarray):\n",
    "        time.sleep(0.5) # simulate model inference.\n",
    "        return [(i, len(inp), inp) for i in inp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(ServeController pid=66733)\u001b[0m INFO 2022-05-20 11:31:29,789 controller 66733 deployment_state.py:1217 - Adding 1 replicas to deployment 'BatchSizePredictor'.\n"
     ]
    }
   ],
   "source": [
    "ModelWrapperDeployment.options(name=\"BatchSizePredictor\").deploy(\n",
    "    predictor_cls=BatchSizePredictor,\n",
    "    checkpoint=local_checkpoint,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use a threadpool executor to send ten requests at the same time to simulate multiple clients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=66737)\u001b[0m INFO 2022-05-20 11:31:35,233 http_proxy 127.0.0.1 http_proxy.py:320 - POST /BatchSizePredictor 200 525.8ms\n",
      "\u001b[2m\u001b[36m(BatchSizePredictor pid=66778)\u001b[0m INFO 2022-05-20 11:31:35,229 BatchSizePredictor BatchSizePredictor#pvZtwj replica.py:483 - HANDLE __call__ OK 520.0ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=66737)\u001b[0m INFO 2022-05-20 11:31:35,742 http_proxy 127.0.0.1 http_proxy.py:320 - POST /BatchSizePredictor 200 1036.9ms\n",
      "\u001b[2m\u001b[36m(BatchSizePredictor pid=66778)\u001b[0m INFO 2022-05-20 11:31:35,738 BatchSizePredictor BatchSizePredictor#pvZtwj replica.py:483 - HANDLE __call__ OK 1015.5ms\n",
      "\u001b[2m\u001b[36m(BatchSizePredictor pid=66778)\u001b[0m INFO 2022-05-20 11:31:36,244 BatchSizePredictor BatchSizePredictor#pvZtwj replica.py:483 - HANDLE __call__ OK 1013.9ms\n",
      "\u001b[2m\u001b[36m(BatchSizePredictor pid=66778)\u001b[0m INFO 2022-05-20 11:31:36,244 BatchSizePredictor BatchSizePredictor#pvZtwj replica.py:483 - HANDLE __call__ OK 1013.4ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=66737)\u001b[0m INFO 2022-05-20 11:31:36,751 http_proxy 127.0.0.1 http_proxy.py:320 - POST /BatchSizePredictor 200 2043.3ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=66737)\u001b[0m INFO 2022-05-20 11:31:36,752 http_proxy 127.0.0.1 http_proxy.py:320 - POST /BatchSizePredictor 200 2043.3ms\n",
      "\u001b[2m\u001b[36m(BatchSizePredictor pid=66778)\u001b[0m INFO 2022-05-20 11:31:36,751 BatchSizePredictor BatchSizePredictor#pvZtwj replica.py:483 - HANDLE __call__ OK 1015.2ms\n",
      "\u001b[2m\u001b[36m(BatchSizePredictor pid=66778)\u001b[0m INFO 2022-05-20 11:31:36,752 BatchSizePredictor BatchSizePredictor#pvZtwj replica.py:483 - HANDLE __call__ OK 1520.5ms\n",
      "\u001b[2m\u001b[36m(BatchSizePredictor pid=66778)\u001b[0m INFO 2022-05-20 11:31:36,752 BatchSizePredictor BatchSizePredictor#pvZtwj replica.py:483 - HANDLE __call__ OK 1015.1ms\n",
      "\u001b[2m\u001b[36m(BatchSizePredictor pid=66778)\u001b[0m INFO 2022-05-20 11:31:36,752 BatchSizePredictor BatchSizePredictor#pvZtwj replica.py:483 - HANDLE __call__ OK 1015.9ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request id: [0.0] is part of batch group: [[0.0]], with batch size 1\n",
      "Request id: [1.0] is part of batch group: [[6.0], [1.0], [9.0], [5.0]], with batch size 4\n",
      "Request id: [2.0] is part of batch group: [[4.0], [2.0]], with batch size 2\n",
      "Request id: [3.0] is part of batch group: [[3.0]], with batch size 1\n",
      "Request id: [4.0] is part of batch group: [[4.0], [2.0]], with batch size 2\n",
      "Request id: [5.0] is part of batch group: [[6.0], [1.0], [9.0], [5.0]], with batch size 4\n",
      "Request id: [6.0] is part of batch group: [[6.0], [1.0], [9.0], [5.0]], with batch size 4\n",
      "Request id: [7.0] is part of batch group: [[8.0], [7.0]], with batch size 2\n",
      "Request id: [8.0] is part of batch group: [[8.0], [7.0]], with batch size 2\n",
      "Request id: [9.0] is part of batch group: [[6.0], [1.0], [9.0], [5.0]], with batch size 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=66737)\u001b[0m INFO 2022-05-20 11:31:37,256 http_proxy 127.0.0.1 http_proxy.py:320 - POST /BatchSizePredictor 200 2541.8ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=66737)\u001b[0m INFO 2022-05-20 11:31:37,257 http_proxy 127.0.0.1 http_proxy.py:320 - POST /BatchSizePredictor 200 2549.0ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=66737)\u001b[0m INFO 2022-05-20 11:31:37,257 http_proxy 127.0.0.1 http_proxy.py:320 - POST /BatchSizePredictor 200 2541.7ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=66737)\u001b[0m INFO 2022-05-20 11:31:37,258 http_proxy 127.0.0.1 http_proxy.py:320 - POST /BatchSizePredictor 200 2542.5ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=66737)\u001b[0m INFO 2022-05-20 11:31:37,258 http_proxy 127.0.0.1 http_proxy.py:320 - POST /BatchSizePredictor 200 2541.8ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=66737)\u001b[0m INFO 2022-05-20 11:31:37,259 http_proxy 127.0.0.1 http_proxy.py:320 - POST /BatchSizePredictor 200 2543.1ms\n",
      "\u001b[2m\u001b[36m(BatchSizePredictor pid=66778)\u001b[0m INFO 2022-05-20 11:31:37,255 BatchSizePredictor BatchSizePredictor#pvZtwj replica.py:483 - HANDLE __call__ OK 1514.7ms\n",
      "\u001b[2m\u001b[36m(BatchSizePredictor pid=66778)\u001b[0m INFO 2022-05-20 11:31:37,256 BatchSizePredictor BatchSizePredictor#pvZtwj replica.py:483 - HANDLE __call__ OK 1517.2ms\n"
     ]
    }
   ],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, wait\n",
    "\n",
    "with ThreadPoolExecutor() as pool:\n",
    "    futs = [\n",
    "        pool.submit(\n",
    "            requests.post,\n",
    "            \"http://localhost:8000/BatchSizePredictor/\",\n",
    "            json={\"array\": [i]},\n",
    "        )\n",
    "        for i in range(10)\n",
    "    ]\n",
    "    wait(futs)\n",
    "for fut in futs:\n",
    "    i, batch_size, batch_group = fut.result().json()\n",
    "    print(f\"Request id: {i} is part of batch group: {batch_group}, with batch size {batch_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, some of the requests are part of a bigger group that's run together.\n",
    "\n",
    "You can also configure the exact details of batching parameters:\n",
    "- `max_batch_size(int)`: the maximum batch size that will be executed in one call to predict.\n",
    "- `batch_wait_timeout_s (float)`: the maximum duration to wait for `max_batch_size` elements before running the predict call.\n",
    "\n",
    "Let's set a `max_batch_size` of 10 to make them into the same batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(ServeController pid=66733)\u001b[0m INFO 2022-05-20 11:31:40,048 controller 66733 deployment_state.py:1176 - Stopping 1 replicas of deployment 'BatchSizePredictor' with outdated versions.\n",
      "\u001b[2m\u001b[36m(ServeController pid=66733)\u001b[0m INFO 2022-05-20 11:31:42,214 controller 66733 deployment_state.py:1217 - Adding 1 replicas to deployment 'BatchSizePredictor'.\n"
     ]
    }
   ],
   "source": [
    "ModelWrapperDeployment.options(name=\"BatchSizePredictor\").deploy(\n",
    "    predictor_cls=BatchSizePredictor,\n",
    "    checkpoint=local_checkpoint,\n",
    "    batching_params={\"max_batch_size\": 10, \"batch_wait_timeout_s\": 5}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's call them again! You should be able to see all ten requests are now part of the same group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request id: [0.0] is part of batch group: [[0.0], [3.0], [6.0], [1.0], [2.0], [5.0], [7.0], [8.0], [4.0], [9.0]], with batch size 10\n",
      "Request id: [1.0] is part of batch group: [[0.0], [3.0], [6.0], [1.0], [2.0], [5.0], [7.0], [8.0], [4.0], [9.0]], with batch size 10\n",
      "Request id: [2.0] is part of batch group: [[0.0], [3.0], [6.0], [1.0], [2.0], [5.0], [7.0], [8.0], [4.0], [9.0]], with batch size 10\n",
      "Request id: [3.0] is part of batch group: [[0.0], [3.0], [6.0], [1.0], [2.0], [5.0], [7.0], [8.0], [4.0], [9.0]], with batch size 10\n",
      "Request id: [4.0] is part of batch group: [[0.0], [3.0], [6.0], [1.0], [2.0], [5.0], [7.0], [8.0], [4.0], [9.0]], with batch size 10\n",
      "Request id: [5.0] is part of batch group: [[0.0], [3.0], [6.0], [1.0], [2.0], [5.0], [7.0], [8.0], [4.0], [9.0]], with batch size 10\n",
      "Request id: [6.0] is part of batch group: [[0.0], [3.0], [6.0], [1.0], [2.0], [5.0], [7.0], [8.0], [4.0], [9.0]], with batch size 10\n",
      "Request id: [7.0] is part of batch group: [[0.0], [3.0], [6.0], [1.0], [2.0], [5.0], [7.0], [8.0], [4.0], [9.0]], with batch size 10\n",
      "Request id: [8.0] is part of batch group: [[0.0], [3.0], [6.0], [1.0], [2.0], [5.0], [7.0], [8.0], [4.0], [9.0]], with batch size 10\n",
      "Request id: [9.0] is part of batch group: [[0.0], [3.0], [6.0], [1.0], [2.0], [5.0], [7.0], [8.0], [4.0], [9.0]], with batch size 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=66737)\u001b[0m INFO 2022-05-20 11:31:47,562 http_proxy 127.0.0.1 http_proxy.py:320 - POST /BatchSizePredictor 200 540.4ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=66737)\u001b[0m INFO 2022-05-20 11:31:47,563 http_proxy 127.0.0.1 http_proxy.py:320 - POST /BatchSizePredictor 200 533.7ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=66737)\u001b[0m INFO 2022-05-20 11:31:47,564 http_proxy 127.0.0.1 http_proxy.py:320 - POST /BatchSizePredictor 200 527.2ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=66737)\u001b[0m INFO 2022-05-20 11:31:47,564 http_proxy 127.0.0.1 http_proxy.py:320 - POST /BatchSizePredictor 200 531.9ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=66737)\u001b[0m INFO 2022-05-20 11:31:47,564 http_proxy 127.0.0.1 http_proxy.py:320 - POST /BatchSizePredictor 200 529.9ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=66737)\u001b[0m INFO 2022-05-20 11:31:47,564 http_proxy 127.0.0.1 http_proxy.py:320 - POST /BatchSizePredictor 200 526.7ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=66737)\u001b[0m INFO 2022-05-20 11:31:47,565 http_proxy 127.0.0.1 http_proxy.py:320 - POST /BatchSizePredictor 200 525.6ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=66737)\u001b[0m INFO 2022-05-20 11:31:47,565 http_proxy 127.0.0.1 http_proxy.py:320 - POST /BatchSizePredictor 200 525.9ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=66737)\u001b[0m INFO 2022-05-20 11:31:47,565 http_proxy 127.0.0.1 http_proxy.py:320 - POST /BatchSizePredictor 200 526.1ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=66737)\u001b[0m INFO 2022-05-20 11:31:47,566 http_proxy 127.0.0.1 http_proxy.py:320 - POST /BatchSizePredictor 200 526.3ms\n",
      "\u001b[2m\u001b[36m(BatchSizePredictor pid=66784)\u001b[0m INFO 2022-05-20 11:31:47,557 BatchSizePredictor BatchSizePredictor#bQKUHH replica.py:483 - HANDLE __call__ OK 529.8ms\n",
      "\u001b[2m\u001b[36m(BatchSizePredictor pid=66784)\u001b[0m INFO 2022-05-20 11:31:47,558 BatchSizePredictor BatchSizePredictor#bQKUHH replica.py:483 - HANDLE __call__ OK 516.4ms\n",
      "\u001b[2m\u001b[36m(BatchSizePredictor pid=66784)\u001b[0m INFO 2022-05-20 11:31:47,558 BatchSizePredictor BatchSizePredictor#bQKUHH replica.py:483 - HANDLE __call__ OK 515.3ms\n",
      "\u001b[2m\u001b[36m(BatchSizePredictor pid=66784)\u001b[0m INFO 2022-05-20 11:31:47,558 BatchSizePredictor BatchSizePredictor#bQKUHH replica.py:483 - HANDLE __call__ OK 515.2ms\n",
      "\u001b[2m\u001b[36m(BatchSizePredictor pid=66784)\u001b[0m INFO 2022-05-20 11:31:47,558 BatchSizePredictor BatchSizePredictor#bQKUHH replica.py:483 - HANDLE __call__ OK 514.6ms\n",
      "\u001b[2m\u001b[36m(BatchSizePredictor pid=66784)\u001b[0m INFO 2022-05-20 11:31:47,559 BatchSizePredictor BatchSizePredictor#bQKUHH replica.py:483 - HANDLE __call__ OK 513.0ms\n",
      "\u001b[2m\u001b[36m(BatchSizePredictor pid=66784)\u001b[0m INFO 2022-05-20 11:31:47,559 BatchSizePredictor BatchSizePredictor#bQKUHH replica.py:483 - HANDLE __call__ OK 512.0ms\n",
      "\u001b[2m\u001b[36m(BatchSizePredictor pid=66784)\u001b[0m INFO 2022-05-20 11:31:47,559 BatchSizePredictor BatchSizePredictor#bQKUHH replica.py:483 - HANDLE __call__ OK 511.7ms\n",
      "\u001b[2m\u001b[36m(BatchSizePredictor pid=66784)\u001b[0m INFO 2022-05-20 11:31:47,560 BatchSizePredictor BatchSizePredictor#bQKUHH replica.py:483 - HANDLE __call__ OK 511.5ms\n",
      "\u001b[2m\u001b[36m(BatchSizePredictor pid=66784)\u001b[0m INFO 2022-05-20 11:31:47,560 BatchSizePredictor BatchSizePredictor#bQKUHH replica.py:483 - HANDLE __call__ OK 510.8ms\n"
     ]
    }
   ],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, wait\n",
    "\n",
    "with ThreadPoolExecutor() as pool:\n",
    "    futs = [\n",
    "        pool.submit(\n",
    "            requests.post,\n",
    "            \"http://localhost:8000/BatchSizePredictor/\",\n",
    "            json={\"array\": [i]},\n",
    "        )\n",
    "        for i in range(10)\n",
    "    ]\n",
    "    wait(futs)\n",
    "for fut in futs:\n",
    "    i, batch_size, batch_group = fut.result().json()\n",
    "    print(f\"Request id: {i} is part of batch group: {batch_group}, with batch size {batch_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The batching behavior is well defined:\n",
    "- When batching arrays, they are all concatenated into a new array with batch dimension added.\n",
    "- When batching dataframes, they are all concatenated row wise.\n",
    "\n",
    "You can also turn off this behavior by setting `batching_params=False`."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3d292ac03404f7ef6351ef2ed79a7f4abdf879ab5af2497c4aeba036dba01cfa"
  },
  "kernelspec": {
   "display_name": "Python 3.6.9 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
