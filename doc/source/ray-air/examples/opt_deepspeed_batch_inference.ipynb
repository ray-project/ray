{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dfdf1047",
   "metadata": {},
   "source": [
    "# Batch Inference with OPT 30B and Ray Data\n",
    "\n",
    "This notebook was tested on a single p3.16xlarge instance with 8 V100 GPUs.\n",
    "\n",
    "## Set Up\n",
    "Initialize Ray and a runtime environment to ensure that all dependent packages are available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36bb842b-b6b6-4cbc-a4f9-a3a65ec069ce",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-22 11:12:15,071\tINFO worker.py:1314 -- Using address localhost:9031 set in the environment variable RAY_ADDRESS\n",
      "fatal: not a git repository (or any parent up to mount point /home/ray)\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n",
      "2023-04-22 11:12:15,676\tINFO worker.py:1432 -- Connecting to existing Ray cluster at address: 172.31.244.129:9031...\n",
      "2023-04-22 11:12:15,724\tINFO worker.py:1607 -- Connected to Ray cluster. View the dashboard at https://console.anyscale.com/api/v2/sessions/ses_jgkdnu2723aleytwqqhebr12vs/services?redirect_to=dashboard \n",
      "2023-04-22 11:12:15,732\tINFO packaging.py:347 -- Pushing file package 'gcs://_ray_pkg_7ad665e3661cefc8f8037daeb0b5ba6e.zip' (0.03MiB) to Ray cluster...\n",
      "2023-04-22 11:12:15,733\tINFO packaging.py:360 -- Successfully pushed file package 'gcs://_ray_pkg_7ad665e3661cefc8f8037daeb0b5ba6e.zip'.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "    <div style=\"margin-left: 50px;display: flex;flex-direction: row;align-items: center\">\n",
       "        <h3 style=\"color: var(--jp-ui-font-color0)\">Ray</h3>\n",
       "        <svg version=\"1.1\" id=\"ray\" width=\"3em\" viewBox=\"0 0 144.5 144.6\" style=\"margin-left: 3em;margin-right: 3em\">\n",
       "            <g id=\"layer-1\">\n",
       "                <path fill=\"#00a2e9\" class=\"st0\" d=\"M97.3,77.2c-3.8-1.1-6.2,0.9-8.3,5.1c-3.5,6.8-9.9,9.9-17.4,9.6S58,88.1,54.8,81.2c-1.4-3-3-4-6.3-4.1\n",
       "                    c-5.6-0.1-9.9,0.1-13.1,6.4c-3.8,7.6-13.6,10.2-21.8,7.6C5.2,88.4-0.4,80.5,0,71.7c0.1-8.4,5.7-15.8,13.8-18.2\n",
       "                    c8.4-2.6,17.5,0.7,22.3,8c1.3,1.9,1.3,5.2,3.6,5.6c3.9,0.6,8,0.2,12,0.2c1.8,0,1.9-1.6,2.4-2.8c3.5-7.8,9.7-11.8,18-11.9\n",
       "                    c8.2-0.1,14.4,3.9,17.8,11.4c1.3,2.8,2.9,3.6,5.7,3.3c1-0.1,2,0.1,3,0c2.8-0.5,6.4,1.7,8.1-2.7s-2.3-5.5-4.1-7.5\n",
       "                    c-5.1-5.7-10.9-10.8-16.1-16.3C84,38,81.9,37.1,78,38.3C66.7,42,56.2,35.7,53,24.1C50.3,14,57.3,2.8,67.7,0.5\n",
       "                    C78.4-2,89,4.7,91.5,15.3c0.1,0.3,0.1,0.5,0.2,0.8c0.7,3.4,0.7,6.9-0.8,9.8c-1.7,3.2-0.8,5,1.5,7.2c6.7,6.5,13.3,13,19.8,19.7\n",
       "                    c1.8,1.8,3,2.1,5.5,1.2c9.1-3.4,17.9-0.6,23.4,7c4.8,6.9,4.6,16.1-0.4,22.9c-5.4,7.2-14.2,9.9-23.1,6.5c-2.3-0.9-3.5-0.6-5.1,1.1\n",
       "                    c-6.7,6.9-13.6,13.7-20.5,20.4c-1.8,1.8-2.5,3.2-1.4,5.9c3.5,8.7,0.3,18.6-7.7,23.6c-7.9,5-18.2,3.8-24.8-2.9\n",
       "                    c-6.4-6.4-7.4-16.2-2.5-24.3c4.9-7.8,14.5-11,23.1-7.8c3,1.1,4.7,0.5,6.9-1.7C91.7,98.4,98,92.3,104.2,86c1.6-1.6,4.1-2.7,2.6-6.2\n",
       "                    c-1.4-3.3-3.8-2.5-6.2-2.6C99.8,77.2,98.9,77.2,97.3,77.2z M72.1,29.7c5.5,0.1,9.9-4.3,10-9.8c0-0.1,0-0.2,0-0.3\n",
       "                    C81.8,14,77,9.8,71.5,10.2c-5,0.3-9,4.2-9.3,9.2c-0.2,5.5,4,10.1,9.5,10.3C71.8,29.7,72,29.7,72.1,29.7z M72.3,62.3\n",
       "                    c-5.4-0.1-9.9,4.2-10.1,9.7c0,0.2,0,0.3,0,0.5c0.2,5.4,4.5,9.7,9.9,10c5.1,0.1,9.9-4.7,10.1-9.8c0.2-5.5-4-10-9.5-10.3\n",
       "                    C72.6,62.3,72.4,62.3,72.3,62.3z M115,72.5c0.1,5.4,4.5,9.7,9.8,9.9c5.6-0.2,10-4.8,10-10.4c-0.2-5.4-4.6-9.7-10-9.7\n",
       "                    c-5.3-0.1-9.8,4.2-9.9,9.5C115,72.1,115,72.3,115,72.5z M19.5,62.3c-5.4,0.1-9.8,4.4-10,9.8c-0.1,5.1,5.2,10.4,10.2,10.3\n",
       "                    c5.6-0.2,10-4.9,9.8-10.5c-0.1-5.4-4.5-9.7-9.9-9.6C19.6,62.3,19.5,62.3,19.5,62.3z M71.8,134.6c5.9,0.2,10.3-3.9,10.4-9.6\n",
       "                    c0.5-5.5-3.6-10.4-9.1-10.8c-5.5-0.5-10.4,3.6-10.8,9.1c0,0.5,0,0.9,0,1.4c-0.2,5.3,4,9.8,9.3,10\n",
       "                    C71.6,134.6,71.7,134.6,71.8,134.6z\"/>\n",
       "            </g>\n",
       "        </svg>\n",
       "        <table>\n",
       "            <tr>\n",
       "                <td style=\"text-align: left\"><b>Python version:</b></td>\n",
       "                <td style=\"text-align: left\"><b>3.9.15</b></td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                <td style=\"text-align: left\"><b>Ray version:</b></td>\n",
       "                <td style=\"text-align: left\"><b> 3.0.0.dev0</b></td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "    <td style=\"text-align: left\"><b>Dashboard:</b></td>\n",
       "    <td style=\"text-align: left\"><b><a href=\"http://console.anyscale.com/api/v2/sessions/ses_jgkdnu2723aleytwqqhebr12vs/services?redirect_to=dashboard\" target=\"_blank\">http://console.anyscale.com/api/v2/sessions/ses_jgkdnu2723aleytwqqhebr12vs/services?redirect_to=dashboard</a></b></td>\n",
       "</tr>\n",
       "\n",
       "        </table>\n",
       "    </div>\n",
       "</div>\n"
      ],
      "text/plain": [
       "RayContext(dashboard_url='console.anyscale.com/api/v2/sessions/ses_jgkdnu2723aleytwqqhebr12vs/services?redirect_to=dashboard', python_version='3.9.15', ray_version='3.0.0.dev0', ray_commit='17df2ef17983406bb178c251044c9dc654b378c0', address_info={'node_ip_address': '172.31.244.129', 'raylet_ip_address': '172.31.244.129', 'redis_address': None, 'object_store_address': '/tmp/ray/session_2023-04-22_11-09-11_790337_150/sockets/plasma_store', 'raylet_socket_name': '/tmp/ray/session_2023-04-22_11-09-11_790337_150/sockets/raylet', 'webui_url': 'console.anyscale.com/api/v2/sessions/ses_jgkdnu2723aleytwqqhebr12vs/services?redirect_to=dashboard', 'session_dir': '/tmp/ray/session_2023-04-22_11-09-11_790337_150', 'metrics_export_port': 61073, 'gcs_address': '172.31.244.129:9031', 'address': '172.31.244.129:9031', 'dashboard_agent_listen_port': 52365, 'node_id': 'e6e9dfeda4469dd816c080bec2cf1cd12abdd978ae74b87e869164eb'})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ray\n",
    "\n",
    "ray.init(\n",
    "    runtime_env={\n",
    "        \"pip\": [\n",
    "            \"numpy==1.23\",\n",
    "            \"protobuf==3.20.0\",\n",
    "            \"transformers==4.27.2\",\n",
    "            \"accelerate==0.17.1\",\n",
    "            \"deepspeed==0.8.3\",\n",
    "        ],\n",
    "        \"env_vars\": {\n",
    "            \"HF_HUB_DISABLE_PROGRESS_BARS\": \"1\",\n",
    "        }\n",
    "    }\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b619a878",
   "metadata": {},
   "source": [
    "## Define Hyperparameters\n",
    "\n",
    "Define a list of hyperparameters as a global dataclass.\n",
    "\n",
    "Refer to https://deepspeed.readthedocs.io/en/stable/inference-init.html#deepspeed.inference.config.DeepSpeedInferenceConfig for more details about the configurations of a DeepSpeed inference job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "613df744",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Optional\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    model_name: str = \"facebook/opt-30b\"\n",
    "    # Path to HuggingFace cache directory. Default is ~/.cache/huggingface/.\n",
    "    cache_dir: Optional[str] = None\n",
    "    # Path to the directory that actually holds model files.\n",
    "    # e.g., ~/.cache/huggingface/models--facebook--opt-30b/snapshots/xxx/\n",
    "    # If this path is not None, we skip download models from HuggingFace.\n",
    "    repo_root: Optional[str] = None\n",
    "    # This is how many DeepSpeed-inference replicas to run for\n",
    "    # this batch inference job.\n",
    "    num_worker_groups: int = 1\n",
    "    # Number of DeepSpeed workers per group.\n",
    "    num_workers_per_group: int = 8\n",
    "\n",
    "    batch_size: int = 1\n",
    "    dtype: str = \"float16\"\n",
    "    # Maximum number of tokens DeepSpeed inference-engine can work with,\n",
    "    # including the input and output tokens.\n",
    "    max_tokens: int = 1024\n",
    "    # Use meta tensors to initialize model.\n",
    "    use_meta_tensor: bool = True\n",
    "    # Use cache for generation.\n",
    "    use_cache: bool = True\n",
    "    # The path for which we want to save the loaded model with a checkpoint.\n",
    "    save_mp_checkpoint_path: Optional[str] = None\n",
    "\n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "28df05bf",
   "metadata": {},
   "source": [
    "## Download and Cache Model\n",
    "\n",
    "Next, we will download and cache model files on all instances of the cluster before we run the job.\n",
    "\n",
    "Notice that when we download model snapshots from HuggingFace, we skip files that end with safetensors, msgpack, and h5 extensions. These are Tensorflow and JAX weight files. We only need PyTorch weights for this example.\n",
    "\n",
    "We execute the ``download_model()`` function on every node of the cluster by using a ``NodeAffinitySchedulingStrategy`` from Ray Core."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63b8a84d-57a6-4430-8fe8-9811760b8b7c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caching model locally ...\n",
      "Done. Model saved in /home/ray/.cache/huggingface/hub/models--facebook--opt-30b/snapshots/ceea0a90ac0f6fae7c2c34bcb40477438c152546\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from huggingface_hub import snapshot_download\n",
    "import ray\n",
    "from ray.util.scheduling_strategies import NodeAffinitySchedulingStrategy\n",
    "\n",
    "\n",
    "@ray.remote\n",
    "def download_model(config: Config):\n",
    "    # This function downloads the specified HF model into a local directory.\n",
    "    # This can also download models from cloud storages like S3.\n",
    "    return snapshot_download(\n",
    "        repo_id=config.model_name,\n",
    "        cache_dir=config.cache_dir,\n",
    "        allow_patterns=[\"*\"],\n",
    "        # Skip downloading TF and FLAX weight files.\n",
    "        ignore_patterns=[\"*.safetensors\", \"*.msgpack\", \"*.h5\"],\n",
    "        revision=None,\n",
    "    )\n",
    "\n",
    "if config.repo_root is None:\n",
    "    # Download model files to all GPU nodes, and set correct repo_root.\n",
    "    refs = []\n",
    "    for node in ray.nodes():\n",
    "        if node[\"Alive\"] and node[\"Resources\"].get(\"GPU\", None):\n",
    "            node_id = node[\"NodeID\"]\n",
    "            scheduling_strategy = NodeAffinitySchedulingStrategy(\n",
    "                node_id=node_id, soft=False\n",
    "            )\n",
    "            options = {\"scheduling_strategy\": scheduling_strategy}\n",
    "            refs.append(\n",
    "                download_model.options(scheduling_strategy=scheduling_strategy).remote(config)\n",
    "            )\n",
    "\n",
    "    print(\"Caching model locally ...\")\n",
    "\n",
    "    # Wait for models to finish downloading.\n",
    "    config.repo_root = ray.get(refs)[0]\n",
    "\n",
    "    print(f\"Done. Model saved in {config.repo_root}\")\n",
    "else:\n",
    "    print(f\"Using existing model saved in {config.repo_root}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6b14b7d9",
   "metadata": {},
   "source": [
    "## Define DeepSpeed Utility Classes\n",
    "\n",
    "Next, we define a few utility classes and functions that are useful for setting up and running the DeepSpeed inference job.\n",
    "\n",
    "Note that the Pipeline is modeled after https://github.com/microsoft/DeepSpeedExamples/tree/efacebb3ddbea86bb20c3af30fd060be0fa41ac8/inference/huggingface/text-generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9aad2a9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ray/anaconda3/lib/python3.9/site-packages/xgboost/compat.py:31: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import io\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "import deepspeed\n",
    "import torch\n",
    "from deepspeed.runtime.utils import see_memory_usage\n",
    "from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "\n",
    "class DSPipeline:\n",
    "    \"\"\"\n",
    "    Example helper class for comprehending DeepSpeed Meta Tensors, meant to mimic HF pipelines.\n",
    "    The DSPipeline can run with and without meta tensors.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name,\n",
    "        dtype=torch.float16,\n",
    "        is_meta=True,\n",
    "        device=-1,\n",
    "        repo_root=None,\n",
    "    ):\n",
    "        self.model_name = model_name\n",
    "        self.dtype = dtype\n",
    "\n",
    "        if isinstance(device, torch.device):\n",
    "            self.device = device\n",
    "        elif isinstance(device, str):\n",
    "            self.device = torch.device(device)\n",
    "        elif device < 0:\n",
    "            self.device = torch.device(\"cpu\")\n",
    "        else:\n",
    "            self.device = torch.device(f\"cuda:{device}\")\n",
    "\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side=\"right\")\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "\n",
    "        if is_meta:\n",
    "            # When meta tensors enabled, use checkpoints\n",
    "            self.config = AutoConfig.from_pretrained(self.model_name)\n",
    "            self.checkpoints_json = self._generate_json(repo_root)\n",
    "\n",
    "            with deepspeed.OnDevice(dtype=dtype, device=\"meta\"):\n",
    "                self.model = AutoModelForCausalLM.from_config(self.config)\n",
    "        else:\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(self.model_name)\n",
    "\n",
    "        self.model.eval()\n",
    "\n",
    "    def __call__(self, inputs, **kwargs):\n",
    "        input_list = [inputs] if isinstance(inputs, str) else inputs\n",
    "        outputs = self.generate_outputs(input_list, **kwargs)\n",
    "        return outputs\n",
    "\n",
    "    def _generate_json(self, repo_root):\n",
    "        if os.path.exists(os.path.join(repo_root, \"ds_inference_config.json\")):\n",
    "            # Simply use the available inference config.\n",
    "            return os.path.join(repo_root, \"ds_inference_config.json\")\n",
    "\n",
    "        # Write a checkpoints config file in local directory.\n",
    "        checkpoints_json = \"checkpoints.json\"\n",
    "\n",
    "        with io.open(checkpoints_json, \"w\", encoding=\"utf-8\") as f:\n",
    "            file_list = [\n",
    "                str(entry).split(\"/\")[-1]\n",
    "                for entry in Path(repo_root).rglob(\"*.[bp][it][n]\")\n",
    "                if entry.is_file()\n",
    "            ]\n",
    "            data = {\n",
    "                # Hardcode bloom for now.\n",
    "                # Possible choices are \"bloom\", \"ds_model\", \"Megatron\".\n",
    "                \"type\": \"bloom\",\n",
    "                \"checkpoints\": file_list,\n",
    "                \"version\": 1.0\n",
    "            }\n",
    "            json.dump(data, f)\n",
    "\n",
    "        return checkpoints_json\n",
    "\n",
    "    def generate_outputs(self, inputs, **generate_kwargs):\n",
    "        input_tokens = self.tokenizer.batch_encode_plus(\n",
    "            inputs, return_tensors=\"pt\", padding=True\n",
    "        )\n",
    "        for t in input_tokens:\n",
    "            if torch.is_tensor(input_tokens[t]):\n",
    "                input_tokens[t] = input_tokens[t].to(self.device)\n",
    "\n",
    "        self.model.cuda().to(self.device)\n",
    "\n",
    "        outputs = self.model.generate(**input_tokens, **generate_kwargs)\n",
    "        outputs = self.tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "\n",
    "def _memory_usage(gpu_id: int, msg: str):\n",
    "    \"\"\"Print memory usage.\"\"\"\n",
    "    if gpu_id != 0:\n",
    "        return\n",
    "    see_memory_usage(msg, True)\n",
    "\n",
    "\n",
    "def init_model(config: Config, world_size: int, gpu_id: int) -> DSPipeline:\n",
    "    \"\"\"Initialize the deepspeed model.\"\"\"\n",
    "    data_type = getattr(torch, config.dtype)\n",
    "\n",
    "    _memory_usage(gpu_id, \"before init\")\n",
    "    pipe = DSPipeline(\n",
    "        model_name=config.model_name,\n",
    "        dtype=data_type,\n",
    "        is_meta=config.use_meta_tensor,\n",
    "        device=gpu_id,\n",
    "        repo_root=config.repo_root,\n",
    "    )\n",
    "    _memory_usage(gpu_id, \"after init\")\n",
    "\n",
    "    if config.use_meta_tensor:\n",
    "        ds_kwargs = dict(\n",
    "            base_dir=config.repo_root, checkpoint=pipe.checkpoints_json\n",
    "        )\n",
    "    else:\n",
    "        ds_kwargs = dict()\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "    pipe.model = deepspeed.init_inference(\n",
    "        pipe.model,\n",
    "        dtype=data_type,\n",
    "        mp_size=world_size,\n",
    "        replace_with_kernel_inject=True,\n",
    "        replace_method=True,\n",
    "        max_tokens=config.max_tokens,\n",
    "        save_mp_checkpoint_path=config.save_mp_checkpoint_path,\n",
    "        **ds_kwargs,\n",
    "    )\n",
    "    _memory_usage(gpu_id, \"after init_inference\")\n",
    "\n",
    "    return pipe\n",
    "\n",
    "\n",
    "def generate(\n",
    "    input_sentences: List[str], pipe: DSPipeline, batch_size: int, **generate_kwargs\n",
    ") -> List[str]:\n",
    "    \"\"\"Generate predictions using a DSPipeline.\"\"\"\n",
    "    if batch_size > len(input_sentences):\n",
    "        # Dynamically extend to support larger bs by repetition.\n",
    "        input_sentences *= math.ceil(batch_size / len(input_sentences))\n",
    "\n",
    "    inputs = input_sentences[:batch_size]\n",
    "    outputs = pipe(inputs, **generate_kwargs)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd20d4d9",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "62eee91d",
   "metadata": {},
   "source": [
    "## Define a DeepSpeed Predictor\n",
    "\n",
    "Define an AIR Predictor to be instantiated by the Dataset pipeline below.\n",
    "\n",
    "Each DeepSpeedPredictor is a stateful Ray actor that understands how to process the input prompt using a group of DeepSpeed inference workers.\n",
    "\n",
    "More specifically, each DeepSpeedPredictor sets up a proper PyTorch DDP process group before spinning up multiple PredictionWorkers. Since the model is loaded using the DeepSpeed inference framework, each PredictionWorker handles a shard of the entire DeepSpeed inference model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "516a200d-14e4-4b52-a615-e09778ba4117",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "import pandas as pd\n",
    "import ray\n",
    "import ray.util\n",
    "from ray.air import Checkpoint, ScalingConfig\n",
    "from ray.air.util.torch_dist import (\n",
    "    TorchDistributedWorker,\n",
    "    init_torch_dist_process_group,\n",
    "    shutdown_torch_dist_process_group,\n",
    ")\n",
    "from ray.train.predictor import Predictor\n",
    "from ray.util.scheduling_strategies import PlacementGroupSchedulingStrategy\n",
    "\n",
    "\n",
    "@ray.remote\n",
    "class PredictionWorker(TorchDistributedWorker):\n",
    "    \"\"\"A PredictionWorker is a Ray remote actor that runs a single shard of a DeepSpeed job.\n",
    "    \n",
    "    Multiple PredictionWorkers of the same WorkerGroup form a PyTorch DDP process\n",
    "    group and work together under the orchestration of DeepSpeed.\n",
    "    \"\"\"\n",
    "    def __init__(self, config: Config, world_size: int):\n",
    "        self.config = config\n",
    "        self.world_size = world_size\n",
    "\n",
    "    def init_model(self, local_rank: int):\n",
    "        \"\"\"Initialize model for inference.\"\"\"\n",
    "        # Note: We have to provide the local_rank that was used to initiate\n",
    "        # the DDP process group here. e.g., a PredictionWorker may be the\n",
    "        # rank 0 worker of a group, but occupies gpu 7.\n",
    "        self.generator = init_model(self.config, self.world_size, local_rank)\n",
    "\n",
    "    def generate(self, data: pd.DataFrame, column: str, **kwargs) -> List[str]:\n",
    "        return generate(\n",
    "            list(data[column]), self.generator, self.config.batch_size, **kwargs\n",
    "        )\n",
    "\n",
    "\n",
    "# TODO: This Predictor should be part of Ray AIR.\n",
    "class DeepSpeedPredictor(Predictor):\n",
    "    def __init__(self, checkpoint: Checkpoint, scaling_config: ScalingConfig) -> None:\n",
    "        self.checkpoint = checkpoint\n",
    "        self.scaling_config = scaling_config\n",
    "        self.init_worker_group(scaling_config)\n",
    "\n",
    "    def __del__(self):\n",
    "        shutdown_torch_dist_process_group(self.prediction_workers)\n",
    "\n",
    "    def init_worker_group(self, scaling_config: ScalingConfig):\n",
    "        \"\"\"Create the worker group.\n",
    "\n",
    "        Each worker in the group communicates with other workers through the\n",
    "        torch distributed backend. The worker group is inelastic (a failure of\n",
    "        one worker destroys the entire group). Each worker in the group\n",
    "        recieves the same input data and outputs the same generated text.\n",
    "        \"\"\"\n",
    "        config = self.checkpoint.to_dict()[\"config\"]\n",
    "\n",
    "        # Start a placement group for the workers.\n",
    "        self.pg = scaling_config.as_placement_group_factory().to_placement_group()\n",
    "        prediction_worker_cls = PredictionWorker.options(\n",
    "            num_cpus=scaling_config.num_cpus_per_worker,\n",
    "            num_gpus=scaling_config.num_gpus_per_worker,\n",
    "            resources=scaling_config.additional_resources_per_worker,\n",
    "            scheduling_strategy=PlacementGroupSchedulingStrategy(\n",
    "                placement_group=self.pg, placement_group_capture_child_tasks=True\n",
    "            ),\n",
    "        )\n",
    "        # Create the prediction workers.\n",
    "        self.prediction_workers = [\n",
    "            prediction_worker_cls.remote(config, scaling_config.num_workers)\n",
    "            for i in range(scaling_config.num_workers)\n",
    "        ]\n",
    "\n",
    "        # Initialize torch distributed process group for the workers.\n",
    "        local_ranks = init_torch_dist_process_group(self.prediction_workers, backend=\"nccl\")\n",
    "\n",
    "        # Initialize the model on each worker.\n",
    "        ray.get([\n",
    "            worker.init_model.remote(local_rank)\n",
    "            for worker, local_rank in zip(self.prediction_workers, local_ranks)\n",
    "        ])\n",
    "\n",
    "    def _predict_pandas(\n",
    "        self,\n",
    "        data: pd.DataFrame,\n",
    "        input_column: str = \"prompt\",\n",
    "        output_column: str = \"output\",\n",
    "        **kwargs\n",
    "    ) -> pd.DataFrame:\n",
    "        data_ref = ray.put(data)\n",
    "        prediction = ray.get(\n",
    "            [\n",
    "                worker.generate.remote(data_ref, column=input_column, **kwargs)\n",
    "                for worker in self.prediction_workers\n",
    "            ]\n",
    "        )[0]\n",
    "\n",
    "        return pd.DataFrame(prediction, columns=[output_column])\n",
    "\n",
    "    @classmethod\n",
    "    def from_checkpoint(cls, checkpoint: Checkpoint, **kwargs) -> \"Predictor\":\n",
    "        return cls(checkpoint=checkpoint, **kwargs)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ca57e150",
   "metadata": {},
   "source": [
    "## Create a Dataset Pipeline\n",
    "\n",
    "Finally, we connect all these pieces together, and use a BatchPredictor to run multiple copies of the DeepSpeedPredictor actors.\n",
    "\n",
    "This step helps parallelize our batch inference job and utilize all available resources in the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48bf4a4f-0ac4-4e77-a05a-710d42e0dc4e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-22 11:14:12,074\tWARNING dataset.py:4124 -- Deprecation warning: use Dataset.materialize() instead of fully_executed().\n",
      "2023-04-22 11:14:12,079\tINFO streaming_executor.py:87 -- Executing DAG InputDataBuffer[Input] -> AllToAllOperator[Repartition] -> AllToAllOperator[RandomShuffle]\n",
      "2023-04-22 11:14:12,081\tINFO streaming_executor.py:88 -- Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=None), locality_with_output=False, preserve_order=False, actor_locality_enabled=True, verbose_progress=False)\n",
      "2023-04-22 11:14:12,082\tINFO streaming_executor.py:90 -- Tip: To enable per-operator progress reporting, set RAY_DATA_VERBOSE_PROGRESS=1.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "- Repartition 1:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "- RandomShuffle 3:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-22 11:14:12,680\tINFO streaming_executor.py:87 -- Executing DAG InputDataBuffer[Input] -> ActorPoolMapOperator[MapBatches(ScoringWrapper)]\n",
      "2023-04-22 11:14:12,682\tINFO streaming_executor.py:88 -- Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=None), locality_with_output=False, preserve_order=False, actor_locality_enabled=True, verbose_progress=False)\n",
      "2023-04-22 11:14:12,683\tINFO streaming_executor.py:90 -- Tip: To enable per-operator progress reporting, set RAY_DATA_VERBOSE_PROGRESS=1.\n",
      "2023-04-22 11:14:12,785\tINFO actor_pool_map_operator.py:114 -- MapBatches(ScoringWrapper): Waiting for 1 pool actors to start...\n",
      "(_MapWorker pid=7005) The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n",
      "0it [00:00, ?it/s]05) \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(PredictionWorker pid=10038) [2023-04-22 11:14:30,762] [INFO] [utils.py:829:see_memory_usage] before init\n",
      "(PredictionWorker pid=10038) [2023-04-22 11:14:30,762] [INFO] [utils.py:830:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.0 GB         Max_CA 0 GB \n",
      "(PredictionWorker pid=10038) [2023-04-22 11:14:30,762] [INFO] [utils.py:838:see_memory_usage] CPU Virtual Memory:  used = 11.63 GB, percent = 2.4%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(PredictionWorker pid=10040) --------------------------------------------------------------------------\n",
      "(PredictionWorker pid=10040)                  Aim collects anonymous usage analytics.                 \n",
      "(PredictionWorker pid=10040)                         Read how to opt-out here:                         \n",
      "(PredictionWorker pid=10040)     https://aimstack.readthedocs.io/en/latest/community/telemetry.html    \n",
      "(PredictionWorker pid=10040) --------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(PredictionWorker pid=10045) [2023-04-22 11:14:33,061] [INFO] [logging.py:93:log_dist] [Rank -1] DeepSpeed info: version=0.8.3, git-hash=unknown, git-branch=unknown\n",
      "(PredictionWorker pid=10045) [2023-04-22 11:14:33,062] [WARNING] [config_utils.py:75:_process_deprecated_field] Config parameter replace_method is deprecated. This parameter is no longer needed, please remove from your call to DeepSpeed-inference\n",
      "(PredictionWorker pid=10045) [2023-04-22 11:14:33,062] [WARNING] [config_utils.py:75:_process_deprecated_field] Config parameter mp_size is deprecated use tensor_parallel.tp_size instead\n",
      "(PredictionWorker pid=10045) [2023-04-22 11:14:33,062] [INFO] [logging.py:93:log_dist] [Rank -1] quantize_bits = 8 mlp_extra_grouping = False, quantize_groups = 1\n",
      "(PredictionWorker pid=10038) [2023-04-22 11:14:33,074] [INFO] [utils.py:829:see_memory_usage] after init\n",
      "(PredictionWorker pid=10038) [2023-04-22 11:14:33,075] [INFO] [utils.py:830:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.0 GB         Max_CA 0 GB \n",
      "(PredictionWorker pid=10038) [2023-04-22 11:14:33,075] [INFO] [utils.py:838:see_memory_usage] CPU Virtual Memory:  used = 12.25 GB, percent = 2.6%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(PredictionWorker pid=10040) Using /home/ray/.cache/torch_extensions/py39_cu116 as PyTorch extensions root...\n",
      "(PredictionWorker pid=10038) Creating extension directory /home/ray/.cache/torch_extensions/py39_cu116/transformer_inference...\n",
      "(PredictionWorker pid=10038) Detected CUDA files, patching ldflags\n",
      "(PredictionWorker pid=10038) Emitting ninja build file /home/ray/.cache/torch_extensions/py39_cu116/transformer_inference/build.ninja...\n",
      "(PredictionWorker pid=10038) Building extension module transformer_inference...\n",
      "(PredictionWorker pid=10038) Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(PredictionWorker pid=10038) [1/9] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=transformer_inference -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/home/ray/anaconda3/lib/python3.9/site-packages/deepspeed/ops/csrc/transformer/inference/includes -I/home/ray/anaconda3/lib/python3.9/site-packages/deepspeed/ops/csrc/includes -isystem /home/ray/anaconda3/lib/python3.9/site-packages/torch/include -isystem /home/ray/anaconda3/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /home/ray/anaconda3/lib/python3.9/site-packages/torch/include/TH -isystem /home/ray/anaconda3/lib/python3.9/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /home/ray/anaconda3/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_70,code=compute_70 -gencode=arch=compute_70,code=sm_70 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++14 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_70,code=compute_70 -c /home/ray/anaconda3/lib/python3.9/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/dequantize.cu -o dequantize.cuda.o \n",
      "(PredictionWorker pid=10038) [2/9] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=transformer_inference -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/home/ray/anaconda3/lib/python3.9/site-packages/deepspeed/ops/csrc/transformer/inference/includes -I/home/ray/anaconda3/lib/python3.9/site-packages/deepspeed/ops/csrc/includes -isystem /home/ray/anaconda3/lib/python3.9/site-packages/torch/include -isystem /home/ray/anaconda3/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /home/ray/anaconda3/lib/python3.9/site-packages/torch/include/TH -isystem /home/ray/anaconda3/lib/python3.9/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /home/ray/anaconda3/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_70,code=compute_70 -gencode=arch=compute_70,code=sm_70 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++14 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_70,code=compute_70 -c /home/ray/anaconda3/lib/python3.9/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/relu.cu -o relu.cuda.o \n",
      "(PredictionWorker pid=10038) [3/9] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=transformer_inference -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/home/ray/anaconda3/lib/python3.9/site-packages/deepspeed/ops/csrc/transformer/inference/includes -I/home/ray/anaconda3/lib/python3.9/site-packages/deepspeed/ops/csrc/includes -isystem /home/ray/anaconda3/lib/python3.9/site-packages/torch/include -isystem /home/ray/anaconda3/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /home/ray/anaconda3/lib/python3.9/site-packages/torch/include/TH -isystem /home/ray/anaconda3/lib/python3.9/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /home/ray/anaconda3/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_70,code=compute_70 -gencode=arch=compute_70,code=sm_70 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++14 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_70,code=compute_70 -c /home/ray/anaconda3/lib/python3.9/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/apply_rotary_pos_emb.cu -o apply_rotary_pos_emb.cuda.o \n",
      "(PredictionWorker pid=10038) [4/9] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=transformer_inference -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/home/ray/anaconda3/lib/python3.9/site-packages/deepspeed/ops/csrc/transformer/inference/includes -I/home/ray/anaconda3/lib/python3.9/site-packages/deepspeed/ops/csrc/includes -isystem /home/ray/anaconda3/lib/python3.9/site-packages/torch/include -isystem /home/ray/anaconda3/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /home/ray/anaconda3/lib/python3.9/site-packages/torch/include/TH -isystem /home/ray/anaconda3/lib/python3.9/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /home/ray/anaconda3/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_70,code=compute_70 -gencode=arch=compute_70,code=sm_70 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++14 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_70,code=compute_70 -c /home/ray/anaconda3/lib/python3.9/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/transform.cu -o transform.cuda.o \n",
      "(PredictionWorker pid=10038) /home/ray/anaconda3/lib/python3.9/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/transform.cu(56): warning #177-D: variable \"lane\" was declared but never referenced\n",
      "(PredictionWorker pid=10038) \n",
      "(PredictionWorker pid=10038) /home/ray/anaconda3/lib/python3.9/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/transform.cu(93): warning #177-D: variable \"half_dim\" was declared but never referenced\n",
      "(PredictionWorker pid=10038) \n",
      "(PredictionWorker pid=10038) /home/ray/anaconda3/lib/python3.9/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/transform.cu(110): warning #177-D: variable \"vals_half\" was declared but never referenced\n",
      "(PredictionWorker pid=10038) \n",
      "(PredictionWorker pid=10038) /home/ray/anaconda3/lib/python3.9/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/transform.cu(111): warning #177-D: variable \"output_half\" was declared but never referenced\n",
      "(PredictionWorker pid=10038) \n",
      "(PredictionWorker pid=10038) /home/ray/anaconda3/lib/python3.9/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/transform.cu(128): warning #177-D: variable \"lane\" was declared but never referenced\n",
      "(PredictionWorker pid=10038) \n",
      "(PredictionWorker pid=10038) [5/9] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=transformer_inference -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/home/ray/anaconda3/lib/python3.9/site-packages/deepspeed/ops/csrc/transformer/inference/includes -I/home/ray/anaconda3/lib/python3.9/site-packages/deepspeed/ops/csrc/includes -isystem /home/ray/anaconda3/lib/python3.9/site-packages/torch/include -isystem /home/ray/anaconda3/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /home/ray/anaconda3/lib/python3.9/site-packages/torch/include/TH -isystem /home/ray/anaconda3/lib/python3.9/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /home/ray/anaconda3/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_70,code=compute_70 -gencode=arch=compute_70,code=sm_70 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++14 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_70,code=compute_70 -c /home/ray/anaconda3/lib/python3.9/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/softmax.cu -o softmax.cuda.o \n",
      "(PredictionWorker pid=10038) /home/ray/anaconda3/lib/python3.9/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/softmax.cu(272): warning #177-D: variable \"alibi_offset\" was declared but never referenced\n",
      "(PredictionWorker pid=10038) \n",
      "(PredictionWorker pid=10038) /home/ray/anaconda3/lib/python3.9/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/softmax.cu(427): warning #177-D: variable \"warp_num\" was declared but never referenced\n",
      "(PredictionWorker pid=10038) \n",
      "(PredictionWorker pid=10038) [6/9] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=transformer_inference -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/home/ray/anaconda3/lib/python3.9/site-packages/deepspeed/ops/csrc/transformer/inference/includes -I/home/ray/anaconda3/lib/python3.9/site-packages/deepspeed/ops/csrc/includes -isystem /home/ray/anaconda3/lib/python3.9/site-packages/torch/include -isystem /home/ray/anaconda3/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /home/ray/anaconda3/lib/python3.9/site-packages/torch/include/TH -isystem /home/ray/anaconda3/lib/python3.9/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /home/ray/anaconda3/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_70,code=compute_70 -gencode=arch=compute_70,code=sm_70 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++14 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_70,code=compute_70 -c /home/ray/anaconda3/lib/python3.9/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/gelu.cu -o gelu.cuda.o \n",
      "(PredictionWorker pid=10038) [2023-04-22 11:14:33,250] [INFO] [logging.py:93:log_dist] [Rank -1] DeepSpeed info: version=0.8.3, git-hash=unknown, git-branch=unknown [repeated 7x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)\n",
      "(PredictionWorker pid=10038) [2023-04-22 11:14:33,251] [WARNING] [config_utils.py:75:_process_deprecated_field] Config parameter replace_method is deprecated. This parameter is no longer needed, please remove from your call to DeepSpeed-inference [repeated 7x across cluster]\n",
      "(PredictionWorker pid=10038) [2023-04-22 11:14:33,251] [WARNING] [config_utils.py:75:_process_deprecated_field] Config parameter mp_size is deprecated use tensor_parallel.tp_size instead [repeated 7x across cluster]\n",
      "(PredictionWorker pid=10038) [2023-04-22 11:14:33,251] [INFO] [logging.py:93:log_dist] [Rank -1] quantize_bits = 8 mlp_extra_grouping = False, quantize_groups = 1 [repeated 7x across cluster]\n",
      "(PredictionWorker pid=10038) [7/9] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=transformer_inference -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/home/ray/anaconda3/lib/python3.9/site-packages/deepspeed/ops/csrc/transformer/inference/includes -I/home/ray/anaconda3/lib/python3.9/site-packages/deepspeed/ops/csrc/includes -isystem /home/ray/anaconda3/lib/python3.9/site-packages/torch/include -isystem /home/ray/anaconda3/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /home/ray/anaconda3/lib/python3.9/site-packages/torch/include/TH -isystem /home/ray/anaconda3/lib/python3.9/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /home/ray/anaconda3/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_70,code=compute_70 -gencode=arch=compute_70,code=sm_70 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++14 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_70,code=compute_70 -c /home/ray/anaconda3/lib/python3.9/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/layer_norm.cu -o layer_norm.cuda.o \n",
      "(PredictionWorker pid=10038) [8/9] c++ -MMD -MF pt_binding.o.d -DTORCH_EXTENSION_NAME=transformer_inference -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/home/ray/anaconda3/lib/python3.9/site-packages/deepspeed/ops/csrc/transformer/inference/includes -I/home/ray/anaconda3/lib/python3.9/site-packages/deepspeed/ops/csrc/includes -isystem /home/ray/anaconda3/lib/python3.9/site-packages/torch/include -isystem /home/ray/anaconda3/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /home/ray/anaconda3/lib/python3.9/site-packages/torch/include/TH -isystem /home/ray/anaconda3/lib/python3.9/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /home/ray/anaconda3/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -O3 -std=c++14 -g -Wno-reorder -c /home/ray/anaconda3/lib/python3.9/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp -o pt_binding.o \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(PredictionWorker pid=10038) Loading extension module transformer_inference...\n",
      "(PredictionWorker pid=10041) -------------------------------------------------------------------------- [repeated 14x across cluster]\n",
      "(PredictionWorker pid=10041)                  Aim collects anonymous usage analytics.                  [repeated 7x across cluster]\n",
      "(PredictionWorker pid=10041)                         Read how to opt-out here:                          [repeated 7x across cluster]\n",
      "(PredictionWorker pid=10041)     https://aimstack.readthedocs.io/en/latest/community/telemetry.html     [repeated 7x across cluster]\n",
      "(PredictionWorker pid=10041) Using /home/ray/.cache/torch_extensions/py39_cu116 as PyTorch extensions root... [repeated 7x across cluster]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(PredictionWorker pid=10038) [9/9] c++ pt_binding.o gelu.cuda.o relu.cuda.o layer_norm.cuda.o softmax.cuda.o dequantize.cuda.o apply_rotary_pos_emb.cuda.o transform.cuda.o -shared -lcurand -L/home/ray/anaconda3/lib/python3.9/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda_cu -ltorch_cuda_cpp -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o transformer_inference.so\n",
      "(PredictionWorker pid=10038) Time to load transformer_inference op: 46.834928035736084 seconds\n",
      "(PredictionWorker pid=10038) [2023-04-22 11:15:21,799] [INFO] [logging.py:93:log_dist] [Rank 0] DeepSpeed-Inference config: {'layer_id': 0, 'hidden_size': 7168, 'intermediate_size': 28672, 'heads': 56, 'num_hidden_layers': -1, 'fp16': True, 'pre_layer_norm': True, 'local_rank': -1, 'stochastic_mode': False, 'epsilon': 1e-12, 'mp_size': 8, 'q_int8': False, 'scale_attention': True, 'triangular_masking': True, 'local_attention': False, 'window_size': 1, 'rotary_dim': -1, 'rotate_half': False, 'rotate_every_two': True, 'return_tuple': True, 'mlp_after_attn': True, 'mlp_act_func_type': <ActivationFuncType.ReLU: 2>, 'specialized_mode': False, 'training_mp_size': 1, 'bigscience_bloom': False, 'max_out_tokens': 1024, 'scale_attn_by_inverse_layer_idx': False, 'enable_qkv_quantization': False, 'use_mup': False, 'return_single_tuple': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(PredictionWorker pid=10040) No modifications detected for re-loaded extension module transformer_inference, skipping build step...\n",
      "Loading 7 checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]\n",
      "Loading 7 checkpoint shards:  14%|█▍        | 1/7 [00:39<03:57, 39.57s/it]\n",
      "(PredictionWorker pid=10041) Loading extension module transformer_inference... [repeated 15x across cluster]\n",
      "(PredictionWorker pid=10041) Using /home/ray/.cache/torch_extensions/py39_cu116 as PyTorch extensions root... [repeated 8x across cluster]\n",
      "(PredictionWorker pid=10041) No modifications detected for re-loaded extension module transformer_inference, skipping build step... [repeated 7x across cluster]\n",
      "Loading 7 checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s] [repeated 7x across cluster]\n",
      "Loading 7 checkpoint shards:  29%|██▊       | 2/7 [01:15<03:06, 37.25s/it] [repeated 8x across cluster]\n",
      "Loading 7 checkpoint shards:  29%|██▊       | 2/7 [01:28<03:42, 44.58s/it] [repeated 7x across cluster]\n",
      "Loading 7 checkpoint shards:  43%|████▎     | 3/7 [01:51<02:26, 36.73s/it]\n",
      "Loading 7 checkpoint shards:  43%|████▎     | 3/7 [01:51<02:26, 36.56s/it]\n",
      "Loading 7 checkpoint shards:  43%|████▎     | 3/7 [01:57<02:34, 38.58s/it] [repeated 5x across cluster]\n",
      "Loading 7 checkpoint shards:  43%|████▎     | 3/7 [02:03<02:41, 40.32s/it]\n",
      "Loading 7 checkpoint shards:  57%|█████▋    | 4/7 [02:24<01:45, 35.29s/it]\n",
      "Loading 7 checkpoint shards:  57%|█████▋    | 4/7 [02:31<01:50, 36.96s/it] [repeated 6x across cluster]\n",
      "Loading 7 checkpoint shards:  71%|███████▏  | 5/7 [02:59<01:09, 34.92s/it] [repeated 2x across cluster]\n",
      "Loading 7 checkpoint shards:  86%|████████▌ | 6/7 [03:05<00:24, 24.84s/it] [repeated 10x across cluster]\n",
      "Loading 7 checkpoint shards:  86%|████████▌ | 6/7 [03:10<00:25, 25.15s/it] [repeated 4x across cluster]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(PredictionWorker pid=10044) checkpoint loading time at rank 6: 216.07904958724976 sec\n",
      "(PredictionWorker pid=10040) Time to load transformer_inference op: 0.03857231140136719 seconds [repeated 15x across cluster]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading 7 checkpoint shards: 100%|██████████| 7/7 [03:36<00:00, 30.87s/it]\n",
      "Loading 7 checkpoint shards: 100%|██████████| 7/7 [03:36<00:00, 30.87s/it]\n",
      "Loading 7 checkpoint shards: 100%|██████████| 7/7 [03:43<00:00, 31.88s/it] [repeated 6x across cluster]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(PredictionWorker pid=10040) checkpoint loading time at rank 1: 223.18208837509155 sec [repeated 6x across cluster]\n",
      "(PredictionWorker pid=10038) [2023-04-22 11:19:13,839] [INFO] [utils.py:829:see_memory_usage] after init_inference\n",
      "(PredictionWorker pid=10038) [2023-04-22 11:19:13,840] [INFO] [utils.py:830:see_memory_usage] MA 7.69 GB         Max_MA 7.69 GB         CA 7.83 GB         Max_CA 8 GB \n",
      "(PredictionWorker pid=10038) [2023-04-22 11:19:13,840] [INFO] [utils.py:838:see_memory_usage] CPU Virtual Memory:  used = 22.22 GB, percent = 4.6%\n",
      "(PredictionWorker pid=10039) [2023-04-22 11:19:13,840] [INFO] [utils.py:838:see_memory_usage] CPU Virtual Memory:  used = 22.22 GB, percent = 4.6%\n",
      "(PredictionWorker pid=10038) ------------------------------------------------------\n",
      "(PredictionWorker pid=10038) Free memory : 6.587830 (GigaBytes)  \n",
      "(PredictionWorker pid=10038) Total memory: 15.781921 (GigaBytes)  \n",
      "(PredictionWorker pid=10038) Requested memory: 0.601562 (GigaBytes) \n",
      "(PredictionWorker pid=10038) Setting maximum total tokens (input + output) to 1024 \n",
      "(PredictionWorker pid=10038) ------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(PredictionWorker pid=10040) 2023-04-22 11:19:26.855845: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "(PredictionWorker pid=10040) 2023-04-22 11:19:26.856002: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "(PredictionWorker pid=10040) 2023-04-22 11:19:26.856022: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "(PredictionWorker pid=10039) 2023-04-22 11:19:26.856022: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               output\n",
      "0   DeepSpeed is the one to go with. No need for a...\n",
      "1   Testimonials:\\n\\nG. SACCHIOULAS (TX)\\n\\n\"We bo...\n",
      "2   Testimonials\\n\\nI received my order today, I'm...\n",
      "3   Testimonials\\n\\nWhat do our clients say about ...\n",
      "4   How can you make them that high?\\nI edited the...\n",
      "..                                                ...\n",
      "59  Please complete the form below to request more...\n",
      "60  DeepSpeed is the most popular way of dealing t...\n",
      "61  How can you not tell that's not a real tweet?\\...\n",
      "62  Testimonials\\n\\n\"The staff and community of H....\n",
      "63  DeepSpeed is an independent, privately held co...\n",
      "\n",
      "[64 rows x 1 columns]\n",
      "(autoscaler +12m27s) Tip: use `ray status` to view detailed cluster status. To disable these messages, set RAY_SCHEDULER_EVENTS=0.\n",
      "(autoscaler +12m27s) Resized to 64 CPUs, 8 GPUs.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ray\n",
    "from ray.air import Checkpoint, ScalingConfig\n",
    "from ray.train.batch_predictor import BatchPredictor\n",
    "\n",
    "# Disable terminal progress bar for notebook environments.\n",
    "ray.data.set_progress_bars(False)\n",
    "\n",
    "# Prompts.\n",
    "# For testing purpose, we create 64 prompts in total.\n",
    "df = pd.DataFrame(\n",
    "    [\n",
    "        \"DeepSpeed is\",\n",
    "        \"Test\",\n",
    "        \"Please complete\",\n",
    "        \"How can you\"\n",
    "    ] * 16,\n",
    "    columns=[\"prompt\"]\n",
    ")\n",
    "ds = (\n",
    "    ray.data.from_pandas(df)\n",
    "    # Make sure there are enough blocks for parallelized execution.\n",
    "    .repartition(config.num_workers_per_group * 2)\n",
    "    .random_shuffle()\n",
    "    .fully_executed()\n",
    ")\n",
    "\n",
    "# Scaling config for one worker group.\n",
    "group_scaling_config = ScalingConfig(\n",
    "    use_gpu=True,\n",
    "    num_workers=config.num_workers_per_group,\n",
    "    # Should not be necessary after we switch to the new API.\n",
    "    trainer_resources={\"CPU\": 0},\n",
    ")\n",
    "batch_predictor = BatchPredictor.from_checkpoint(\n",
    "    # TODO: Use HugginFaceDeepSpeedCheckpoint when it's available.\n",
    "    Checkpoint.from_dict({\"config\": config}),\n",
    "    DeepSpeedPredictor,\n",
    "    scaling_config=group_scaling_config,\n",
    ")\n",
    "\n",
    "# Batch prediction.\n",
    "pred = batch_predictor.predict(\n",
    "    ds,\n",
    "    batch_size=1,\n",
    "    num_cpus_per_worker=0,\n",
    "    min_scoring_workers=config.num_worker_groups,\n",
    "    max_scoring_workers=config.num_worker_groups,\n",
    "    # Kwargs passed to model.generate()\n",
    "    do_sample=True,\n",
    "    temperature=0.9,\n",
    "    max_length=100,\n",
    ")\n",
    "\n",
    "# Let's see the genreated texts.\n",
    "print(pred.to_pandas())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
