{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VaFMt6AIhYbK"
      },
      "source": [
        "This notebook is adapted from \"components keras\" [tutorial](https://www.tensorflow.org/tfx/tutorials/tfx/components_keras).\n",
        "The goal is to predict whether a trip may generate a big tip.\n",
        "\n",
        "In this example, we showcase how to convert a tfx pipeline to [Ray AIR](https://docs.ray.io/en/latest/ray-air/getting-started.html), covering\n",
        "every step from data ingestion to pushing a model to serving.\n",
        "\n",
        "1. Read a CSV file into ray dataset.\n",
        "2. Process the dataset by chaining a variety of off-the-shelf preprocessors.\n",
        "3. Train the model using distributed tensorflow with few lines of code.\n",
        "4. Serve the model that will apply the same preprocessing to the incoming requests.\n",
        "\n",
        "Note, ``ray.ml.checkpoint.Checkpoint`` serves as the bridge between step 3 and step 4.\n",
        "By capturing both model and preprocessing steps in a way compatible with Ray Serve, this\n",
        "abstraction makes sure ml workload can transition seamlessly between training and\n",
        "serving."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQbdfyWQhYbO"
      },
      "source": [
        "Uncomment and run the following line in order to install all the necessary dependencies:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "YajFzmkthYbO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6dd3a4f-8435-4202-9322-827222de4509"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow>=2.8.0 in /usr/local/lib/python3.7/dist-packages (2.8.0+zzzcolab20220506162203)\n",
            "Collecting ray[data,serve,tune]>=1.12.1\n",
            "  Downloading ray-1.12.1-cp37-cp37m-manylinux2014_x86_64.whl (53.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 53.2 MB 128 kB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.8.0) (0.25.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.8.0) (1.15.0)\n",
            "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.8.0) (2.8.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.8.0) (1.1.0)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.8.0) (1.0.0)\n",
            "Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.8.0) (2.8.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.8.0) (14.0.1)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.8.0) (3.1.0)\n",
            "Collecting tf-estimator-nightly==2.8.0.dev2021122109\n",
            "  Downloading tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl (462 kB)\n",
            "\u001b[K     |████████████████████████████████| 462 kB 38.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.8.0) (3.3.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.8.0) (3.17.3)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.8.0) (1.46.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.8.0) (57.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.8.0) (0.2.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.8.0) (1.14.1)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.8.0) (1.6.3)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.8.0) (0.5.3)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.8.0) (2.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.8.0) (1.1.2)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.8.0) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.8.0) (4.2.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow>=2.8.0) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow>=2.8.0) (1.5.2)\n",
            "Collecting grpcio<2.0,>=1.24.3\n",
            "  Downloading grpcio-1.43.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.1 MB 20.4 MB/s \n",
            "\u001b[?25hCollecting aiosignal\n",
            "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from ray[data,serve,tune]>=1.12.1) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from ray[data,serve,tune]>=1.12.1) (3.7.0)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.7/dist-packages (from ray[data,serve,tune]>=1.12.1) (21.4.0)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.7/dist-packages (from ray[data,serve,tune]>=1.12.1) (7.1.2)\n",
            "Collecting frozenlist\n",
            "  Downloading frozenlist-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
            "\u001b[K     |████████████████████████████████| 144 kB 36.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ray[data,serve,tune]>=1.12.1) (1.0.3)\n",
            "Collecting virtualenv\n",
            "  Downloading virtualenv-20.14.1-py2.py3-none-any.whl (8.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.8 MB 6.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from ray[data,serve,tune]>=1.12.1) (3.13)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.7/dist-packages (from ray[data,serve,tune]>=1.12.1) (4.3.3)\n",
            "Collecting fsspec\n",
            "  Downloading fsspec-2022.3.0-py3-none-any.whl (136 kB)\n",
            "\u001b[K     |████████████████████████████████| 136 kB 40.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyarrow<7.0.0,>=4.0.1 in /usr/local/lib/python3.7/dist-packages (from ray[data,serve,tune]>=1.12.1) (6.0.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from ray[data,serve,tune]>=1.12.1) (1.3.5)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from ray[data,serve,tune]>=1.12.1) (0.8.9)\n",
            "Collecting tensorboardX>=1.9\n",
            "  Downloading tensorboardX-2.5-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[K     |████████████████████████████████| 125 kB 37.9 MB/s \n",
            "\u001b[?25hCollecting aiohttp-cors\n",
            "  Downloading aiohttp_cors-0.7.0-py3-none-any.whl (27 kB)\n",
            "Collecting starlette\n",
            "  Downloading starlette-0.20.0-py3-none-any.whl (61 kB)\n",
            "\u001b[K     |████████████████████████████████| 61 kB 246 kB/s \n",
            "\u001b[?25hCollecting uvicorn==0.16.0\n",
            "  Downloading uvicorn-0.16.0-py3-none-any.whl (54 kB)\n",
            "\u001b[K     |████████████████████████████████| 54 kB 1.7 MB/s \n",
            "\u001b[?25hCollecting gpustat>=1.0.0b1\n",
            "  Downloading gpustat-1.0.0b1.tar.gz (82 kB)\n",
            "\u001b[K     |████████████████████████████████| 82 kB 221 kB/s \n",
            "\u001b[?25hCollecting colorful\n",
            "  Downloading colorful-0.5.4-py2.py3-none-any.whl (201 kB)\n",
            "\u001b[K     |████████████████████████████████| 201 kB 36.4 MB/s \n",
            "\u001b[?25hCollecting aiorwlock\n",
            "  Downloading aiorwlock-1.3.0-py3-none-any.whl (10.0 kB)\n",
            "Requirement already satisfied: smart-open in /usr/local/lib/python3.7/dist-packages (from ray[data,serve,tune]>=1.12.1) (6.0.0)\n",
            "Collecting py-spy>=0.2.0\n",
            "  Downloading py_spy-0.3.12-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (3.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1 MB 37.0 MB/s \n",
            "\u001b[?25hCollecting fastapi\n",
            "  Downloading fastapi-0.78.0-py3-none-any.whl (54 kB)\n",
            "\u001b[K     |████████████████████████████████| 54 kB 2.7 MB/s \n",
            "\u001b[?25hCollecting prometheus-client<0.14.0,>=0.7.1\n",
            "  Downloading prometheus_client-0.13.1-py3-none-any.whl (57 kB)\n",
            "\u001b[K     |████████████████████████████████| 57 kB 4.4 MB/s \n",
            "\u001b[?25hCollecting opencensus\n",
            "  Downloading opencensus-0.9.0-py2.py3-none-any.whl (128 kB)\n",
            "\u001b[K     |████████████████████████████████| 128 kB 19.6 MB/s \n",
            "\u001b[?25hCollecting aiohttp>=3.7\n",
            "  Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 35.7 MB/s \n",
            "\u001b[?25hCollecting asgiref>=3.4.0\n",
            "  Downloading asgiref-3.5.2-py3-none-any.whl (22 kB)\n",
            "Collecting h11>=0.8\n",
            "  Downloading h11-0.13.0-py3-none-any.whl (58 kB)\n",
            "\u001b[K     |████████████████████████████████| 58 kB 5.5 MB/s \n",
            "\u001b[?25hCollecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (94 kB)\n",
            "\u001b[K     |████████████████████████████████| 94 kB 3.8 MB/s \n",
            "\u001b[?25hCollecting asynctest==0.13.0\n",
            "  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n",
            "\u001b[K     |████████████████████████████████| 271 kB 39.3 MB/s \n",
            "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp>=3.7->ray[data,serve,tune]>=1.12.1) (2.0.12)\n",
            "Requirement already satisfied: nvidia-ml-py3>=7.352.0 in /usr/local/lib/python3.7/dist-packages (from gpustat>=1.0.0b1->ray[data,serve,tune]>=1.12.1) (7.352.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from gpustat>=1.0.0b1->ray[data,serve,tune]>=1.12.1) (5.4.8)\n",
            "Collecting blessed>=1.17.1\n",
            "  Downloading blessed-1.19.1-py2.py3-none-any.whl (58 kB)\n",
            "\u001b[K     |████████████████████████████████| 58 kB 5.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wcwidth>=0.1.4 in /usr/local/lib/python3.7/dist-packages (from blessed>=1.17.1->gpustat>=1.0.0b1->ray[data,serve,tune]>=1.12.1) (0.2.5)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.8.0) (1.8.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.8.0) (0.4.6)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.8.0) (1.35.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.8.0) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.8.0) (0.6.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.8.0) (3.3.7)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow>=2.8.0) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow>=2.8.0) (4.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow>=2.8.0) (4.2.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow>=2.8.0) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow>=2.8.0) (4.11.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow>=2.8.0) (3.8.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow>=2.8.0) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->ray[data,serve,tune]>=1.12.1) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->ray[data,serve,tune]>=1.12.1) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->ray[data,serve,tune]>=1.12.1) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->ray[data,serve,tune]>=1.12.1) (2021.10.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow>=2.8.0) (3.2.0)\n",
            "Collecting pydantic!=1.7,!=1.7.1,!=1.7.2,!=1.7.3,!=1.8,!=1.8.1,<2.0.0,>=1.6.2\n",
            "  Downloading pydantic-1.9.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.9 MB 33.8 MB/s \n",
            "\u001b[?25hCollecting starlette\n",
            "  Downloading starlette-0.19.1-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 971 kB/s \n",
            "\u001b[?25hCollecting anyio<5,>=3.4.0\n",
            "  Downloading anyio-3.6.1-py3-none-any.whl (80 kB)\n",
            "\u001b[K     |████████████████████████████████| 80 kB 6.9 MB/s \n",
            "\u001b[?25hCollecting sniffio>=1.1\n",
            "  Downloading sniffio-1.2.0-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema->ray[data,serve,tune]>=1.12.1) (0.18.1)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema->ray[data,serve,tune]>=1.12.1) (5.7.1)\n",
            "Collecting opencensus-context>=0.1.2\n",
            "  Downloading opencensus_context-0.1.2-py2.py3-none-any.whl (4.4 kB)\n",
            "Requirement already satisfied: google-api-core<3.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from opencensus->ray[data,serve,tune]>=1.12.1) (1.31.5)\n",
            "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.7/dist-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[data,serve,tune]>=1.12.1) (21.3)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[data,serve,tune]>=1.12.1) (1.56.1)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[data,serve,tune]>=1.12.1) (2022.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=14.3->google-api-core<3.0.0,>=1.0.0->opencensus->ray[data,serve,tune]>=1.12.1) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->ray[data,serve,tune]>=1.12.1) (2.8.2)\n",
            "Collecting platformdirs<3,>=2\n",
            "  Downloading platformdirs-2.5.2-py3-none-any.whl (14 kB)\n",
            "Collecting distlib<1,>=0.3.1\n",
            "  Downloading distlib-0.3.4-py2.py3-none-any.whl (461 kB)\n",
            "\u001b[K     |████████████████████████████████| 461 kB 46.1 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: gpustat\n",
            "  Building wheel for gpustat (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gpustat: filename=gpustat-1.0.0b1-py3-none-any.whl size=15979 sha256=fe7f9f35755ce74800dbcab9b7eb836d7df50036409de16e304b62560e164881\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/16/e2/3e2437fba4c4b6a97a97bd96fce5d14e66cff5c4966fb1cc8c\n",
            "Successfully built gpustat\n",
            "Installing collected packages: sniffio, multidict, frozenlist, yarl, platformdirs, distlib, asynctest, async-timeout, anyio, aiosignal, virtualenv, starlette, pydantic, opencensus-context, h11, grpcio, blessed, asgiref, aiohttp, uvicorn, tf-estimator-nightly, tensorboardX, ray, py-spy, prometheus-client, opencensus, gpustat, fsspec, fastapi, colorful, aiorwlock, aiohttp-cors\n",
            "  Attempting uninstall: grpcio\n",
            "    Found existing installation: grpcio 1.46.1\n",
            "    Uninstalling grpcio-1.46.1:\n",
            "      Successfully uninstalled grpcio-1.46.1\n",
            "  Attempting uninstall: prometheus-client\n",
            "    Found existing installation: prometheus-client 0.14.1\n",
            "    Uninstalling prometheus-client-0.14.1:\n",
            "      Successfully uninstalled prometheus-client-0.14.1\n",
            "Successfully installed aiohttp-3.8.1 aiohttp-cors-0.7.0 aiorwlock-1.3.0 aiosignal-1.2.0 anyio-3.6.1 asgiref-3.5.2 async-timeout-4.0.2 asynctest-0.13.0 blessed-1.19.1 colorful-0.5.4 distlib-0.3.4 fastapi-0.78.0 frozenlist-1.3.0 fsspec-2022.3.0 gpustat-1.0.0b1 grpcio-1.43.0 h11-0.13.0 multidict-6.0.2 opencensus-0.9.0 opencensus-context-0.1.2 platformdirs-2.5.2 prometheus-client-0.13.1 py-spy-0.3.12 pydantic-1.9.0 ray-1.12.1 sniffio-1.2.0 starlette-0.19.1 tensorboardX-2.5 tf-estimator-nightly-2.8.0.dev2021122109 uvicorn-0.16.0 virtualenv-20.14.1 yarl-1.7.2\n",
            "Running: /usr/bin/python3 -m pip install -U https://s3-us-west-2.amazonaws.com/ray-wheels/latest/ray-2.0.0.dev0-cp37-cp37m-manylinux2014_x86_64.whl.\n",
            "Collecting ray==2.0.0.dev0\n",
            "  Downloading https://s3-us-west-2.amazonaws.com/ray-wheels/latest/ray-2.0.0.dev0-cp37-cp37m-manylinux2014_x86_64.whl (54.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 54.9 MB 1.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: frozenlist in /usr/local/lib/python3.7/dist-packages (from ray==2.0.0.dev0) (1.3.0)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.7/dist-packages (from ray==2.0.0.dev0) (21.4.0)\n",
            "Requirement already satisfied: click<=8.0.4,>=7.0 in /usr/local/lib/python3.7/dist-packages (from ray==2.0.0.dev0) (7.1.2)\n",
            "Requirement already satisfied: virtualenv in /usr/local/lib/python3.7/dist-packages (from ray==2.0.0.dev0) (20.14.1)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.7/dist-packages (from ray==2.0.0.dev0) (4.3.3)\n",
            "Requirement already satisfied: protobuf>=3.15.3 in /usr/local/lib/python3.7/dist-packages (from ray==2.0.0.dev0) (3.17.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from ray==2.0.0.dev0) (3.7.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from ray==2.0.0.dev0) (2.23.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from ray==2.0.0.dev0) (3.13)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ray==2.0.0.dev0) (1.0.3)\n",
            "Requirement already satisfied: grpcio!=1.44.0,>=1.28.1 in /usr/local/lib/python3.7/dist-packages (from ray==2.0.0.dev0) (1.43.0)\n",
            "Requirement already satisfied: aiosignal in /usr/local/lib/python3.7/dist-packages (from ray==2.0.0.dev0) (1.2.0)\n",
            "Requirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.7/dist-packages (from ray==2.0.0.dev0) (1.21.6)\n",
            "Requirement already satisfied: six>=1.5.2 in /usr/local/lib/python3.7/dist-packages (from grpcio!=1.44.0,>=1.28.1->ray==2.0.0.dev0) (1.15.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from jsonschema->ray==2.0.0.dev0) (4.2.0)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema->ray==2.0.0.dev0) (5.7.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from jsonschema->ray==2.0.0.dev0) (4.11.3)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema->ray==2.0.0.dev0) (0.18.1)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources>=1.4.0->jsonschema->ray==2.0.0.dev0) (3.8.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->ray==2.0.0.dev0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->ray==2.0.0.dev0) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->ray==2.0.0.dev0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->ray==2.0.0.dev0) (3.0.4)\n",
            "Requirement already satisfied: platformdirs<3,>=2 in /usr/local/lib/python3.7/dist-packages (from virtualenv->ray==2.0.0.dev0) (2.5.2)\n",
            "Requirement already satisfied: distlib<1,>=0.3.1 in /usr/local/lib/python3.7/dist-packages (from virtualenv->ray==2.0.0.dev0) (0.3.4)\n",
            "Installing collected packages: ray\n",
            "  Attempting uninstall: ray\n",
            "    Found existing installation: ray 1.12.1\n",
            "    Uninstalling ray-1.12.1:\n",
            "      Successfully uninstalled ray-1.12.1\n",
            "Successfully installed ray-2.0.0.dev0\n",
            "\u001b[0mRequirement already satisfied: fastapi in /usr/local/lib/python3.7/dist-packages (0.78.0)\n",
            "Requirement already satisfied: pydantic!=1.7,!=1.7.1,!=1.7.2,!=1.7.3,!=1.8,!=1.8.1,<2.0.0,>=1.6.2 in /usr/local/lib/python3.7/dist-packages (from fastapi) (1.9.0)\n",
            "Requirement already satisfied: starlette==0.19.1 in /usr/local/lib/python3.7/dist-packages (from fastapi) (0.19.1)\n",
            "Requirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from starlette==0.19.1->fastapi) (3.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.10.0 in /usr/local/lib/python3.7/dist-packages (from starlette==0.19.1->fastapi) (4.2.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.7/dist-packages (from anyio<5,>=3.4.0->starlette==0.19.1->fastapi) (2.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.7/dist-packages (from anyio<5,>=3.4.0->starlette==0.19.1->fastapi) (1.2.0)\n"
          ]
        }
      ],
      "source": [
        "! pip install \"tensorflow>=2.8.0\" \"ray[tune, data, serve]>=1.12.1\"\n",
        "! ray install-nightly\n",
        "! pip install fastapi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pvSRaEHChYbP"
      },
      "source": [
        "# Set up Ray"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRdL3kWBhYbQ"
      },
      "source": [
        "We will use `ray.init()` to initialize a local cluster. By default, this cluster will be compromised of only the machine you are running this notebook on. You can also run this notebook on an Anyscale cluster."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MOsHUjgdIrIW",
        "outputId": "7bb04994-71ba-40b9-e9af-fc02720d55c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-17 23:03:42,211\tINFO services.py:1484 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RayContext(dashboard_url='127.0.0.1:8265', python_version='3.7.13', ray_version='2.0.0.dev0', ray_commit='68d4dd3a8b2defa5549cfa70e59aa26f2d4825a3', address_info={'node_ip_address': '172.28.0.2', 'raylet_ip_address': '172.28.0.2', 'redis_address': None, 'object_store_address': '/tmp/ray/session_2022-05-17_23-03-37_890498_73/sockets/plasma_store', 'raylet_socket_name': '/tmp/ray/session_2022-05-17_23-03-37_890498_73/sockets/raylet', 'webui_url': '127.0.0.1:8265', 'session_dir': '/tmp/ray/session_2022-05-17_23-03-37_890498_73', 'metrics_export_port': 61896, 'gcs_address': '172.28.0.2:59395', 'address': '172.28.0.2:59395', 'node_id': '2a8c95d7f2bad4a80e4f5bad2292906077bde17aa35673ea6533da97'})"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "from pprint import pprint\n",
        "import ray\n",
        "\n",
        "ray.init()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJiSdWy2hYbR"
      },
      "source": [
        "We can check the resources our cluster is composed of. If you are running this notebook on your local machine or Google Colab, you should see the number of CPU cores and GPUs available on the said machine."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KlMz0dt9hYbS",
        "outputId": "d3163e99-2d04-49b6-a08f-2afdc2d986b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'CPU': 2.0,\n",
            " 'GPU': 1.0,\n",
            " 'accelerator_type:K80': 1.0,\n",
            " 'memory': 7858067867.0,\n",
            " 'node:172.28.0.2': 1.0,\n",
            " 'object_store_memory': 3929033932.0}\n"
          ]
        }
      ],
      "source": [
        "pprint(ray.cluster_resources())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Getting the data"
      ],
      "metadata": {
        "id": "jAgvLbhT8nB0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's start with defining a helper function to get the data to work with. Some columns are dropped for simplicity."
      ],
      "metadata": {
        "id": "IXQb4--97_Cf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "gAbhv9OqhYbT"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def get_data() -> pd.DataFrame:\n",
        "    \"\"\"Fetch the taxi fare data to work on.\"\"\"\n",
        "    _data = pd.read_csv(\n",
        "        \"https://raw.githubusercontent.com/tensorflow/tfx/master/\"\n",
        "        \"tfx/examples/chicago_taxi_pipeline/data/simple/data.csv\"\n",
        "    )\n",
        "    _data[\"is_big_tip\"] = _data[\"tips\"] / _data[\"fare\"] > 0.2\n",
        "    # We drop some columns here for the sake of simplicity.\n",
        "    return _data.drop(\n",
        "        [\n",
        "            \"tips\",\n",
        "            \"fare\",\n",
        "            \"dropoff_latitude\",\n",
        "            \"dropoff_longitude\",\n",
        "            \"pickup_latitude\",\n",
        "            \"pickup_longitude\",\n",
        "            \"pickup_census_tract\",\n",
        "        ],\n",
        "        axis=1,\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = get_data()"
      ],
      "metadata": {
        "id": "FbeYf1aF8ISK"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's take a look at the data. Notice that some values are missing. This is exactly where preprocessing comes into the picture. We will come back to this in the preprocessing session below."
      ],
      "metadata": {
        "id": "1WALC3kT8WgL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data.head(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "8tugpr5S8gPq",
        "outputId": "5398291d-c14b-4b66-a9d6-2b4634e0585e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   pickup_community_area  trip_start_month  trip_start_hour  trip_start_day  \\\n",
              "0                    NaN                 5               19               6   \n",
              "1                    NaN                 3               19               5   \n",
              "2                   60.0                10                2               3   \n",
              "3                   10.0                10                1               2   \n",
              "4                   14.0                 5                7               5   \n",
              "\n",
              "   trip_start_timestamp  trip_miles  dropoff_census_tract payment_type  \\\n",
              "0            1400269500         0.0                   NaN  Credit Card   \n",
              "1            1362683700         0.0                   NaN      Unknown   \n",
              "2            1380593700        12.6                   NaN         Cash   \n",
              "3            1382319000         0.0                   NaN         Cash   \n",
              "4            1369897200         0.0                   NaN         Cash   \n",
              "\n",
              "                                    company  trip_seconds  \\\n",
              "0  Chicago Elite Cab Corp. (Chicago Carriag           0.0   \n",
              "1                   Chicago Elite Cab Corp.         300.0   \n",
              "2                 Taxi Affiliation Services        1380.0   \n",
              "3                 Taxi Affiliation Services         180.0   \n",
              "4                 Dispatch Taxi Affiliation        1080.0   \n",
              "\n",
              "   dropoff_community_area  is_big_tip  \n",
              "0                     NaN       False  \n",
              "1                     NaN       False  \n",
              "2                     NaN       False  \n",
              "3                     NaN       False  \n",
              "4                     NaN       False  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c6d7a137-8f62-4f49-b6da-3cf4d704f834\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>pickup_community_area</th>\n",
              "      <th>trip_start_month</th>\n",
              "      <th>trip_start_hour</th>\n",
              "      <th>trip_start_day</th>\n",
              "      <th>trip_start_timestamp</th>\n",
              "      <th>trip_miles</th>\n",
              "      <th>dropoff_census_tract</th>\n",
              "      <th>payment_type</th>\n",
              "      <th>company</th>\n",
              "      <th>trip_seconds</th>\n",
              "      <th>dropoff_community_area</th>\n",
              "      <th>is_big_tip</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>NaN</td>\n",
              "      <td>5</td>\n",
              "      <td>19</td>\n",
              "      <td>6</td>\n",
              "      <td>1400269500</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Credit Card</td>\n",
              "      <td>Chicago Elite Cab Corp. (Chicago Carriag</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>NaN</td>\n",
              "      <td>3</td>\n",
              "      <td>19</td>\n",
              "      <td>5</td>\n",
              "      <td>1362683700</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Unknown</td>\n",
              "      <td>Chicago Elite Cab Corp.</td>\n",
              "      <td>300.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>60.0</td>\n",
              "      <td>10</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>1380593700</td>\n",
              "      <td>12.6</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Cash</td>\n",
              "      <td>Taxi Affiliation Services</td>\n",
              "      <td>1380.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>10.0</td>\n",
              "      <td>10</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1382319000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Cash</td>\n",
              "      <td>Taxi Affiliation Services</td>\n",
              "      <td>180.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>14.0</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>1369897200</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Cash</td>\n",
              "      <td>Dispatch Taxi Affiliation</td>\n",
              "      <td>1080.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c6d7a137-8f62-4f49-b6da-3cf4d704f834')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c6d7a137-8f62-4f49-b6da-3cf4d704f834 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c6d7a137-8f62-4f49-b6da-3cf4d704f834');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We continue to split the data into training and test data.\n",
        "For the test data, we separate out the features to run serving on as well as labels to compare serving results with."
      ],
      "metadata": {
        "id": "xzNQKJMA9YV-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from typing import Tuple\n",
        "\n",
        "\n",
        "def split_data(data: pd.DataFrame) -> Tuple[ray.data.Dataset, pd.DataFrame, np.array]:\n",
        "    \"\"\"Split the data in a stratified way.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing train dataset, test data and test label.\n",
        "    \"\"\"\n",
        "    train_data, test_data = train_test_split(\n",
        "        data, stratify=data[[\"is_big_tip\"]], random_state=1113\n",
        "    )\n",
        "    _train_ds = ray.data.from_pandas(train_data)\n",
        "    _test_label = test_data[\"is_big_tip\"].values\n",
        "    _test_df = test_data.drop([\"is_big_tip\"], axis=1)\n",
        "    return _train_ds, _test_df, _test_label\n",
        "\n",
        "train_ds, test_df, test_label = split_data(data)"
      ],
      "metadata": {
        "id": "YSLvrBMC9aRv"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"There are {train_ds.count()} samples for training and {test_df.shape[0]} samples for testing.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xfhRl7eO981w",
        "outputId": "6b079564-f2fe-45b3-c6d8-81fcd90f017d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 11251 samples for training and 3751 samples for testing.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing"
      ],
      "metadata": {
        "id": "N7tiwqdP-zVS"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4RRkXuteIrIh"
      },
      "source": [
        "Let's focus on preprocessing first.\n",
        "Usually input data needs to go through some preprocessing before being\n",
        "fed into model. It is a good idea to package preprocessing logic into\n",
        "a modularized component so that the same logic can be applied to both\n",
        "training data as well as data for online serving or offline batch prediction.\n",
        "\n",
        "In AIR, this component is `ray.ml.preprocessor.Preprocessor`.\n",
        "It is constructed in a way that allows easy composition.\n",
        "\n",
        "Now let's construct a chained preprocessor composed of simple preprocessors, including\n",
        "1. Imputer for filling missing features;\n",
        "2. OneHotEncoder for encoding categorical features;\n",
        "3. BatchMapper where arbitrary udf can be applied to batches of records;\n",
        "and so on. Take a look at `ray.ml.preprocessor.Preprocessor` for more details.\n",
        "The output of the preprocessing step goes into model for training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "zVvslsfMIrIh"
      },
      "outputs": [],
      "source": [
        "from ray.ml.preprocessors import (\n",
        "    BatchMapper,\n",
        "    Chain,\n",
        "    OneHotEncoder,\n",
        "    SimpleImputer,\n",
        ")\n",
        "\n",
        "def get_preprocessor():\n",
        "    \"\"\"Construct a chain of preprocessors.\"\"\"\n",
        "    imputer1 = SimpleImputer(\n",
        "        [\"dropoff_census_tract\"], strategy=\"constant\", fill_value=17031839100\n",
        "    )\n",
        "    imputer2 = SimpleImputer(\n",
        "        [\"pickup_community_area\", \"dropoff_community_area\"],\n",
        "        strategy=\"constant\",\n",
        "        fill_value=8,\n",
        "    )\n",
        "    imputer3 = SimpleImputer([\"payment_type\"], strategy=\"constant\", fill_value=\"Cash\")\n",
        "    imputer4 = SimpleImputer(\n",
        "        [\"company\"], strategy=\"constant\", fill_value=\"Taxi Affiliation Services\"\n",
        "    )\n",
        "    imputer5 = SimpleImputer(\n",
        "        [\"trip_start_timestamp\", \"trip_miles\", \"trip_seconds\"], strategy=\"mean\"\n",
        "    )\n",
        "\n",
        "    ohe = OneHotEncoder(\n",
        "        columns=[\n",
        "            \"trip_start_hour\",\n",
        "            \"trip_start_day\",\n",
        "            \"trip_start_month\",\n",
        "            \"dropoff_census_tract\",\n",
        "            \"pickup_community_area\",\n",
        "            \"dropoff_community_area\",\n",
        "            \"payment_type\",\n",
        "            \"company\",\n",
        "        ],\n",
        "        limit={\n",
        "            \"dropoff_census_tract\": 25,\n",
        "            \"pickup_community_area\": 20,\n",
        "            \"dropoff_community_area\": 20,\n",
        "            \"payment_type\": 2,\n",
        "            \"company\": 7,\n",
        "        },\n",
        "    )\n",
        "\n",
        "    def fn(df):\n",
        "        df[\"trip_start_year\"] = pd.to_datetime(df[\"trip_start_timestamp\"], unit=\"s\").dt.year\n",
        "        return df\n",
        "\n",
        "    chained_pp = Chain(\n",
        "        imputer1,\n",
        "        imputer2,\n",
        "        imputer3,\n",
        "        imputer4,\n",
        "        imputer5,\n",
        "        ohe,\n",
        "        BatchMapper(fn),\n",
        "        BatchMapper(lambda x: x.drop([\"trip_start_timestamp\"], axis=1)),\n",
        "    )\n",
        "    return chained_pp\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's define some constants for clarity."
      ],
      "metadata": {
        "id": "V2BIiegi_brE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Note that `INPUT_SIZE` here is corresponding to the output dimension\n",
        "# of the previously defined processing steps.\n",
        "# This is used to specify the input shape of Keras model as well as\n",
        "# when converting from training data from `ray.data.Dataset` to `tf.Tensor`.\n",
        "INPUT_SIZE = 120\n",
        "# The training batch size. Based on `NUM_WORKERS`, each worker\n",
        "# will get its own share of this batch size. For example, if\n",
        "# `NUM_WORKERS = 2`, each worker will work on 4 samples per batch.\n",
        "BATCH_SIZE = 8\n",
        "# Number of epoch. Adjust it based on how quickly you want the run to be.\n",
        "EPOCH = 1\n",
        "# Number of training workers.\n",
        "NUM_WORKERS = 2"
      ],
      "metadata": {
        "id": "ejGVU-uN_dVP"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whPRbBNbIrIl"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7QYTpxXIrIl"
      },
      "source": [
        "Let's starting with defining a simple Keras model for the classification task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "MwhAeEOuhYbV"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "def build_model():\n",
        "    model = tf.keras.models.Sequential()\n",
        "    model.add(tf.keras.Input(shape=(INPUT_SIZE,)))\n",
        "    model.add(tf.keras.layers.Dense(50, activation=\"relu\"))\n",
        "    model.add(tf.keras.layers.Dense(1, activation=\"sigmoid\"))\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's define the training loop. This code will be run on each training\n",
        "worker in a distributed fashion."
      ],
      "metadata": {
        "id": "UVVji2YKADrh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ray import train\n",
        "from ray.train.tensorflow import prepare_dataset_shard\n",
        "\n",
        "def train_loop_per_worker():\n",
        "    dataset_shard = train.get_dataset_shard(\"train\")\n",
        "\n",
        "    strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()\n",
        "    with strategy.scope():\n",
        "        model = build_model()\n",
        "        model.compile(\n",
        "            loss=\"binary_crossentropy\",\n",
        "            optimizer=\"adam\",\n",
        "            metrics=[\"accuracy\"],\n",
        "        )\n",
        "\n",
        "    for epoch in range(EPOCH):\n",
        "        # This will make sure that the training workers will get their own\n",
        "        # share of batch to work on.\n",
        "        # See `ray.train.tensorflow.prepare_dataset_shard` for more information.\n",
        "        tf_dataset = prepare_dataset_shard(\n",
        "            dataset_shard.to_tf(\n",
        "                label_column=\"is_big_tip\",\n",
        "                output_signature=(\n",
        "                    tf.TensorSpec(shape=(BATCH_SIZE, INPUT_SIZE), dtype=tf.float32),\n",
        "                    tf.TensorSpec(shape=(BATCH_SIZE,), dtype=tf.int64),\n",
        "                ),\n",
        "                batch_size=BATCH_SIZE,\n",
        "                drop_last=True,\n",
        "            )\n",
        "        )\n",
        "\n",
        "        model.fit(tf_dataset)\n",
        "        # This saves checkpoint in a way that can be used by Ray Serve coherently.\n",
        "        train.save_checkpoint(epoch=epoch, model=model.get_weights())"
      ],
      "metadata": {
        "id": "U5pdjIzoAGRd"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RzfPtOMoIrIu"
      },
      "source": [
        "Now let's define a trainer that takes in the training loop,\n",
        "the training dataset as well the preprocessor that we just defined.\n",
        "\n",
        "And run it!\n",
        "\n",
        "Notice that you can tune how long you want the run to be by changing ``EPOCH``."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ray.ml.train.integrations.tensorflow import TensorflowTrainer\n",
        "\n",
        "trainer = TensorflowTrainer(\n",
        "    train_loop_per_worker=train_loop_per_worker,\n",
        "    scaling_config={\"num_workers\": NUM_WORKERS},\n",
        "    datasets={\"train\": train_ds},\n",
        "    preprocessor=get_preprocessor(),\n",
        ")\n",
        "result = trainer.fit()"
      ],
      "metadata": {
        "id": "fzpWK7nuTJmN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Moving on to Serve"
      ],
      "metadata": {
        "id": "Nb0HkOV2R4uL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ray Serve serves the trained model through constructs of `ray.serve.model_wrappers.ModelWrapper` and `ray.serve.model_wrappers.ModelWrapperDeployment`. These constructs facilitate wraps a `ray.ml.checkpoint.Checkpoint` into an endpoint that can readily serve http requests.\n",
        "\n",
        "This removes the boilerplate code and minimizes the effort to bring your model to production!\n",
        "\n",
        "The http payload in this case will be a json payload of one record.\n",
        "Upon receiving this payload, Ray Serve would need some adapter to convert the payload into a pandas DataFrame, and this will be achieved by the following `dataframe_adapter`.\n",
        "\n",
        "Let's first define this adapter function."
      ],
      "metadata": {
        "id": "OlzjlW8QR_q6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from fastapi import Request\n",
        "\n",
        "async def dataframe_adapter(request: Request):\n",
        "    \"\"\"Serve HTTP Adapter that reads JSON and convert to pandas DataFrame.\"\"\"\n",
        "    content = await request.json()\n",
        "    return pd.DataFrame.from_dict(content)"
      ],
      "metadata": {
        "id": "BBbcMwc9Rz66"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's wrap everything in a serve endpoint that exposes a URL to where requests can be sent to."
      ],
      "metadata": {
        "id": "SOnl90IuRywD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ray import serve\n",
        "from ray.ml.checkpoint import Checkpoint\n",
        "from ray.ml.predictors.integrations.tensorflow import TensorflowPredictor\n",
        "from ray.serve.model_wrappers import ModelWrapperDeployment\n",
        "\n",
        "\n",
        "def serve_model(checkpoint: Checkpoint, model_definition, adapter, name=\"Model\") -> str:\n",
        "    \"\"\"Expose a serve endpoint.\n",
        "\n",
        "    Returns:\n",
        "        serve URL.\n",
        "    \"\"\"\n",
        "    serve.start(detached=True)\n",
        "    deployment = ModelWrapperDeployment.options(name=name)\n",
        "    deployment.deploy(\n",
        "        TensorflowPredictor,\n",
        "        checkpoint,\n",
        "        # This is due to a current limitation on Serve that's\n",
        "        # being addressed.\n",
        "        # TODO(xwjiang): Change to True.\n",
        "        batching_params=False,\n",
        "        model_definition=model_definition,\n",
        "        http_adapter=adapter,\n",
        "    )\n",
        "    return deployment.url\n",
        "\n",
        "endpoint_uri = serve_model(result.checkpoint, build_model, dataframe_adapter)"
      ],
      "metadata": {
        "id": "ujmwT8ZhScq1"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's write a helper function to send requests to this endpoint and compare the results with labels."
      ],
      "metadata": {
        "id": "rzHSwa2bSyee"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "NUM_SERVE_REQUESTS = 100\n",
        "\n",
        "def send_requests(df: pd.DataFrame, label: np.array):\n",
        "    for i in range(NUM_SERVE_REQUESTS):\n",
        "        one_row = df.iloc[[i]].to_dict()\n",
        "        serve_result = requests.post(endpoint_uri, json=one_row).json()\n",
        "        print(\n",
        "            f\"request[{i}] prediction: {serve_result['predictions']['0']} \"\n",
        "            f\"- label: {str(label[i])}\"\n",
        "        )"
      ],
      "metadata": {
        "id": "E9m80HDmSz66"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "send_requests(test_df, test_label)"
      ],
      "metadata": {
        "id": "GFPwKc5JTgnI"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "jAgvLbhT8nB0"
      ],
      "name": "tfx.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "3c0d54d489a08ae47a06eae2fd00ff032d6cddb527c382959b7b2575f6a8167f"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
