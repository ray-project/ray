{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VaFMt6AIhYbK"
   },
   "source": [
    "This notebook is adapted from \"components keras\" [tutorial](https://www.tensorflow.org/tfx/tutorials/tfx/components_keras).\n",
    "The goal is to predict whether a trip may generate a big tip.\n",
    "\n",
    "In this example, we showcase how to convert a tfx pipeline to [Ray AIR](https://docs.ray.io/en/latest/ray-air/getting-started.html), covering\n",
    "every step from data ingestion to pushing a model to serving.\n",
    "\n",
    "1. Read a CSV file into ray dataset.\n",
    "2. Process the dataset by chaining a variety of off-the-shelf preprocessors.\n",
    "3. Train the model using distributed tensorflow with few lines of code.\n",
    "4. Serve the model that will apply the same preprocessing to the incoming requests.\n",
    "\n",
    "Note, ``ray.ml.checkpoint.Checkpoint`` serves as the bridge between step 3 and step 4.\n",
    "By capturing both model and preprocessing steps in a way compatible with Ray Serve, this\n",
    "abstraction makes sure ml workload can transition seamlessly between training and\n",
    "serving."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sQbdfyWQhYbO"
   },
   "source": [
    "Uncomment and run the following line in order to install all the necessary dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YajFzmkthYbO",
    "outputId": "b6dd3a4f-8435-4202-9322-827222de4509"
   },
   "outputs": [],
   "source": [
    "# ! pip install \"tensorflow>=2.8.0\" \"ray[tune, data, serve]>=1.12.1\"\n",
    "# ! ray install-nightly\n",
    "# ! pip install fastapi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pvSRaEHChYbP"
   },
   "source": [
    "# Set up Ray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LRdL3kWBhYbQ"
   },
   "source": [
    "We will use `ray.init()` to initialize a local cluster. By default, this cluster will be compromised of only the machine you are running this notebook on. You can also run this notebook on an Anyscale cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MOsHUjgdIrIW",
    "outputId": "7bb04994-71ba-40b9-e9af-fc02720d55c0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RayContext(dashboard_url='127.0.0.1:8265', python_version='3.7.7', ray_version='3.0.0.dev0', ray_commit='d76ef9add589ad8c6f057b2de2d51b0421611d43', address_info={'node_ip_address': '172.31.75.80', 'raylet_ip_address': '172.31.75.80', 'redis_address': None, 'object_store_address': '/tmp/ray/session_2022-05-24_12-43-10_437405_182/sockets/plasma_store', 'raylet_socket_name': '/tmp/ray/session_2022-05-24_12-43-10_437405_182/sockets/raylet', 'webui_url': '127.0.0.1:8265', 'session_dir': '/tmp/ray/session_2022-05-24_12-43-10_437405_182', 'metrics_export_port': 61160, 'gcs_address': '172.31.75.80:9031', 'address': '172.31.75.80:9031', 'node_id': 'f569e1f390f710339bbdbe18f5090f2a2dc5a94932ccd81c6fe29312'})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "import ray\n",
    "\n",
    "ray.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oJiSdWy2hYbR"
   },
   "source": [
    "We can check the resources our cluster is composed of. If you are running this notebook on your local machine or Google Colab, you should see the number of CPU cores and GPUs available on the said machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KlMz0dt9hYbS",
    "outputId": "d3163e99-2d04-49b6-a08f-2afdc2d986b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'CPU': 8.0,\n",
      " 'memory': 19425912423.0,\n",
      " 'node:172.31.75.80': 1.0,\n",
      " 'object_store_memory': 9712956211.0}\n"
     ]
    }
   ],
   "source": [
    "pprint(ray.cluster_resources())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jAgvLbhT8nB0"
   },
   "source": [
    "# Getting the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IXQb4--97_Cf"
   },
   "source": [
    "Let's start with defining a helper function to get the data to work with. Some columns are dropped for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "gAbhv9OqhYbT"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "LABEL = \"is_big_tip\"\n",
    "\n",
    "def get_data() -> pd.DataFrame:\n",
    "    \"\"\"Fetch the taxi fare data to work on.\"\"\"\n",
    "    _data = pd.read_csv(\n",
    "        \"https://raw.githubusercontent.com/tensorflow/tfx/master/\"\n",
    "        \"tfx/examples/chicago_taxi_pipeline/data/simple/data.csv\"\n",
    "    )\n",
    "    _data[LABEL] = _data[\"tips\"] / _data[\"fare\"] > 0.2\n",
    "    # We drop some columns here for the sake of simplicity.\n",
    "    return _data.drop(\n",
    "        [\n",
    "            \"tips\",\n",
    "            \"fare\",\n",
    "            \"dropoff_latitude\",\n",
    "            \"dropoff_longitude\",\n",
    "            \"pickup_latitude\",\n",
    "            \"pickup_longitude\",\n",
    "            \"pickup_census_tract\",\n",
    "        ],\n",
    "        axis=1,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "FbeYf1aF8ISK"
   },
   "outputs": [],
   "source": [
    "data = get_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1WALC3kT8WgL"
   },
   "source": [
    "Now let's take a look at the data. Notice that some values are missing. This is exactly where preprocessing comes into the picture. We will come back to this in the preprocessing session below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "8tugpr5S8gPq",
    "outputId": "5398291d-c14b-4b66-a9d6-2b4634e0585e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pickup_community_area</th>\n",
       "      <th>trip_start_month</th>\n",
       "      <th>trip_start_hour</th>\n",
       "      <th>trip_start_day</th>\n",
       "      <th>trip_start_timestamp</th>\n",
       "      <th>trip_miles</th>\n",
       "      <th>dropoff_census_tract</th>\n",
       "      <th>payment_type</th>\n",
       "      <th>company</th>\n",
       "      <th>trip_seconds</th>\n",
       "      <th>dropoff_community_area</th>\n",
       "      <th>is_big_tip</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>19</td>\n",
       "      <td>6</td>\n",
       "      <td>1400269500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Credit Card</td>\n",
       "      <td>Chicago Elite Cab Corp. (Chicago Carriag</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>19</td>\n",
       "      <td>5</td>\n",
       "      <td>1362683700</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Chicago Elite Cab Corp.</td>\n",
       "      <td>300.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>60.0</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1380593700</td>\n",
       "      <td>12.6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Cash</td>\n",
       "      <td>Taxi Affiliation Services</td>\n",
       "      <td>1380.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.0</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1382319000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Cash</td>\n",
       "      <td>Taxi Affiliation Services</td>\n",
       "      <td>180.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14.0</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>1369897200</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Cash</td>\n",
       "      <td>Dispatch Taxi Affiliation</td>\n",
       "      <td>1080.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pickup_community_area  trip_start_month  trip_start_hour  trip_start_day  \\\n",
       "0                    NaN                 5               19               6   \n",
       "1                    NaN                 3               19               5   \n",
       "2                   60.0                10                2               3   \n",
       "3                   10.0                10                1               2   \n",
       "4                   14.0                 5                7               5   \n",
       "\n",
       "   trip_start_timestamp  trip_miles  dropoff_census_tract payment_type  \\\n",
       "0            1400269500         0.0                   NaN  Credit Card   \n",
       "1            1362683700         0.0                   NaN      Unknown   \n",
       "2            1380593700        12.6                   NaN         Cash   \n",
       "3            1382319000         0.0                   NaN         Cash   \n",
       "4            1369897200         0.0                   NaN         Cash   \n",
       "\n",
       "                                    company  trip_seconds  \\\n",
       "0  Chicago Elite Cab Corp. (Chicago Carriag           0.0   \n",
       "1                   Chicago Elite Cab Corp.         300.0   \n",
       "2                 Taxi Affiliation Services        1380.0   \n",
       "3                 Taxi Affiliation Services         180.0   \n",
       "4                 Dispatch Taxi Affiliation        1080.0   \n",
       "\n",
       "   dropoff_community_area  is_big_tip  \n",
       "0                     NaN       False  \n",
       "1                     NaN       False  \n",
       "2                     NaN       False  \n",
       "3                     NaN       False  \n",
       "4                     NaN       False  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xzNQKJMA9YV-"
   },
   "source": [
    "We continue to split the data into training and test data.\n",
    "For the test data, we separate out the features to run serving on as well as labels to compare serving results with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "YSLvrBMC9aRv"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from typing import Tuple\n",
    "\n",
    "\n",
    "def split_data(data: pd.DataFrame) -> Tuple[ray.data.Dataset, pd.DataFrame, np.array]:\n",
    "    \"\"\"Split the data in a stratified way.\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing train dataset, test data and test label.\n",
    "    \"\"\"\n",
    "    train_data, test_data = train_test_split(\n",
    "        data, stratify=data[[LABEL]], random_state=1113\n",
    "    )\n",
    "    _train_ds = ray.data.from_pandas(train_data)\n",
    "    _test_label = test_data[LABEL].values\n",
    "    _test_df = test_data.drop([LABEL], axis=1)\n",
    "    return _train_ds, _test_df, _test_label\n",
    "\n",
    "train_ds, test_df, test_label = split_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xfhRl7eO981w",
    "outputId": "6b079564-f2fe-45b3-c6d8-81fcd90f017d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 11251 samples for training and 3751 samples for testing.\n"
     ]
    }
   ],
   "source": [
    "print(f\"There are {train_ds.count()} samples for training and {test_df.shape[0]} samples for testing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N7tiwqdP-zVS"
   },
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4RRkXuteIrIh"
   },
   "source": [
    "Let's focus on preprocessing first.\n",
    "Usually input data needs to go through some preprocessing before being\n",
    "fed into model. It is a good idea to package preprocessing logic into\n",
    "a modularized component so that the same logic can be applied to both\n",
    "training data as well as data for online serving or offline batch prediction.\n",
    "\n",
    "In AIR, this component is `ray.ml.preprocessor.Preprocessor`.\n",
    "It is constructed in a way that allows easy composition.\n",
    "\n",
    "Now let's construct a chained preprocessor composed of simple preprocessors, including\n",
    "1. Imputer for filling missing features;\n",
    "2. OneHotEncoder for encoding categorical features;\n",
    "3. BatchMapper where arbitrary udf can be applied to batches of records;\n",
    "and so on. Take a look at `ray.ml.preprocessor.Preprocessor` for more details.\n",
    "The output of the preprocessing step goes into model for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "zVvslsfMIrIh"
   },
   "outputs": [],
   "source": [
    "from ray.ml.preprocessors import (\n",
    "    BatchMapper,\n",
    "    Chain,\n",
    "    OneHotEncoder,\n",
    "    SimpleImputer,\n",
    ")\n",
    "\n",
    "def get_preprocessor():\n",
    "    \"\"\"Construct a chain of preprocessors.\"\"\"\n",
    "    imputer1 = SimpleImputer(\n",
    "        [\"dropoff_census_tract\"], strategy=\"most_frequent\"\n",
    "    )\n",
    "    imputer2 = SimpleImputer(\n",
    "        [\"pickup_community_area\", \"dropoff_community_area\"],\n",
    "        strategy=\"most_frequent\",\n",
    "    )\n",
    "    imputer3 = SimpleImputer([\"payment_type\"], strategy=\"most_frequent\")\n",
    "    imputer4 = SimpleImputer(\n",
    "        [\"company\"], strategy=\"most_frequent\")\n",
    "    imputer5 = SimpleImputer(\n",
    "        [\"trip_start_timestamp\", \"trip_miles\", \"trip_seconds\"], strategy=\"mean\"\n",
    "    )\n",
    "\n",
    "    ohe = OneHotEncoder(\n",
    "        columns=[\n",
    "            \"trip_start_hour\",\n",
    "            \"trip_start_day\",\n",
    "            \"trip_start_month\",\n",
    "            \"dropoff_census_tract\",\n",
    "            \"pickup_community_area\",\n",
    "            \"dropoff_community_area\",\n",
    "            \"payment_type\",\n",
    "            \"company\",\n",
    "        ],\n",
    "        limit={\n",
    "            \"dropoff_census_tract\": 25,\n",
    "            \"pickup_community_area\": 20,\n",
    "            \"dropoff_community_area\": 20,\n",
    "            \"payment_type\": 2,\n",
    "            \"company\": 7,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    def batch_mapper_fn(df):\n",
    "        df[\"trip_start_year\"] = pd.to_datetime(df[\"trip_start_timestamp\"], unit=\"s\").dt.year\n",
    "        df = df.drop([\"trip_start_timestamp\"], axis=1)\n",
    "        return df\n",
    "\n",
    "    chained_pp = Chain(\n",
    "        imputer1,\n",
    "        imputer2,\n",
    "        imputer3,\n",
    "        imputer4,\n",
    "        imputer5,\n",
    "        ohe,\n",
    "        BatchMapper(batch_mapper_fn),\n",
    "    )\n",
    "    return chained_pp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V2BIiegi_brE"
   },
   "source": [
    "Now let's define some constants for clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "ejGVU-uN_dVP"
   },
   "outputs": [],
   "source": [
    "# Note that `INPUT_SIZE` here is corresponding to the output dimension\n",
    "# of the previously defined processing steps.\n",
    "# This is used to specify the input shape of Keras model as well as\n",
    "# when converting from training data from `ray.data.Dataset` to `tf.Tensor`.\n",
    "INPUT_SIZE = 120\n",
    "# The training batch size. Based on `NUM_WORKERS`, each worker\n",
    "# will get its own share of this batch size. For example, if\n",
    "# `NUM_WORKERS = 2`, each worker will work on 4 samples per batch.\n",
    "BATCH_SIZE = 8\n",
    "# Number of epoch. Adjust it based on how quickly you want the run to be.\n",
    "EPOCH = 1\n",
    "# Number of training workers.\n",
    "# Adjust this accordingly based on the resources you have!\n",
    "NUM_WORKERS = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "whPRbBNbIrIl"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W7QYTpxXIrIl"
   },
   "source": [
    "Let's starting with defining a simple Keras model for the classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "MwhAeEOuhYbV"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def build_model():\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(tf.keras.Input(shape=(INPUT_SIZE,)))\n",
    "    model.add(tf.keras.layers.Dense(50, activation=\"relu\"))\n",
    "    model.add(tf.keras.layers.Dense(1, activation=\"sigmoid\"))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UVVji2YKADrh"
   },
   "source": [
    "Now let's define the training loop. This code will be run on each training\n",
    "worker in a distributed fashion. See more details [here](https://docs.ray.io/en/latest/train/train.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "U5pdjIzoAGRd"
   },
   "outputs": [],
   "source": [
    "from ray import train\n",
    "from ray.train.tensorflow import prepare_dataset_shard\n",
    "\n",
    "def train_loop_per_worker():\n",
    "    dataset_shard = train.get_dataset_shard(\"train\")\n",
    "\n",
    "    strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()\n",
    "    with strategy.scope():\n",
    "        model = build_model()\n",
    "        model.compile(\n",
    "            loss=\"binary_crossentropy\",\n",
    "            optimizer=\"adam\",\n",
    "            metrics=[\"accuracy\"],\n",
    "        )\n",
    "\n",
    "    for epoch in range(EPOCH):\n",
    "        # This will make sure that the training workers will get their own\n",
    "        # share of batch to work on.\n",
    "        # See `ray.train.tensorflow.prepare_dataset_shard` for more information.\n",
    "        tf_dataset = prepare_dataset_shard(\n",
    "            dataset_shard.to_tf(\n",
    "                label_column=LABEL,\n",
    "                output_signature=(\n",
    "                    tf.TensorSpec(shape=(BATCH_SIZE, INPUT_SIZE), dtype=tf.float32),\n",
    "                    tf.TensorSpec(shape=(BATCH_SIZE,), dtype=tf.int64),\n",
    "                ),\n",
    "                batch_size=BATCH_SIZE,\n",
    "                drop_last=True,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        model.fit(tf_dataset)\n",
    "        # This saves checkpoint in a way that can be used by Ray Serve coherently.\n",
    "        train.save_checkpoint(epoch=epoch, model=model.get_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RzfPtOMoIrIu"
   },
   "source": [
    "Now let's define a trainer that takes in the training loop,\n",
    "the training dataset as well the preprocessor that we just defined.\n",
    "\n",
    "And run it!\n",
    "\n",
    "Notice that you can tune how long you want the run to be by changing ``EPOCH``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "fzpWK7nuTJmN"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-05-24 14:04:51 (running for 00:00:22.16)<br>Memory usage on this node: 3.5/30.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/8 CPUs, 0/0 GPUs, 0.0/18.09 GiB heap, 0.0/9.05 GiB objects<br>Result logdir: /home/ray/ray_results/TensorflowTrainer_2022-05-24_14-04-28<br>Number of trials: 1/1 (1 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                   </th><th>status    </th><th>loc               </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>TensorflowTrainer_1933d_00000</td><td>TERMINATED</td><td>172.31.75.80:15060</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=15164)\u001b[0m WARNING:tensorflow:From /tmp/ipykernel_14933/3319996406.py:7: _CollectiveAllReduceStrategyExperimental.__init__ (from tensorflow.python.distribute.collective_all_reduce_strategy) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=15164)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=15164)\u001b[0m use distribute.MultiWorkerMirroredStrategy instead\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=15164)\u001b[0m 2022-05-24 14:04:37.731813: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=15164)\u001b[0m 2022-05-24 14:04:37.731842: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=15164)\u001b[0m 2022-05-24 14:04:37.731875: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ip-172-31-75-80): /proc/driver/nvidia/version does not exist\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=15164)\u001b[0m 2022-05-24 14:04:37.732375: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=15164)\u001b[0m To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=15164)\u001b[0m 2022-05-24 14:04:37.738006: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> 172.31.75.80:34187, 1 -> 172.31.75.80:42149}\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=15164)\u001b[0m 2022-05-24 14:04:37.738518: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:427] Started server with target: grpc://172.31.75.80:34187\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=15165)\u001b[0m WARNING:tensorflow:From /tmp/ipykernel_14933/3319996406.py:7: _CollectiveAllReduceStrategyExperimental.__init__ (from tensorflow.python.distribute.collective_all_reduce_strategy) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=15165)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=15165)\u001b[0m use distribute.MultiWorkerMirroredStrategy instead\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=15165)\u001b[0m 2022-05-24 14:04:37.731813: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=15165)\u001b[0m 2022-05-24 14:04:37.731842: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=15165)\u001b[0m 2022-05-24 14:04:37.731869: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ip-172-31-75-80): /proc/driver/nvidia/version does not exist\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=15165)\u001b[0m 2022-05-24 14:04:37.732344: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=15165)\u001b[0m To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=15165)\u001b[0m 2022-05-24 14:04:37.738036: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> 172.31.75.80:34187, 1 -> 172.31.75.80:42149}\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=15165)\u001b[0m 2022-05-24 14:04:37.738568: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:427] Started server with target: grpc://172.31.75.80:42149\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=15164)\u001b[0m 2022-05-24 14:04:38.114301: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=15165)\u001b[0m 2022-05-24 14:04:38.113499: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=15164)\u001b[0m 2022-05-24 14:04:38.147905: W tensorflow/core/framework/dataset.cc:679] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=15165)\u001b[0m 2022-05-24 14:04:38.163387: W tensorflow/core/framework/dataset.cc:679] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      1/Unknown - 3s 3s/step - loss: 56.8760 - accuracy: 0.7500\n",
      "      1/Unknown - 3s 3s/step - loss: 56.8760 - accuracy: 0.7500\n",
      "     18/Unknown - 3s 7ms/step - loss: 31.5487 - accuracy: 0.7708\n",
      "     18/Unknown - 3s 7ms/step - loss: 31.5487 - accuracy: 0.7708\n",
      "     37/Unknown - 3s 6ms/step - loss: 22.3617 - accuracy: 0.7027\n",
      "     37/Unknown - 3s 6ms/step - loss: 22.3617 - accuracy: 0.7027\n",
      "     54/Unknown - 3s 6ms/step - loss: 18.9552 - accuracy: 0.6713\n",
      "     52/Unknown - 3s 6ms/step - loss: 19.5164 - accuracy: 0.6755\n",
      "     71/Unknown - 3s 6ms/step - loss: 16.0888 - accuracy: 0.6655\n",
      "     69/Unknown - 3s 6ms/step - loss: 16.3788 - accuracy: 0.6703\n",
      "     79/Unknown - 3s 6ms/step - loss: 15.1199 - accuracy: 0.6503\n",
      "     85/Unknown - 3s 6ms/step - loss: 14.4635 - accuracy: 0.6618\n",
      "     97/Unknown - 3s 6ms/step - loss: 12.9514 - accuracy: 0.6637\n",
      "    102/Unknown - 3s 6ms/step - loss: 12.4481 - accuracy: 0.6667\n",
      "    114/Unknown - 3s 6ms/step - loss: 11.2579 - accuracy: 0.6754\n",
      "    118/Unknown - 3s 6ms/step - loss: 10.9078 - accuracy: 0.6737\n",
      "    122/Unknown - 3s 6ms/step - loss: 10.5599 - accuracy: 0.6824\n",
      "    124/Unknown - 3s 7ms/step - loss: 10.4076 - accuracy: 0.6825\n",
      "    124/Unknown - 3s 7ms/step - loss: 10.4076 - accuracy: 0.6825\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=15164)\u001b[0\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=15164)\u001b[0m     130/Unknown - 4s 7ms/step - loss: 9.9490 - accuracy: 0.6885 \n",
      "    144/Unknown - 4s 7ms/step - loss: 9.0588 - accuracy: 0.6892\n",
      "    138/Unknown - 4s 7ms/step - loss: 9.4135 - accuracy: 0.6911\n",
      "    152/Unknown - 4s 7ms/step - loss: 8.6824 - accuracy: 0.6883\n",
      "    154/Unknown - 4s 7ms/step - loss: 8.5775 - accuracy: 0.6883\n",
      "    169/Unknown - 4s 7ms/step - loss: 7.9367 - accuracy: 0.6842\n",
      "    171/Unknown - 4s 7ms/step - loss: 7.8511 - accuracy: 0.6857\n",
      "    186/Unknown - 4s 7ms/step - loss: 7.3203 - accuracy: 0.6868\n",
      "    188/Unknown - 4s 7ms/step - loss: 7.2652 - accuracy: 0.6875\n",
      "    202/Unknown - 4s 7ms/step - loss: 6.8200 - accuracy: 0.6931\n",
      "    204/Unknown - 4s 7ms/step - loss: 6.7886 - accuracy: 0.6906\n",
      "    219/Unknown - 4s 7ms/step - loss: 6.3873 - accuracy: 0.6895\n",
      "    221/Unknown - 4s 7ms/step - loss: 6.3406 - accuracy: 0.6900\n",
      "    235/Unknown - 4s 7ms/step - loss: 6.0469 - accuracy: 0.6936\n",
      "    237/Unknown - 4s 7ms/step - loss: 6.0011 - accuracy: 0.6936\n",
      "    250/Unknown - 4s 7ms/step - loss: 5.7335 - accuracy: 0.6975\n",
      "    252/Unknown - 4s 7ms/step - loss: 5.6937 - accuracy: 0.6959\n",
      "    266/Unknown - 4s 7ms/step - loss: 5.4833 - accuracy: 0.6992\n",
      "    268/Unknown - 4s 7ms/step - loss: 5.4511 - accuracy: 0.6996\n",
      "    283/Unknown - 5s 7ms/step - loss: 5.2458 - accuracy: 0.7010\n",
      "    285/Unknown - 5s 7ms/step - loss: 5.2157 - accuracy: 0.7013\n",
      "    300/Unknown - 5s 7ms/step - loss: 5.0530 - accuracy: 0.7050\n",
      "    301/Unknown - 5s 7ms/step - loss: 5.0379 - accuracy: 0.7056\n",
      "    318/Unknown - 5s 7ms/step - loss: 4.8257 - accuracy: 0.7064\n",
      "    320/Unknown - 5s 7ms/step - loss: 4.8065 - accuracy: 0.7051\n",
      "    334/Unknown - 5s 7ms/step - loss: 4.6559 - accuracy: 0.7047\n",
      "    336/Unknown - 5s 7ms/step - loss: 4.6376 - accuracy: 0.7042\n",
      "    350/Unknown - 5s 7ms/step - loss: 4.4719 - accuracy: 0.7093\n",
      "    352/Unknown - 5s 7ms/step - loss: 4.4527 - accuracy: 0.7095\n",
      "    366/Unknown - 5s 7ms/step - loss: 4.3045 - accuracy: 0.7128\n",
      "    360/Unknown - 5s 7ms/step - loss: 4.3694 - accuracy: 0.7115\n",
      "    374/Unknown - 5s 7ms/step - loss: 4.2267 - accuracy: 0.7156\n",
      "    376/Unknown - 5s 7ms/step - loss: 4.2098 - accuracy: 0.7158\n",
      "    390/Unknown - 5s 7ms/step - loss: 4.1001 - accuracy: 0.7147\n",
      "    392/Unknown - 5s 7ms/step - loss: 4.0877 - accuracy: 0.7149\n",
      "    406/Unknown - 5s 7ms/step - loss: 3.9733 - accuracy: 0.7161\n",
      "    407/Unknown - 5s 7ms/step - loss: 3.9648 - accuracy: 0.7162\n",
      "    421/Unknown - 6s 7ms/step - loss: 3.8848 - accuracy: 0.7164\n",
      "    421/Unknown - 6s 7ms/step - loss: 3.8848 - accuracy: 0.7164\n",
      "    437/Unknown - 6s 7ms/step - loss: 3.7959 - accuracy: 0.7140\n",
      "    437/Unknown - 6s 7ms/step - loss: 3.7959 - accuracy: 0.7140\n",
      "    456/Unknown - 6s 7ms/step - loss: 3.6761 - accuracy: 0.7160\n",
      "    456/Unknown - 6s 7ms/step - loss: 3.6761 - accuracy: 0.7160\n",
      "    472/Unknown - 6s 7ms/step - loss: 3.5813 - accuracy: 0.7172\n",
      "    473/Unknown - 6s 7ms/step - loss: 3.5757 - accuracy: 0.7170\n",
      "    489/Unknown - 6s 7ms/step - loss: 3.4816 - accuracy: 0.7165\n",
      "    490/Unknown - 6s 7ms/step - loss: 3.4758 - accuracy: 0.7168\n",
      "    505/Unknown - 6s 7ms/step - loss: 3.3854 - accuracy: 0.7205\n",
      "    506/Unknown - 6s 7ms/step - loss: 3.3795 - accuracy: 0.7208\n",
      "    524/Unknown - 6s 7ms/step - loss: 3.3017 - accuracy: 0.7188\n",
      "    524/Unknown - 6s 7ms/step - loss: 3.3017 - accuracy: 0.7188\n",
      "    532/Unknown - 6s 7ms/step - loss: 3.2560 - accuracy: 0.7216\n",
      "    532/Unknown - 6s 7ms/step - loss: 3.2560 - accuracy: 0.7216\n",
      "    548/Unknown - 6s 7ms/step - loss: 3.1777 - accuracy: 0.7256\n",
      "    548/Unknown - 6s 7ms/step - loss: 3.1777 - accuracy: 0.7256\n",
      "    564/Unknown - 7s 7ms/step - loss: 3.0993 - accuracy: 0.7285\n",
      "    564/Unknown - 6s 7ms/step - loss: 3.0993 - accuracy: 0.7285\n",
      "    580/Unknown - 7s 7ms/step - loss: 3.0223 - accuracy: 0.7325\n",
      "    580/Unknown - 7s 7ms/step - loss: 3.0223 - accuracy: 0.7325\n",
      "    596/Unknown - 7s 7ms/step - loss: 2.9670 - accuracy: 0.7326\n",
      "    594/Unknown - 7s 7ms/step - loss: 2.9756 - accuracy: 0.7323\n",
      "    612/Unknown - 7s 7ms/step - loss: 2.9096 - accuracy: 0.7322\n",
      "    610/Unknown - 7s 7ms/step - loss: 2.9158 - accuracy: 0.7328\n",
      "    628/Unknown - 7s 7ms/step - loss: 2.8519 - accuracy: 0.7319\n",
      "    627/Unknown - 7s 7ms/step - loss: 2.8532 - accuracy: 0.7323\n",
      "    646/Unknown - 7s 7ms/step - loss: 2.8153 - accuracy: 0.7336\n",
      "    645/Unknown - 7s 7ms/step - loss: 2.8166 - accuracy: 0.7333\n",
      "    663/Unknown - 7s 7ms/step - loss: 2.7730 - accuracy: 0.7342\n",
      "    662/Unknown - 7s 7ms/step - loss: 2.7743 - accuracy: 0.7347\n",
      "    680/Unknown - 7s 7ms/step - loss: 2.7262 - accuracy: 0.7344\n",
      "    679/Unknown - 7s 7ms/step - loss: 2.7298 - accuracy: 0.7342\n",
      "    697/Unknown - 7s 7ms/step - loss: 2.6723 - accuracy: 0.7367\n",
      "    695/Unknown - 7s 7ms/step - loss: 2.6783 - accuracy: 0.7367\n",
      "    713/Unknown - 7s 7ms/step - loss: 2.6210 - accuracy: 0.7393\n",
      "    711/Unknown - 7s 7ms/step - loss: 2.6273 - accuracy: 0.7391\n",
      "    730/Unknown - 8s 7ms/step - loss: 2.5704 - accuracy: 0.7416\n",
      "    728/Unknown - 8s 7ms/step - loss: 2.5761 - accuracy: 0.7412\n",
      "    738/Unknown - 8s 7ms/step - loss: 2.5531 - accuracy: 0.7410\n",
      "    746/Unknown - 8s 7ms/step - loss: 2.5382 - accuracy: 0.7413\n",
      "    757/Unknown - 8s 7ms/step - loss: 2.5126 - accuracy: 0.7412\n",
      "    762/Unknown - 8s 7ms/step - loss: 2.5005 - accuracy: 0.7406\n",
      "    764/Unknown - 8s 7ms/step - loss: 2.4976 - accuracy: 0.7407\n",
      "    771/Unknown - 8s 7ms/step - loss: 2.4797 - accuracy: 0.7409\n",
      "    774/Unknown - 8s 7ms/step - loss: 2.4822 - accuracy: 0.7411\n",
      "    774/Unknown - 8s 7ms/step - loss: 2.4822 - accuracy: 0.7411\n",
      "    790/Unknown - 8s 7ms/step - loss: 2.4480 - accuracy: 0.7410\n",
      "    790/Unknown - 8s 7ms/step - loss: 2.4480 - accuracy: 0.7410\n",
      "    806/Unknown - 8s 7ms/step - loss: 2.4131 - accuracy: 0.7430\n",
      "    806/Unknown - 8s 7ms/step - loss: 2.4131 - accuracy: 0.7430\n",
      "    821/Unknown - 8s 7ms/step - loss: 2.3855 - accuracy: 0.7438\n",
      "    822/Unknown - 8s 7ms/step - loss: 2.3836 - accuracy: 0.7438\n",
      "    834/Unknown - 8s 7ms/step - loss: 2.3724 - accuracy: 0.7439\n",
      "    837/Unknown - 8s 7ms/step - loss: 2.3792 - accuracy: 0.7433\n",
      "    851/Unknown - 8s 7ms/step - loss: 2.3597 - accuracy: 0.7424\n",
      "    853/Unknown - 8s 7ms/step - loss: 2.3600 - accuracy: 0.7416\n",
      "    868/Unknown - 9s 7ms/step - loss: 2.3412 - accuracy: 0.7428\n",
      "    861/Unknown - 9s 7ms/step - loss: 2.3560 - accuracy: 0.7420\n",
      "    886/Unknown - 9s 7ms/step - loss: 2.3019 - accuracy: 0.7442\n",
      "    879/Unknown - 9s 7ms/step - loss: 2.3160 - accuracy: 0.7442\n",
      "    903/Unknown - 9s 7ms/step - loss: 2.2666 - accuracy: 0.7461\n",
      "    896/Unknown - 9s 7ms/step - loss: 2.2817 - accuracy: 0.7451\n",
      "    911/Unknown - 9s 7ms/step - loss: 2.2515 - accuracy: 0.7470\n",
      "    913/Unknown - 9s 7ms/step - loss: 2.2470 - accuracy: 0.7474\n",
      "    928/Unknown - 9s 7ms/step - loss: 2.2249 - accuracy: 0.7478\n",
      "    930/Unknown - 9s 7ms/step - loss: 2.2213 - accuracy: 0.7477\n",
      "    945/Unknown - 9s 7ms/step - loss: 2.2092 - accuracy: 0.7467\n",
      "    946/Unknown - 9s 7ms/step - loss: 2.2073 - accuracy: 0.7467\n",
      "    960/Unknown - 9s 7ms/step - loss: 2.1991 - accuracy: 0.7449\n",
      "    962/Unknown - 9s 7ms/step - loss: 2.1976 - accuracy: 0.7452\n",
      "    977/Unknown - 9s 7ms/step - loss: 2.1778 - accuracy: 0.7455\n",
      "    979/Unknown - 9s 7ms/step - loss: 2.1753 - accuracy: 0.7455\n",
      "    993/Unknown - 9s 7ms/step - loss: 2.1530 - accuracy: 0.7458\n",
      "    995/Unknown - 9s 7ms/step - loss: 2.1494 - accuracy: 0.7457\n",
      "   1010/Unknown - 9s 7ms/step - loss: 2.1288 - accuracy: 0.7459\n",
      "   1013/Unknown - 10s 7ms/step - loss: 2.1234 - accuracy: 0.7461\n",
      "   1026/Unknown - 10s 7ms/step - loss: 2.1017 - accuracy: 0.7470\n",
      "   1028/Unknown - 10s 7ms/step - loss: 2.0985 - accuracy: 0.7470\n",
      "   1042/Unknown - 10s 7ms/step - loss: 2.0757 - accuracy: 0.7482\n",
      "   1044/Unknown - 10s 7ms/step - loss: 2.0721 - accuracy: 0.7486\n",
      "   1059/Unknown - 10s 7ms/step - loss: 2.0475 - accuracy: 0.7506\n",
      "   1059/Unknown - 10s 7ms/step - loss: 2.0475 - accuracy: 0.7506\n",
      "   1072/Unknown - 10s 7ms/step - loss: 2.0272 - accuracy: 0.7515\n",
      "   1072/Unknown - 10s 7ms/step - loss: 2.0272 - accuracy: 0.7515\n",
      "   1090/Unknown - 10s 7ms/step - loss: 2.0123 - accuracy: 0.7521\n",
      "   1090/Unknown - 10s 7ms/step - loss: 2.0123 - accuracy: 0.7521\n",
      "   1107/Unknown - 10s 7ms/step - loss: 2.0133 - accuracy: 0.7518\n",
      "   1098/Unknown - 10s 7ms/step - loss: 2.0192 - accuracy: 0.7517\n",
      "   1115/Unknown - 10s 7ms/step - loss: 2.0037 - accuracy: 0.7519\n",
      "   1116/Unknown - 10s 7ms/step - loss: 2.0029 - accuracy: 0.7519\n",
      "   1134/Unknown - 10s 7ms/step - loss: 1.9771 - accuracy: 0.7536\n",
      "   1134/Unknown - 10s 7ms/step - loss: 1.9771 - accuracy: 0.7536\n",
      "   1149/Unknown - 11s 7ms/step - loss: 1.9574 - accuracy: 0.7545\n",
      "   1149/Unknown - 11s 7ms/step - loss: 1.9574 - accuracy: 0.7545\n",
      "   1164/Unknown - 11s 7ms/step - loss: 1.9400 - accuracy: 0.7546\n",
      "   1163/Unknown - 11s 7ms/step - loss: 1.9415 - accuracy: 0.7544\n",
      "   1181/Unknown - 11s 7ms/step - loss: 1.9370 - accuracy: 0.7531\n",
      "   1180/Unknown - 11s 7ms/step - loss: 1.9370 - accuracy: 0.7531\n",
      "   1199/Unknown - 11s 7ms/step - loss: 1.9322 - accuracy: 0.7529\n",
      "   1198/Unknown - 11s 7ms/step - loss: 1.9314 - accuracy: 0.7530\n",
      "   1208/Unknown - 11s 7ms/step - loss: 1.9263 - accuracy: 0.7533\n",
      "   1216/Unknown - 11s 7ms/step - loss: 1.9263 - accuracy: 0.75\n",
      "   1216/Unknown - 11s 7ms/step - loss: 1.9234 - accuracy: 0.7527\n",
      "\u001b[2m\u001b[36m(BaseWorkerMixin pid=15165)\u001b[0m  - 11s 7ms/step - loss: 1.9234 - accuracy: 0.7527\n",
      "   1224/Unknown - 11s 7ms/step - loss: 1.9162 - accuracy: 0.7533\n",
      "   1232/Unknown - 11s 7ms/step - loss: 1.9062 - accuracy: 0.7539\n",
      "   1240/Unknown - 11s 7ms/step - loss: 1.8957 - accuracy: 0.7544\n",
      "   1240/Unknown - 11s 7ms/step - loss: 1.8957 - accuracy: 0.7544\n",
      "   1255/Unknown - 11s 7ms/step - loss: 1.8821 - accuracy: 0.7541\n",
      "   1255/Unknown - 11s 7ms/step - loss: 1.8821 - accuracy: 0.7541\n",
      "   1271/Unknown - 11s 7ms/step - loss: 1.8674 - accuracy: 0.7545\n",
      "   1271/Unknown - 11s 7ms/step - loss: 1.8674 - accuracy: 0.7545\n",
      "   1286/Unknown - 11s 7ms/step - loss: 1.8623 - accuracy: 0.7538\n",
      "   1286/Unknown - 11s 7ms/step - loss: 1.8623 - accuracy: 0.7538\n",
      "   1303/Unknown - 12s 7ms/step - loss: 1.8504 - accuracy: 0.7536\n",
      "   1303/Unknown - 12s 7ms/step - loss: 1.8504 - accuracy: 0.7536\n",
      "   1320/Unknown - 12s 7ms/step - loss: 1.8411 - accuracy: 0.7538\n",
      "   1320/Unknown - 12s 7ms/step - loss: 1.8411 - accuracy: 0.7538\n",
      "   1338/Unknown - 12s 7ms/step - loss: 1.8234 - accuracy: 0.7547\n",
      "   1338/Unknown - 12s 7ms/step - loss: 1.8234 - accuracy: 0.7547\n",
      "   1356/Unknown - 12s 7ms/step - loss: 1.8062 - accuracy: 0.7555\n",
      "   1356/Unknown - 12s 7ms/step - loss: 1.8062 - accuracy: 0.7555\n",
      "   1374/Unknown - 12s 7ms/step - loss: 1.7875 - accuracy: 0.7565\n",
      "   1374/Unknown - 12s 7ms/step - loss: 1.7875 - accuracy: 0.7565\n",
      "   1391/Unknown - 12s 7ms/step - loss: 1.7748 - accuracy: 0.7572\n",
      "   1390/Unknown - 12s 7ms/step - loss: 1.7760 - accuracy: 0.7570\n",
      "1406/1406 [==============================] - 12s 7ms/step - loss: 1.7651 - accuracy: 0.7576\n",
      "1406/1406 [==============================] - 12s 7ms/step - loss: 1.7651 - accuracy: 0.7576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-24 14:04:51,032\tERROR checkpoint_manager.py:193 -- Result dict has no key: training_iteration. checkpoint_score_attr must be set to a key of the result dict. Valid keys are ['trial_id', 'experiment_id', 'date', 'timestamp', 'pid', 'hostname', 'node_ip', 'config', 'done']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial TensorflowTrainer_1933d_00000 completed. Last result: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-24 14:04:51,142\tINFO tune.py:753 -- Total run time: 23.11 seconds (22.16 seconds for the tuning loop).\n"
     ]
    }
   ],
   "source": [
    "from ray.ml.train.integrations.tensorflow import TensorflowTrainer\n",
    "\n",
    "trainer = TensorflowTrainer(\n",
    "    train_loop_per_worker=train_loop_per_worker,\n",
    "    scaling_config={\"num_workers\": NUM_WORKERS},\n",
    "    datasets={\"train\": train_ds},\n",
    "    preprocessor=get_preprocessor(),\n",
    ")\n",
    "result = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nb0HkOV2R4uL"
   },
   "source": [
    "# Moving on to Serve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OlzjlW8QR_q6"
   },
   "source": [
    "Ray Serve serves the trained model through constructs of `ray.serve.model_wrappers.ModelWrapper` and `ray.serve.model_wrappers.ModelWrapperDeployment`. These constructs wrap a `ray.ml.checkpoint.Checkpoint` into an endpoint that can readily serve http requests.\n",
    "\n",
    "This removes the boilerplate code and minimizes the effort to bring your model to production!\n",
    "\n",
    "Generally speaking the http request can either send in json or data.\n",
    "Upon receiving this payload, Ray Serve would need some \"adapter\" to convert the request payload into some shape or form that can be accepted as input to the preprocessing steps. In this case, we send in a json request and converts it to a pandas DataFrame through `dataframe_adapter`, defined below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "BBbcMwc9Rz66"
   },
   "outputs": [],
   "source": [
    "from fastapi import Request\n",
    "\n",
    "async def dataframe_adapter(request: Request):\n",
    "    \"\"\"Serve HTTP Adapter that reads JSON and converts to pandas DataFrame.\"\"\"\n",
    "    content = await request.json()\n",
    "    return pd.DataFrame.from_dict(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SOnl90IuRywD"
   },
   "source": [
    "Now let's wrap everything in a serve endpoint that exposes a URL to where requests can be sent to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "ujmwT8ZhScq1"
   },
   "outputs": [],
   "source": [
    "from ray import serve\n",
    "from ray.ml.checkpoint import Checkpoint\n",
    "from ray.ml.predictors.integrations.tensorflow import TensorflowPredictor\n",
    "from ray.serve.model_wrappers import ModelWrapperDeployment\n",
    "\n",
    "\n",
    "def serve_model(checkpoint: Checkpoint, model_definition, adapter, name=\"Model\") -> str:\n",
    "    \"\"\"Expose a serve endpoint.\n",
    "\n",
    "    Returns:\n",
    "        serve URL.\n",
    "    \"\"\"\n",
    "    serve.start(detached=True)\n",
    "    deployment = ModelWrapperDeployment.options(name=name)\n",
    "    deployment.deploy(\n",
    "        TensorflowPredictor,\n",
    "        checkpoint,\n",
    "        # This is due to a current limitation on Serve that's\n",
    "        # being addressed.\n",
    "        # TODO(xwjiang): Change to True.\n",
    "        batching_params=False,\n",
    "        model_definition=model_definition,\n",
    "        http_adapter=adapter,\n",
    "    )\n",
    "    return deployment.url\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-24 14:04:52,586\tINFO worker.py:863 -- Using address localhost:9031 set in the environment variable RAY_ADDRESS\n",
      "2022-05-24 14:04:52,587\tINFO worker.py:965 -- Connecting to existing Ray cluster at address: 172.31.75.80:9031\n",
      "\u001b[2m\u001b[36m(ServeController pid=15863)\u001b[0m INFO 2022-05-24 14:04:53,529 controller 15863 checkpoint_path.py:17 - Using RayInternalKVStore for controller checkpoint and recovery.\n",
      "\u001b[2m\u001b[36m(ServeController pid=15863)\u001b[0m INFO 2022-05-24 14:04:53,532 controller 15863 http_state.py:115 - Starting HTTP proxy with name 'SERVE_CONTROLLER_ACTOR:SERVE_PROXY_ACTOR-node:172.31.75.80-0' on node 'node:172.31.75.80-0' listening on '127.0.0.1:8000'\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO:     Started server process [15898]\n",
      "\u001b[2m\u001b[36m(ServeController pid=15863)\u001b[0m INFO 2022-05-24 14:04:56,155 controller 15863 deployment_state.py:1217 - Adding 1 replicas to deployment 'Model'.\n"
     ]
    }
   ],
   "source": [
    "# Generally speaking, training and serving are done in totally different ray clusters.\n",
    "# To simulate that, let's shutdown the old ray cluster in preparation for serving.\n",
    "ray.shutdown()\n",
    "\n",
    "endpoint_uri = serve_model(result.checkpoint, build_model, dataframe_adapter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rzHSwa2bSyee"
   },
   "source": [
    "Let's write a helper function to send requests to this endpoint and compare the results with labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "E9m80HDmSz66"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "NUM_SERVE_REQUESTS = 100\n",
    "\n",
    "def send_requests(df: pd.DataFrame, label: np.array):\n",
    "    for i in range(NUM_SERVE_REQUESTS):\n",
    "        one_row = df.iloc[[i]].to_dict()\n",
    "        serve_result = requests.post(endpoint_uri, json=one_row).json()\n",
    "        print(\n",
    "            f\"request[{i}] prediction: {serve_result['predictions']['0']} \"\n",
    "            f\"- label: {str(label[i])}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "GFPwKc5JTgnI"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:04:59,203 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 307 4.0ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:04:59,202 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 0.3ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m 2022-05-24 14:04:59.269428: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m 2022-05-24 14:04:59.269458: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m 2022-05-24 14:04:59.269475: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ip-172-31-75-80): /proc/driver/nvidia/version does not exist\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m 2022-05-24 14:04:59.269719: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:04:59,299 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 200 94.1ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:04:59,306 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 307 2.2ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:04:59,298 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 91.8ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:04:59,305 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 0.2ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:04:59,385 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 76.4ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "request[0] prediction: 1.1491288205434103e-05 - label: True\n",
      "request[1] prediction: 0.003555983304977417 - label: False\n",
      "request[2] prediction: 1.3029743684001005e-07 - label: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:04:59,386 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 200 78.8ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:04:59,392 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 307 2.3ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:04:59,473 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 200 78.0ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:04:59,479 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 307 2.4ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:04:59,392 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 0.2ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:04:59,471 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 75.7ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:04:59,478 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 0.3ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:04:59,563 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 200 81.9ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:04:59,569 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 307 2.6ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:04:59,561 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 79.6ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:04:59,568 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 0.3ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "request[3] prediction: 2.1414199125047162e-07 - label: False\n",
      "request[4] prediction: 2.606009559258382e-07 - label: False\n",
      "request[5] prediction: 2.4299682266359923e-08 - label: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:04:59,656 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 200 84.3ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:04:59,662 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 307 2.6ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:04:59,654 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 81.6ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:04:59,661 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 0.2ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:04:59,745 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 200 80.2ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:04:59,751 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 307 2.6ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:04:59,743 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 77.6ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:04:59,750 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 0.2ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "request[6] prediction: 6.910659777759065e-08 - label: False\n",
      "request[7] prediction: 0.005982130765914917 - label: False\n",
      "request[8] prediction: 1.0000212569138967e-07 - label: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:04:59,835 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 200 81.1ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:04:59,841 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 307 2.4ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:04:59,834 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 78.7ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:04:59,841 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 0.2ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:04:59,922 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 200 78.5ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:04:59,928 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 307 2.2ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:04:59,921 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 76.2ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:04:59,927 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 0.2ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "request[9] prediction: 0.0004226863384246826 - label: True\n",
      "request[10] prediction: 4.646678462449927e-06 - label: False\n",
      "request[11] prediction: 4.0754843212198466e-05 - label: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:00,011 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 200 80.7ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:00,017 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 307 2.4ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:00,097 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 200 77.5ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:00,009 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 78.5ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:00,016 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 0.3ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:00,095 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 75.3ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:00,103 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 0.3ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:00,104 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 307 2.9ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:00,184 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 200 77.7ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:00,191 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 307 2.4ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:00,183 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 75.3ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:00,190 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 0.2ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:00,275 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 200 82.5ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:00,281 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 307 2.1ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:00,274 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 80.4ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:00,280 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 0.2ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:00,363 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 200 80.0ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:00,369 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 307 2.3ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:00,362 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 77.8ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:00,369 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 0.2ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "request[12] prediction: 0.0003007054328918457 - label: False\n",
      "request[13] prediction: 1.7516806849471322e-07 - label: False\n",
      "request[14] prediction: 0.0004551708698272705 - label: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:00,453 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 200 81.9ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:00,459 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 307 2.2ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:00,452 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 79.7ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:00,458 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 0.2ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:00,539 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 200 77.8ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:00,545 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 307 2.1ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:00,538 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 75.7ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:00,544 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 0.2ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "request[15] prediction: 3.4552940633147955e-05 - label: True\n",
      "request[16] prediction: 6.294904153492098e-08 - label: False\n",
      "request[17] prediction: 1.8173336968629883e-07 - label: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:00,631 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 200 83.8ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:00,637 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 307 2.2ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:00,717 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 200 77.7ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:00,629 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 81.6ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:00,636 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 0.2ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:00,715 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 75.5ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:00,723 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 307 2.5ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:00,802 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 200 77.1ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:00,808 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 307 2.2ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:00,722 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 0.2ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:00,801 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 74.9ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:00,807 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 0.2ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "request[18] prediction: 1.0331812205777169e-07 - label: False\n",
      "request[19] prediction: 1.1827465584701713e-07 - label: False\n",
      "request[20] prediction: 1.6453311602049325e-08 - label: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:00,891 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 200 80.9ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:00,898 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 307 2.4ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:00,890 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 78.7ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:00,897 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 0.2ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:00,978 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 200 78.1ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:00,984 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 307 2.4ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:00,976 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 75.8ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:00,983 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 0.3ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:01,073 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 200 86.5ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:01,079 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 307 2.3ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:01,071 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 84.0ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:01,078 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 0.3ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:01,159 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 200 78.6ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:01,166 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 307 2.2ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:01,158 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 76.4ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:01,165 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 0.2ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "request[21] prediction: 4.500541805896319e-08 - label: False\n",
      "request[22] prediction: 0.5504510998725891 - label: False\n",
      "request[23] prediction: 4.865139999310486e-05 - label: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:01,246 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 200 78.2ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:01,252 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 307 2.3ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:01,245 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 75.9ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:01,251 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 0.2ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:01,335 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 79.9ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:01,336 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 200 82.3ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:01,345 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 307 2.6ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:01,429 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 200 81.8ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:01,435 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 307 2.2ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:01,343 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 0.3ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:01,427 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 78.7ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:01,434 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 0.2ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "request[24] prediction: 5.4028344464995826e-08 - label: False\n",
      "request[25] prediction: 1.943743797028219e-07 - label: False\n",
      "request[26] prediction: 2.9754369634815703e-08 - label: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:01,519 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 200 82.4ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:01,525 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 307 2.2ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:01,518 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 80.2ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:01,524 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 0.2ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:01,607 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 200 80.0ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:01,613 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 307 2.4ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:01,606 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 77.8ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:01,613 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 0.2ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "request[27] prediction: 1.3197384873819828e-07 - label: False\n",
      "request[28] prediction: 2.137282280045838e-07 - label: False\n",
      "request[29] prediction: 1.6149725468039833e-07 - label: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:01,696 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 200 80.5ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:01,702 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 307 2.3ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:01,695 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 78.2ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:01,702 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 0.2ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:01,789 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 200 84.4ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:01,795 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 307 2.3ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:01,788 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 82.1ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:01,794 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 0.3ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:01,875 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 200 78.5ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:01,881 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 307 2.2ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:01,874 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 76.2ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:01,880 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 0.2ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:01,969 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 200 85.4ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:01,975 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 307 2.5ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:02,055 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 200 77.8ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:01,967 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 83.2ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:01,974 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 0.2ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:02,054 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 75.5ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "request[30] prediction: 1.2152982264979073e-07 - label: False\n",
      "request[31] prediction: 1.8627473252763593e-07 - label: False\n",
      "request[32] prediction: 1.1352103257422641e-07 - label: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:02,062 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 307 2.3ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:02,144 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 200 79.8ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:02,150 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 307 2.1ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:02,061 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 0.2ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:02,143 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 77.7ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:02,149 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 0.2ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:02,235 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 200 82.6ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:02,241 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 307 2.2ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:02,233 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 80.4ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:02,240 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 0.2ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "request[33] prediction: 1.1688795709119404e-08 - label: False\n",
      "request[34] prediction: 4.857337262365036e-05 - label: True\n",
      "request[35] prediction: 5.4835744833781064e-08 - label: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:02,324 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 200 80.5ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:02,330 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 307 2.5ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:02,322 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 78.3ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:02,329 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 0.2ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:02,415 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 200 82.8ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:02,421 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 307 2.4ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:02,413 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 80.2ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:02,420 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 0.2ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "request[36] prediction: 0.0001519620418548584 - label: True\n",
      "request[37] prediction: 4.3995584064759896e-08 - label: False\n",
      "request[38] prediction: 2.2130504362394277e-07 - label: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:02,505 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 200 81.9ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:02,512 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 307 2.6ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:02,504 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 79.6ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:02,511 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 0.3ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:02,594 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 200 79.8ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:02,600 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 307 2.5ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:02,592 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 77.2ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:02,599 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 0.2ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:02,688 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 200 85.7ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:02,695 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 307 2.6ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:02,687 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 83.1ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:02,694 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 0.3ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:02,775 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 76.7ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:02,777 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 200 79.7ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:02,784 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 307 2.8ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:02,871 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 200 84.5ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:02,877 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 307 2.2ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:02,783 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 0.2ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:02,869 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 82.1ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:02,876 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 0.2ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "request[39] prediction: 3.043066681129858e-05 - label: True\n",
      "request[40] prediction: 1.6134484326357779e-07 - label: False\n",
      "request[41] prediction: 4.23210185545031e-05 - label: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:02,957 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 200 78.0ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:02,963 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 307 2.2ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:02,956 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 75.6ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:02,962 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 0.2ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:03,045 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 200 79.6ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:03,051 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 307 2.3ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:03,043 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 77.4ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:03,050 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 0.2ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "request[42] prediction: 3.361811468494125e-05 - label: False\n",
      "request[43] prediction: 1.5734133285150165e-07 - label: False\n",
      "request[44] prediction: 9.001221769722179e-05 - label: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:03,135 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 200 81.5ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:03,141 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 307 2.3ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:03,133 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 79.2ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:03,140 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 0.2ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:03,227 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 200 84.6ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:03,233 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 307 2.2ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:03,226 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 82.3ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:03,232 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 0.2ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "request[45] prediction: 3.0814808837931196e-07 - label: False\n",
      "request[46] prediction: 2.628052607178688e-05 - label: True\n",
      "request[47] prediction: 3.590690198507218e-07 - label: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:03,314 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 200 78.3ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:03,320 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 307 2.2ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:03,312 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 76.2ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:03,319 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 0.2ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:03,399 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 200 77.5ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:03,405 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 307 2.2ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:03,484 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 200 76.3ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:03,490 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 307 2.2ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:03,398 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 75.3ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:03,404 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 0.2ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:03,483 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 74.1ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:03,489 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 0.2ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "request[48] prediction: 6.896811828482896e-05 - label: True\n",
      "request[49] prediction: 1.0443396547543671e-07 - label: False\n",
      "request[50] prediction: 0.0003883242607116699 - label: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:03,574 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 200 82.3ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:03,580 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 307 2.1ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:03,573 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 80.2ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:03,579 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 0.2ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:03,672 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 200 89.2ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:03,678 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 307 3.0ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:03,670 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 87.0ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:03,677 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 0.3ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:03,774 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 200 93.8ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:03,781 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 307 2.4ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:03,773 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 91.4ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:03,780 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 0.2ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:03,862 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 200 78.8ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:03,869 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 307 2.3ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:03,861 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 76.6ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:03,868 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 0.2ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "request[51] prediction: 6.354945725206562e-08 - label: False\n",
      "request[52] prediction: 2.4775346219030325e-07 - label: False\n",
      "request[53] prediction: 0.18227052688598633 - label: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:03,951 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 200 79.8ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:03,957 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 307 2.4ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:03,950 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 77.5ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:03,956 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 0.2ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:04,038 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 200 79.4ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:04,044 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 307 2.3ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:04,037 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 77.1ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:04,044 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 0.2ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "request[54] prediction: 2.6946693765239615e-08 - label: False\n",
      "request[55] prediction: 7.76203194163827e-07 - label: False\n",
      "request[56] prediction: 0.0007852911949157715 - label: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:04,139 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 200 92.5ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:04,146 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 307 2.6ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:04,138 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 90.1ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:04,145 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 0.2ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:04,240 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 200 91.4ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:04,246 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 307 2.5ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:04,238 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 88.9ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:04,245 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 0.2ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "request[57] prediction: 2.4366783691220917e-05 - label: True\n",
      "request[58] prediction: 1.0067540046065915e-07 - label: False\n",
      "request[59] prediction: 7.350580943921159e-08 - label: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:04,331 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 200 83.5ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:04,338 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 307 2.4ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:04,418 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 200 78.4ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:04,330 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 81.1ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:04,337 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 0.2ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:04,417 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 76.1ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:04,426 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 307 2.4ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:04,505 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 200 77.4ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:04,511 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 307 2.2ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:04,425 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 0.2ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:04,504 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 75.3ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:04,510 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 0.2ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:04,591 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 200 77.9ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:04,597 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 307 2.3ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:04,590 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 75.7ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:04,596 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 0.2ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:04,682 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 200 83.1ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:04,689 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 307 2.2ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:04,681 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 80.9ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:04,688 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 0.2ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "request[60] prediction: 1.1088037581430399e-06 - label: False\n",
      "request[61] prediction: 1.6559798154958116e-07 - label: False\n",
      "request[62] prediction: 2.7227918053540634e-07 - label: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:04,773 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 200 81.7ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:04,779 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 307 2.3ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:04,772 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 79.4ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:04,778 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 0.3ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:04,859 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 200 78.2ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:04,866 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 307 2.2ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:04,858 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 76.1ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:04,865 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 0.2ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "request[63] prediction: 0.000486224889755249 - label: True\n",
      "request[64] prediction: 3.870736620115167e-08 - label: False\n",
      "request[65] prediction: 1.0610143164058172e-07 - label: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:04,946 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 200 78.2ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:04,952 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 307 2.2ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:05,032 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 200 77.5ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:05,038 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 307 2.3ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:04,945 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 76.1ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:04,951 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 0.2ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:05,031 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 75.3ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:05,037 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 0.2ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:05,120 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 200 80.4ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:05,126 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 307 2.2ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:05,119 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 78.0ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:05,125 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 0.2ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "request[66] prediction: 2.262856071411079e-07 - label: False\n",
      "request[67] prediction: 7.113482780596314e-08 - label: False\n",
      "request[68] prediction: 1.201359989266848e-08 - label: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:05,211 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 200 82.5ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:05,217 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 307 2.3ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:05,209 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 80.3ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:05,216 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 0.2ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:05,297 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 200 77.9ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:05,303 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 307 2.2ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:05,296 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 75.7ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:05,302 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 0.2ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:05,384 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 200 78.9ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:05,390 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 307 2.3ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:05,383 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 76.5ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:05,389 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 0.2ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:05,471 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 200 79.1ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:05,477 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 307 2.2ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:05,470 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 76.8ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:05,477 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 0.2ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "request[69] prediction: 7.79878391767852e-05 - label: True\n",
      "request[70] prediction: 1.586334548164814e-07 - label: False\n",
      "request[71] prediction: 6.570118671334058e-08 - label: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:05,562 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 200 82.4ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:05,568 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 307 2.3ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:05,652 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 200 82.3ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:05,561 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 80.1ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:05,567 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 0.2ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:05,651 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 80.0ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:05,659 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 0.3ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:05,660 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 307 2.9ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:05,752 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 200 89.8ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:05,759 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 307 2.5ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:05,751 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 87.4ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:05,758 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 0.3ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "request[72] prediction: 0.0004475414752960205 - label: False\n",
      "request[73] prediction: 1.2184953845917335e-07 - label: False\n",
      "request[74] prediction: 4.380761993161286e-07 - label: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:05,841 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 200 79.9ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:05,847 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 307 2.2ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:05,840 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 77.6ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:05,846 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 0.2ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:05,927 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 200 77.9ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:05,932 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 307 2.1ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:05,926 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 75.8ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:05,932 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 0.2ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "request[75] prediction: 6.315800874290289e-06 - label: False\n",
      "request[76] prediction: 1.179628543468425e-05 - label: True\n",
      "request[77] prediction: 3.807217296980525e-08 - label: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:06,016 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 200 81.3ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:06,022 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 307 2.2ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:06,015 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 79.1ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:06,021 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 0.2ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:06,102 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 200 77.5ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:06,107 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 307 2.1ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:06,100 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 75.4ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:06,106 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 0.2ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "request[78] prediction: 4.287464253138751e-05 - label: False\n",
      "request[79] prediction: 3.848262153383075e-08 - label: False\n",
      "request[80] prediction: 7.571935043415579e-07 - label: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:06,188 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 200 78.5ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:06,193 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 307 2.1ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:06,275 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 200 79.2ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:06,187 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 76.5ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:06,193 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 0.2ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:06,273 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 76.9ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:06,282 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 307 2.4ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:06,367 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 200 82.7ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:06,373 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 307 2.2ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:06,281 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 0.2ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:06,366 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 80.6ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:06,372 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 0.2ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:06,454 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 200 78.5ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:06,460 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 307 2.3ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:06,452 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 76.4ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:06,459 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 0.2ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:06,539 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 200 76.9ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:06,545 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 307 2.1ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:06,538 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 74.8ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:06,544 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 0.2ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "request[81] prediction: 0.0005091428756713867 - label: False\n",
      "request[82] prediction: 6.751630365897654e-08 - label: False\n",
      "request[83] prediction: 2.0060606686911342e-07 - label: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:06,627 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 200 80.5ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:06,633 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 307 2.2ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:06,626 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 78.4ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:06,632 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 0.2ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:06,719 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 200 83.4ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:06,724 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 307 2.2ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:06,717 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 81.2ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:06,724 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 0.2ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "request[84] prediction: 2.957314748641693e-08 - label: False\n",
      "request[85] prediction: 8.676134939378244e-07 - label: False\n",
      "request[86] prediction: 0.0006256103515625 - label: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:06,805 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 200 78.1ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:06,811 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 307 2.4ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:06,803 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 75.8ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:06,810 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 0.2ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:06,896 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 200 83.0ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:06,902 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 307 2.3ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:06,983 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 200 79.3ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:06,990 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 307 2.4ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:06,895 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 80.6ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:06,901 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 0.2ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:06,982 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 77.0ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:06,989 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 0.3ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "request[87] prediction: 1.4276582760430756e-07 - label: False\n",
      "request[88] prediction: 2.6005329800682375e-07 - label: False\n",
      "request[89] prediction: 5.810511538584251e-07 - label: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:07,076 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 200 83.5ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:07,082 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 307 2.5ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:07,074 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 81.0ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:07,081 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 0.3ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:07,165 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 200 80.7ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:07,171 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 307 2.4ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:07,164 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 78.4ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:07,170 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 0.2ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:07,254 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 200 80.3ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:07,260 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 307 2.3ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:07,253 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 78.0ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:07,259 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 0.2ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:07,345 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 200 82.5ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:07,351 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 307 2.5ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:07,343 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 80.3ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:07,350 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 0.2ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "request[90] prediction: 5.8869878557743505e-05 - label: False\n",
      "request[91] prediction: 0.0016419589519500732 - label: False\n",
      "request[92] prediction: 9.698381830958169e-08 - label: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:07,432 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 200 78.3ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:07,438 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 307 2.3ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:07,430 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 76.0ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:07,437 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 0.2ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:07,522 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 200 82.7ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:07,528 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 307 2.1ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:07,521 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 80.6ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:07,527 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 0.2ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:07,607 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 76.2ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:07,609 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 200 78.4ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "request[93] prediction: 1.5432956956829003e-07 - label: False\n",
      "request[94] prediction: 8.49949515213666e-08 - label: False\n",
      "request[95] prediction: 3.555568400770426e-05 - label: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:07,616 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 307 2.3ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:07,695 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 200 77.2ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:07,701 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 307 2.2ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:07,615 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 0.2ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:07,694 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 75.0ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:07,700 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 0.2ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:07,786 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 200 82.7ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:07,791 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 307 2.2ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:07,784 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 80.5ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:07,791 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 0.2ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "request[96] prediction: 2.7247064281255007e-05 - label: True\n",
      "request[97] prediction: 2.0579797421760304e-07 - label: False\n",
      "request[98] prediction: 1.1204150496268994e-07 - label: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:07,872 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 200 78.7ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:07,879 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 307 2.3ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:07,871 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 76.5ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:07,878 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 0.3ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:07,962 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 200 81.4ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:07,968 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 307 2.2ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:07,961 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 79.2ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:07,967 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 0.2ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "request[99] prediction: 0.00015854835510253906 - label: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:08,047 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 200 77.2ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:08,053 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 307 2.0ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:08,046 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 75.2ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:08,052 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 0.2ms\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=15898)\u001b[0m INFO 2022-05-24 14:05:08,134 http_proxy 172.31.75.80 http_proxy.py:320 - POST /Model 200 79.7ms\n",
      "\u001b[2m\u001b[36m(Model pid=15941)\u001b[0m INFO 2022-05-24 14:05:08,133 Model Model#lIJyYz replica.py:483 - HANDLE __call__ OK 77.6ms\n"
     ]
    }
   ],
   "source": [
    "send_requests(test_df, test_label)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "jAgvLbhT8nB0"
   ],
   "name": "tfx.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "3c0d54d489a08ae47a06eae2fd00ff032d6cddb527c382959b7b2575f6a8167f"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
