{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT-J-6B Inference with Ray AIR\n",
    "\n",
    "In this example, we will showcase how to use the Ray AIR for **GPT-J batch inference (offline inference) and serving (online inference)**. GPT-J is a GPT-2-like causal language model trained on the Pile dataset. This particular model has 6 billion parameters. For more information on GPT-J, click [here](https://huggingface.co/docs/transformers/model_doc/gptj).\n",
    "\n",
    "We will use Ray Data for batch inference, Ray Serve for online inference and a pretrained model from Hugging Face hub. Note that you can easily adapt this example to use other similar models.\n",
    "\n",
    "It is highly recommended to read [Ray AIR Key Concepts](air-key-concepts), [Ray Data Key Concepts](data_key_concepts) and [Ray Serve Key Concepts](serve-key-concepts) before starting this example.\n",
    "\n",
    "```{note}\n",
    "In order to run this example, make sure your Ray cluster has access to at least one GPU with 16 or more GBs of memory. The amount of memory needed will depend on the model.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"EleutherAI/gpt-j-6B\"\n",
    "revision = \"float16\"  # use float16 weights to fit in 16GB GPUs\n",
    "prompt = (\n",
    "    \"In a shocking finding, scientists discovered a herd of unicorns living in a remote, \"\n",
    "    \"previously unexplored valley, in the Andes Mountains. Even more surprising to the \"\n",
    "    \"researchers was the fact that the unicorns spoke perfect English.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a {ref}`runtime environment <runtime-environments>` to ensure that the Ray workers have access to all the necessary packages. You can omit the `runtime_env` argument if you have all of the packages already installed on each node in your cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.init(\n",
    "    runtime_env={\n",
    "        \"pip\": [\n",
    "            \"accelerate>=0.16.0\",\n",
    "            \"transformers>=4.26.0\",\n",
    "            \"torch\",\n",
    "        ]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch inference with Ray Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purposes of this example, we will use a very small toy dataset composed of multiple copies of our prompt. Ray Data can handle much bigger datasets with ease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray.data\n",
    "import pandas as pd\n",
    "\n",
    "ds = ray.data.from_pandas(pd.DataFrame([prompt] * 10, columns=[\"prompt\"]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we will be using a pretrained model from Hugging Face hub, the simplest way is to use {meth}`map_batches <ray.data.Dataset.map_batches>` with a [callable class UDF](transform_datasets_callable_classes). This will allow us to save time by initializing a model just once and then feed it multiple batches of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredictCallable:\n",
    "    def __init__(self, model_id: str, revision: str = None):\n",
    "        from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "        import torch\n",
    "\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_id,\n",
    "            revision=revision,\n",
    "            torch_dtype=torch.float16,\n",
    "            low_cpu_mem_usage=True,\n",
    "            device_map=\"auto\",  # automatically makes use of all GPUs available to the Actor\n",
    "        )\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "    def __call__(self, batch: pd.DataFrame) -> pd.DataFrame:\n",
    "        input_ids = self.tokenizer(\n",
    "            list(batch[\"prompt\"]), return_tensors=\"pt\"\n",
    "        ).input_ids.to(self.model.device)\n",
    "\n",
    "        gen_tokens = self.model.generate(\n",
    "            input_ids,\n",
    "            do_sample=True,\n",
    "            temperature=0.9,\n",
    "            max_length=100,\n",
    "        )\n",
    "        return pd.DataFrame(\n",
    "            self.tokenizer.batch_decode(gen_tokens), columns=[\"responses\"]\n",
    "        )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All that is left is to run the `map_batches` method on the dataset. We specify that we want to use one GPU for each Ray Actor that will be running our callable class.\n",
    "\n",
    "```{tip}\n",
    "If you have access to large GPUs, you may want to increase the batch size to better saturate them.\n",
    "\n",
    "If you want to use inter-node model parallelism, you can also increase `num_gpus`. As we have created the model with `device_map=\"auto\"`, it will be automatically placed on correct devices. Note that this requires nodes with multiple GPUs.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = ds.map_batches(\n",
    "    PredictCallable,\n",
    "    batch_size=4,\n",
    "    fn_constructor_kwargs=dict(model_id=model_id, revision=revision),\n",
    "    compute=\"actors\",\n",
    "    num_gpus=1,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After `map_batches` is done, we can view our generated text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-28 10:40:50,530\tINFO bulk_executor.py:41 -- Executing DAG InputDataBuffer[Input] -> ActorPoolMapOperator[MapBatches(PredictCallable)]\n",
      "MapBatches(PredictCallable), 0 actors [0 locality hits, 1 misses]: 100%|██████████| 1/1 [12:10<00:00, 730.80s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'responses': 'In a shocking finding, scientists discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\\n\\nThe finding comes from the team of researchers, which includes Dr. Michael Goldberg, a professor and chair of the Zoology Department at the University of Maryland. Dr. Goldberg spent a year collecting and conducting research in the Ecuadorian Andes, including the Pinchahu'},\n",
       " {'responses': 'In a shocking finding, scientists discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\\n\\nThe team of British, Argentine and Chilean scientists found that the elusive unicorns had been living in the valley for at least 50 years, and had even interacted with humans.\\n\\nThe team’s findings published in the journal Scientific Reports has been hailed as a'},\n",
       " {'responses': 'In a shocking finding, scientists discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\\n\\nAs far as the rest of human kind knew, unicorns had never existed on Earth, but the presence of this herd has left some very confused. Are the scientists simply overreacting? Or has the valley become the new Unicorn Valley?\\n\\nThere are only'},\n",
       " {'responses': 'In a shocking finding, scientists discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English. The discovery was announced by Oxford University and was published in the journal Science. According to the researchers, this is proof of an alien life. This time around the aliens are definitely not from outer space – they are quite cozy.\\n\\n“I saw the herd for the'},\n",
       " {'responses': 'In a shocking finding, scientists discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\\n\\nIn the article, The Daily Beast and NewScientist report on these \"extraordinary find[s], reported this week to the Royal Society.\" According to the article:\\n\\nThe authors, who were part of a team from the University of Lincoln’s'},\n",
       " {'responses': 'In a shocking finding, scientists discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English. This was no ordinary herd of animals.\\n\\nThe discovery was made by the team while they were riding horses in the wilds of the Peruvian Andes. As they rode through the area, they came upon a herd of white alpacas, which were quite exotic'},\n",
       " {'responses': 'In a shocking finding, scientists discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\\n\\nThe mountain valley that the unicorns lived in sat under the shadow of an active volcano emitting smoke as big as Mount St. Helens. The scientists named the newly discovered unicorn herd the Andes Biodiversity Center—or ABC for short.\\n\\nThe discovery'},\n",
       " {'responses': 'In a shocking finding, scientists discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\\n\\nUnicorns have been depicted in the fairy tales and legends of many cultures throughout history, but scientists have been unable to explain the species.\\n\\nIn a paper published in the journal Biology Letters, the researchers studied five male and five female unicorns and their offspring'},\n",
       " {'responses': 'In a shocking finding, scientists discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English. How did this amazing discovery occur?\\n\\nBefore I tell you exactly how unicorns managed to come into existence, allow me to explain how I think unicorns occur. I think they exist in the same way some people believe Jesus rose from the dead.\\n\\nI think'},\n",
       " {'responses': 'In a shocking finding, scientists discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\\n\\nIt is well known that horses, zebras, and other hoofed beasts have long since left their ancestral lands in the grasslands of South America, and are now found throughout Eurasia and North Africa. However, there are also a number of other,'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.take_all()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may notice that we are not using an AIR {class}`Predictor <ray.train.predictor.Predictor>` here. This is because Predictors are mainly intended to be used with AIR {class}`Checkpoints <ray.air.checkpoint.Checkpoint>`, which we don't for this example. See {ref}`air-predictors` for more information and usage examples."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serving with Ray Serve\n",
    "\n",
    "Setting up basic serving with Ray Serve is very similar to batch inference with Ray Data. First, we define a callable class that will serve as the [Serve deployment](serve-key-concepts-deployment). At runtime, a deployment consists of a number of *replicas*, which are individual copies of the class or function that are started in separate Ray Actors (processes). The number of replicas can be scaled up or down (or even autoscaled) to match the incoming request load.\n",
    "\n",
    "We make sure to set the deployment to use 1 GPU by setting `\"num_gpus\"` in `ray_actor_options`. The rest of the class is very similar to `PredictCallable` we have used for batch inference - the only difference is the use of `async` for the `__call__` method, which will be used to handle incoming HTTP requests.\n",
    "\n",
    "```{tip}\n",
    "Same as in the batch inference workload, if you want to use inter-node model parallelism, you can also increase `num_gpus`. As we have created the model with `device_map=\"auto\"`, it will be automatically placed on correct devices. Note that this requires nodes with multiple GPUs.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray import serve\n",
    "from starlette.requests import Request\n",
    "\n",
    "\n",
    "@serve.deployment(ray_actor_options={\"num_gpus\": 1})\n",
    "class PredictDeployment:\n",
    "    def __init__(self, model_id: str, revision: str = None):\n",
    "        from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "        import torch\n",
    "\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_id,\n",
    "            revision=revision,\n",
    "            torch_dtype=torch.float16,\n",
    "            low_cpu_mem_usage=True,\n",
    "            device_map=\"auto\",  # automatically makes use of all GPUs available to the Actor\n",
    "        )\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "    def generate(self, text: str) -> pd.DataFrame:\n",
    "        input_ids = self.tokenizer(text, return_tensors=\"pt\").input_ids.to(\n",
    "            self.model.device\n",
    "        )\n",
    "\n",
    "        gen_tokens = self.model.generate(\n",
    "            input_ids,\n",
    "            do_sample=True,\n",
    "            temperature=0.9,\n",
    "            max_length=100,\n",
    "        )\n",
    "        return pd.DataFrame(\n",
    "            self.tokenizer.batch_decode(gen_tokens), columns=[\"responses\"]\n",
    "        )\n",
    "\n",
    "    async def __call__(self, http_request: Request) -> str:\n",
    "        json_request: str = await http_request.json()\n",
    "        prompts = []\n",
    "        for prompt in json_request:\n",
    "            text = prompt[\"text\"]\n",
    "            if isinstance(text, list):\n",
    "                prompts.extend(text)\n",
    "            else:\n",
    "                prompts.append(text)\n",
    "        return self.generate(prompts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now `bind` the deployment with our arguments, and use {meth}`~ray.serve.run` to start it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RayServeSyncHandle(deployment='PredictDeployment')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deployment = PredictDeployment.bind(model_id=model_id, revision=revision)\n",
    "serve.run(deployment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try submitting a request to our deployment. We will use the same prompt as before, and send a POST request. The deployment will generate a response and return it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(ServeReplica:PredictDeployment pid=651, ip=10.0.8.161) The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "(ServeReplica:PredictDeployment pid=651, ip=10.0.8.161) Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'responses': 'In a shocking finding, scientists discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\\n\\nThe findings come from a recent expedition to the region of Cordillera del Divisor, in northern Peru. The region was previously known to have an unusually high number of native animals.\\n\\n\"Our team was conducting a population census of the region’'}]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "sample_input = {\"text\": prompt}\n",
    "\n",
    "output = requests.post(\"http://localhost:8000/\", json=[sample_input]).json()\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10 (default, Nov 14 2022, 12:59:47) \n[GCC 9.4.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3c0d54d489a08ae47a06eae2fd00ff032d6cddb527c382959b7b2575f6a8167f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
