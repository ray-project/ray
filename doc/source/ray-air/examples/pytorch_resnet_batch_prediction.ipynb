{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Prediction with Pytorch Pretrained Model\n",
    "\n",
    "In this example, we will introduce how to use {class}`BatchPredictor <ray.train.BatchPredictor>` for large-scale efficient batch inference.\n",
    "\n",
    "Next we will load a pretrained ResNet-152 model for image classification. We choose [Imagenette](https://github.com/fastai/imagenette) dataset for demo, which is a subset of Imagenet with 10 easily classified classes(tench, English springer, cassette player, chain saw, church, French horn, garbage truck, gas pump, golf ball, parachute).\n",
    "\n",
    "First we use {meth}`ray.data.read_images <ray.data.read_images>` to load the validation set from S3. Since the dataset folders are already structured, we can use {class}`Partitioning <ray.data.datasource.Partitioning>` API to automatically extract image labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-11 14:38:07,841\tINFO worker.py:1352 -- Connecting to existing Ray cluster at address: 10.0.29.191:6379...\n",
      "2023-02-11 14:38:08,111\tINFO worker.py:1529 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2023-02-11 14:38:08,117\tINFO packaging.py:373 -- Pushing file package 'gcs://_ray_pkg_448ef99926e5e34674f2510a0848d7c6.zip' (0.71MiB) to Ray cluster...\n",
      "2023-02-11 14:38:08,124\tINFO packaging.py:386 -- Successfully pushed file package 'gcs://_ray_pkg_448ef99926e5e34674f2510a0848d7c6.zip'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: extension<arrow.py_extension_type<ArrowVariableShapedTensorType>>\n",
      "class: string\n"
     ]
    }
   ],
   "source": [
    "import ray\n",
    "from ray.data.datasource.partitioning import Partitioning\n",
    "\n",
    "s3_uri = \"s3://air-example-data-2/imagenette2/val/\"\n",
    "partitioning = Partitioning(\"dir\", field_names=[\"class\"], base_dir=s3_uri)\n",
    "ds = ray.data.read_images(s3_uri, partitioning=partitioning, mode=\"RGB\")\n",
    "print(ds.schema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "ds = ds.limit(1000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the label for images are strings of folder names(e.g. n01728920). Let's download a `imagenet_class_index.json` to construct a mapping from class names to indices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "s3.meta.client.download_file('air-example-data-2', 'imagenette2/imagenet_class_index.json', '/tmp/imagenet_class_index.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "idx_to_class = json.load(open(\"/tmp/imagenet_class_index.json\"))\n",
    "class_to_idx = {cls_name: int(index) for index, (cls_name, _) in idx_to_class.items()}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define a preprocessor to crop and normalize input images. Likewise, you can define any customized preprocessing function for a {class}`BatchMapper <ray.data.preprocessors.BatchMapper>`. It applies data transformations to the input dataset before feeding it into the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from ray.data.preprocessors import BatchMapper\n",
    "\n",
    "def preprocess_image(batch):\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    batch[\"image\"] = np.array([preprocess(img).numpy() for img in batch[\"image\"]])\n",
    "    batch[\"label\"] = np.array([class_to_idx[cls_name] for cls_name in batch[\"class\"]])\n",
    "    return batch\n",
    "\n",
    "preprocessor = BatchMapper(fn=preprocess_image, batch_format=\"numpy\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a BatchPredictor\n",
    "\n",
    "A BatchPredictor takes a checkpoint and a predictor class(e.g. `TorchPredictor`, `TensorflowPredictor`) and provides an interface to run batch scoring on Ray datasets. It will distribute inference workload across all the workers when calling `predict()`. You can find more details in [Using Predictors for Inference](https://docs.ray.io/en/latest/ray-air/predictors.html).\n",
    "\n",
    "Here we directly load a pretrained ResNet model from `torchvision.models`, and construct a TorchCheckpoint with the preprocessor. You can also reload your own Ray AIR checkpoint from your previous experiments. You can find more details about checkpoint loading in {class}`Checkpoint <ray.air.checkpoint.Checkpoint>` api."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "from ray.train.batch_predictor import BatchPredictor\n",
    "from ray.train.torch import TorchCheckpoint, TorchPredictor\n",
    "\n",
    "# Load the pretrained resnet model and construct a checkpoint\n",
    "model = models.resnet152(pretrained=True)\n",
    "checkpoint = TorchCheckpoint.from_model(model=model, preprocessor=preprocessor)\n",
    "\n",
    "# Build a BatchPredictor from checkpoint\n",
    "batch_predictor = BatchPredictor(checkpoint, TorchPredictor)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call {meth}`predict() <ray.train.batch_predictor.BatchPredictor.predict>` on target datasets:\n",
    "- `feature_columns` specifies which columns are required for the model. The selected columns will be concatenated to build a batch tensor.\n",
    "- The columns in `keep_columns` will be returned together with the prediction results. For example, you can keep image labels for evaluation later.\n",
    "- The BatchPredictor uses CPUs for inference by default, please specify `num_gpus_per_worker` if you want to use GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-11 14:38:27,442\tINFO batch_predictor.py:184 -- `num_gpus_per_worker` is set for `BatchPreditor`.Automatically enabling GPU prediction for this predictor. To disable set `use_gpu` to `False` in `BatchPredictor.predict`.\n",
      "Map_Batches: 100%|██████████| 50/50 [00:05<00:00,  9.27it/s]\n",
      "Map Progress (2 actors 1 pending): 100%|██████████| 9/9 [00:16<00:00,  1.82s/it]\n"
     ]
    }
   ],
   "source": [
    "predictions = batch_predictor.predict(\n",
    "    ds, feature_columns=[\"image\"], keep_columns=[\"label\"], batch_size=128, max_scoring_workers=3, num_gpus_per_worker=1\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result Evaluation and Saving\n",
    "\n",
    "`BatchPredictor.predict()` will return a ray dataset with a column of model output with key \"predictions\", and all columns specified in `keep_columns`.\n",
    "\n",
    "In this example, the output of the ResNet model is a 1000-dimensional tensor containing the logits of each class. We select the top-5 classes with the highest classification probabilities and calculate the Top-1 and Top-5 errors:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map_Batches: 100%|██████████| 9/9 [00:00<00:00, 139.29it/s]\n",
      "Shuffle Map: 100%|██████████| 9/9 [00:00<00:00, 613.36it/s]\n",
      "Shuffle Reduce: 100%|██████████| 1/1 [00:00<00:00, 88.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-1 error:  0.15100000000000002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Shuffle Map: 100%|██████████| 9/9 [00:00<00:00, 699.70it/s]\n",
      "Shuffle Reduce: 100%|██████████| 1/1 [00:00<00:00, 130.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-5 error:  0.010000000000000009\n"
     ]
    }
   ],
   "source": [
    "def calculate_matches(batch):\n",
    "    batch[\"top_5_pred\"] = batch[\"predictions\"].apply(lambda x: np.argsort(-x)[:5])\n",
    "    batch[\"top_5_match\"] = batch.apply(lambda x: x[\"label\"] in x[\"top_5_pred\"], axis=1)\n",
    "    batch[\"top_1_match\"] = batch.apply(lambda x: x[\"label\"] == x[\"top_5_pred\"][0], axis=1)\n",
    "    return batch\n",
    "\n",
    "predictions = predictions.map_batches(calculate_matches)\n",
    "print(\"Top-1 error: \", 1 - predictions.mean(on=\"top_1_match\"))\n",
    "print(\"Top-5 error: \", 1 - predictions.mean(on=\"top_5_match\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save your prediction results:\n",
    "\n",
    "- You can call `ds.repartition(n)` to split your prediction results into n partitions, then n files will be created with `write_csv()` later.\n",
    "- You can either store files to your local disk or S3 bucket by passing local path or S3 uri to `write_csv()`.\n",
    "- Other output file formats are described here: [Ray Data Input/Output](https://docs.ray.io/en/latest/data/api/input_output.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.repartition(1).write_csv(\"/tmp/single_csv\")\n",
    "# >>> /tmp/single_csv/d757569dfb2845589b0ccbcb263e8cc3_000000.csv\n",
    "\n",
    "predictions.repartition(3).write_csv(\"/tmp/multiple_csv\")\n",
    "# >>> /tmp/multiple_csv/2b529dc5d8eb45e5ad03e69fb7ad8bc0_000000.csv\n",
    "# >>> /tmp/multiple_csv/2b529dc5d8eb45e5ad03e69fb7ad8bc0_000001.csv\n",
    "# >>> /tmp/multiple_csv/2b529dc5d8eb45e5ad03e69fb7ad8bc0_000002.csv\n",
    "\n",
    "# You can also save results to S3 by replacing local path to S3 URI\n",
    "# predictions.write_csv(YOUR_S3_BUCKET_URI)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ray",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13 | packaged by conda-forge | (default, Mar 25 2022, 06:05:16) \n[Clang 12.0.1 ]"
  },
  "vscode": {
   "interpreter": {
    "hash": "a8c1140d108077f4faeb76b2438f85e4ed675f93d004359552883616a1acd54c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
