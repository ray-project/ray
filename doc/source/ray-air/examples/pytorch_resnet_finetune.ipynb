{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wxZqPGrYrLTz"
   },
   "source": [
    "# Finetuning a Pytorch Image Classifier with Ray AIR\n",
    "In this example we will finetune a pretrained ResNet model with Ray Train. \n",
    "\n",
    "For this example, our network architecture consists of the intermediate layer output of a pretrained ResNet model, which feeds into a randomly initialized linear layer that outputs classification logits for our new task.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_acbhQI9rLT0"
   },
   "source": [
    "## Load and preprocess finetuning dataset with Ray Data\n",
    "This example is adapted from Pytorch's [Fintuning Torchvision Models](https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html) tutorial.\n",
    "We will use *hymenoptera_data* as the finetuning dataset, which contains two classes (bees and ants) and 397 total images (across training and validation). This is a quite small dataset and we use this only for demenstration purpose. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "sqE6Qpn3rLT0",
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# If you want to run full example, please set this to False\n",
    "SMOKE_TEST = True\n",
    "if SMOKE_TEST:\n",
    "    import os\n",
    "    os.system(\"pip install -U moto[s3,server]==2.4.1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "uFAEWBNMrLT1"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, models, transforms\n",
    "from typing import Dict\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import ray\n",
    "from ray.data.datasource.partitioning import Partitioning\n",
    "from ray.train.torch import TorchCheckpoint\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GBMPwgS2rLT1"
   },
   "source": [
    "The dataset can be downloaded [here](https://download.pytorch.org/tutorial/hymenoptera_data.zip)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xUhiuBV3rLT1"
   },
   "source": [
    "First, we use {meth}`ray.data.read_images <ray.data.read_images>` to load the images. Since the dataset is already structured with directory names as the labels, we can use the {class}`Partitioning <ray.data.datasource.Partitioning>` API to automatically extract image labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "YK1fRwjYrLT1",
    "outputId": "3caeb848-8afc-4310-d32c-8de6b243ab8a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-22 10:40:25,388\tINFO worker.py:1360 -- Connecting to existing Ray cluster at address: 10.0.62.233:6379...\n",
      "2023-02-22 10:40:25,422\tINFO worker.py:1548 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2023-02-22 10:40:25,426\tINFO packaging.py:330 -- Pushing file package 'gcs://_ray_pkg_a3a790f8196b6a46eed365be4d734a9d.zip' (0.52MiB) to Ray cluster...\n",
      "2023-02-22 10:40:25,432\tINFO packaging.py:343 -- Successfully pushed file package 'gcs://_ray_pkg_a3a790f8196b6a46eed365be4d734a9d.zip'.\n"
     ]
    }
   ],
   "source": [
    "ray_img_datasets = {}\n",
    "for split in [\"train\", \"val\"]:\n",
    "    data_folder = f\"s3://anonymous@air-example-data-2/hymenoptera_data/{split}\"\n",
    "    partitioning = Partitioning(\"dir\", field_names=[\"class\"], base_dir=data_folder)\n",
    "    ray_img_datasets[split] = ray.data.read_images(\n",
    "        data_folder, size=(256, 256), partitioning=partitioning, mode=\"RGB\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R3or9OgXrLT1"
   },
   "source": [
    "We have already load the images from to a Ray dataset, which will partition the whole dataset and distribute the data blocks across the nodes in cluster. You will benefit from faster parallel pre-processing and data ingestion. Notice that the ResNet model was pretrained with hard-coded normalization values. We'll keep these numbers the same for fine-tuning, as shown in *data_transforms*. More details can be found [here](https://pytorch.org/hub/pytorch_vision_resnet/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a label preprocessor with {class}`BatchMapper <ray.data.preprocessors.BatchMapper>`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.data.preprocessors import BatchMapper\n",
    "\n",
    "class_to_idx = {\"ants\": 0, \"bees\": 1}\n",
    "\n",
    "# 1. Map the image folder names to label ids\n",
    "def map_labels(batch: np.ndarray) -> np.ndarray:\n",
    "    batch[\"label\"] = np.vectorize(class_to_idx.__getitem__)(batch[\"class\"])\n",
    "    batch.pop(\"class\")\n",
    "    return batch\n",
    "\n",
    "\n",
    "label_preprocessor = BatchMapper(fn=map_labels, batch_format=\"numpy\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GCc2hzkVrLT2"
   },
   "source": [
    "```{note}\n",
    "Note that **batch** here refers to the chunk of data for preprocessing, not the batch for model training. To learn more about writing functions for mapping batches, read [writing user-defined functions](transform_datasets_writing_udfs).\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, define an image preprocessor with {class}`TorchVisionPreprocessor <ray.data.preprocessors.TorchVisionPreprocessor>`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.data.preprocessors import TorchVisionPreprocessor\n",
    "\n",
    "# 2. Convert input image to tensors\n",
    "def to_tensor(batch: np.ndarray) -> torch.Tensor:\n",
    "    tensor = torch.as_tensor(batch, dtype=torch.float)\n",
    "    # (B, H, W, C) -> (B, C, H, W)\n",
    "    tensor = tensor.permute(0, 3, 1, 2).contiguous()\n",
    "    # [0., 255.] -> [0., 1.]\n",
    "    tensor = tensor.div(255)\n",
    "    return tensor\n",
    "\n",
    "\n",
    "# Data augmentation and normalization for training\n",
    "# Just normalization for validation\n",
    "data_transforms = {\n",
    "    \"train\": transforms.Compose(\n",
    "        [\n",
    "            transforms.Lambda(to_tensor),\n",
    "            transforms.RandomResizedCrop(224),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "        ]\n",
    "    ),\n",
    "    \"val\": transforms.Compose(\n",
    "        [\n",
    "            transforms.Lambda(to_tensor),\n",
    "            transforms.Resize(224),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "        ]\n",
    "    ),\n",
    "}\n",
    "\n",
    "# Accelerate image processing with batched transformations\n",
    "image_preprocessors = {\n",
    "    split: TorchVisionPreprocessor(\n",
    "        columns=[\"image\"], transform=data_transforms[split], batched=True\n",
    "    )\n",
    "    for split in [\"train\", \"val\"]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine two preprocessors with {class}`Chain <ray.data.preprocessors.Chain>` and transform raw datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-22 10:53:10,874\tINFO bulk_executor.py:41 -- Executing DAG InputDataBuffer[Input] -> TaskPoolMapOperator[read->TorchVisionPreprocessor]\n",
      "read->TorchVisionPreprocessor: 100%|██████████| 128/128 [00:03<00:00, 37.63it/s]\n",
      "2023-02-22 10:53:14,346\tINFO bulk_executor.py:41 -- Executing DAG InputDataBuffer[Input] -> TaskPoolMapOperator[BatchMapper]\n",
      "BatchMapper: 100%|██████████| 128/128 [00:00<00:00, 4499.57it/s]\n",
      "2023-02-22 10:53:14,518\tINFO bulk_executor.py:41 -- Executing DAG InputDataBuffer[Input] -> TaskPoolMapOperator[read->TorchVisionPreprocessor]\n",
      "read->TorchVisionPreprocessor: 100%|██████████| 128/128 [00:03<00:00, 41.15it/s]\n",
      "2023-02-22 10:53:17,699\tINFO bulk_executor.py:41 -- Executing DAG InputDataBuffer[Input] -> TaskPoolMapOperator[BatchMapper]\n",
      "BatchMapper: 100%|██████████| 128/128 [00:00<00:00, 3793.20it/s]\n"
     ]
    }
   ],
   "source": [
    "from ray.data.preprocessors import Chain\n",
    "\n",
    "ray_datasets = {}\n",
    "for split in [\"train\", \"val\"]:\n",
    "    preprocessor = Chain(image_preprocessors[split], label_preprocessor)\n",
    "    ray_datasets[split] = preprocessor.fit_transform(ray_img_datasets[split])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3U1ALTMPrLT2"
   },
   "source": [
    "## Initialize Model and Fine-tuning configs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "osM035rBrLT2"
   },
   "source": [
    "Next let's define our model, you can create a model from a pretrained ResNet, or reload the model checkpoint from a previous run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "kATI6D3ErLT2"
   },
   "outputs": [],
   "source": [
    "def initialize_model():\n",
    "    # Load pretrained model params\n",
    "    model = models.resnet50(pretrained=True)\n",
    "\n",
    "    # Replace the original classifier with a new Linear layer\n",
    "    num_features = model.fc.in_features\n",
    "    model.fc = nn.Linear(num_features, 2)\n",
    "\n",
    "    # Ensure all params get updated during fintuning\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = True\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DnDEPBBOrLT2"
   },
   "source": [
    "Next, let's define the training configuration, which will be passed into training loop function later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "l02n9lCJrLT2"
   },
   "outputs": [],
   "source": [
    "train_loop_config = {\n",
    "    \"input_size\": 224,  # Input image size (224 x 224)\n",
    "    \"batch_size\": 32,  # Batch size for training\n",
    "    \"num_epochs\": 10,  # Number of epochs to train for\n",
    "    \"lr\": 0.001,  # Learning Rate\n",
    "    \"momentum\": 0.9,  # SGD optimizer momentum\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1udtp2s0rLT2"
   },
   "source": [
    "## Define the Training Loop\n",
    "\n",
    "The `train_loop_per_worker` function defines the finetuning procedure for each worker.\n",
    "\n",
    "**1. Load dataset shard for each worker**:\n",
    "- The Trainer will take a dictionary of Ray {class}`~ray.data.Dataset`s as input. These will be preprocessed and accessible in the worker's training loop via {meth}`session.get_dataset_shard() <ray.air.session.get_dataset_shard>`.\n",
    "- By default, only the dataset under the key \"train\" will be split into multiple shards. `session.get_dataset_shard()` will return the full dataset for other keys. To configure this, see {class}`~ray.air.DatasetConfig`.\n",
    "- Use {meth}`iter_torch_batches <ray.data.Dataset.iter_torch_batches>` to iterate the datasets with automatic tensor batching and device placement. If you need a more flexible customized batching function, please refer to our lower-level {meth}`iter_batches <ray.data.Dataset.iter_batches>` API.\n",
    "\n",
    "**2. Prepare your model**:\n",
    "- {meth}`train.torch.prepare_model() <ray.train.torch.prepare_model>` will prepares the model for distributed training. Under the hood, it converts your torch model to `DistributedDataParallel` model, which will synchronize its weights across all workers.\n",
    "\n",
    "**3. Report metrics and checkpoint**:\n",
    "- {meth}`session.report() <ray.air.session.report>` will report metrics and checkpoints to Ray AIR.\n",
    "- Saving checkpoints through {meth}`session.report(metrics, checkpoint=...) <ray.air.session.report>` will automatically [upload checkpoints to cloud storage](tune-cloud-checkpointing) (if configured), and allow you to easily enable Ray AIR worker fault tolerance in the future.\n",
    "- The best checkpoints will be saved according to the specified `checkpoint_score_attribute` in {class}`CheckpointConfig <ray.air.config.CheckpointConfig>`. Here we only save the best model with highest validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "5pPf2kqKrLT2"
   },
   "outputs": [],
   "source": [
    "import ray.train as train\n",
    "from ray.air import session\n",
    "from ray.train.torch import TorchCheckpoint\n",
    "\n",
    "\n",
    "def evaluate(logits, labels):\n",
    "    _, preds = torch.max(logits, 1)\n",
    "    corrects = torch.sum(preds == labels).item()\n",
    "    return corrects\n",
    "\n",
    "\n",
    "def train_loop_per_worker(configs):\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    # Prepare dataloader for each worker\n",
    "    datasets = dict()\n",
    "    datasets[\"train\"] = session.get_dataset_shard(\"train\")\n",
    "    datasets[\"val\"] = session.get_dataset_shard(\"val\")\n",
    "\n",
    "    # Calculate the batch size for a single worker\n",
    "    worker_batch_size = configs[\"batch_size\"] // session.get_world_size()\n",
    "\n",
    "    device = train.torch.get_device()\n",
    "\n",
    "    # Prepare DDP Model, optimizer, and loss function\n",
    "    model = initialize_model()  # [TODO]\n",
    "\n",
    "    model = train.torch.prepare_model(model)\n",
    "\n",
    "    optimizer = optim.SGD(\n",
    "        model.parameters(), lr=configs[\"lr\"], momentum=configs[\"momentum\"]\n",
    "    )\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Start training loops\n",
    "    for epoch in range(configs[\"num_epochs\"]):\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in [\"train\", \"val\"]:\n",
    "            if phase == \"train\":\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()  # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Create a dataset iterator for the shard on the current worker\n",
    "            dataset_iterator = datasets[phase].iter_torch_batches(\n",
    "                batch_size=worker_batch_size, device=device\n",
    "            )\n",
    "            for batch in dataset_iterator:\n",
    "                inputs = batch[\"image\"]\n",
    "                labels = batch[\"label\"]\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                with torch.set_grad_enabled(phase == \"train\"):\n",
    "                    # Get model outputs and calculate loss\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == \"train\":\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # calculate statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += evaluate(outputs, labels)\n",
    "\n",
    "            epoch_loss = running_loss / datasets[phase].count()\n",
    "            epoch_acc = running_corrects / datasets[phase].count()\n",
    "\n",
    "            if session.get_world_rank() == 0:\n",
    "                print(\n",
    "                    \"Epoch {}-{} Loss: {:.4f} Acc: {:.4f}\".format(\n",
    "                        epoch, phase, epoch_loss, epoch_acc\n",
    "                    )\n",
    "                )\n",
    "\n",
    "            # Report metrics and checkpoint every epoch\n",
    "            if phase == \"val\":\n",
    "                checkpoint = TorchCheckpoint.from_dict(\n",
    "                    {\n",
    "                        \"epoch\": epoch,\n",
    "                        \"model\": model.module.state_dict(),\n",
    "                        \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                    }\n",
    "                )\n",
    "                session.report(\n",
    "                    metrics={\"loss\": epoch_loss, \"acc\": epoch_acc},\n",
    "                    checkpoint=checkpoint,\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VmZQKtWnrLT3"
   },
   "source": [
    "Next, setup the TorchTrainer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "cellView": "form",
    "id": "6jeTo6YTrLT3",
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# @title +\n",
    "if SMOKE_TEST:\n",
    "    from moto.server import ThreadedMotoServer\n",
    "    from moto import mock_s3\n",
    "    import boto3\n",
    "    import os\n",
    "    import logging\n",
    "\n",
    "    server = ThreadedMotoServer(port=5002)\n",
    "    server.start()\n",
    "\n",
    "    s3 = boto3.client(\"s3\", endpoint_url=\"http://localhost:5002\")\n",
    "\n",
    "    bucket_name = \"checkpoint-bucket\"\n",
    "    s3.create_bucket(Bucket=bucket_name)\n",
    "    logging.getLogger(\"werkzeug\").setLevel(logging.WARNING)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "FTOW-JHXrLT3"
   },
   "outputs": [],
   "source": [
    "UPLOAD_DIR = \"s3://YOUR_BUCKET_NAME\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "AnBOpS2irLT3",
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# @title +\n",
    "if SMOKE_TEST:\n",
    "    UPLOAD_DIR = f\"s3://{bucket_name}/results?endpoint_override=http://localhost:5002\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "LOF_aqrLrLT3"
   },
   "outputs": [],
   "source": [
    "from ray.train.torch import TorchTrainer, TorchCheckpoint\n",
    "from ray.air.config import ScalingConfig, RunConfig, CheckpointConfig\n",
    "from ray.tune.syncer import SyncConfig\n",
    "\n",
    "# Scale out model training across 4 workers, each assigned 1 CPU and 1 GPU.\n",
    "scaling_config = ScalingConfig(\n",
    "    num_workers=4, use_gpu=True, resources_per_worker={\"CPU\": 1, \"GPU\": 1}\n",
    ")\n",
    "\n",
    "# Save only the latest checkpoint\n",
    "checkpoint_config = CheckpointConfig(num_to_keep=1)\n",
    "\n",
    "# Set experiment name and checkpoint configs\n",
    "run_config = RunConfig(\n",
    "    name=\"finetune-resnet\",\n",
    "    sync_config=SyncConfig(upload_dir=UPLOAD_DIR),\n",
    "    checkpoint_config=checkpoint_config,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "cellView": "form",
    "id": "n7HO1fQCrLT3",
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# @title +\n",
    "if SMOKE_TEST:\n",
    "    scaling_config = ScalingConfig(\n",
    "        num_workers=8, use_gpu=False, resources_per_worker={\"CPU\": 1}\n",
    "    )\n",
    "    train_loop_config[\"num_epochs\"] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "iGE04uMfrLT3",
    "outputId": "40225496-b8a3-431e-aee0-e762337b8264"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2023-02-22 10:54:34</td></tr>\n",
       "<tr><td>Running for: </td><td>00:01:03.44        </td></tr>\n",
       "<tr><td>Memory:      </td><td>9.4/62.0 GiB       </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using FIFO scheduling algorithm.<br>Resources requested: 0/64 CPUs, 0/4 GPUs, 0.0/164.18 GiB heap, 0.0/72.95 GiB objects (0.0/4.0 accelerator_type:T4)\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th>status    </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    loss</th><th style=\"text-align: right;\">     acc</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>TorchTrainer_3303a_00000</td><td>TERMINATED</td><td>10.0.56.151:3546</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         54.9253</td><td style=\"text-align: right;\">0.168786</td><td style=\"text-align: right;\">0.928105</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=3633, ip=10.0.56.151) 2023-02-22 10:53:37,556\tINFO config.py:86 -- Setting up process group for: env:// [rank=0, world_size=4]\n",
      "(TorchTrainer pid=3546, ip=10.0.56.151) 2023-02-22 10:53:38,162\tINFO bulk_executor.py:41 -- Executing DAG InputDataBuffer[Input] -> AllToAllOperator[randomize_block_order]\n",
      "(TorchTrainer pid=3546, ip=10.0.56.151) /home/ray/anaconda3/lib/python3.9/site-packages/ray/train/_internal/dataset_iterator.py:64: UserWarning: session.get_dataset_shard returns a ray.data.DatasetIterator instead of a Dataset/DatasetPipeline as of Ray v2.3. Use iter_torch_batches(), to_tf(), or iter_batches() to iterate over one epoch. See https://docs.ray.io/en/latest/data/api/dataset_iterator.html for full DatasetIterator docs.\n",
      "(TorchTrainer pid=3546, ip=10.0.56.151)   warnings.warn(\n",
      "(RayTrainWorker pid=52252) 2023-02-22 10:53:40,867\tINFO train_loop_utils.py:307 -- Moving model to device: cuda:0\n",
      "(RayTrainWorker pid=3767, ip=10.0.18.83) 2023-02-22 10:53:40,877\tINFO train_loop_utils.py:307 -- Moving model to device: cuda:0\n",
      "(RayTrainWorker pid=3633, ip=10.0.56.151) 2023-02-22 10:53:40,882\tINFO train_loop_utils.py:307 -- Moving model to device: cuda:0\n",
      "(RayTrainWorker pid=3572, ip=10.0.53.242) 2023-02-22 10:53:40,869\tINFO train_loop_utils.py:307 -- Moving model to device: cuda:0\n",
      "(RayTrainWorker pid=3572, ip=10.0.53.242) 2023-02-22 10:53:42,522\tINFO train_loop_utils.py:367 -- Wrapping provided model in DistributedDataParallel.\n",
      "(RayTrainWorker pid=52252) 2023-02-22 10:53:42,535\tINFO train_loop_utils.py:367 -- Wrapping provided model in DistributedDataParallel.\n",
      "(RayTrainWorker pid=3767, ip=10.0.18.83) 2023-02-22 10:53:42,565\tINFO train_loop_utils.py:367 -- Wrapping provided model in DistributedDataParallel.\n",
      "(RayTrainWorker pid=3633, ip=10.0.56.151) 2023-02-22 10:53:42,635\tINFO train_loop_utils.py:367 -- Wrapping provided model in DistributedDataParallel.\n",
      "(RayTrainWorker pid=52252) 2023-02-22 10:53:46,123\tINFO bulk_executor.py:41 -- Executing DAG InputDataBuffer[Input] -> AllToAllOperator[randomize_block_order]\n",
      "(RayTrainWorker pid=3767, ip=10.0.18.83) 2023-02-22 10:53:46,124\tINFO bulk_executor.py:41 -- Executing DAG InputDataBuffer[Input] -> AllToAllOperator[randomize_block_order]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=3633, ip=10.0.56.151) Epoch 0-train Loss: 0.6499 Acc: 0.6721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=3633, ip=10.0.56.151) 2023-02-22 10:53:46,178\tINFO bulk_executor.py:41 -- Executing DAG InputDataBuffer[Input] -> AllToAllOperator[randomize_block_order]\n",
      "(RayTrainWorker pid=3572, ip=10.0.53.242) 2023-02-22 10:53:46,124\tINFO bulk_executor.py:41 -- Executing DAG InputDataBuffer[Input] -> AllToAllOperator[randomize_block_order]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=3633, ip=10.0.56.151) Epoch 0-val Loss: 0.5461 Acc: 0.7059\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"trialProgress\">\n",
       "  <h3>Trial Progress</h3>\n",
       "  <table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th style=\"text-align: right;\">     acc</th><th>date               </th><th>done  </th><th style=\"text-align: right;\">  experiment_tag</th><th>hostname      </th><th style=\"text-align: right;\">  iterations_since_restore</th><th style=\"text-align: right;\">    loss</th><th>node_ip    </th><th style=\"text-align: right;\">  pid</th><th>should_checkpoint  </th><th style=\"text-align: right;\">  time_since_restore</th><th style=\"text-align: right;\">  time_this_iter_s</th><th style=\"text-align: right;\">  time_total_s</th><th style=\"text-align: right;\">  timestamp</th><th style=\"text-align: right;\">  training_iteration</th><th>trial_id   </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>TorchTrainer_3303a_00000</td><td style=\"text-align: right;\">0.928105</td><td>2023-02-22_10-54-29</td><td>True  </td><td style=\"text-align: right;\">               0</td><td>ip-10-0-56-151</td><td style=\"text-align: right;\">                        10</td><td style=\"text-align: right;\">0.168786</td><td>10.0.56.151</td><td style=\"text-align: right;\"> 3546</td><td>True               </td><td style=\"text-align: right;\">             54.9253</td><td style=\"text-align: right;\">           4.22899</td><td style=\"text-align: right;\">       54.9253</td><td style=\"text-align: right;\"> 1677092067</td><td style=\"text-align: right;\">                  10</td><td>3303a_00000</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".trialProgress {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".trialProgress h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".trialProgress td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=3633, ip=10.0.56.151) Epoch 1-train Loss: 0.4958 Acc: 0.7869\n",
      "(RayTrainWorker pid=3633, ip=10.0.56.151) Epoch 1-val Loss: 0.3475 Acc: 0.9281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-22 10:53:53,788\tWARNING util.py:244 -- The `process_trial_save` operation took 5.093 s, which may be a performance bottleneck.\n",
      "2023-02-22 10:53:53,789\tWARNING trial_runner.py:674 -- Consider turning off forced head-worker trial checkpoint syncs by setting sync_on_checkpoint=False. Note that this may result in faulty trial restoration if a failure occurs while the checkpoint is being synced from the worker to the head node.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=3633, ip=10.0.56.151) Epoch 2-train Loss: 0.3287 Acc: 0.8852\n",
      "(RayTrainWorker pid=3633, ip=10.0.56.151) Epoch 2-val Loss: 0.2657 Acc: 0.9346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-22 10:53:58,102\tWARNING util.py:244 -- The `process_trial_save` operation took 3.018 s, which may be a performance bottleneck.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=3633, ip=10.0.56.151) Epoch 3-train Loss: 0.2355 Acc: 0.9344\n",
      "(RayTrainWorker pid=3633, ip=10.0.56.151) Epoch 3-val Loss: 0.2206 Acc: 0.9346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-22 10:54:02,476\tWARNING util.py:244 -- The `process_trial_save` operation took 3.088 s, which may be a performance bottleneck.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=3633, ip=10.0.56.151) Epoch 4-train Loss: 0.1555 Acc: 0.9672\n",
      "(RayTrainWorker pid=3633, ip=10.0.56.151) Epoch 4-val Loss: 0.1965 Acc: 0.9281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-22 10:54:06,669\tWARNING util.py:244 -- The `process_trial_save` operation took 2.998 s, which may be a performance bottleneck.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=3633, ip=10.0.56.151) Epoch 5-train Loss: 0.1060 Acc: 0.9836\n",
      "(RayTrainWorker pid=3633, ip=10.0.56.151) Epoch 5-val Loss: 0.1854 Acc: 0.9346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-22 10:54:10,881\tWARNING util.py:244 -- The `process_trial_save` operation took 3.023 s, which may be a performance bottleneck.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=3633, ip=10.0.56.151) Epoch 6-train Loss: 0.0758 Acc: 1.0000\n",
      "(RayTrainWorker pid=3633, ip=10.0.56.151) Epoch 6-val Loss: 0.1783 Acc: 0.9346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-22 10:54:15,229\tWARNING util.py:244 -- The `process_trial_save` operation took 3.020 s, which may be a performance bottleneck.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=3633, ip=10.0.56.151) Epoch 7-train Loss: 0.0546 Acc: 1.0000\n",
      "(RayTrainWorker pid=3633, ip=10.0.56.151) Epoch 7-val Loss: 0.1729 Acc: 0.9281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-22 10:54:19,467\tWARNING util.py:244 -- The `process_trial_save` operation took 3.043 s, which may be a performance bottleneck.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=3633, ip=10.0.56.151) Epoch 8-train Loss: 0.0415 Acc: 1.0000\n",
      "(RayTrainWorker pid=3633, ip=10.0.56.151) Epoch 8-val Loss: 0.1703 Acc: 0.9281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-22 10:54:23,675\tWARNING util.py:244 -- The `process_trial_save` operation took 3.019 s, which may be a performance bottleneck.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=3633, ip=10.0.56.151) Epoch 9-train Loss: 0.0333 Acc: 1.0000\n",
      "(RayTrainWorker pid=3633, ip=10.0.56.151) Epoch 9-val Loss: 0.1688 Acc: 0.9281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-22 10:54:27,905\tWARNING util.py:244 -- The `process_trial_save` operation took 3.034 s, which may be a performance bottleneck.\n",
      "2023-02-22 10:54:31,934\tWARNING util.py:244 -- The `process_trial_save` operation took 2.834 s, which may be a performance bottleneck.\n",
      "2023-02-22 10:54:36,004\tINFO tune.py:825 -- Total run time: 65.35 seconds (63.44 seconds for the tuning loop).\n"
     ]
    }
   ],
   "source": [
    "trainer = TorchTrainer(\n",
    "    train_loop_per_worker=train_loop_per_worker,\n",
    "    train_loop_config=train_loop_config,\n",
    "    scaling_config=scaling_config,\n",
    "    run_config=run_config,\n",
    "    datasets=ray_datasets,\n",
    ")\n",
    "\n",
    "result = trainer.fit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZLZmbxhurLT3"
   },
   "source": [
    "Training has finished! The best checkpoint has been saved to the experiment directory, and you can now check the experiment metrics and checkpoint information:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "98QKucy3rLT3"
   },
   "source": [
    "## Load the fine-tuned model for batch prediction\n",
    "\n",
    "Now, we want to load the trained model and evaluation it on test data.\n",
    "We can use {ref}`TorchCheckpoint.from_directory() <ray.train.torch.TorchCheckpoint.from_directory>` to load the resulting checkpoint from our fine-tuning run. The {class}`~ray.train.batch_predictor.BatchPredictor` will identify the dict key `\"model\"` and load the corresponding parameters into the model.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k0gLMEj6rLT3"
   },
   "source": [
    "The log and checkpoints will be saved into `upload_dir` specified in TrainerTrainer. \n",
    "\n",
    "For example:\n",
    "\"s3://YOUR_BUCKET_NAME/finetune-resnet/TorchTrainer_94bb5_00000_0_2023-02-14_14-40-28/checkpoint_000009\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "GvYSeoihrLT3"
   },
   "outputs": [],
   "source": [
    "from ray.train.batch_predictor import BatchPredictor\n",
    "from ray.train.torch import TorchCheckpoint, TorchPredictor\n",
    "\n",
    "ckpt = TorchCheckpoint.from_uri(result.checkpoint.uri)\n",
    "predictor = BatchPredictor.from_checkpoint(\n",
    "    ckpt, TorchPredictor, model=initialize_model()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t724hgjTrLT3",
    "outputId": "2a55681f-48e8-45ab-9394-fa5fe7e01b05"
   },
   "outputs": [],
   "source": [
    "prediction_ds = predictor.predict(\n",
    "    ray_datasets[\"val\"],\n",
    "    feature_columns=[\"image\"],\n",
    "    keep_columns=[\"label\"],\n",
    "    num_gpus_per_worker=1,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dLURjJZcrLT3"
   },
   "source": [
    "## Evaluate prediction results\n",
    "\n",
    "The prediction has finished! We can use `ds.schema()` and `ds.take()` to inspect the data types and record structure.\n",
    "\n",
    "We can see that there are two keys in the prediction results:\n",
    "- \"predictions\": The output logits of our ResNet model, which is a 1000 dimensional tensor.\n",
    "- \"label\": The image label. Specified by `keep_columns` in `predictor.predict()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "4HUbnCLOrLT3",
    "outputId": "6eea7585-7c72-4a51-e688-33c9d2462d6b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predictions: extension<arrow.py_extension_type<ArrowTensorType>>\n",
       "label: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_ds.schema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "goh248omrLT4",
    "outputId": "34140cb5-a262-4eea-9a34-5c0789b792dc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'predictions': array([ 0.37351528, -0.7467256 ], dtype=float32), 'label': 0}]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_ds.take(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WI_pdhGkrLT4"
   },
   "source": [
    "Here we define a function `convert_logits_to_classes` to convert tensor outputs to labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "i8yNXfcVrLT4",
    "outputId": "30f1c71a-4461-4552-a84a-1da35ff5431c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-22 10:57:15,005\tINFO bulk_executor.py:41 -- Executing DAG InputDataBuffer[Input] -> TaskPoolMapOperator[MapBatches(convert_logits_to_classes)]\n",
      "MapBatches(convert_logits_to_classes): 100%|██████████| 1/1 [00:00<00:00, 243.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'predictions': array([ 0.37351528, -0.7467256 ], dtype=float32), 'label': 0, 'pred_label': 0, 'correct': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-22 10:57:15,026\tINFO bulk_executor.py:41 -- Executing DAG InputDataBuffer[Input] -> AllToAllOperator[aggregate]\n",
      "Shuffle Map: 100%|██████████| 1/1 [00:00<00:00, 138.26it/s]\n",
      "Shuffle Reduce: 100%|██████████| 1/1 [00:00<00:00, 121.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Accuracy =  0.9281045751633987\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def convert_logits_to_classes(batch):\n",
    "    batch[\"pred_label\"] = np.argmax(batch[\"predictions\"], axis=1)\n",
    "    batch[\"correct\"] = batch[\"pred_label\"] == batch[\"label\"]\n",
    "    return batch\n",
    "\n",
    "\n",
    "predictions = prediction_ds.map_batches(convert_logits_to_classes, batch_format=\"numpy\")\n",
    "predictions.show(1)\n",
    "\n",
    "print(\"Evaluation Accuracy = \", predictions.mean(on=\"correct\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A9ZeMZz0rLT4"
   },
   "source": [
    "You can also reuse the evaluation function defined in the training loop by iterating over the dataset. Note that the previous approach using `map_batches()` is more efficient because it parallelizes the evaluation on each partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "38zX_DzKrLT4",
    "outputId": "70fe6c4a-fb88-4183-c183-b2bfb26854c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Accuracy =  0.9281045751633987\n"
     ]
    }
   ],
   "source": [
    "def evaluate(logits, labels):\n",
    "    _, preds = torch.max(logits, 1)\n",
    "    corrects = torch.sum(preds == labels).item()\n",
    "    return corrects\n",
    "\n",
    "\n",
    "accuracy = 0\n",
    "for batch in prediction_ds.iter_torch_batches(batch_size=10):\n",
    "    accuracy += evaluate(batch[\"predictions\"], batch[\"label\"])\n",
    "accuracy /= prediction_ds.count()\n",
    "\n",
    "print(\"Evaluation Accuracy = \", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "2LWrS79BrLT4",
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# @title +\n",
    "if SMOKE_TEST:\n",
    "    server.stop()\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "a8c1140d108077f4faeb76b2438f85e4ed675f93d004359552883616a1acd54c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
