{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning a Pytorch Image Classifier with Ray AIR\n",
    "In this example we will finetune a pretrained ResNet model with Ray Train. \n",
    "\n",
    "For this example, our network architecture consists of the intermediate layer output of a pretrained ResNet model, which feeds into a randomly initialized linear layer that outputs classification logits for our new task.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and preprocess finetuning dataset with Ray Data\n",
    "This example is adapted from Pytorch's [Fintuning Torchvision Models](https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html) tutorial.\n",
    "We will use *hymenoptera_data* as the finetuning dataset, which contains two classes (bees and ants) and 397 total images (across training and validation). This is a quite small dataset and we use this only for demenstration purpose. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, models, transforms\n",
    "import numpy as np\n",
    "from typing import Dict\n",
    "import random\n",
    "\n",
    "import ray\n",
    "from ray.data.datasource.partitioning import Partitioning\n",
    "from ray.train.torch import TorchCheckpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset can be downloaded [here](https://download.pytorch.org/tutorial/hymenoptera_data.zip)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "! wget https://download.pytorch.org/tutorial/hymenoptera_data.zip\n",
    "! unzip -o hymenoptera_data.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we use {meth}`ray.data.read_images <ray.data.read_images>` to load the images. Since the dataset is already structured with directory names as the labels, we can use the {class}`Partitioning <ray.data.datasource.Partitioning>` API to automatically extract image labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-15 14:28:35,638\tINFO worker.py:1352 -- Connecting to existing Ray cluster at address: 10.0.37.49:6379...\n",
      "2023-02-15 14:28:35,671\tINFO worker.py:1529 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2023-02-15 14:28:36,353\tINFO packaging.py:373 -- Pushing file package 'gcs://_ray_pkg_ffdd1afa7144a1c7df59859eb1ae314b.zip' (226.84MiB) to Ray cluster...\n",
      "2023-02-15 14:28:40,655\tINFO packaging.py:386 -- Successfully pushed file package 'gcs://_ray_pkg_ffdd1afa7144a1c7df59859eb1ae314b.zip'.\n"
     ]
    }
   ],
   "source": [
    "ray_img_datasets = {}\n",
    "for split in [\"train\", \"val\"]:\n",
    "    data_folder = f\"./hymenoptera_data/{split}\"\n",
    "    partitioning = Partitioning(\"dir\", field_names=[\"class\"], base_dir=data_folder)\n",
    "    ray_img_datasets[split] = ray.data.read_images(\n",
    "        data_folder, partitioning=partitioning, mode=\"RGB\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have already load the images from to a Ray dataset, which will partition the whole dataset and distribute the data blocks across the nodes in cluster. You will benefit from faster parallel pre-processing and data ingestion. Notice that the ResNet model was pretrained with hard-coded normalization values. We'll keep these numbers the same for fine-tuning, as shown in *data_transforms*. More details can be found [here](https://pytorch.org/hub/pytorch_vision_resnet/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read->Map_Batches: 100%|██████████| 128/128 [00:05<00:00, 22.53it/s]\n",
      "Read->Map_Batches: 100%|██████████| 128/128 [00:00<00:00, 252.42it/s]\n"
     ]
    }
   ],
   "source": [
    "# Data augmentation and normalization for training\n",
    "# Just normalization for validation\n",
    "data_transforms = {\n",
    "    \"train\": transforms.Compose(\n",
    "        [\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.RandomResizedCrop(224),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "        ]\n",
    "    ),\n",
    "    \"val\": transforms.Compose(\n",
    "        [\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.Resize(224),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "        ]\n",
    "    ),\n",
    "}\n",
    "\n",
    "class_to_idx = {\"ants\": 0, \"bees\": 1}\n",
    "\n",
    "\n",
    "def preprocess(batch: Dict[str, np.ndarray], split: str) -> Dict[str, np.ndarray]:\n",
    "    transform = data_transforms[split]\n",
    "    batch[\"image\"] = np.array([transform(img).numpy() for img in batch[\"image\"]])\n",
    "    batch[\"label\"] = np.array([class_to_idx[cls_name] for cls_name in batch[\"class\"]])\n",
    "    batch.pop(\"class\")\n",
    "    return batch\n",
    "\n",
    "\n",
    "ray_datasets = {\n",
    "    split: ds.map_batches(\n",
    "        fn=preprocess, fn_kwargs={\"split\": split}, batch_format=\"numpy\"\n",
    "    )\n",
    "    for split, ds in ray_img_datasets.items()\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{note}\n",
    "Note that **batch** here refers to the chunk of data for preprocessing, not the batch for model training. To learn more about writing functions for {meth}`map_batches <ray.data.Dataset.map_batches>`, read [writing user-defined functions](transform_datasets_writing_udfs).\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Model and Fine-tuning configs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next let's define our model, you can create a model from a pretrained ResNet, or reload the model checkpoint from a previous run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model():\n",
    "    # Load pretrained model params\n",
    "    model = models.resnet50(pretrained=True)\n",
    "\n",
    "    # Replace the original classifier with a new Linear layer\n",
    "    num_features = model.fc.in_features\n",
    "    model.fc = nn.Linear(num_features, 2)\n",
    "\n",
    "    # Ensure all params get updated during fintuning\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = True\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's define the training configuration, which will be passed into training loop function later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loop_config = {\n",
    "    \"input_size\": 224,  # Input image size (224 x 224)\n",
    "    \"batch_size\": 32,  # Batch size for training\n",
    "    \"num_epochs\": 10,  # Number of epochs to train for\n",
    "    \"lr\": 0.001,  # Learning Rate\n",
    "    \"momentum\": 0.9,  # SGD optimizer momentum\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Training Loop\n",
    "\n",
    "The `train_loop_per_worker` function defines the finetuning procedure for each worker.\n",
    "\n",
    "**1. Load dataset shard for each worker**:\n",
    "- The Trainer will take a dictionary of Ray {class}`~ray.data.Dataset`s as input. These will be preprocessed and accessible in the worker's training loop via {meth}`session.get_dataset_shard() <ray.air.session.get_dataset_shard>`.\n",
    "- By default, only the dataset under the key \"train\" will be split into multiple shards. `session.get_dataset_shard()` will return the full dataset for other keys. To configure this, see {class}`~ray.air.DatasetConfig`.\n",
    "- Use {meth}`iter_torch_batches <ray.data.Dataset.iter_torch_batches>` to iterate the datasets with automatic tensor batching and device placement. If you need a more flexible customized batching function, please refer to our lower-level {meth}`iter_batches <ray.data.Dataset.iter_batches>` API.\n",
    "\n",
    "**2. Prepare your model**:\n",
    "- {meth}`train.torch.prepare_model() <ray.train.torch.prepare_model>` will prepares the model for distributed training. Under the hood, it converts your torch model to `DistributedDataParallel` model, which will synchronize its weights across all workers.\n",
    "\n",
    "**3. Report metrics and checkpoint**:\n",
    "- {meth}`session.report() <ray.air.session.report>` will report metrics and checkpoints to Ray AIR.\n",
    "- Saving checkpoints through {meth}`session.report(metrics, checkpoint=...) <ray.air.session.report>` will automatically [upload checkpoints to cloud storage](tune-cloud-checkpointing) (if configured), and allow you to easily enable Ray AIR worker fault tolerance in the future.\n",
    "- The best checkpoints will be saved according to the specified `checkpoint_score_attribute` in {class}`CheckpointConfig <ray.air.config.CheckpointConfig>`. Here we only save the best model with highest validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray.train as train\n",
    "from ray.air import session\n",
    "from ray.train.torch import TorchCheckpoint\n",
    "\n",
    "\n",
    "def evaluate(logits, labels):\n",
    "    _, preds = torch.max(logits, 1)\n",
    "    corrects = torch.sum(preds == labels).item()\n",
    "    return corrects\n",
    "\n",
    "\n",
    "def train_loop_per_worker(configs):\n",
    "    #     set_random_seed(420)\n",
    "    # Prepare dataloader for each worker\n",
    "    datasets = dict()\n",
    "    datasets[\"train\"] = session.get_dataset_shard(\"train\")\n",
    "    datasets[\"val\"] = session.get_dataset_shard(\"val\")\n",
    "\n",
    "    # Calculate the batch size for a single worker\n",
    "    worker_batch_size = configs[\"batch_size\"] // session.get_world_size()\n",
    "\n",
    "    device = train.torch.get_device()\n",
    "\n",
    "    # Prepare DDP Model, optimizer, and loss function\n",
    "    model = initialize_model()  # [TODO]\n",
    "\n",
    "    model = train.torch.prepare_model(model)\n",
    "\n",
    "    optimizer = optim.SGD(\n",
    "        model.parameters(), lr=configs[\"lr\"], momentum=configs[\"momentum\"]\n",
    "    )\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Start training loops\n",
    "    for epoch in range(configs[\"num_epochs\"]):\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in [\"train\", \"val\"]:\n",
    "            if phase == \"train\":\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()  # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Create a dataset iterator for the shard on the current worker\n",
    "            dataset_iterator = datasets[phase].iter_torch_batches(\n",
    "                batch_size=worker_batch_size, device=device\n",
    "            )\n",
    "            for batch in dataset_iterator:\n",
    "                inputs = batch[\"image\"]\n",
    "                labels = batch[\"label\"]\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                with torch.set_grad_enabled(phase == \"train\"):\n",
    "                    # Get model outputs and calculate loss\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == \"train\":\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # calculate statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += evaluate(outputs, labels)\n",
    "\n",
    "            epoch_loss = running_loss / datasets[phase].count()\n",
    "            epoch_acc = running_corrects / datasets[phase].count()\n",
    "\n",
    "            if session.get_world_rank() == 0:\n",
    "                print(\n",
    "                    \"Epoch {}-{} Loss: {:.4f} Acc: {:.4f}\".format(\n",
    "                        epoch, phase, epoch_loss, epoch_acc\n",
    "                    )\n",
    "                )\n",
    "\n",
    "            # Report metrics and checkpoint every epoch\n",
    "            if phase == \"val\":\n",
    "                checkpoint = TorchCheckpoint.from_dict(\n",
    "                    {\n",
    "                        \"epoch\": epoch,\n",
    "                        \"model\": model.module.state_dict(),\n",
    "                        \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                    }\n",
    "                )\n",
    "                session.report(\n",
    "                    metrics={\"loss\": epoch_loss, \"acc\": epoch_acc},\n",
    "                    checkpoint=checkpoint,\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, setup the TorchTrainer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.train.torch import TorchTrainer, TorchCheckpoint\n",
    "from ray.air.config import ScalingConfig, RunConfig, CheckpointConfig\n",
    "from ray.tune.syncer import SyncConfig\n",
    "\n",
    "# Scale out model training across 4 workers, each assigned 1 CPU and 1 GPU.\n",
    "scaling_config = ScalingConfig(\n",
    "    num_workers=4, use_gpu=True, resources_per_worker={\"CPU\": 1, \"GPU\": 1}\n",
    ")\n",
    "\n",
    "# Save only the latest checkpoint\n",
    "checkpoint_config = CheckpointConfig(num_to_keep=1)\n",
    "\n",
    "# Set experiment name and checkpoint configs\n",
    "run_config = RunConfig(\n",
    "    name=\"finetune-resnet\",\n",
    "    local_dir=\"/tmp/ray_results\",\n",
    "    checkpoint_config=checkpoint_config,\n",
    ")\n",
    "\n",
    "# [TODO] You can also initialize a model from previous checkpoint\n",
    "# CHECKPOINT_URI = \"s3://air-example-data/finetune-resnet-checkpoint/TorchTrainer_4f69f_00000_0_2023-02-14_14-04-09/checkpoint_000000/\"\n",
    "# def initialize_model_from_ckpt():\n",
    "#     checkpoint = TorchCheckpoint.from_uri(CHECKPOINT_URI)\n",
    "#     resnet18 = initialize_model()\n",
    "#     return checkpoint.get_model(model=resnet18)\n",
    "\n",
    "trainer = TorchTrainer(\n",
    "    train_loop_per_worker=train_loop_per_worker,\n",
    "    train_loop_config=train_loop_config,\n",
    "    scaling_config=scaling_config,\n",
    "    run_config=run_config,\n",
    "    datasets=ray_datasets,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training has finished! The best checkpoint has been saved to the experiment directory, and you can now check the experiment metrics and checkpoint information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2023-02-15 14:30:01</td></tr>\n",
       "<tr><td>Running for: </td><td>00:01:08.29        </td></tr>\n",
       "<tr><td>Memory:      </td><td>7.5/62.0 GiB       </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using FIFO scheduling algorithm.<br>Resources requested: 0/64 CPUs, 0/4 GPUs, 0.0/163.93 GiB heap, 0.0/72.84 GiB objects (0.0/4.0 accelerator_type:T4)\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th>status    </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    loss</th><th style=\"text-align: right;\">     acc</th><th style=\"text-align: right;\">  _timestamp</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>TorchTrainer_20605_00000</td><td>TERMINATED</td><td>10.0.51.23:15947</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         56.2452</td><td style=\"text-align: right;\">0.234332</td><td style=\"text-align: right;\">0.934641</td><td style=\"text-align: right;\">  1676500191</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=16024, ip=10.0.51.23) 2023-02-15 14:28:59,031\tINFO config.py:86 -- Setting up process group for: env:// [rank=0, world_size=4]\n",
      "(RayTrainWorker pid=79304) /home/ray/anaconda3/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "(RayTrainWorker pid=79304)   warnings.warn(\n",
      "(RayTrainWorker pid=79304) /home/ray/anaconda3/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "(RayTrainWorker pid=79304)   warnings.warn(msg)\n",
      "(RayTrainWorker pid=15545, ip=10.0.18.48) /home/ray/anaconda3/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "(RayTrainWorker pid=15545, ip=10.0.18.48)   warnings.warn(\n",
      "(RayTrainWorker pid=15545, ip=10.0.18.48) /home/ray/anaconda3/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "(RayTrainWorker pid=15545, ip=10.0.18.48)   warnings.warn(msg)\n",
      "(RayTrainWorker pid=12110, ip=10.0.26.26) /home/ray/anaconda3/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "(RayTrainWorker pid=12110, ip=10.0.26.26)   warnings.warn(\n",
      "(RayTrainWorker pid=12110, ip=10.0.26.26) /home/ray/anaconda3/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "(RayTrainWorker pid=12110, ip=10.0.26.26)   warnings.warn(msg)\n",
      "(RayTrainWorker pid=16024, ip=10.0.51.23) /home/ray/anaconda3/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "(RayTrainWorker pid=16024, ip=10.0.51.23)   warnings.warn(\n",
      "(RayTrainWorker pid=16024, ip=10.0.51.23) /home/ray/anaconda3/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "(RayTrainWorker pid=16024, ip=10.0.51.23)   warnings.warn(msg)\n",
      "(RayTrainWorker pid=79304) Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /home/ray/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
      "(RayTrainWorker pid=15545, ip=10.0.18.48) Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /home/ray/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
      "(RayTrainWorker pid=12110, ip=10.0.26.26) Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /home/ray/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
      "(RayTrainWorker pid=16024, ip=10.0.51.23) Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /home/ray/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
      "  0%|          | 0.00/97.8M [00:00<?, ?B/s]\n",
      "  1%|▏         | 1.45M/97.8M [00:00<00:06, 15.2MB/s]\n",
      "  0%|          | 0.00/97.8M [00:00<?, ?B/s]\n",
      "  0%|          | 0.00/97.8M [00:00<?, ?B/s]\n",
      "  1%|          | 1.18M/97.8M [00:00<00:08, 12.3MB/s]\n",
      "  4%|▍         | 3.87M/97.8M [00:00<00:04, 20.2MB/s]\n",
      "  4%|▍         | 3.73M/97.8M [00:00<00:04, 20.3MB/s]\n",
      "  3%|▎         | 2.77M/97.8M [00:00<00:03, 28.1MB/s]\n",
      "  4%|▍         | 4.25M/97.8M [00:00<00:04, 23.9MB/s]\n",
      "  6%|▌         | 6.01M/97.8M [00:00<00:04, 21.2MB/s]\n",
      "  6%|▌         | 5.88M/97.8M [00:00<00:04, 20.9MB/s]\n",
      "  6%|▌         | 5.45M/97.8M [00:00<00:03, 24.9MB/s]\n",
      "  7%|▋         | 6.75M/97.8M [00:00<00:03, 24.8MB/s]\n",
      "  9%|▉         | 8.62M/97.8M [00:00<00:03, 23.5MB/s]\n",
      "  9%|▉         | 8.62M/97.8M [00:00<00:03, 23.8MB/s]\n",
      "  8%|▊         | 8.02M/97.8M [00:00<00:03, 24.9MB/s]\n",
      "  9%|▉         | 9.12M/97.8M [00:00<00:04, 22.7MB/s]\n",
      " 11%|█         | 10.9M/97.8M [00:00<00:03, 22.8MB/s]\n",
      " 11%|█         | 10.9M/97.8M [00:00<00:04, 22.8MB/s]\n",
      " 11%|█         | 10.4M/97.8M [00:00<00:03, 23.4MB/s]\n",
      " 12%|█▏        | 11.6M/97.8M [00:00<00:03, 23.6MB/s]\n",
      " 14%|█▍        | 13.6M/97.8M [00:00<00:03, 24.6MB/s]\n",
      " 13%|█▎        | 12.8M/97.8M [00:00<00:03, 24.1MB/s]\n",
      " 15%|█▍        | 14.2M/97.8M [00:00<00:03, 24.9MB/s]\n",
      " 16%|█▋        | 15.9M/97.8M [00:00<00:03, 24.6MB/s]\n",
      " 17%|█▋        | 16.4M/97.8M [00:00<00:03, 24.4MB/s]\n",
      " 16%|█▌        | 15.3M/97.8M [00:00<00:03, 24.7MB/s]\n",
      " 17%|█▋        | 16.6M/97.8M [00:00<00:03, 22.4MB/s]\n",
      " 18%|█▊        | 17.7M/97.8M [00:00<00:03, 23.3MB/s]\n",
      " 20%|█▉        | 19.1M/97.8M [00:00<00:03, 22.8MB/s]\n",
      " 19%|█▊        | 18.3M/97.8M [00:00<00:03, 22.7MB/s]\n",
      " 19%|█▉        | 18.7M/97.8M [00:00<00:03, 22.8MB/s]\n",
      " 20%|██        | 20.0M/97.8M [00:00<00:03, 22.0MB/s]\n",
      " 22%|██▏       | 21.3M/97.8M [00:00<00:03, 22.9MB/s]\n",
      " 21%|██        | 20.5M/97.8M [00:00<00:03, 21.7MB/s]\n",
      " 21%|██▏       | 20.9M/97.8M [00:00<00:03, 22.0MB/s]\n",
      " 23%|██▎       | 22.3M/97.8M [00:00<00:03, 22.7MB/s]\n",
      " 24%|██▍       | 23.5M/97.8M [00:01<00:03, 21.0MB/s]\n",
      " 23%|██▎       | 22.9M/97.8M [00:01<00:03, 22.6MB/s]\n",
      " 24%|██▍       | 23.3M/97.8M [00:01<00:03, 22.9MB/s]\n",
      " 26%|██▌       | 25.6M/97.8M [00:01<00:02, 26.1MB/s]\n",
      " 28%|██▊       | 27.6M/97.8M [00:01<00:02, 26.6MB/s]\n",
      " 27%|██▋       | 26.1M/97.8M [00:01<00:02, 25.6MB/s]\n",
      " 27%|██▋       | 26.5M/97.8M [00:01<00:02, 25.8MB/s]\n",
      " 29%|██▊       | 28.1M/97.8M [00:01<00:02, 24.7MB/s]\n",
      " 29%|██▉       | 28.5M/97.8M [00:01<00:03, 23.7MB/s]\n",
      " 30%|██▉       | 29.0M/97.8M [00:01<00:02, 24.1MB/s]\n",
      " 31%|███       | 30.5M/97.8M [00:01<00:02, 24.7MB/s]\n",
      " 31%|███       | 30.2M/97.8M [00:01<00:02, 24.7MB/s]\n",
      " 34%|███▍      | 33.5M/97.8M [00:01<00:02, 27.2MB/s]\n",
      " 32%|███▏      | 31.7M/97.8M [00:01<00:02, 26.2MB/s]\n",
      " 33%|███▎      | 32.4M/97.8M [00:01<00:02, 27.3MB/s]\n",
      " 34%|███▍      | 33.4M/97.8M [00:01<00:02, 26.4MB/s]\n",
      " 35%|███▌      | 34.2M/97.8M [00:01<00:02, 26.1MB/s]\n",
      " 36%|███▌      | 35.0M/97.8M [00:01<00:02, 25.7MB/s]\n",
      " 37%|███▋      | 36.0M/97.8M [00:01<00:02, 25.3MB/s]\n",
      " 37%|███▋      | 36.2M/97.8M [00:01<00:02, 25.9MB/s]\n",
      " 38%|███▊      | 36.8M/97.8M [00:01<00:02, 25.2MB/s]\n",
      " 38%|███▊      | 37.6M/97.8M [00:01<00:02, 25.4MB/s]\n",
      " 39%|███▉      | 38.5M/97.8M [00:01<00:02, 25.4MB/s]\n",
      " 40%|███▉      | 38.7M/97.8M [00:01<00:02, 25.9MB/s]\n",
      " 40%|████      | 39.2M/97.8M [00:01<00:02, 24.5MB/s]\n",
      " 41%|████      | 40.0M/97.8M [00:01<00:02, 25.1MB/s]\n",
      " 43%|████▎     | 41.6M/97.8M [00:01<00:02, 27.5MB/s]\n",
      " 44%|████▍     | 43.5M/97.8M [00:01<00:01, 30.2MB/s]\n",
      " 47%|████▋     | 46.2M/97.8M [00:01<00:01, 32.6MB/s]\n",
      " 45%|████▍     | 44.0M/97.8M [00:01<00:01, 29.6MB/s]\n",
      " 47%|████▋     | 45.8M/97.8M [00:01<00:01, 32.1MB/s]\n",
      " 48%|████▊     | 46.9M/97.8M [00:01<00:01, 31.5MB/s]\n",
      " 49%|████▊     | 47.6M/97.8M [00:01<00:01, 31.8MB/s]\n",
      " 50%|████▉     | 48.8M/97.8M [00:01<00:01, 31.6MB/s]\n",
      " 50%|█████     | 49.4M/97.8M [00:01<00:01, 30.7MB/s]\n",
      " 52%|█████▏    | 50.7M/97.8M [00:02<00:01, 33.7MB/s]\n",
      " 56%|█████▋    | 55.1M/97.8M [00:02<00:01, 35.5MB/s]\n",
      " 54%|█████▎    | 52.4M/97.8M [00:02<00:01, 33.2MB/s]\n",
      " 55%|█████▌    | 54.2M/97.8M [00:02<00:01, 34.5MB/s]\n",
      " 55%|█████▍    | 53.7M/97.8M [00:02<00:01, 34.9MB/s]\n",
      " 57%|█████▋    | 55.9M/97.8M [00:02<00:01, 34.0MB/s]\n",
      " 59%|█████▉    | 57.5M/97.8M [00:02<00:01, 30.7MB/s]\n",
      " 58%|█████▊    | 57.1M/97.8M [00:02<00:01, 31.4MB/s]\n",
      " 60%|█████▉    | 58.5M/97.8M [00:02<00:01, 30.2MB/s]\n",
      " 60%|██████    | 59.1M/97.8M [00:02<00:01, 28.4MB/s]\n",
      " 62%|██████▏   | 60.2M/97.8M [00:02<00:01, 27.6MB/s]\n",
      " 63%|██████▎   | 61.5M/97.8M [00:02<00:01, 28.3MB/s]\n",
      " 64%|██████▎   | 62.2M/97.8M [00:02<00:01, 29.2MB/s]\n",
      " 62%|██████▏   | 60.5M/97.8M [00:02<00:01, 26.6MB/s]\n",
      " 66%|██████▌   | 64.3M/97.8M [00:02<00:01, 31.3MB/s]\n",
      " 67%|██████▋   | 65.6M/97.8M [00:02<00:01, 32.1MB/s]\n",
      " 68%|██████▊   | 66.4M/97.8M [00:02<00:00, 33.0MB/s]\n",
      " 66%|██████▋   | 64.9M/97.8M [00:02<00:01, 31.5MB/s]\n",
      " 70%|███████   | 68.7M/97.8M [00:02<00:00, 35.3MB/s]\n",
      " 71%|███████▏  | 69.8M/97.8M [00:02<00:00, 34.8MB/s]\n",
      " 71%|███████   | 69.7M/97.8M [00:02<00:01, 29.0MB/s]\n",
      " 71%|███████   | 69.5M/97.8M [00:02<00:00, 35.6MB/s]\n",
      " 74%|███████▍  | 72.3M/97.8M [00:02<00:00, 35.0MB/s]\n",
      " 76%|███████▌  | 74.2M/97.8M [00:02<00:00, 38.0MB/s]\n",
      " 75%|███████▌  | 73.4M/97.8M [00:02<00:00, 36.9MB/s]\n",
      " 78%|███████▊  | 76.1M/97.8M [00:02<00:00, 38.7MB/s]\n",
      " 78%|███████▊  | 76.2M/97.8M [00:02<00:00, 36.5MB/s]\n",
      " 79%|███████▉  | 77.1M/97.8M [00:02<00:00, 34.2MB/s]\n",
      " 80%|███████▉  | 78.0M/97.8M [00:02<00:00, 32.4MB/s]\n",
      " 82%|████████▏ | 80.5M/97.8M [00:03<00:00, 29.0MB/s]\n",
      " 82%|████████▏ | 80.1M/97.8M [00:02<00:00, 31.9MB/s]\n",
      " 82%|████████▏ | 79.8M/97.8M [00:02<00:00, 30.9MB/s]\n",
      " 83%|████████▎ | 81.3M/97.8M [00:03<00:00, 28.8MB/s]\n",
      " 85%|████████▌ | 83.5M/97.8M [00:03<00:00, 29.3MB/s]\n",
      " 85%|████████▍ | 83.0M/97.8M [00:03<00:00, 27.6MB/s]\n",
      " 86%|████████▌ | 84.2M/97.8M [00:03<00:00, 29.3MB/s]\n",
      " 85%|████████▌ | 83.5M/97.8M [00:03<00:00, 28.4MB/s]\n",
      " 89%|████████▉ | 87.2M/97.8M [00:03<00:00, 31.0MB/s]\n",
      " 89%|████████▉ | 86.8M/97.8M [00:03<00:00, 30.5MB/s]\n",
      " 89%|████████▉ | 87.2M/97.8M [00:03<00:00, 31.5MB/s]\n",
      " 90%|████████▉ | 87.7M/97.8M [00:03<00:00, 31.1MB/s]\n",
      " 94%|█████████▎| 91.6M/97.8M [00:03<00:00, 34.8MB/s]\n",
      " 93%|█████████▎| 91.3M/97.8M [00:03<00:00, 34.9MB/s]\n",
      " 94%|█████████▎| 91.6M/97.8M [00:03<00:00, 34.9MB/s]\n",
      " 94%|█████████▍| 91.9M/97.8M [00:03<00:00, 34.4MB/s]\n",
      " 97%|█████████▋| 95.3M/97.8M [00:03<00:00, 35.9MB/s]\n",
      " 97%|█████████▋| 94.9M/97.8M [00:03<00:00, 35.2MB/s]\n",
      " 98%|█████████▊| 95.4M/97.8M [00:03<00:00, 36.1MB/s]\n",
      " 98%|█████████▊| 95.4M/97.8M [00:03<00:00, 34.9MB/s]\n",
      "100%|██████████| 97.8M/97.8M [00:03<00:00, 28.7MB/s]\n",
      "100%|██████████| 97.8M/97.8M [00:03<00:00, 28.8MB/s]\n",
      "100%|██████████| 97.8M/97.8M [00:03<00:00, 29.0MB/s]\n",
      "100%|██████████| 97.8M/97.8M [00:03<00:00, 28.7MB/s]\n",
      "(RayTrainWorker pid=12110, ip=10.0.26.26) 2023-02-15 14:29:05,978\tINFO train_loop_utils.py:270 -- Moving model to device: cuda:0\n",
      "(RayTrainWorker pid=79304) 2023-02-15 14:29:05,975\tINFO train_loop_utils.py:270 -- Moving model to device: cuda:0\n",
      "(RayTrainWorker pid=16024, ip=10.0.51.23) 2023-02-15 14:29:05,978\tINFO train_loop_utils.py:270 -- Moving model to device: cuda:0\n",
      "(RayTrainWorker pid=15545, ip=10.0.18.48) 2023-02-15 14:29:05,981\tINFO train_loop_utils.py:270 -- Moving model to device: cuda:0\n",
      "(RayTrainWorker pid=79304) 2023-02-15 14:29:07,526\tINFO train_loop_utils.py:330 -- Wrapping provided model in DistributedDataParallel.\n",
      "(RayTrainWorker pid=12110, ip=10.0.26.26) 2023-02-15 14:29:07,581\tINFO train_loop_utils.py:330 -- Wrapping provided model in DistributedDataParallel.\n",
      "(RayTrainWorker pid=16024, ip=10.0.51.23) 2023-02-15 14:29:07,569\tINFO train_loop_utils.py:330 -- Wrapping provided model in DistributedDataParallel.\n",
      "(RayTrainWorker pid=15545, ip=10.0.18.48) 2023-02-15 14:29:07,602\tINFO train_loop_utils.py:330 -- Wrapping provided model in DistributedDataParallel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=16024, ip=10.0.51.23) Epoch 0-train Loss: 1.0319 Acc: 0.3443\n",
      "(RayTrainWorker pid=16024, ip=10.0.51.23) Epoch 0-val Loss: 0.5913 Acc: 0.5817\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"trialProgress\">\n",
       "  <h3>Trial Progress</h3>\n",
       "  <table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th style=\"text-align: right;\">  _time_this_iter_s</th><th style=\"text-align: right;\">  _timestamp</th><th style=\"text-align: right;\">  _training_iteration</th><th style=\"text-align: right;\">     acc</th><th>date               </th><th>done  </th><th>episodes_total  </th><th>experiment_id                   </th><th style=\"text-align: right;\">  experiment_tag</th><th>hostname     </th><th style=\"text-align: right;\">  iterations_since_restore</th><th style=\"text-align: right;\">    loss</th><th>node_ip   </th><th style=\"text-align: right;\">  pid</th><th>should_checkpoint  </th><th style=\"text-align: right;\">  time_since_restore</th><th style=\"text-align: right;\">  time_this_iter_s</th><th style=\"text-align: right;\">  time_total_s</th><th style=\"text-align: right;\">  timestamp</th><th style=\"text-align: right;\">  timesteps_since_restore</th><th>timesteps_total  </th><th style=\"text-align: right;\">  training_iteration</th><th style=\"text-align: right;\">   trial_id</th><th style=\"text-align: right;\">  warmup_time</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>TorchTrainer_20605_00000</td><td style=\"text-align: right;\">            4.13338</td><td style=\"text-align: right;\">  1676500191</td><td style=\"text-align: right;\">                   10</td><td style=\"text-align: right;\">0.934641</td><td>2023-02-15_14-29-52</td><td>True  </td><td>                </td><td>17be262490e740238915a0697f5c1ef7</td><td style=\"text-align: right;\">               0</td><td>ip-10-0-51-23</td><td style=\"text-align: right;\">                        10</td><td style=\"text-align: right;\">0.234332</td><td>10.0.51.23</td><td style=\"text-align: right;\">15947</td><td>True               </td><td style=\"text-align: right;\">             56.2452</td><td style=\"text-align: right;\">           4.20876</td><td style=\"text-align: right;\">       56.2452</td><td style=\"text-align: right;\"> 1676500192</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">20605_00000</td><td style=\"text-align: right;\">     0.185671</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".trialProgress {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".trialProgress h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".trialProgress td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=16024, ip=10.0.51.23) Epoch 1-train Loss: 0.6640 Acc: 0.5574\n",
      "(RayTrainWorker pid=16024, ip=10.0.51.23) Epoch 1-val Loss: 0.4608 Acc: 0.9281\n",
      "(RayTrainWorker pid=16024, ip=10.0.51.23) Epoch 2-train Loss: 0.5956 Acc: 0.7213\n",
      "(RayTrainWorker pid=16024, ip=10.0.51.23) Epoch 2-val Loss: 0.3758 Acc: 0.8758\n",
      "(RayTrainWorker pid=16024, ip=10.0.51.23) Epoch 3-train Loss: 0.5258 Acc: 0.7377\n",
      "(RayTrainWorker pid=16024, ip=10.0.51.23) Epoch 3-val Loss: 0.3247 Acc: 0.9477\n",
      "(RayTrainWorker pid=16024, ip=10.0.51.23) Epoch 4-train Loss: 0.4128 Acc: 0.7869\n",
      "(RayTrainWorker pid=16024, ip=10.0.51.23) Epoch 4-val Loss: 0.2768 Acc: 0.9412\n",
      "(RayTrainWorker pid=16024, ip=10.0.51.23) Epoch 5-train Loss: 0.3988 Acc: 0.7541\n",
      "(RayTrainWorker pid=16024, ip=10.0.51.23) Epoch 5-val Loss: 0.2693 Acc: 0.9542\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-15 14:29:34,586\tWARNING util.py:244 -- The `process_trial_save` operation took 2.894 s, which may be a performance bottleneck.\n",
      "2023-02-15 14:29:34,586\tWARNING trial_runner.py:1059 -- Consider turning off forced head-worker trial checkpoint syncs by setting sync_on_checkpoint=False. Note that this may result in faulty trial restoration if a failure occurs while the checkpoint is being synced from the worker to the head node.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=16024, ip=10.0.51.23) Epoch 6-train Loss: 0.3330 Acc: 0.7869\n",
      "(RayTrainWorker pid=16024, ip=10.0.51.23) Epoch 6-val Loss: 0.2469 Acc: 0.9542\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-15 14:29:38,719\tWARNING util.py:244 -- The `process_trial_save` operation took 2.892 s, which may be a performance bottleneck.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=16024, ip=10.0.51.23) Epoch 7-train Loss: 0.3207 Acc: 0.7541\n",
      "(RayTrainWorker pid=16024, ip=10.0.51.23) Epoch 7-val Loss: 0.2467 Acc: 0.9412\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-15 14:29:42,806\tWARNING util.py:244 -- The `process_trial_save` operation took 2.877 s, which may be a performance bottleneck.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=16024, ip=10.0.51.23) Epoch 8-train Loss: 0.2849 Acc: 0.8197\n",
      "(RayTrainWorker pid=16024, ip=10.0.51.23) Epoch 8-val Loss: 0.2342 Acc: 0.9412\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-15 14:29:46,947\tWARNING util.py:244 -- The `process_trial_save` operation took 2.916 s, which may be a performance bottleneck.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=16024, ip=10.0.51.23) Epoch 9-train Loss: 0.2719 Acc: 0.8197\n",
      "(RayTrainWorker pid=16024, ip=10.0.51.23) Epoch 9-val Loss: 0.2343 Acc: 0.9346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-15 14:29:51,080\tWARNING util.py:244 -- The `process_trial_save` operation took 2.915 s, which may be a performance bottleneck.\n",
      "2023-02-15 14:29:55,059\tWARNING util.py:244 -- The `process_trial_save` operation took 2.686 s, which may be a performance bottleneck.\n",
      "2023-02-15 14:29:59,508\tWARNING util.py:244 -- The `process_trial_save` operation took 1.780 s, which may be a performance bottleneck.\n",
      "2023-02-15 14:30:01,519\tINFO tune.py:762 -- Total run time: 68.63 seconds (68.29 seconds for the tuning loop).\n"
     ]
    }
   ],
   "source": [
    "result = trainer.fit()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the fine-tuned model for batch prediction\n",
    "\n",
    "Now, we want to load the trained model and evaluation it on test data.\n",
    "We can use `TorchCheckpoint.from_uri()` to load the resulting checkpoint from our fine-tuning run. The {class}`~ray.train.batch_predictor.BatchPredictor` will identify the dict key `\"model\"` and load the corresponding parameters into the model.\n",
    " "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example path for a checkpoint folder:\n",
    "`\"/tmp/ray_results/finetune-resnet/TorchTrainer_94bb5_00000_0_2023-02-14_14-40-28/checkpoint_000009\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_folder = result.checkpoint.uri\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.train.batch_predictor import BatchPredictor\n",
    "from ray.train.torch import TorchCheckpoint, TorchPredictor\n",
    "\n",
    "ckpt = TorchCheckpoint.from_uri(checkpoint_folder)\n",
    "predictor = BatchPredictor.from_checkpoint(\n",
    "    ckpt, TorchPredictor, model=initialize_model()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-15 14:30:03,395\tINFO batch_predictor.py:184 -- `num_gpus_per_worker` is set for `BatchPreditor`.Automatically enabling GPU prediction for this predictor. To disable set `use_gpu` to `False` in `BatchPredictor.predict`.\n",
      "2023-02-15 14:30:03,737\tWARNING compute.py:549 -- Requested batch size 4096 results in batches of 2466299904 bytes for map_batches tasks, which is larger than the configured target max block size 536870912. This may result in out-of-memory errors for certain workloads, and you may want to decrease your batch size or increase the configured target max block size, e.g.: from ray.data.context import DatasetContext; DatasetContext.get_current().target_max_block_size = 4_000_000_000\n",
      "2023-02-15 14:30:03,738\tWARNING compute.py:520 -- `batch_size` is set to 4096, which reduces parallelism from 128 to 1. If the performance is worse than expected, this may indicate that the batch size is too large or the input block size is too small. To reduce batch size, consider decreasing `batch_size` or use the default in `map_batches`. To increase input block size, consider decreasing `parallelism` in read.\n",
      "Map_Batches:   0%|          | 0/1 [00:00<?, ?it/s]2023-02-15 14:30:05,016\tWARNING worker.py:1851 -- Warning: The actor BlockWorker is very large (90 MiB). Check that its definition is not implicitly capturing a large array or other object in scope. Tip: use ray.put() to put large objects in the Ray object store.\n",
      "Map Progress (1 actors 1 pending): 100%|██████████| 1/1 [00:10<00:00, 10.38s/it]\n"
     ]
    }
   ],
   "source": [
    "prediction_ds = predictor.predict(\n",
    "    ray_datasets[\"val\"],\n",
    "    feature_columns=[\"image\"],\n",
    "    keep_columns=[\"label\"],\n",
    "    num_gpus_per_worker=1,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate prediction results\n",
    "\n",
    "The prediction has finished! We can use `ds.schema()` and `ds.take()` to inspect the data types and record structure.\n",
    "\n",
    "We can see that there are two keys in the prediction results:\n",
    "- \"predictions\": The output logits of our ResNet model, which is a 1000 dimensional tensor.\n",
    "- \"label\": The image label. Specified by `keep_columns` in `predictor.predict()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predictions: extension<arrow.py_extension_type<ArrowTensorType>>\n",
       "label: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_ds.schema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ArrowRow({'predictions': array([ 0.9406432 , -0.83088267], dtype=float32),\n",
       "           'label': 0})]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_ds.take(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define a function `convert_logits_to_classes` to convert tensor outputs to labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map_Batches: 100%|██████████| 1/1 [00:00<00:00, 75.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'predictions': array([ 0.9406432 , -0.83088267], dtype=float32), 'label': 0, 'pred_label': 0, 'correct': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Shuffle Map: 100%|██████████| 1/1 [00:00<00:00, 118.18it/s]\n",
      "Shuffle Reduce: 100%|██████████| 1/1 [00:00<00:00, 129.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Accuracy =  0.934640522875817\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def convert_logits_to_classes(batch):\n",
    "    batch[\"pred_label\"] = np.argmax(batch[\"predictions\"], axis=1)\n",
    "    batch[\"correct\"] = batch[\"pred_label\"] == batch[\"label\"]\n",
    "    return batch\n",
    "\n",
    "\n",
    "predictions = prediction_ds.map_batches(convert_logits_to_classes, batch_format=\"numpy\")\n",
    "predictions.show(1)\n",
    "\n",
    "print(\"Evaluation Accuracy = \", predictions.mean(on=\"correct\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also reuse the evaluation function defined in the training loop by iterating over the dataset. Note that the previous approach using `map_batches()` is more efficient because it parallelizes the evaluation on each partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Accuracy =  0.934640522875817\n"
     ]
    }
   ],
   "source": [
    "def evaluate(logits, labels):\n",
    "    _, preds = torch.max(logits, 1)\n",
    "    corrects = torch.sum(preds == labels).item()\n",
    "    return corrects\n",
    "\n",
    "\n",
    "accuracy = 0\n",
    "for batch in prediction_ds.iter_torch_batches(batch_size=10):\n",
    "    accuracy += evaluate(batch[\"predictions\"], batch[\"label\"])\n",
    "accuracy /= prediction_ds.count()\n",
    "\n",
    "print(\"Evaluation Accuracy = \", accuracy)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ray",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13 | packaged by conda-forge | (default, Mar 25 2022, 06:05:16) \n[Clang 12.0.1 ]"
  },
  "vscode": {
   "interpreter": {
    "hash": "a8c1140d108077f4faeb76b2438f85e4ed675f93d004359552883616a1acd54c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
