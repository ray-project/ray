{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning a Pytorch ResNet Model for Image Classification\n",
    "In this example we will finetune a pretrained ResNet model with Ray Train. You should be familiar with [PyTorch](https://pytorch.org/) before starting the tutorial. \n",
    "\n",
    "For fine-tuning, our network architecture consists of a pretrained ResNet model as the backbone and a randomly initialized linear layer as the classifier. The ResNet model is pretrained on the 1000-class Imagenet dataset. We will unfreeze and retrain all parameters of the model for the new task.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and transform datasets\n",
    "We will use the *hymenoptera_data* as the fintuning dataset, which contains two classes(bees and ants) and 397 images(244 for train, 153 for validation). The dataset is provided by Pytorch and can be downloaded [here](https://download.pytorch.org/tutorial/hymenoptera_data.zip). The dataset folder is structured such that we can load with Pytorch [ImageFolder](https://pytorch.org/vision/main/generated/torchvision.datasets.ImageFolder.html) dataset.\n",
    "\n",
    "Notice that the ResNet model was pretrained with hard-coded normalization values. We'll keep these numbers the same for fine-tuning, as shown in *data_transforms*. More details can be found [here](https://pytorch.org/hub/pytorch_vision_resnet/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.system(\"wget https://download.pytorch.org/tutorial/hymenoptera_data.zip\")\n",
    "os.system(\"unzip hymenoptera_data.zip\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, models, transforms\n",
    "import numpy as np\n",
    "\n",
    "torch.manual_seed(620)\n",
    "# torch.manual_seed(450)\n",
    "\n",
    "# Replace with your own path of the dataset\n",
    "DATA_DIR = \"./hymenoptera_data\"\n",
    "\n",
    "# Data augmentation and normalization for training\n",
    "# Just normalization for validation\n",
    "input_size = 224\n",
    "data_transforms = {\n",
    "    \"train\": transforms.Compose(\n",
    "        [\n",
    "            transforms.RandomResizedCrop(input_size),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "        ]\n",
    "    ),\n",
    "    \"val\": transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize(input_size),\n",
    "            transforms.CenterCrop(input_size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "        ]\n",
    "    ),\n",
    "}\n",
    "\n",
    "torch_datasets = dict()\n",
    "for split in [\"train\", \"val\"]:\n",
    "    torch_datasets[split] = datasets.ImageFolder(\n",
    "        os.path.join(DATA_DIR, split), data_transforms[split]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will transform our ImageFolder dataset into a Ray dataset, which will partition the whole dataset and distribute the data blocks across the nodes in cluster. You will benefit from faster parallel pre-processing and data ingestion.\n",
    "\n",
    "Note that **batch** here refers to the chunk of data that the map function will execute on, not the batch we use for model training. To learn more about writing functions for {meth}`map_batches <ray.data.Dataset.map_batches>`, read [writing user-defined functions](https://docs.ray.io/en/latest/data/transforming-datasets.html#transform-datasets-writing-udfs) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map_Batches: 100%|██████████| 244/244 [00:00<00:00, 245.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: extension<arrow.py_extension_type<ArrowTensorType>>\n",
      "label: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map_Batches: 100%|██████████| 153/153 [00:00<00:00, 284.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: extension<arrow.py_extension_type<ArrowTensorType>>\n",
      "label: int64\n"
     ]
    }
   ],
   "source": [
    "import ray\n",
    "\n",
    "\n",
    "def convert_batch_to_numpy(batch):\n",
    "    images = np.array([image.numpy() for image, _ in batch])\n",
    "    labels = np.array([label for _, label in batch])\n",
    "    return {\"image\": images, \"label\": labels}\n",
    "\n",
    "\n",
    "ray_datasets = dict()\n",
    "for split in [\"train\", \"val\"]:\n",
    "    ray_datasets[split] = ray.data.from_torch(torch_datasets[split]).map_batches(\n",
    "        convert_batch_to_numpy\n",
    "    )\n",
    "    print(ray_datasets[split].schema())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Model and Fine-tuning configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = dict()\n",
    "\n",
    "# Input image size (224 x 224)\n",
    "configs[\"input_size\"] = 224\n",
    "\n",
    "# Batch size for training (change depending on how much memory you have)\n",
    "configs[\"batch_size\"] = 32\n",
    "\n",
    "# Number of epochs to train for\n",
    "configs[\"num_epochs\"] = 10\n",
    "\n",
    "# Hyper-parameters for optimizer\n",
    "configs[\"lr\"] = 0.001\n",
    "configs[\"momentum\"] = 0.9\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next let's define our model, you can create a model from a pretrained ResNet, or reload the model checkpoint from a previous run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.train.torch import TorchCheckpoint\n",
    "\n",
    "\n",
    "def initialize_model():\n",
    "    # Load pretrained model params\n",
    "    model = models.resnet18(pretrained=True)\n",
    "\n",
    "    # Replace the original classifier with a new Linear layer\n",
    "    num_features = model.fc.in_features\n",
    "    model.fc = nn.Linear(num_features, 2)\n",
    "\n",
    "    # Ensure all params get updated during fintuning\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = True\n",
    "    return model\n",
    "\n",
    "\n",
    "# You can also initialize a model from previous checkpoint\n",
    "CHECKPOINT_URI = \"s3://air-example-data/finetune-resnet-checkpoint/TorchTrainer_4f69f_00000_0_2023-02-14_14-04-09/checkpoint_000000/\"\n",
    "\n",
    "\n",
    "def initialize_model_from_ckpt():\n",
    "    checkpoint = TorchCheckpoint.from_uri(CHECKPOINT_URI)\n",
    "    resnet18 = initialize_model()\n",
    "    return checkpoint.get_model(model=resnet18)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Training Loop\n",
    "\n",
    "The `train_loop_per_worker` function defines the finetuning procedure for each worker.\n",
    "\n",
    "**1. Load dataset shard for each worker**:\n",
    "- A ray trainer will take a dictionary of ray datasets as input. One can accessed these data by `session.get_dataset_shard(DATASET_KEY)` in the workers.\n",
    "- Only the dataset with key \"train\" will be split into multiple shards, while all the others will remain the same. \n",
    "- One can use {meth}`iter_torch_batches <ray.data.Dataset.iter_torch_batches>` to iterate the datasets with automatic tensor batching. If you need more flexible customized batching function, please refer to our lower-level api {meth}`iter_batches <ray.data.Dataset.iter_batches>`.\n",
    "\n",
    "**2. Prepare your model**:\n",
    "- `train.torch.prepare_model` will prepares the model for distributed training. Under the hood, it converts your torch model to `DistributedDataParallel` model, and synchronizes the gradients and buffers across all workers.\n",
    "\n",
    "**3. Report metrics and checkpoint**:\n",
    "- `session.report` will gather the metrics from each worker and save them into log files.\n",
    "- You don't have to save checkpoints manually with `torch.save()`, `session.report()` will help you sync checkpoints to local/cloud storage.\n",
    "- The best checkpoints will be saved according to the specified `checkpoint_score_attribute` in {class}`CheckpointConfig <ray.air.config.CheckpointConfig>`. Here we only save the best model with highest validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray.train as train\n",
    "from ray.air import session\n",
    "from ray.train.torch import TorchCheckpoint\n",
    "\n",
    "\n",
    "def evaluate(logits, labels):\n",
    "    _, preds = torch.max(logits, 1)\n",
    "    corrects = torch.sum(preds == labels).item()\n",
    "    return corrects\n",
    "\n",
    "\n",
    "def train_loop_per_worker(configs):\n",
    "    # Prepare dataloader for each worker\n",
    "    datasets = dict()\n",
    "    datasets[\"train\"] = session.get_dataset_shard(\"train\")\n",
    "    datasets[\"val\"] = session.get_dataset_shard(\"val\")\n",
    "\n",
    "    # Calculate the batch size for a single worker\n",
    "    worker_batch_size = configs[\"batch_size\"] // session.get_world_size()\n",
    "\n",
    "    device = \"cpu\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda:0\"\n",
    "\n",
    "    # Prepare DDP Model, optimizer, and loss function\n",
    "    model = initialize_model_from_ckpt()\n",
    "    model = train.torch.prepare_model(model)\n",
    "\n",
    "    optimizer = optim.SGD(\n",
    "        model.parameters(), lr=configs[\"lr\"], momentum=configs[\"momentum\"]\n",
    "    )\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Start training loops\n",
    "    for epoch in range(configs[\"num_epochs\"]):\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in [\"train\", \"val\"]:\n",
    "            if phase == \"train\":\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()  # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Create a dataset iterator for the shard on the current worker\n",
    "            dataset_iterator = datasets[phase].iter_torch_batches(\n",
    "                batch_size=worker_batch_size, device=device\n",
    "            )\n",
    "            for batch in dataset_iterator:\n",
    "                inputs = batch[\"image\"]\n",
    "                labels = batch[\"label\"]\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                with torch.set_grad_enabled(phase == \"train\"):\n",
    "                    # Get model outputs and calculate loss\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == \"train\":\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # calculate statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += evaluate(outputs, labels)\n",
    "\n",
    "            epoch_loss = running_loss / datasets[phase].count()\n",
    "            epoch_acc = running_corrects / datasets[phase].count()\n",
    "\n",
    "            if session.get_world_rank() == 0:\n",
    "                print(\n",
    "                    \"Epoch {}-{} Loss: {:.4f} Acc: {:.4f}\".format(\n",
    "                        epoch, phase, epoch_loss, epoch_acc\n",
    "                    )\n",
    "                )\n",
    "\n",
    "            # Report metrics and checkpoint every epoch\n",
    "            if phase == \"val\":\n",
    "                checkpoint = TorchCheckpoint.from_dict(\n",
    "                    {\n",
    "                        \"epoch\": epoch,\n",
    "                        \"model\": model.module.state_dict(),\n",
    "                        \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                    }\n",
    "                )\n",
    "                session.report(\n",
    "                    metrics={\"loss\": epoch_loss, \"acc\": epoch_acc},\n",
    "                    checkpoint=checkpoint,\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, setup the TorchTrainer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.train.torch import TorchTrainer, TorchCheckpoint\n",
    "from ray.air.config import ScalingConfig, RunConfig, CheckpointConfig\n",
    "from ray.tune.syncer import SyncConfig\n",
    "\n",
    "# Scale out model training across 4 GPUs.\n",
    "scaling_config = ScalingConfig(\n",
    "    num_workers=4, use_gpu=True, resources_per_worker={\"CPU\": 1, \"GPU\": 1}\n",
    ")\n",
    "\n",
    "# Save the latest checkpoint\n",
    "checkpoint_config = CheckpointConfig(num_to_keep=1)\n",
    "\n",
    "# Set experiment name and checkpoint configs\n",
    "run_config = RunConfig(\n",
    "    name=\"finetune-resnet\",\n",
    "    local_dir=\"/tmp/ray_results\",\n",
    "    sync_config=SyncConfig(),\n",
    "    checkpoint_config=checkpoint_config,\n",
    ")\n",
    "\n",
    "trainer = TorchTrainer(\n",
    "    train_loop_per_worker=train_loop_per_worker,\n",
    "    train_loop_config=configs,\n",
    "    scaling_config=scaling_config,\n",
    "    run_config=run_config,\n",
    "    datasets=ray_datasets,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training procedure completed in 53 seconds, it saved the best checkpoint in the `local_dir` provided to the trainer. You can now check the experiment metrics and checkpoint information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2023-02-14 14:41:27</td></tr>\n",
       "<tr><td>Running for: </td><td>00:00:59.23        </td></tr>\n",
       "<tr><td>Memory:      </td><td>10.1/62.0 GiB      </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using FIFO scheduling algorithm.<br>Resources requested: 0/64 CPUs, 0/4 GPUs, 0.0/163.86 GiB heap, 0.0/72.81 GiB objects (0.0/4.0 accelerator_type:T4)\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th>status    </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    loss</th><th style=\"text-align: right;\">     acc</th><th style=\"text-align: right;\">  _timestamp</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>TorchTrainer_94bb5_00000</td><td>TERMINATED</td><td>10.0.13.194:6490</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">          47.268</td><td style=\"text-align: right;\">0.211976</td><td style=\"text-align: right;\">0.934641</td><td style=\"text-align: right;\">  1676414478</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=6567, ip=10.0.13.194) 2023-02-14 14:40:34,581\tINFO config.py:86 -- Setting up process group for: env:// [rank=0, world_size=4]\n",
      "(RayTrainWorker pid=4757, ip=10.0.6.12) /home/ray/anaconda3/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "(RayTrainWorker pid=4757, ip=10.0.6.12)   warnings.warn(\n",
      "(RayTrainWorker pid=4757, ip=10.0.6.12) /home/ray/anaconda3/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "(RayTrainWorker pid=4757, ip=10.0.6.12)   warnings.warn(msg)\n",
      "(RayTrainWorker pid=7300, ip=10.0.43.115) /home/ray/anaconda3/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "(RayTrainWorker pid=7300, ip=10.0.43.115)   warnings.warn(\n",
      "(RayTrainWorker pid=7300, ip=10.0.43.115) /home/ray/anaconda3/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "(RayTrainWorker pid=7300, ip=10.0.43.115)   warnings.warn(msg)\n",
      "(RayTrainWorker pid=967689) /home/ray/anaconda3/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "(RayTrainWorker pid=967689)   warnings.warn(\n",
      "(RayTrainWorker pid=967689) /home/ray/anaconda3/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "(RayTrainWorker pid=967689)   warnings.warn(msg)\n",
      "(RayTrainWorker pid=6567, ip=10.0.13.194) /home/ray/anaconda3/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "(RayTrainWorker pid=6567, ip=10.0.13.194)   warnings.warn(\n",
      "(RayTrainWorker pid=6567, ip=10.0.13.194) /home/ray/anaconda3/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "(RayTrainWorker pid=6567, ip=10.0.13.194)   warnings.warn(msg)\n",
      "(RayTrainWorker pid=967689) 2023-02-14 14:40:44,017\tINFO train_loop_utils.py:270 -- Moving model to device: cuda:0\n",
      "(RayTrainWorker pid=7300, ip=10.0.43.115) 2023-02-14 14:40:44,041\tINFO train_loop_utils.py:270 -- Moving model to device: cuda:0\n",
      "(RayTrainWorker pid=4757, ip=10.0.6.12) 2023-02-14 14:40:44,052\tINFO train_loop_utils.py:270 -- Moving model to device: cuda:0\n",
      "(RayTrainWorker pid=6567, ip=10.0.13.194) 2023-02-14 14:40:44,038\tINFO train_loop_utils.py:270 -- Moving model to device: cuda:0\n",
      "(RayTrainWorker pid=7300, ip=10.0.43.115) 2023-02-14 14:40:45,546\tINFO train_loop_utils.py:330 -- Wrapping provided model in DistributedDataParallel.\n",
      "(RayTrainWorker pid=6567, ip=10.0.13.194) 2023-02-14 14:40:45,542\tINFO train_loop_utils.py:330 -- Wrapping provided model in DistributedDataParallel.\n",
      "(RayTrainWorker pid=967689) 2023-02-14 14:40:45,560\tINFO train_loop_utils.py:330 -- Wrapping provided model in DistributedDataParallel.\n",
      "(RayTrainWorker pid=4757, ip=10.0.6.12) 2023-02-14 14:40:45,580\tINFO train_loop_utils.py:330 -- Wrapping provided model in DistributedDataParallel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=6567, ip=10.0.13.194) Epoch 0-train Loss: 0.5375 Acc: 0.7213\n",
      "(RayTrainWorker pid=6567, ip=10.0.13.194) Epoch 0-val Loss: 0.5027 Acc: 0.7320\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"trialProgress\">\n",
       "  <h3>Trial Progress</h3>\n",
       "  <table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th style=\"text-align: right;\">  _time_this_iter_s</th><th style=\"text-align: right;\">  _timestamp</th><th style=\"text-align: right;\">  _training_iteration</th><th style=\"text-align: right;\">     acc</th><th>date               </th><th>done  </th><th>episodes_total  </th><th>experiment_id                   </th><th style=\"text-align: right;\">  experiment_tag</th><th>hostname      </th><th style=\"text-align: right;\">  iterations_since_restore</th><th style=\"text-align: right;\">    loss</th><th>node_ip    </th><th style=\"text-align: right;\">  pid</th><th>should_checkpoint  </th><th style=\"text-align: right;\">  time_since_restore</th><th style=\"text-align: right;\">  time_this_iter_s</th><th style=\"text-align: right;\">  time_total_s</th><th style=\"text-align: right;\">  timestamp</th><th style=\"text-align: right;\">  timesteps_since_restore</th><th>timesteps_total  </th><th style=\"text-align: right;\">  training_iteration</th><th>trial_id   </th><th style=\"text-align: right;\">  warmup_time</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>TorchTrainer_94bb5_00000</td><td style=\"text-align: right;\">            3.00367</td><td style=\"text-align: right;\">  1676414478</td><td style=\"text-align: right;\">                   10</td><td style=\"text-align: right;\">0.934641</td><td>2023-02-14_14-41-19</td><td>True  </td><td>                </td><td>f8c3ff154b024c7b8741099e97057f54</td><td style=\"text-align: right;\">               0</td><td>ip-10-0-13-194</td><td style=\"text-align: right;\">                        10</td><td style=\"text-align: right;\">0.211976</td><td>10.0.13.194</td><td style=\"text-align: right;\"> 6490</td><td>True               </td><td style=\"text-align: right;\">              47.268</td><td style=\"text-align: right;\">           3.01411</td><td style=\"text-align: right;\">        47.268</td><td style=\"text-align: right;\"> 1676414479</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                  10</td><td>94bb5_00000</td><td style=\"text-align: right;\">     0.197936</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".trialProgress {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".trialProgress h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".trialProgress td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=6567, ip=10.0.13.194) Epoch 1-train Loss: 0.4045 Acc: 0.8689\n",
      "(RayTrainWorker pid=6567, ip=10.0.13.194) Epoch 1-val Loss: 0.3641 Acc: 0.9085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-14 14:40:54,169\tWARNING util.py:244 -- The `process_trial_save` operation took 4.271 s, which may be a performance bottleneck.\n",
      "2023-02-14 14:40:54,170\tWARNING trial_runner.py:1059 -- Consider turning off forced head-worker trial checkpoint syncs by setting sync_on_checkpoint=False. Note that this may result in faulty trial restoration if a failure occurs while the checkpoint is being synced from the worker to the head node.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=6567, ip=10.0.13.194) Epoch 2-train Loss: 0.2317 Acc: 0.9508\n",
      "(RayTrainWorker pid=6567, ip=10.0.13.194) Epoch 2-val Loss: 0.2869 Acc: 0.9085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-14 14:40:57,280\tWARNING util.py:244 -- The `process_trial_save` operation took 2.391 s, which may be a performance bottleneck.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=6567, ip=10.0.13.194) Epoch 3-train Loss: 0.1685 Acc: 0.9672\n",
      "(RayTrainWorker pid=6567, ip=10.0.13.194) Epoch 3-val Loss: 0.2549 Acc: 0.9281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-14 14:41:00,329\tWARNING util.py:244 -- The `process_trial_save` operation took 2.393 s, which may be a performance bottleneck.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=6567, ip=10.0.13.194) Epoch 4-train Loss: 0.1130 Acc: 0.9836\n",
      "(RayTrainWorker pid=6567, ip=10.0.13.194) Epoch 4-val Loss: 0.2357 Acc: 0.9281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-14 14:41:03,344\tWARNING util.py:244 -- The `process_trial_save` operation took 2.376 s, which may be a performance bottleneck.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=6567, ip=10.0.13.194) Epoch 5-train Loss: 0.0835 Acc: 1.0000\n",
      "(RayTrainWorker pid=6567, ip=10.0.13.194) Epoch 5-val Loss: 0.2250 Acc: 0.9281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-14 14:41:06,411\tWARNING util.py:244 -- The `process_trial_save` operation took 2.408 s, which may be a performance bottleneck.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=6567, ip=10.0.13.194) Epoch 6-train Loss: 0.0631 Acc: 1.0000\n",
      "(RayTrainWorker pid=6567, ip=10.0.13.194) Epoch 6-val Loss: 0.2205 Acc: 0.9346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-14 14:41:09,416\tWARNING util.py:244 -- The `process_trial_save` operation took 2.364 s, which may be a performance bottleneck.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=6567, ip=10.0.13.194) Epoch 7-train Loss: 0.0475 Acc: 1.0000\n",
      "(RayTrainWorker pid=6567, ip=10.0.13.194) Epoch 7-val Loss: 0.2165 Acc: 0.9346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-14 14:41:12,457\tWARNING util.py:244 -- The `process_trial_save` operation took 2.367 s, which may be a performance bottleneck.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=6567, ip=10.0.13.194) Epoch 8-train Loss: 0.0376 Acc: 1.0000\n",
      "(RayTrainWorker pid=6567, ip=10.0.13.194) Epoch 8-val Loss: 0.2134 Acc: 0.9346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-14 14:41:15,575\tWARNING util.py:244 -- The `process_trial_save` operation took 2.465 s, which may be a performance bottleneck.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=6567, ip=10.0.13.194) Epoch 9-train Loss: 0.0306 Acc: 1.0000\n",
      "(RayTrainWorker pid=6567, ip=10.0.13.194) Epoch 9-val Loss: 0.2120 Acc: 0.9346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-14 14:41:18,579\tWARNING util.py:244 -- The `process_trial_save` operation took 2.354 s, which may be a performance bottleneck.\n",
      "2023-02-14 14:41:21,498\tWARNING util.py:244 -- The `process_trial_save` operation took 2.262 s, which may be a performance bottleneck.\n",
      "2023-02-14 14:41:26,182\tWARNING util.py:244 -- The `process_trial_save` operation took 1.863 s, which may be a performance bottleneck.\n",
      "2023-02-14 14:41:28,107\tINFO tune.py:762 -- Total run time: 59.35 seconds (59.23 seconds for the tuning loop).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result(metrics={'loss': 0.21197572525809794, 'acc': 0.934640522875817, '_timestamp': 1676414478, '_time_this_iter_s': 3.0036728382110596, '_training_iteration': 10, 'should_checkpoint': True, 'done': True, 'trial_id': '94bb5_00000', 'experiment_tag': '0'}, error=None, log_dir=PosixPath('/tmp/ray_results/finetune-resnet/TorchTrainer_94bb5_00000_0_2023-02-14_14-40-28'))\n"
     ]
    }
   ],
   "source": [
    "result = trainer.fit()\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the checkpoint for batch prediction:\n",
    "\n",
    "TorchTrainer has already saved the best model parameters in `log_dir`. Now we want to load this model into memory and perform batch prediction and evaluation on test data.\n",
    "`TorchCheckpoint.from_directory` will automatically extract pickled params. BatchPredictor will identify the dict key \"model\", and load the corresponding parameters into model. You can also specify the \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The log and checkpoints will be saved into `local_dir` specified in TrainerTrainer. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_folder = \"/tmp/ray_results/finetune-resnet/TorchTrainer_94bb5_00000_0_2023-02-14_14-40-28/checkpoint_000009\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "checkpoint_folder = result.checkpoint.uri[7:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.train.batch_predictor import BatchPredictor\n",
    "from ray.train.torch import TorchCheckpoint, TorchPredictor\n",
    "\n",
    "ckpt = TorchCheckpoint.from_directory(checkpoint_folder)\n",
    "predictor = BatchPredictor.from_checkpoint(\n",
    "    ckpt, TorchPredictor, model=initialize_model()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-14 14:41:29,186\tINFO batch_predictor.py:184 -- `num_gpus_per_worker` is set for `BatchPreditor`.Automatically enabling GPU prediction for this predictor. To disable set `use_gpu` to `False` in `BatchPredictor.predict`.\n",
      "2023-02-14 14:41:29,470\tWARNING compute.py:549 -- Requested batch size 4096 results in batches of 2466299904 bytes for map_batches tasks, which is larger than the configured target max block size 536870912. This may result in out-of-memory errors for certain workloads, and you may want to decrease your batch size or increase the configured target max block size, e.g.: from ray.data.context import DatasetContext; DatasetContext.get_current().target_max_block_size = 4_000_000_000\n",
      "2023-02-14 14:41:29,470\tWARNING compute.py:520 -- `batch_size` is set to 4096, which reduces parallelism from 153 to 1. If the performance is worse than expected, this may indicate that the batch size is too large or the input block size is too small. To reduce batch size, consider decreasing `batch_size` or use the default in `map_batches`. To increase input block size, consider decreasing `parallelism` in read.\n",
      "Map_Batches:   0%|          | 0/1 [00:00<?, ?it/s]2023-02-14 14:41:29,588\tWARNING worker.py:1851 -- Warning: The actor BlockWorker is very large (42 MiB). Check that its definition is not implicitly capturing a large array or other object in scope. Tip: use ray.put() to put large objects in the Ray object store.\n",
      "Map Progress (1 actors 1 pending): 100%|██████████| 1/1 [00:08<00:00,  8.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions: extension<arrow.py_extension_type<ArrowTensorType>>\n",
      "label: int64\n",
      "[{'predictions': array([ 0.6944345, -2.0196059], dtype=float32), 'label': 0}]\n"
     ]
    }
   ],
   "source": [
    "prediction_ds = predictor.predict(\n",
    "    ray_datasets[\"val\"],\n",
    "    feature_columns=[\"image\"],\n",
    "    keep_columns=[\"label\"],\n",
    "    num_gpus_per_worker=1,\n",
    ")\n",
    "print(prediction_ds.schema())\n",
    "print(prediction_ds.take(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate predictions results\n",
    "The BatchPredictor returns a ray dataset as result, which consists a column of `predictions` and the columns specified by `keep_columns` argument. The `predictions` column contains the model's tensor output. Here we define a function `convert_logits_to_classes` to convert tensor outputs to labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map_Batches: 100%|██████████| 1/1 [00:00<00:00, 94.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'predictions': array([ 0.6944345, -2.0196059], dtype=float32), 'label': 0, 'pred_label': 0, 'correct': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Shuffle Map: 100%|██████████| 1/1 [00:00<00:00, 145.21it/s]\n",
      "Shuffle Reduce: 100%|██████████| 1/1 [00:00<00:00, 147.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Accuracy =  0.934640522875817\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def convert_logits_to_classes(batch):\n",
    "    batch[\"pred_label\"] = np.argmax(batch[\"predictions\"], axis=1)\n",
    "    batch[\"correct\"] = batch[\"pred_label\"] == batch[\"label\"]\n",
    "    return batch\n",
    "\n",
    "\n",
    "predictions = prediction_ds.map_batches(convert_logits_to_classes, batch_format=\"numpy\")\n",
    "predictions.show(1)\n",
    "\n",
    "print(\"Evaluation Accuracy = \", predictions.mean(on=\"correct\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also reuse the evaluation function defined in the training loop by iterating over the dataset. Note that the previous approach using `map_batches()` is more efficient because it parallelizes the evaluation on each partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Accuracy =  0.934640522875817\n"
     ]
    }
   ],
   "source": [
    "def evaluate(logits, labels):\n",
    "    _, preds = torch.max(logits, 1)\n",
    "    corrects = torch.sum(preds == labels).item()\n",
    "    return corrects\n",
    "\n",
    "\n",
    "accuracy = 0\n",
    "for batch in prediction_ds.iter_torch_batches(batch_size=10):\n",
    "    accuracy += evaluate(batch[\"predictions\"], batch[\"label\"])\n",
    "accuracy /= prediction_ds.count()\n",
    "\n",
    "print(\"Evaluation Accuracy = \", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example is adapted from Pytorch's [Fintuning Torchvision Models](https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html) tutorial."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
