{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning a Pytorch ResNet Model for Image Classification\n",
    "In this example we will finetune a pretrained ResNet model with Ray Train. You should be familiar with [PyTorch](https://pytorch.org/) before starting the tutorial. \n",
    "\n",
    "For fine-tuning, our network architecture consists of a pretrained ResNet model as the backbone and a randomly initialized linear layer as the classifier. The ResNet model is pretrained on the 1000-class Imagenet dataset. We will unfreeze and retrain all parameters of the model for the new task.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and transform datasets\n",
    "We will use the *hymenoptera_data* as the fintuning dataset, which contains two classes(bees and ants) and 397 images(244 for train, 153 for validation). The dataset is provided by Pytorch and can be downloaded [here](https://download.pytorch.org/tutorial/hymenoptera_data.zip). The dataset folder is structured such that we can load with Pytorch [ImageFolder](https://pytorch.org/vision/main/generated/torchvision.datasets.ImageFolder.html) dataset.\n",
    "\n",
    "Notice that the ResNet model was pretrained with hard-coded normalization values. We'll keep these numbers the same for fine-tuning, as shown in *data_transforms*. More details can be found [here](https://pytorch.org/hub/pytorch_vision_resnet/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, models, transforms\n",
    "import numpy as np\n",
    "\n",
    "# Replace with your own path of the dataset\n",
    "DATA_DIR = \"/mnt/cluster_storage/hymenoptera_data\"\n",
    "\n",
    "# Data augmentation and normalization for training\n",
    "# Just normalization for validation\n",
    "input_size = 224\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(input_size),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(input_size),\n",
    "        transforms.CenterCrop(input_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "torch_datasets = dict()\n",
    "for split in [\"train\", \"val\"]:\n",
    "    torch_datasets[split] = datasets.ImageFolder(os.path.join(DATA_DIR, split), data_transforms[split])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will transform our ImageFolder dataset into a Ray dataset, which will partition the whole dataset and distribute the data blocks across the nodes in cluster. You will benefit from faster parallel pre-processing and data ingestion.\n",
    "\n",
    "Note that **batch** here refers to the chunk of data that the map function will execute on, not the batch we use for model training. To learn more about writing functions for {meth}`map_batches <ray.data.Dataset.map_batches>`, read [writing user-defined functions](https://docs.ray.io/en/latest/data/transforming-datasets.html#transform-datasets-writing-udfs) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-05 18:14:51,368\tINFO worker.py:1352 -- Connecting to existing Ray cluster at address: 10.0.7.113:6379...\n",
      "2023-02-05 18:14:51,376\tINFO worker.py:1529 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2023-02-05 18:14:51,380\tINFO packaging.py:373 -- Pushing file package 'gcs://_ray_pkg_e5286d4376f7908c7b2027efcd21fa25.zip' (0.13MiB) to Ray cluster...\n",
      "2023-02-05 18:14:51,381\tINFO packaging.py:386 -- Successfully pushed file package 'gcs://_ray_pkg_e5286d4376f7908c7b2027efcd21fa25.zip'.\n",
      "Map_Batches:   0%|          | 0/244 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(scheduler +5s) Tip: use `ray status` to view detailed cluster status. To disable these messages, set RAY_SCHEDULER_EVENTS=0.\n",
      "(scheduler +5s) Warning: The following resource request cannot be scheduled right now: {'CPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map_Batches: 100%|██████████| 244/244 [00:04<00:00, 49.58it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: extension<arrow.py_extension_type<ArrowTensorType>>\n",
      "label: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map_Batches: 100%|██████████| 153/153 [00:00<00:00, 307.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: extension<arrow.py_extension_type<ArrowTensorType>>\n",
      "label: int64\n"
     ]
    }
   ],
   "source": [
    "import ray\n",
    "\n",
    "def convert_batch_to_numpy(batch):\n",
    "    images = np.array([image.numpy() for image, _ in batch])\n",
    "    labels = np.array([label for _, label in batch])\n",
    "    return {\"image\": images, \"label\": labels}\n",
    "\n",
    "ray_datasets = dict()\n",
    "for split in [\"train\", \"val\"]:\n",
    "    ray_datasets[split] = ray.data.from_torch(torch_datasets[split]).map_batches(convert_batch_to_numpy)\n",
    "    print(ray_datasets[split].schema())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Model and Fine-tuning configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model(num_classes):\n",
    "    # Load pretrained model params\n",
    "    model = models.resnet18(pretrained=True)\n",
    "\n",
    "    # Replace the original classifier with a new Linear layer\n",
    "    num_features = model.fc.in_features\n",
    "    model.fc = nn.Linear(num_features, num_classes)\n",
    "\n",
    "    # Ensure all params get updated during fintuning\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = True\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = dict()\n",
    "\n",
    "# Input image size (224 x 224)\n",
    "configs[\"input_size\"] = 224\n",
    "\n",
    "# Number of label classes\n",
    "configs[\"num_classes\"] = 2\n",
    "\n",
    "# Batch size for training (change depending on how much memory you have)\n",
    "configs[\"batch_size\"] = 8\n",
    "\n",
    "# Number of epochs to train for\n",
    "configs[\"num_epochs\"] = 15\n",
    "\n",
    "# Hyper-parameters for optimizer\n",
    "configs[\"lr\"] = 0.001\n",
    "configs[\"momentum\"] = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define and Run Training Loop\n",
    "\n",
    "The `train_loop_per_worker` function handles the training and validation of a given model.\n",
    "1. Load dataset shard for each worker:\n",
    "- A ray trainer will take a dictionary of ray datasets as input, the one denoted by the \"train\" key will be automatically be split into multiple dataset shards that can then be accessed by `session.get_dataset_shard(\"train\")`. All other datasets will not be split.\n",
    "- One can use {meth}`iter_torch_batches <ray.data.Dataset.iter_torch_batches>` to iterate the datasets with automatic tensor batching. If you need more flexible customized batching function, please refer to {meth}`iter_batches <ray.data.Dataset.iter_batches>`, which is a lower-level API.\n",
    "2. Prepare your model:\n",
    "- `train.torch.prepare_model` will prepares the model for distributed execution. It will transform your model into DistributedDataParallel under the hood to synchronize gradients and buffers.\n",
    "3. Report metrics and checkpoint:\n",
    "- `session.report` will gather the metrics from each worker and save into log files.\n",
    "- The best checkpoints will be saved according to the reported metrics specified in {class}`CheckpointConfig <ray.air.config.CheckpointConfig>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray.train as train\n",
    "from ray.air import session\n",
    "from ray.train.torch import TorchCheckpoint\n",
    "\n",
    "def evaluate(logits, labels):\n",
    "    _, preds = torch.max(logits, 1)\n",
    "    corrects = torch.sum(preds == labels).item()\n",
    "    return corrects\n",
    "\n",
    "def train_loop_per_worker(configs):\n",
    "    # Prepare dataloader for each worker\n",
    "    datasets = dict()\n",
    "    datasets[\"train\"] = session.get_dataset_shard(\"train\")\n",
    "    datasets[\"val\"] = session.get_dataset_shard(\"val\")\n",
    "\n",
    "    # Calculate the batch size for a single worker\n",
    "    worker_batch_size = configs[\"batch_size\"] // session.get_world_size()\n",
    "\n",
    "    device = \"cpu\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda:0\"\n",
    "\n",
    "    # Prepare DDP Model, optimizer, and loss function\n",
    "    model = initialize_model(num_classes=configs[\"num_classes\"])\n",
    "    model = train.torch.prepare_model(model)\n",
    "\n",
    "    optimizer = optim.SGD(model.parameters(), lr=configs[\"lr\"], momentum=configs[\"momentum\"])\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Start training loops\n",
    "    for epoch in range(configs[\"num_epochs\"]):\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Create a dataset iterator for the shard on the current worker\n",
    "            dataset_iterator = datasets[phase].iter_torch_batches(batch_size=worker_batch_size)\n",
    "            for batch in dataset_iterator:\n",
    "                inputs = batch[\"image\"].to(device)\n",
    "                labels = batch[\"label\"].to(device)\n",
    "            \n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    # Get model outputs and calculate loss\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # calculate statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += evaluate(outputs, labels)\n",
    "\n",
    "            epoch_loss = running_loss / datasets[phase].count()\n",
    "            epoch_acc = running_corrects / datasets[phase].count()\n",
    "\n",
    "            print('Epoch {}-{} Loss: {:.4f} Acc: {:.4f}'.format(epoch, phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # Report metrics and checkpoint every epoch\n",
    "            if phase == \"val\":\n",
    "                checkpoint = TorchCheckpoint.from_dict(\n",
    "                    {\n",
    "                        \"epoch\": epoch,\n",
    "                        \"model\": model.module.state_dict(),\n",
    "                        \"optimizer_state_dict\": optimizer.state_dict()\n",
    "                    }\n",
    "                )\n",
    "                session.report(metrics={\"loss\": epoch_loss, \"acc\": epoch_acc}, checkpoint=checkpoint)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, setup the TorchTrainer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.train.torch import TorchTrainer, TorchCheckpoint\n",
    "from ray.air.config import ScalingConfig, RunConfig, CheckpointConfig\n",
    "from ray.tune.syncer import SyncConfig\n",
    "\n",
    "# Scale out model training across 4 GPUs.\n",
    "scaling_config = ScalingConfig(num_workers=2, use_gpu=True, resources_per_worker={\"CPU\": 4, \"GPU\": 1})\n",
    "\n",
    "# Save the best checkpoint with highest validation accuracy \n",
    "checkpoint_config = CheckpointConfig(num_to_keep=1, checkpoint_score_attribute=\"acc\", checkpoint_score_order=\"max\")\n",
    "\n",
    "# Set experiment name and checkpoint configs\n",
    "run_config = RunConfig(\n",
    "    name=\"resnet-finetune\",\n",
    "    local_dir=\"/mnt/cluster_storage/ray_results\",  # Use shared filesystem for checkpointing, no sync required\n",
    "    sync_config=SyncConfig(syncer=None),\n",
    "    checkpoint_config=checkpoint_config\n",
    ")\n",
    "\n",
    "trainer = TorchTrainer(\n",
    "    train_loop_per_worker=train_loop_per_worker,\n",
    "    train_loop_config=configs,\n",
    "    scaling_config=scaling_config,\n",
    "    run_config=run_config,\n",
    "    datasets=ray_datasets,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2023-02-05 18:15:51</td></tr>\n",
       "<tr><td>Running for: </td><td>00:00:52.27        </td></tr>\n",
       "<tr><td>Memory:      </td><td>10.3/62.0 GiB      </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using FIFO scheduling algorithm.<br>Resources requested: 0/48 CPUs, 0/3 GPUs, 0.0/123.43 GiB heap, 0.0/55.53 GiB objects\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th>status    </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    loss</th><th style=\"text-align: right;\">     acc</th><th style=\"text-align: right;\">  _timestamp</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>TorchTrainer_0e7c2_00000</td><td>TERMINATED</td><td>10.0.34.113:14558</td><td style=\"text-align: right;\">    15</td><td style=\"text-align: right;\">         46.7192</td><td style=\"text-align: right;\">0.236638</td><td style=\"text-align: right;\">0.921569</td><td style=\"text-align: right;\">  1675649747</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=14635, ip=10.0.34.113) 2023-02-05 18:15:04,714\tINFO config.py:86 -- Setting up process group for: env:// [rank=0, world_size=2]\n",
      "(RayTrainWorker pid=14635, ip=10.0.34.113) /home/ray/anaconda3/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "(RayTrainWorker pid=14635, ip=10.0.34.113)   warnings.warn(\n",
      "(RayTrainWorker pid=14635, ip=10.0.34.113) /home/ray/anaconda3/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "(RayTrainWorker pid=14635, ip=10.0.34.113)   warnings.warn(msg)\n",
      "(RayTrainWorker pid=18732) /home/ray/anaconda3/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "(RayTrainWorker pid=18732)   warnings.warn(\n",
      "(RayTrainWorker pid=18732) /home/ray/anaconda3/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "(RayTrainWorker pid=18732)   warnings.warn(msg)\n",
      "(RayTrainWorker pid=14635, ip=10.0.34.113) 2023-02-05 18:15:05,366\tINFO train_loop_utils.py:270 -- Moving model to device: cuda:0\n",
      "(RayTrainWorker pid=18732) 2023-02-05 18:15:05,367\tINFO train_loop_utils.py:270 -- Moving model to device: cuda:0\n",
      "(RayTrainWorker pid=18732) 2023-02-05 18:15:06,975\tINFO train_loop_utils.py:330 -- Wrapping provided model in DistributedDataParallel.\n",
      "(RayTrainWorker pid=14635, ip=10.0.34.113) 2023-02-05 18:15:06,985\tINFO train_loop_utils.py:330 -- Wrapping provided model in DistributedDataParallel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=14635, ip=10.0.34.113) Epoch 0-train Loss: 0.5568 Acc: 0.6803\n",
      "(RayTrainWorker pid=18732) Epoch 0-train Loss: 0.7022 Acc: 0.6066\n",
      "(RayTrainWorker pid=18732) Epoch 0-val Loss: 0.2785 Acc: 0.8954\n",
      "(RayTrainWorker pid=14635, ip=10.0.34.113) Epoch 0-val Loss: 0.2785 Acc: 0.8954\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"trialProgress\">\n",
       "  <h3>Trial Progress</h3>\n",
       "  <table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th style=\"text-align: right;\">  _time_this_iter_s</th><th style=\"text-align: right;\">  _timestamp</th><th style=\"text-align: right;\">  _training_iteration</th><th style=\"text-align: right;\">     acc</th><th>date               </th><th>done  </th><th>episodes_total  </th><th>experiment_id                   </th><th style=\"text-align: right;\">  experiment_tag</th><th>hostname      </th><th style=\"text-align: right;\">  iterations_since_restore</th><th style=\"text-align: right;\">    loss</th><th>node_ip    </th><th style=\"text-align: right;\">  pid</th><th>should_checkpoint  </th><th style=\"text-align: right;\">  time_since_restore</th><th style=\"text-align: right;\">  time_this_iter_s</th><th style=\"text-align: right;\">  time_total_s</th><th style=\"text-align: right;\">  timestamp</th><th style=\"text-align: right;\">  timesteps_since_restore</th><th>timesteps_total  </th><th style=\"text-align: right;\">  training_iteration</th><th>trial_id   </th><th style=\"text-align: right;\">  warmup_time</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>TorchTrainer_0e7c2_00000</td><td style=\"text-align: right;\">            2.69991</td><td style=\"text-align: right;\">  1675649747</td><td style=\"text-align: right;\">                   15</td><td style=\"text-align: right;\">0.921569</td><td>2023-02-05_18-15-49</td><td>True  </td><td>                </td><td>6527e0b6261b4a24814e1f0bb5fedc3a</td><td style=\"text-align: right;\">               0</td><td>ip-10-0-34-113</td><td style=\"text-align: right;\">                        15</td><td style=\"text-align: right;\">0.236638</td><td>10.0.34.113</td><td style=\"text-align: right;\">14558</td><td>True               </td><td style=\"text-align: right;\">             46.7192</td><td style=\"text-align: right;\">           2.58934</td><td style=\"text-align: right;\">       46.7192</td><td style=\"text-align: right;\"> 1675649749</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                  15</td><td>0e7c2_00000</td><td style=\"text-align: right;\">     0.187365</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".trialProgress {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".trialProgress h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".trialProgress td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=14635, ip=10.0.34.113) Epoch 1-train Loss: 0.2617 Acc: 0.8934\n",
      "(RayTrainWorker pid=18732) Epoch 1-train Loss: 0.3784 Acc: 0.8443\n",
      "(RayTrainWorker pid=18732) Epoch 1-val Loss: 0.3210 Acc: 0.8693\n",
      "(RayTrainWorker pid=14635, ip=10.0.34.113) Epoch 1-val Loss: 0.3210 Acc: 0.8693\n",
      "(RayTrainWorker pid=18732) Epoch 2-train Loss: 0.2643 Acc: 0.9098\n",
      "(RayTrainWorker pid=14635, ip=10.0.34.113) Epoch 2-train Loss: 0.2164 Acc: 0.9262\n",
      "(RayTrainWorker pid=14635, ip=10.0.34.113) Epoch 2-val Loss: 0.3782 Acc: 0.8497\n",
      "(RayTrainWorker pid=18732) Epoch 2-val Loss: 0.3782 Acc: 0.8497\n",
      "(RayTrainWorker pid=18732) Epoch 3-train Loss: 0.2506 Acc: 0.9262\n",
      "(RayTrainWorker pid=14635, ip=10.0.34.113) Epoch 3-train Loss: 0.3308 Acc: 0.8525\n",
      "(RayTrainWorker pid=18732) Epoch 3-val Loss: 0.5937 Acc: 0.7908\n",
      "(RayTrainWorker pid=14635, ip=10.0.34.113) Epoch 3-val Loss: 0.5937 Acc: 0.7908\n",
      "(RayTrainWorker pid=18732) Epoch 4-train Loss: 0.3079 Acc: 0.8934\n",
      "(RayTrainWorker pid=14635, ip=10.0.34.113) Epoch 4-train Loss: 0.6821 Acc: 0.7951\n",
      "(RayTrainWorker pid=14635, ip=10.0.34.113) Epoch 4-val Loss: 0.5834 Acc: 0.8039\n",
      "(RayTrainWorker pid=18732) Epoch 4-val Loss: 0.5834 Acc: 0.8039\n",
      "(RayTrainWorker pid=14635, ip=10.0.34.113) Epoch 5-train Loss: 1.1389 Acc: 0.7213\n",
      "(RayTrainWorker pid=18732) Epoch 5-train Loss: 0.3934 Acc: 0.9180\n",
      "(RayTrainWorker pid=18732) Epoch 5-val Loss: 0.3274 Acc: 0.8954\n",
      "(RayTrainWorker pid=14635, ip=10.0.34.113) Epoch 5-val Loss: 0.3274 Acc: 0.8954\n",
      "(RayTrainWorker pid=14635, ip=10.0.34.113) Epoch 6-train Loss: 1.0568 Acc: 0.7541\n",
      "(RayTrainWorker pid=18732) Epoch 6-train Loss: 0.2587 Acc: 0.9180\n",
      "(RayTrainWorker pid=18732) Epoch 6-val Loss: 0.3381 Acc: 0.9085\n",
      "(RayTrainWorker pid=14635, ip=10.0.34.113) Epoch 6-val Loss: 0.3381 Acc: 0.9085\n",
      "(RayTrainWorker pid=18732) Epoch 7-train Loss: 0.2482 Acc: 0.9262\n",
      "(RayTrainWorker pid=14635, ip=10.0.34.113) Epoch 7-train Loss: 0.8222 Acc: 0.7787\n",
      "(RayTrainWorker pid=14635, ip=10.0.34.113) Epoch 7-val Loss: 0.2623 Acc: 0.9085\n",
      "(RayTrainWorker pid=18732) Epoch 7-val Loss: 0.2623 Acc: 0.9085\n",
      "(RayTrainWorker pid=14635, ip=10.0.34.113) Epoch 8-train Loss: 0.7468 Acc: 0.7869\n",
      "(RayTrainWorker pid=18732) Epoch 8-train Loss: 0.2113 Acc: 0.9344\n",
      "(RayTrainWorker pid=18732) Epoch 8-val Loss: 0.3076 Acc: 0.8758\n",
      "(RayTrainWorker pid=14635, ip=10.0.34.113) Epoch 8-val Loss: 0.3076 Acc: 0.8758\n",
      "(RayTrainWorker pid=18732) Epoch 9-train Loss: 0.1849 Acc: 0.9426\n",
      "(RayTrainWorker pid=14635, ip=10.0.34.113) Epoch 9-train Loss: 0.5654 Acc: 0.7705\n",
      "(RayTrainWorker pid=14635, ip=10.0.34.113) Epoch 9-val Loss: 0.2642 Acc: 0.9085\n",
      "(RayTrainWorker pid=18732) Epoch 9-val Loss: 0.2642 Acc: 0.9085\n",
      "(RayTrainWorker pid=14635, ip=10.0.34.113) Epoch 10-train Loss: 0.4190 Acc: 0.7951\n",
      "(RayTrainWorker pid=18732) Epoch 10-train Loss: 0.1360 Acc: 0.9344\n",
      "(RayTrainWorker pid=14635, ip=10.0.34.113) Epoch 10-val Loss: 0.3112 Acc: 0.9085\n",
      "(RayTrainWorker pid=18732) Epoch 10-val Loss: 0.3112 Acc: 0.9085\n",
      "(RayTrainWorker pid=18732) Epoch 11-train Loss: 0.1088 Acc: 0.9590\n",
      "(RayTrainWorker pid=14635, ip=10.0.34.113) Epoch 11-train Loss: 0.3833 Acc: 0.8361\n",
      "(RayTrainWorker pid=18732) Epoch 11-val Loss: 0.2442 Acc: 0.9216\n",
      "(RayTrainWorker pid=14635, ip=10.0.34.113) Epoch 11-val Loss: 0.2442 Acc: 0.9216\n",
      "(RayTrainWorker pid=14635, ip=10.0.34.113) Epoch 12-train Loss: 0.3481 Acc: 0.8689\n",
      "(RayTrainWorker pid=18732) Epoch 12-train Loss: 0.1190 Acc: 0.9672\n",
      "(RayTrainWorker pid=14635, ip=10.0.34.113) Epoch 12-val Loss: 0.2543 Acc: 0.9085\n",
      "(RayTrainWorker pid=18732) Epoch 12-val Loss: 0.2543 Acc: 0.9085\n",
      "(RayTrainWorker pid=18732) Epoch 13-train Loss: 0.1122 Acc: 0.9590\n",
      "(RayTrainWorker pid=14635, ip=10.0.34.113) Epoch 13-train Loss: 0.3194 Acc: 0.8689\n",
      "(RayTrainWorker pid=18732) Epoch 13-val Loss: 0.2809 Acc: 0.8889\n",
      "(RayTrainWorker pid=14635, ip=10.0.34.113) Epoch 13-val Loss: 0.2809 Acc: 0.8889\n",
      "(RayTrainWorker pid=18732) Epoch 14-train Loss: 0.1137 Acc: 0.9508\n",
      "(RayTrainWorker pid=14635, ip=10.0.34.113) Epoch 14-train Loss: 0.2544 Acc: 0.8689\n",
      "(RayTrainWorker pid=18732) Epoch 14-val Loss: 0.2366 Acc: 0.9216\n",
      "(RayTrainWorker pid=14635, ip=10.0.34.113) Epoch 14-val Loss: 0.2366 Acc: 0.9216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-05 18:15:51,839\tINFO tune.py:762 -- Total run time: 52.49 seconds (52.24 seconds for the tuning loop).\n"
     ]
    }
   ],
   "source": [
    "result = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training procedure completed in 53 seconds, it saved the best checkpoint in the `local_dir` provided to the trainer. You can now check the experiment metrics and checkpoint information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result(metrics={'loss': 0.23663791493524863, 'acc': 0.9215686274509803, '_timestamp': 1675649747, '_time_this_iter_s': 2.6999073028564453, '_training_iteration': 15, 'should_checkpoint': True, 'done': True, 'trial_id': '0e7c2_00000', 'experiment_tag': '0'}, error=None, log_dir=PosixPath('/mnt/cluster_storage/ray_results/resnet-finetune/TorchTrainer_0e7c2_00000_0_2023-02-05_18-14-59'))\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the checkpoint for batch prediction:\n",
    "\n",
    "TorchTrainer has already saved the best model parameters in `log_dir`. Now we want to load this model into memory and perform batch prediction and evaluation on test data.\n",
    "`TorchCheckpoint.from_directory` will automatically extract pickled params. BatchPredictor will identify the dict key \"model\", and load the corresponding parameters into model. You can also specify the \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ray/anaconda3/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/ray/anaconda3/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from ray.train.batch_predictor import BatchPredictor\n",
    "from ray.train.torch import TorchCheckpoint, TorchPredictor\n",
    "\n",
    "ckpt = TorchCheckpoint.from_directory(\"/mnt/cluster_storage/ray_results/resnet-finetune/TorchTrainer_0e7c2_00000_0_2023-02-05_18-14-59/checkpoint_000014\")\n",
    "predictor = BatchPredictor.from_checkpoint(ckpt, TorchPredictor, model=initialize_model(configs[\"num_classes\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-05 18:16:44,629\tWARNING compute.py:549 -- Requested batch size 4096 results in batches of 2466299904 bytes for map_batches tasks, which is larger than the configured target max block size 536870912. This may result in out-of-memory errors for certain workloads, and you may want to decrease your batch size or increase the configured target max block size, e.g.: from ray.data.context import DatasetContext; DatasetContext.get_current().target_max_block_size = 4_000_000_000\n",
      "2023-02-05 18:16:44,630\tWARNING compute.py:520 -- `batch_size` is set to 4096, which reduces parallelism from 153 to 1. If the performance is worse than expected, this may indicate that the batch size is too large or the input block size is too small. To reduce batch size, consider decreasing `batch_size` or use the default in `map_batches`. To increase input block size, consider decreasing `parallelism` in read.\n",
      "Map_Batches:   0%|          | 0/1 [00:00<?, ?it/s]2023-02-05 18:16:45,238\tWARNING worker.py:1851 -- Warning: The actor BlockWorker is very large (42 MiB). Check that its definition is not implicitly capturing a large array or other object in scope. Tip: use ray.put() to put large objects in the Ray object store.\n",
      "Map Progress (3 actors 1 pending): 100%|██████████| 1/1 [00:13<00:00, 13.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions: extension<arrow.py_extension_type<ArrowTensorType>>\n",
      "label: int64\n",
      "[{'predictions': array([ 1.525892 , -2.5796869], dtype=float32), 'label': 0}, {'predictions': array([ 1.4257711 , -0.95987207], dtype=float32), 'label': 0}, {'predictions': array([ 2.4064574, -3.74627  ], dtype=float32), 'label': 0}, {'predictions': array([ 1.2741024, -1.4860458], dtype=float32), 'label': 0}, {'predictions': array([ 4.2864375, -3.927444 ], dtype=float32), 'label': 0}]\n"
     ]
    }
   ],
   "source": [
    "prediction_ds = predictor.predict(ray_datasets[\"val\"], feature_columns=[\"image\"], keep_columns=[\"label\"])\n",
    "print(prediction_ds.schema())\n",
    "print(prediction_ds.take(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate predictions results\n",
    "The BatchPredictor returns a ray dataset as result, which consists a column of `predictions` and the columns specified by `keep_columns` argument. The `predictions` column contains the model's tensor output. Here we define a function `convert_logits_to_classes` to convert tensor outputs to labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map_Batches: 100%|██████████| 1/1 [00:00<00:00, 63.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prediction': 0, 'label': 0, 'correct': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Shuffle Map: 100%|██████████| 1/1 [00:00<00:00, 111.99it/s]\n",
      "Shuffle Reduce: 100%|██████████| 1/1 [00:00<00:00,  7.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Accuracy =  0.9215686274509803\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd \n",
    "\n",
    "def convert_logits_to_classes(df):\n",
    "    pred_class = df[\"predictions\"].map(lambda x: x.argmax())\n",
    "    df[\"prediction\"] = pred_class\n",
    "    df[\"correct\"] = df[\"prediction\"] == df[\"label\"]\n",
    "    return df[[\"prediction\", \"label\", \"correct\"]]\n",
    "\n",
    "predictions = prediction_ds.map_batches(convert_logits_to_classes)\n",
    "predictions.show(1)\n",
    "\n",
    "print(\"Evaluation Accuracy = \", predictions.mean(on=\"correct\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of rewriting a new evaluation function in pandas format, one can also reuse the evaluation function they used in the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Accuracy =  0.9215686274509803\n"
     ]
    }
   ],
   "source": [
    "def evaluate(logits, labels):\n",
    "    _, preds = torch.max(logits, 1)\n",
    "    corrects = torch.sum(preds == labels).item()\n",
    "    return corrects\n",
    "\n",
    "accuracy = 0\n",
    "for batch in prediction_ds.iter_torch_batches(batch_size=10):\n",
    "    accuracy += evaluate(batch[\"predictions\"], batch[\"label\"])\n",
    "accuracy /= prediction_ds.count()\n",
    "\n",
    "print(\"Evaluation Accuracy = \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example is adapted from Pytorch's [Fintuning Torchvision Models](https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html) tutorial."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
