{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Tabular workload pattern\n\n<div align=\"left\">\n<a target=\"_blank\" href=\"https://console.anyscale.com/template-preview/ray_train_workloads?file=%252Ffiles%252Fworkload-patterns%252F04b_tabular_workload_pattern.ipynb\"><img src=\"https://img.shields.io/badge/ðŸš€ Run_on-Anyscale-9hf\"></a>&nbsp;\n<a href=\"https://github.com/ray-project/ray/tree/master/doc/source/train/tutorials/content/workload-patterns/04b_tabular_workload_pattern.ipynb\" role=\"button\"><img src=\"https://img.shields.io/static/v1?label=&amp;message=View%20On%20GitHub&amp;color=586069&amp;logo=github&amp;labelColor=2f363d\"></a>&nbsp;\n</div>\n\nIn this tutorial you take the classic **Cover type forest-cover dataset** (580 k rows, 54 tabular features) and scale an **XGBoost** model across an Anyscale cluster using **Ray Train**.\n\n## Learning objectives\n\n- Ingest tabular data at scale using **Ray Data** and persist it to Parquet for reproducibility  \n- Launch a fault-tolerant, checkpoint enabled **XGBoost training loop** on multiple CPUs using **Ray Train**  \n- Resume training from checkpoints for protection against job restarts and hardware failures  \n- Evaluate model accuracy, visualize feature importance, and scale batch inference using **Ray Data**  \n- Understand how to port classic gradient boosting workflows into a **fully distributed, multi-node training setup on Anyscale**"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What problem are you solving? (Forest cover classification with XGBoost)\n",
    "\n",
    "You're predicting which **type of forest vegetation** (for example, Lodge-pole Pine, Spruce/Fir, Aspen) is present at a given land location, using only numeric and binary cartographic features such as elevation, slope, soil type, and proximity to roads or hydrology.\n",
    "\n",
    "---\n",
    "\n",
    "## What's XGBoost?\n",
    "\n",
    "**XGBoost** (Extreme Gradient Boosting) is a fast, scalable machine learning algorithm based on **gradient-boosted decision trees**. It builds a sequence of shallow decision trees, where each new tree tries to correct the errors of the previous ensemble by minimizing a differentiable loss (like log-loss).\n",
    "\n",
    "In your case, minimize the **multi-class Softmax log-loss**, learning a function:\n",
    "\n",
    "$$\n",
    "f_\\theta: \\mathbb{R}^{54} \\rightarrow \\{0, 1, \\dots, 6\\}\n",
    "$$\n",
    "\n",
    "that maps a 54-dimensional tabular input (raw geo-spatial features) to a forest cover type. Each boosting round fits a new tree on the gradient of the loss, gradually improving accuracy over hundreds of rounds.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to migrate this tabular workload to a distributed setup using Ray on Anyscale\n",
    "\n",
    "This tutorial walks through the end-to-end process of **migrating a local XGBoost training pipeline to a distributed Ray cluster running on Anyscale**.\n",
    "\n",
    "The following steps make that transition:\n",
    "\n",
    "1. **Store local data as remote data**  \n",
    "   Store the raw data as Parquet in a shared cloud directory and load it using **Ray Data**, which streams and shards the dataset across workers automatically.\n",
    "\n",
    "2. **Convert a single-process to multi-worker training**  \n",
    "   Define a custom `train_func`, then let **Ray Train** spin up 16 distributed training workers (1 per CPU) and run `xgb.train` in parallel, each with its own data shard.\n",
    "\n",
    "3. **Configure Ray for automated fault tolerance**  \n",
    "   With `RayTrainReportCallback` and `CheckpointConfig`, Ray saves checkpoints every 10 boosting rounds and can resume mid-training if any worker crashes or a job is re-launched.\n",
    "\n",
    "4. **Use Ray's cluster-scale abstractions**  \n",
    "   Skip the boilerplate of manually slicing datasets, coordinating workers, or building launch scripts. Instead, declare intent (with `ScalingConfig`, `RunConfig`, and `FailureConfig`) and let **Ray and Anyscale** manage the execution.\n",
    "\n",
    "5. **Use offline inference**  \n",
    "   Batch inference is done with **Ray Data** on CPU workers. This is useful for seamlessly evolving the pipeline to large-scale production environments.\n",
    "\n",
    "This pattern turns a traditional single-node workflow into a scalable, resilient training pipeline with minimal code changes, and it works seamlessly on any cluster you provision through Anyscale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports  \n",
    "Before you touch any data, import every tool you need.  \n",
    "Alongside the standard scientific-Python stack, bring in **XGBoost** for gradient-boosted decision trees and **Ray** for distributed data loading and training. Ray Trainâ€™s helper classes (RunConfig, ScalingConfig, CheckpointConfig, FailureConfig) give you fault-tolerant, CPU training with almost no extra code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 00. Runtime setup \n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "# Non-secret env var \n",
    "os.environ[\"RAY_TRAIN_V2_ENABLED\"] = \"1\"\n",
    "\n",
    "# Install Python dependencies \n",
    "subprocess.check_call([\n",
    "    sys.executable, \"-m\", \"pip\", \"install\", \"--no-cache-dir\",\n",
    "    \"matplotlib==3.10.6\",\n",
    "    \"scikit-learn==1.7.2\",\n",
    "    \"pyarrow==14.0.2\",    \n",
    "    \"xgboost==3.0.5\",\n",
    "    \"seaborn==0.13.2\",\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 01. Imports\n",
    "import os\n",
    "import shutil\n",
    "import json\n",
    "import uuid\n",
    "import tempfile\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import fetch_covtype\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "import pyarrow as pa\n",
    "\n",
    "import ray\n",
    "import ray.data as rd\n",
    "from ray.data import ActorPoolStrategy\n",
    "from ray.train import RunConfig, ScalingConfig, CheckpointConfig, FailureConfig, get_dataset_shard, get_checkpoint, get_context\n",
    "from ray.train.xgboost import XGBoostTrainer, RayTrainReportCallback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load the University of California, Irvine (UCI) Cover type dataset  \n",
    "The Cover type dataset contains ~580,000 forest-cover observations with 54 tabular features and a 7-class label. Fetch it from `sklearn.datasets`, rename the target column to `label` (Ray's default), and shift the classes from **1-7** to **0-6** so they're zero-indexed as XGBoost expects. A quick `value_counts` sanity-check confirms the mapping worked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 02. Load the UCI Cover type dataset (~580k rows, 54 features)\n",
    "data = fetch_covtype(as_frame=True)\n",
    "df = data.frame\n",
    "df.rename(columns={\"Cover_Type\": \"label\"}, inplace=True)   # Ray expects \"label\"\n",
    "df[\"label\"] = df[\"label\"] - 1          # 1-7  â†’  0-6\n",
    "assert df[\"label\"].between(0, 6).all()\n",
    "print(df.shape, df.label.value_counts(normalize=True).head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Visualize class balance  \n",
    "Highly imbalanced targets can bias tree-based models, so plot the raw label counts. The cover type distribution shows skew, but not muchâ€”the bar chart lets you judge whether extra re-scaling or class-weighting is necessary. Rely on XGBoostâ€™s built-in handling for this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 03. Visualize class distribution\n",
    "df.label.value_counts().plot(kind=\"bar\", figsize=(6,3), title=\"Cover Type distribution\")\n",
    "plt.ylabel(\"Frequency\"); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Write train / validation Parquet files  \n",
    "\n",
    "Rather than splitting a large dataset in memory later, you persist **train** and **validation** splits up front.  \n",
    "Each split is written to the clusterâ€™s shared volume (`/mnt/cluster_storage`) so that all Ray workers can access it directly.  \n",
    "This approach keeps the workflow reproducible and avoids rematerializing the dataset during distributed training.  \n",
    "\n",
    "You perform a **stratified 80 / 20 split** to preserve class balance across splits, then write each subset to its own Parquet file.  \n",
    "Parquet is columnar and compressed, making it ideal for Ray Data ingestion and parallel reads.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 04. Write separate train/val Parquets to /mnt/cluster_storage/covtype/\n",
    "\n",
    "PARQUET_DIR = \"/mnt/cluster_storage/covtype/parquet\"\n",
    "os.makedirs(PARQUET_DIR, exist_ok=True)\n",
    "\n",
    "TRAIN_PARQUET = os.path.join(PARQUET_DIR, \"train.parquet\")\n",
    "VAL_PARQUET   = os.path.join(PARQUET_DIR, \"val.parquet\")\n",
    "\n",
    "# Stratified 80/20 split for reproducibility\n",
    "train_df, val_df = train_test_split(\n",
    "    df, test_size=0.2, random_state=42, stratify=df[\"label\"]\n",
    ")\n",
    "\n",
    "train_df.to_parquet(TRAIN_PARQUET, index=False)\n",
    "val_df.to_parquet(VAL_PARQUET, index=False)\n",
    "\n",
    "print(f\"Wrote Train â†’ {TRAIN_PARQUET} ({len(train_df):,} rows)\")\n",
    "print(f\"Wrote Val   â†’ {VAL_PARQUET}   ({len(val_df):,} rows)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load the train and validation splits as Ray Datasets  \n",
    "\n",
    "Now that the data is stored in Parquet, you load each split directly with `ray.data.read_parquet`.  \n",
    "Each call returns a **lazy, columnar Ray Dataset** that supports distributed reads and transformations across the cluster.  \n",
    "\n",
    "Calling `.random_shuffle()` on the training split ensures balanced sampling during training,  \n",
    "while leaving the validation split unshuffled preserves its deterministic order for evaluation.  \n",
    "\n",
    "From this point forward, all data access is **parallel and streaming**, eliminating single-node I/O bottlenecks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 05. Load the two splits as Ray Datasets (lazy, columnar)\n",
    "train_ds = rd.read_parquet(TRAIN_PARQUET).random_shuffle()\n",
    "val_ds   = rd.read_parquet(VAL_PARQUET)\n",
    "\n",
    "print(train_ds)\n",
    "print(val_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Inspect dataset sizes (optional)\n",
    "\n",
    "After loading the Parquet files, quickly confirm that both splits were read correctly by counting their rows.  \n",
    "This step triggers a lightweight distributed count across the cluster and verifies that the  \n",
    "**train / validation partitioning** matches the expected 80 / 20 ratio before moving on to distributed training.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Train rows: {train_ds.count():,},  Val rows: {val_ds.count():,}\")  # Note that this will materialize the dataset (skip at scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Inspect a mini-batch  \n",
    "Taking a tiny pandas batch helps verify that feature columns and labels have the expected shapes and types. You also build `feature_columns`, a list you reuse when building XGBoostâ€™s `DMatrix`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 07. Look into one batch to confirm feature dimensionality\n",
    "batch = train_ds.take_batch(batch_size=5, batch_format=\"pandas\")\n",
    "print(batch.head())\n",
    "feature_columns = [c for c in batch.columns if c != \"label\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Define the Ray Train worker loop (Arrow-based, memory-efficient)  \n",
    "\n",
    "Each Ray Train worker runs its own copy of `train_func`.  \n",
    "Inside the loop, the worker pulls its **shard** of the train and validation datasets directly as **Arrow tables**. \n",
    "\n",
    "You then:  \n",
    "1. **Materialize each shard** into a `pyarrow.Table` and drop any accidental index columns (like `__index_level_0__`)  \n",
    "   that might have been added during Parquet serialization.  \n",
    "2. **Convert Arrow â†’ NumPy â†’ XGBoost DMatrix** with explicit `feature_names`, ensuring consistent column order  \n",
    "   across all workers and splits.  \n",
    "3. **Optionally resume** from a prior checkpoint using `get_checkpoint()`.  \n",
    "4. **Train the booster** with `xgb.train`, using the built-in `RayTrainReportCallback()` to automatically stream  \n",
    "   per-round metrics and checkpoints back to Ray Train.  \n",
    "\n",
    "This design keeps the data path fully distributed and avoids unnecessary copies or manual metric handling.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INDEX_COLS = {\"__index_level_0__\"}  # extend if needed\n",
    "\n",
    "def _arrow_table_from_shard(name: str) -> pa.Table:\n",
    "    \"\"\"Collect this worker's Ray Dataset shard into one pyarrow. Table and\n",
    "    drop accidental index columns (e.g., from pandas Parquet).\"\"\"\n",
    "    ds_iter = get_dataset_shard(name)\n",
    "    arrow_refs = ds_iter.materialize().to_arrow_refs()\n",
    "    tables = [ray.get(r) for r in arrow_refs]\n",
    "    tbl = pa.concat_tables(tables, promote_options=\"none\") if tables else pa.table({})\n",
    "    # Drop index columns if present\n",
    "    keep = [c for c in tbl.column_names if c not in INDEX_COLS]\n",
    "    if len(keep) != len(tbl.column_names):\n",
    "        tbl = tbl.select(keep)\n",
    "    return tbl\n",
    "\n",
    "def _dmat_from_arrow(table: pa.Table, feature_cols, label_col: str):\n",
    "    \"\"\"Build XGBoost DMatrix from pyarrow.Table with explicit feature_names.\"\"\"\n",
    "    X = np.column_stack([table[c].to_numpy(zero_copy_only=False) for c in feature_cols])\n",
    "    y = table[label_col].to_numpy(zero_copy_only=False)\n",
    "    return xgb.DMatrix(X, label=y, feature_names=feature_cols)\n",
    "\n",
    "def train_func(config):\n",
    "    label_col = config[\"label_column\"]\n",
    "\n",
    "    # Arrow tables \n",
    "    train_arrow = _arrow_table_from_shard(\"train\")\n",
    "    eval_arrow  = _arrow_table_from_shard(\"evaluation\")\n",
    "\n",
    "    # Use the SAME ordered feature list for both splits\n",
    "    feature_cols = [c for c in train_arrow.column_names if c != label_col]\n",
    "\n",
    "    dtrain = _dmat_from_arrow(train_arrow, feature_cols, label_col)\n",
    "    deval  = _dmat_from_arrow(eval_arrow,  feature_cols, label_col)\n",
    "\n",
    "    # -------- 2) Optional resume from checkpoint ------------------------------\n",
    "    ckpt = get_checkpoint()\n",
    "    if ckpt:\n",
    "        with ckpt.as_directory() as d:\n",
    "            model_path = os.path.join(d, RayTrainReportCallback.CHECKPOINT_NAME)\n",
    "            booster = xgb.Booster()\n",
    "            booster.load_model(model_path)\n",
    "            print(f\"[Rank {get_context().get_world_rank()}] Resumed from checkpoint\")\n",
    "    else:\n",
    "        booster = None\n",
    "\n",
    "    # -------- 3) Train with per-round reporting & checkpointing ---------------\n",
    "    evals_result = {}\n",
    "    xgb.train(\n",
    "        params          = config[\"params\"],\n",
    "        dtrain          = dtrain,\n",
    "        evals           = [(dtrain, \"train\"), (deval, \"validation\")],\n",
    "        num_boost_round = config[\"num_boost_round\"],\n",
    "        xgb_model       = booster,\n",
    "        evals_result    = evals_result,\n",
    "        callbacks       = [RayTrainReportCallback()],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Configure XGBoost and build the Trainer  \n",
    "Next, define the XGBoost hyperparameters and wrap the `train_func` in an `XGBoostTrainer` for distributed execution.  \n",
    "Each worker is assigned an entire CPU node (`resources_per_worker={\"CPU\": CPUS_PER_WORKER}`),  \n",
    "allowing XGBoost to use all local cores efficiently through the `nthread` parameter.  \n",
    "\n",
    "Key settings:  \n",
    "- **`ScalingConfig`** â€” controls how many workers to launch and their CPU/GPU allocation.  \n",
    "- **`CheckpointConfig`** â€” saves a checkpoint every 10 boosting rounds and scores each checkpoint by  \n",
    "  validation log-loss (`validation-mlogloss`), retaining only the best model.  \n",
    "- **`FailureConfig`** â€” automatically retries failed workers once for fault tolerance.  \n",
    "\n",
    "By passing the Ray Datasets directly into the trainer, Ray handles dataset sharding and distributed streaming automatically,  \n",
    "so each worker trains on its own slice of the data without manual coordination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 09. XGBoost config and Trainer (full-node CPU workers)\n",
    "\n",
    "# Adjust this to your node size if different (e.g., 16, 32, etc.)\n",
    "CPUS_PER_WORKER = 4\n",
    "\n",
    "xgb_params = {\n",
    "    \"objective\": \"multi:softprob\",\n",
    "    \"num_class\": 7,\n",
    "    \"eval_metric\": \"mlogloss\",\n",
    "    \"tree_method\": \"hist\",\n",
    "    \"eta\": 0.3,\n",
    "    \"max_depth\": 8,\n",
    "    \"nthread\": CPUS_PER_WORKER,  \n",
    "}\n",
    "\n",
    "trainer = XGBoostTrainer(\n",
    "    train_func,\n",
    "    scaling_config=ScalingConfig(\n",
    "        num_workers=2,\n",
    "        use_gpu=False,\n",
    "        resources_per_worker={\"CPU\": CPUS_PER_WORKER},\n",
    "    ),\n",
    "    datasets={\"train\": train_ds, \"evaluation\": val_ds},\n",
    "    train_loop_config={\n",
    "        \"label_column\": \"label\",\n",
    "        \"params\": xgb_params,\n",
    "        \"num_boost_round\": 50,\n",
    "    },\n",
    "    run_config=RunConfig(\n",
    "        name=\"covtype_xgb_cpu\",\n",
    "        storage_path=\"/mnt/cluster_storage/covtype/results\",\n",
    "        checkpoint_config=CheckpointConfig(\n",
    "            num_to_keep=1,\n",
    "            checkpoint_score_attribute=\"validation-mlogloss\",  # score by val loss\n",
    "            checkpoint_score_order=\"min\",\n",
    "        ),\n",
    "        failure_config=FailureConfig(max_failures=1),\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Start distributed training  \n",
    "`trainer.fit()` blocks until all boosting rounds finish, or until Ray exhausts retries.  The result object contains the last reported metrics and the best checkpoint found so far. Print the final validation log-loss and keep a handle to the checkpoint for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. Fit the trainer (reports eval metrics every boosting round)\n",
    "result = trainer.fit()\n",
    "best_ckpt = result.checkpoint            # saved automatically by Trainer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Evaluate the trained model  \n",
    "Pull the XGBoost `Booster` back from the checkpoint, run predictions on the entire validation set, and compute overall accuracy. Converting the Ray Dataset to pandas keeps the example short. In production you stream batches instead of materializing the whole frame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11. Retrieve Booster object from Ray checkpoint\n",
    "booster = RayTrainReportCallback.get_model(best_ckpt)\n",
    "\n",
    "# Convert Ray Dataset to pandas for quick local scoring\n",
    "val_pd = val_ds.to_pandas()\n",
    "dmatrix = xgb.DMatrix(val_pd[feature_columns])\n",
    "pred_prob = booster.predict(dmatrix)\n",
    "pred_labels = np.argmax(pred_prob, axis=1)\n",
    "\n",
    "acc = accuracy_score(val_pd.label, pred_labels)\n",
    "print(f\"Validation accuracy: {acc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Confusion matrix visualization  \n",
    "Raw counts and row-normalized ratios highlight which cover types the model confuses most often. Diagonal dominance indicates good performance; off-diagonal hot spots may suggest a need for more data or feature engineering for those specific classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12. Confusion matrix\n",
    "\n",
    "cm = confusion_matrix(val_pd.label, pred_labels)  # or sample_batch.label if used\n",
    "\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"viridis\")\n",
    "plt.title(\"Confusion Matrix with Counts\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.show()\n",
    "\n",
    "cm_norm = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis]\n",
    "sns.heatmap(cm_norm, annot=True, fmt=\".2f\", cmap=\"viridis\")\n",
    "plt.title(\"Normalized Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. CPU batch inference with Ray Data  \n",
    "\n",
    "Use **Ray Data** for scalable, parallel inference.  \n",
    "Each actor loads the trained model once and processes data batches in parallel,  \n",
    "providing better throughput than ad-hoc remote tasks and avoiding repeated model loads.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13. CPU batch inference with Ray Data \n",
    "\n",
    "# Assumes: val_ds, feature_columns, best_ckpt already defined.\n",
    "\n",
    "class XGBPredictor:\n",
    "    \"\"\"Stateful actor: load Booster once, reuse across batches.\"\"\"\n",
    "    def __init__(self, ckpt, feature_cols):\n",
    "        self.model = RayTrainReportCallback.get_model(ckpt)\n",
    "        self.feature_cols = feature_cols\n",
    "\n",
    "    def __call__(self, batch: pd.DataFrame) -> pd.DataFrame:\n",
    "        dmatrix = xgb.DMatrix(batch[self.feature_cols])\n",
    "        probs = self.model.predict(dmatrix)\n",
    "        preds = np.argmax(probs, axis=1)\n",
    "        return pd.DataFrame(\n",
    "            {\"pred\": preds.astype(np.int32), \"label\": batch[\"label\"].astype(np.int32)}\n",
    "        )\n",
    "\n",
    "# Use an ActorPoolStrategy instead of compute=\"actors\"\n",
    "pred_ds = val_ds.map_batches(\n",
    "    XGBPredictor,\n",
    "    fn_constructor_args=(best_ckpt, feature_columns),\n",
    "    batch_format=\"pandas\",\n",
    "    compute=ActorPoolStrategy(),   \n",
    "    num_cpus=1,                    # per-actor CPU; tune as needed\n",
    ")\n",
    "\n",
    "# Aggregate accuracy without collecting to driver\n",
    "stats_ds = pred_ds.map_batches(\n",
    "    lambda df: pd.DataFrame({\n",
    "        \"correct\": [int((df[\"pred\"].to_numpy() == df[\"label\"].to_numpy()).sum())],\n",
    "        \"n\": [int(len(df))]\n",
    "    }),\n",
    "    batch_format=\"pandas\",\n",
    ")\n",
    "\n",
    "correct = int(stats_ds.sum(\"correct\"))\n",
    "n = int(stats_ds.sum(\"n\"))\n",
    "print(f\"Validation accuracy (Ray Data inference): {correct / n:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Feature-importance diagnostics  \n",
    "XGBoost's built-in `get_score(importance_type=\"gain\")` ranks each feature by its average gain across all splits. Visualizing the top-15 helps connect model behaviour back to domain knowledge. For example, elevation and soil type often dominate forest-cover prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 14. Gainâ€‘based feature importance\n",
    "importances = booster.get_score(importance_type=\"gain\")\n",
    "keys, gains = zip(*sorted(importances.items(), key=lambda kv: kv[1], reverse=True)[:15])\n",
    "\n",
    "plt.barh(range(len(gains)), gains)\n",
    "plt.yticks(range(len(gains)), keys)\n",
    "plt.gca().invert_yaxis()\n",
    "plt.title(\"Top-15 Feature Importances (gain)\"); plt.xlabel(\"Average gain\"); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Continue training from the latest checkpoint  \n",
    "Because `train_func` always checks for `get_checkpoint()`, re-invoking `trainer.fit()` automatically resumes boosting from where you left off. Call `fit()` a second time and print the new best validation log-loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15. Run 50 more training iterations from the last saved checkpoint\n",
    "result = trainer.fit()\n",
    "best_ckpt = result.checkpoint            # Saved automatically by Trainer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Verify post-training inference  \n",
    "\n",
    "Rerun the Ray Data inference pipeline with the latest checkpoint to confirm that  \n",
    "additional boosting rounds improved validation accuracy.  \n",
    "This reuses the same distributed actors, ensuring consistent and scalable evaluation.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 16. Rerun Ray Data inference to verify improved accuracy after continued training\n",
    "\n",
    "# Reuse the existing Ray Data inference setup with the latest checkpoint\n",
    "pred_ds = val_ds.map_batches(\n",
    "    XGBPredictor,\n",
    "    fn_constructor_args=(best_ckpt, feature_columns),\n",
    "    batch_format=\"pandas\",\n",
    "    compute=ActorPoolStrategy(),\n",
    "    num_cpus=1,\n",
    ")\n",
    "\n",
    "# Aggregate accuracy across all batches\n",
    "stats_ds = pred_ds.map_batches(\n",
    "    lambda df: pd.DataFrame({\n",
    "        \"correct\": [int((df[\"pred\"] == df[\"label\"]).sum())],\n",
    "        \"n\": [int(len(df))]\n",
    "    }),\n",
    "    batch_format=\"pandas\",\n",
    ")\n",
    "\n",
    "correct = int(stats_ds.sum(\"correct\"))\n",
    "n = int(stats_ds.sum(\"n\"))\n",
    "print(f\"Validation accuracy after continued training: {correct / n:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17. Clean up  \n",
    "Finally, tidy up by deleting temporary checkpoint folders, the metrics CSV, and any intermediate result directories. Clearing out old artifacts frees disk space and leaves your workspace clean for whatever comes next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 17. Optional cleanup to free space\n",
    "ARTIFACT_DIR = \"/mnt/cluster_storage/covtype\"\n",
    "if os.path.exists(ARTIFACT_DIR):\n",
    "    shutil.rmtree(ARTIFACT_DIR)\n",
    "    print(f\"Deleted {ARTIFACT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrap up and next steps\n",
    "\n",
    "You built a fast and fault-tolerant XGBoost training loop that runs on real data, scales across CPUs, recovers from worker failures, and supports batch inference, all inside a single notebook.\n",
    "\n",
    "This tutorial demonstrates:\n",
    "\n",
    "* Using **Ray Data** to ingest, shuffle, and shard large tabular datasets across a cluster.  \n",
    "* Defining custom `train_func`s that run on **Ray Train** workers and resume seamlessly from checkpoints.  \n",
    "* Tracking per-round metrics and saving checkpoints with **RayTrainReportCallback**.  \n",
    "* Leveraging **Ray's distributed execution model** to evaluate and monitor models without manual orchestration.  \n",
    "* Launching remote CPU-powered inference tasks using **Ray Data** for scalable batch scoring.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Next steps\n",
    "\n",
    "Below are a few directions you might explore to adapt or extend the pattern:\n",
    "\n",
    "1. **Early stopping and best iteration tracking**  \n",
    "   * Add `early_stopping_rounds=10` to `xgb.train` and log the best round.  \n",
    "   * Track performance delta across resumed runs.\n",
    "\n",
    "2. **Hyperparameter sweeps**  \n",
    "   * Wrap the trainer with **Ray Tune** and search over `eta`, `max_depth`, or `subsample`.  \n",
    "   * Use Tune's built-in checkpoint pruning and log callbacks.\n",
    "\n",
    "3. **Feature engineering at scale**  \n",
    "   * Create new features using `Ray Dataset.map_batches`, such as terrain interactions or log-scaled distances.  \n",
    "   * Materialize multiple Parquet shards and benchmark load time.\n",
    "\n",
    "4. **Model interpretability**  \n",
    "   * Use XGBoost's built-in `Booster.get_score` for feature attributions.  \n",
    "   * Rank features by importance and validate with domain knowledge.\n",
    "\n",
    "5. **Serving the model**  \n",
    "   * Package the Booster as a Ray task or **Ray Serve** endpoint.  \n",
    "   * Deploy an API that takes a feature vector and returns the predicted cover type.\n",
    "\n",
    "6. **Real-time logging**  \n",
    "   * Integrate with MLflow or Weights & Biases to store logs, plots, and checkpoints.  \n",
    "   * Use tags and metadata to track experiments over time.\n",
    "\n",
    "7. **Alternative objectives**  \n",
    "   * Try a binary objective (for example, presence versus absence of a species) or regression target (for example, canopy height).  \n",
    "   * Fine-tune loss functions for specific ecological tasks.\n",
    "\n",
    "8. **End-to-end MLOps**  \n",
    "   * Schedule retraining with Ray Jobs or Anyscale Jobs.  \n",
    "   * Upload new data snapshots and trigger daily training runs with automatic checkpoint cleanup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}