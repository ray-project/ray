{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Generative computer vision pattern\n\n<div align=\"left\">\n<a target=\"_blank\" href=\"https://console.anyscale.com/template-preview/ray_train_workloads?file=%252Ffiles%252Fworkload-patterns%252F04d1_generative_cv_pattern.ipynb\"><img src=\"https://img.shields.io/badge/üöÄ Run_on-Anyscale-9hf\"></a>&nbsp;\n<a href=\"https://github.com/ray-project/ray/tree/master/doc/source/train/tutorials/content/workload-patterns/04d1_generative_cv_pattern.ipynb\" role=\"button\"><img src=\"https://img.shields.io/static/v1?label=&amp;message=View%20On%20GitHub&amp;color=586069&amp;logo=github&amp;labelColor=2f363d\"></a>&nbsp;\n</div>\n\nThis notebook builds a **mini diffusion pipeline** on the **Food-101-Lite** dataset and runs it end-to-end on an Anyscale cluster with **Ray Train**.\n\n## Learning objectives  \n* How to use **Ray Data** to decode and preprocess large image datasets in parallel.  \n* How to split and shard datasets for **distributed training** across multiple Ray workers.  \n* How to wrap a custom `LightningModule` with Ray Train to scale out **PyTorch code without boilerplate**.  \n* How to **enable fault tolerance** by saving and restoring model checkpoints.  \n* How to run training and evaluation with **no changes to your core model code** as Ray handles multi-node orchestration.  \n* How to generate images post-training using the same Ray-hosted environment on Anyscale."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What problem are you solving? (Diffusion as image de-noising)\n",
    "\n",
    "You‚Äôre training a **generative model** that learns to produce realistic Red-Green-Blue (RGB) images from pure noise  \n",
    "by learning how to *reverse* a noising process.\n",
    "\n",
    "This approach builds on **de-noising diffusion models**: instead of modeling the full image distribution $p(x)$ directly,  \n",
    "teach the model to reverse a *known* corruption process that gradually adds noise to clean images.\n",
    "\n",
    "---\n",
    "\n",
    "## Input: Images as tensors\n",
    "\n",
    "Each training example is a 3-channel RGB image:\n",
    "\n",
    "$$\n",
    "x_0 \\in [-1, 1]^{3 \\times H \\times W}\n",
    "$$\n",
    "\n",
    "Normalize pixel values to \\[-1, 1\\] and train on **Food-101-Lite**, a small 10-class subset of Food-101.\n",
    "\n",
    "---\n",
    "\n",
    "## Forward process: adding noise\n",
    "\n",
    "During training, sample a timestep $t \\in \\{0, \\dots, T{-}1\\}$  \n",
    "and inject Gaussian noise into the image:\n",
    "\n",
    "$$\\varepsilon \\sim \\mathcal{N}(0, 1), \\quad x_{t} = x_0 + \\varepsilon$$\n",
    "\n",
    "The model sees $x_{t}$ and must learn to recover the corrupting noise $\\varepsilon$.\n",
    "\n",
    "---\n",
    "\n",
    "## Training objective\n",
    "\n",
    "Train a convolutional network $f_\\theta$ to predict the noise:\n",
    "\n",
    "$$\\mathcal{L} = \\mathbb{E}_{x_0, \\varepsilon, t}\\ \\big\\|f_\\theta(x_{t}, t) - \\varepsilon\\big\\|_2^2$$\n",
    "\n",
    "This is an **Mean Squared Error (MSE) loss**, and it encourages the model to de-noise corrupted images.\n",
    "\n",
    "---\n",
    "\n",
    "## Reverse diffusion: sampling new images\n",
    "\n",
    "At generation time, start from pure noise $x_T \\sim \\mathcal{N}(0, 1)$ and step backward:\n",
    "\n",
    "$$x_{t} \\leftarrow x_{t} - \\eta \\cdot f_\\theta(x_{t}, t), \\quad t = T{-}1, \\dots, 0$$\n",
    "\n",
    "After $T$ steps, $x_0$ is a fully generated image ‚Äî a sample from the learned data distribution.\n",
    "\n",
    "---\n",
    "\n",
    "## Why this works\n",
    "\n",
    "- Diffusion models sidestep unstable Generative Adversarial Network (GAN) training and can model complex, multimodal image distributions  \n",
    "- The forward process stays fixed and simple (just add noise), which makes the learning problem tractable  \n",
    "- At inference time, sampling becomes iterative de-noising ‚Äî easy to debug, modify, and extend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to migrate this diffusion-policy workload to a distributed setup using Ray on Anyscale\n",
    "\n",
    "This tutorial walks through the end-to-end process of **migrating a local image-based diffusion policy to a distributed Ray cluster running on Anyscale**.\n",
    "\n",
    "Here's how you make that transition:\n",
    "\n",
    "1. **Local Joint Photographic Experts Groups (JPEG) ‚Üí Distributed Ray Dataset**  \n",
    "   Preprocess and store Food-101 images as Parquet, then use **Ray Data** to load and decode the dataset in parallel across the cluster. Each worker gets its own shard, streamed efficiently for GPU training.\n",
    "\n",
    "2. **Single-GPU PyTorch ‚Üí Multi-node Distributed Training**  \n",
    "   Wrap your Lightning model in a Ray Train `train_loop`, then launch distributed training using **TorchTrainer** with eight GPU workers‚Äîeach operating on its own data partition with no manual coordination.\n",
    "\n",
    "3. **Manual Checkpoints ‚Üí Lightning-Integrated Fault Tolerance**  \n",
    "   Checkpointing and recovery are now handled automatically by **PyTorch Lightning** in combination with **Ray Train V2**.  \n",
    "   The `RayTrainReportCallback()` forwards each Lightning `checkpoint.ckpt` to Ray, enabling **structured, automatic resume and fault-tolerant training** with no manual save or report logic required.\n",
    "\n",
    "4. **Manual Data Management ‚Üí Declarative Scaling with Ray**  \n",
    "   Instead of slicing data or managing worker processes yourself, declare your intent with `ScalingConfig`, `CheckpointConfig`, and `FailureConfig`, and let **Ray + Anyscale handle the orchestration**.\n",
    "\n",
    "This pattern transforms a simple single-node PyTorch loop into a **scalable, fault-tolerant, multi-node training pipeline** with just a few lines of Ray-specific code, and it runs seamlessly on any cluster provisioned with Anyscale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports and setup  \n",
    "Pull in standard Python utilities, Ray (core, Data, Train, Lightning), and PyTorch Lightning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 00. Runtime setup\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "# Non-secret env var (safe to set here)\n",
    "os.environ[\"RAY_TRAIN_V2_ENABLED\"] = \"1\"\n",
    "\n",
    "# Install Python dependencies (same pinned versions as build.sh)\n",
    "subprocess.check_call([\n",
    "    sys.executable, \"-m\", \"pip\", \"install\", \"--no-cache-dir\",\n",
    "    \"torch==2.8.0\",\n",
    "    \"torchvision==0.23.0\",\n",
    "    \"matplotlib==3.10.6\",\n",
    "    \"pyarrow==14.0.2\",\n",
    "    \"datasets==2.19.2\",\n",
    "    \"lightning==2.5.5\",\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 01. Imports\n",
    "\n",
    "# Standard libraries\n",
    "import os\n",
    "import io\n",
    "import json\n",
    "import shutil\n",
    "import tempfile\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "# Ray\n",
    "import ray, ray.data\n",
    "from ray.train import ScalingConfig, get_context, RunConfig, FailureConfig, CheckpointConfig, Checkpoint, get_checkpoint\n",
    "from ray.train.torch import TorchTrainer\n",
    "from ray.train.lightning import RayLightningEnvironment\n",
    "\n",
    "# PyTorch / Lightning\n",
    "import lightning.pytorch as pl\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# Dataset\n",
    "from datasets import load_dataset\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from tqdm import tqdm  \n",
    "from torchvision.transforms import Compose, Resize, CenterCrop\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load 10 % of Food-101  \n",
    "Next, grab roughly 7,500 images, exactly 10 % of Food-101‚Äîusing a single call to `load_dataset`. This trimmed subset trains quickly while still being large enough to demonstrate Ray's scaling behaviour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 02. Load 10% of food101 (~7,500 images)\n",
    "hf_ds = load_dataset(\"food101\", split=\"train[:10%]\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Resize and encode images  \n",
    "Use **Ray Data** to preprocess images in parallel across the cluster.  \n",
    "Convert each image to raw JPEG bytes (a serializable format) and then decoded, resized to 256 pixels, center-cropped to 224 pixels, and re-encoded.  \n",
    "Processing with Ray Data makes the pipeline distributed, fault-tolerant, and Parquet-friendly‚Äîkeeping the dataset compact while scaling efficiently across workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 03. Resize + encode as JPEG bytes (Ray Data; BYTES-BASED)\n",
    "\n",
    "# Build Ray items with RAW BYTES (serializable) + label\n",
    "rows = []\n",
    "buf = io.BytesIO()\n",
    "for ex in hf_ds:\n",
    "    img = ex[\"image\"].convert(\"RGB\")\n",
    "    buf.seek(0); buf.truncate(0)\n",
    "    img.save(buf, format=\"JPEG\")\n",
    "    rows.append({\"image_bytes_raw\": buf.getvalue(), \"label\": ex[\"label\"]})\n",
    "\n",
    "# Create a Ray Dataset from serializable dicts\n",
    "ds = ray.data.from_items(rows)\n",
    "\n",
    "# Define preprocessing (runs on Ray workers)\n",
    "transform = Compose([Resize(256), CenterCrop(224)])\n",
    "\n",
    "def preprocess_images(batch_df):\n",
    "    out_img_bytes, out_labels = [], []\n",
    "    for b, lbl in zip(batch_df[\"image_bytes_raw\"], batch_df[\"label\"]):\n",
    "        try:\n",
    "            img = Image.open(io.BytesIO(b)).convert(\"RGB\")\n",
    "            img = transform(img)\n",
    "            out = io.BytesIO()\n",
    "            img.save(out, format=\"JPEG\")\n",
    "            out_img_bytes.append(out.getvalue())\n",
    "            out_labels.append(lbl)\n",
    "        except Exception:\n",
    "            # Skip unreadable/corrupt rows but don't kill the batch\n",
    "            continue\n",
    "    return {\"image_bytes\": out_img_bytes, \"label\": out_labels}\n",
    "\n",
    "# Parallel preprocessing\n",
    "processed_ds = ds.map_batches(\n",
    "    preprocess_images,\n",
    "    batch_format=\"pandas\",\n",
    "    num_cpus=1,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Processed records:\", processed_ds.count())\n",
    "processed_ds.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visual sanity check  \n",
    "Before committing to hours of training, take nine random samples and plot them with their class names. This quick inspection lets you confirm that images are correctly resized and preprocessed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 04. Visualize the dataset (Ray Data version)\n",
    "label_names = hf_ds.features[\"label\"].names  # int -> class name\n",
    "\n",
    "samples = processed_ds.random_shuffle().take(9)\n",
    "\n",
    "fig, axs = plt.subplots(3, 3, figsize=(8, 8))\n",
    "fig.suptitle(\"Sample Resized Images from food101-lite\", fontsize=16)\n",
    "\n",
    "for ax, rec in zip(axs.flatten(), samples):\n",
    "    img = Image.open(io.BytesIO(rec[\"image_bytes\"]))\n",
    "    ax.imshow(img)\n",
    "    ax.set_title(label_names[rec[\"label\"]])\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Persist to Parquet  \n",
    "Now, write the images and labels to a Parquet file. Because Parquet is columnar, you can read just the columns you need during training, which speeds up IO---especially when multiple workers are reading in parallel under Ray."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 05. Persist Ray Dataset to Parquet\n",
    "import os\n",
    "\n",
    "output_dir = \"/mnt/cluster_storage/food101_lite/parquet_256\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Write each block as its own Parquet shard\n",
    "processed_ds.write_parquet(output_dir)\n",
    "\n",
    "print(f\"‚úÖ Wrote {processed_ds.count()} records to {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Load and decode with Ray Data  \n",
    "Read the Parquet shard into a **Ray Dataset**, decode the JPEG bytes to **Channel-Height-Width (CHW) float32 tensors**, scale to \\[-1, 1\\], and drop the original byte column.  \n",
    "Because `decode_and_normalize` is stateless, the default **task-based** execution is perfect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 06. Load & Decode Food-101-Lite\n",
    "\n",
    "# Path to Parquet shards written earlier\n",
    "PARQUET_PATH = \"/mnt/cluster_storage/food101_lite/parquet_256\"\n",
    "\n",
    "# Read the Parquet files (‚âà7‚ÄØ500 rows with JPEG bytes + label)\n",
    "ds = ray.data.read_parquet(PARQUET_PATH)\n",
    "print(\"Raw rows:\", ds.count())\n",
    "\n",
    "# Decode JPEG ‚Üí CHW float32 in [‚Äë1,‚ÄØ1]\n",
    "\n",
    "def decode_and_normalize(batch_df):\n",
    "    \"\"\"Decode JPEG bytes and scale to [-1, 1].\"\"\"\n",
    "    images = []\n",
    "    for b in batch_df[\"image_bytes\"]:\n",
    "        img = Image.open(io.BytesIO(b)).convert(\"RGB\")\n",
    "        arr = np.asarray(img, dtype=np.float32) / 255.0       # H‚ÄØ√ó‚ÄØW‚ÄØ√ó‚ÄØ3, 0‚Äë1\n",
    "        arr = (arr - 0.5) / 0.5                               # ‚Äë1‚ÄØ‚Ä¶‚ÄØ1\n",
    "        arr = arr.transpose(2, 0, 1)                          # 3‚ÄØ√ó‚ÄØH‚ÄØ√ó‚ÄØW (CHW)\n",
    "        images.append(arr)\n",
    "    return {\"image\": images}\n",
    "\n",
    "# Apply in parallel\n",
    "#   batch_format=\"pandas\" ‚Üí batch_df is a DataFrame, return dict of lists.\n",
    "#   default task‚Äëbased compute is sufficient for a stateless function.\n",
    "\n",
    "ds = ds.map_batches(\n",
    "    decode_and_normalize,\n",
    "    batch_format=\"pandas\",\n",
    "    # Use the default (task‚Äëbased) compute strategy since `decode_and_normalize` is a plain function.\n",
    "    num_cpus=1,\n",
    ")\n",
    "\n",
    "# Drop the original JPEG column to save memory\n",
    "if \"image_bytes\" in ds.schema().names:\n",
    "    ds = ds.drop_columns([\"image_bytes\", \"label\"])\n",
    "\n",
    "print(\"Decoded rows:\", ds.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Shuffle and Train/Val split  \n",
    "Perform a reproducible shuffle, then split 80 % / 20 % into `train_ds` and `val_ds`.  \n",
    "Each split remains a first-class Ray Dataset, enabling distributed, sharded DataLoaders later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 07. Shuffle & Train/Val Split\n",
    "\n",
    "# Typical 80‚ÄØ/‚ÄØ20 split\n",
    "TOTAL = ds.count()\n",
    "train_count = int(TOTAL * 0.8)\n",
    "ds = ds.random_shuffle()  # expensive operation -- for large datasets, consider file shuffling or local shuffling. Ray offers both options\n",
    "train_ds, val_ds = ds.split_at_indices([train_count])\n",
    "print(\"Train rows:\", train_ds.count())\n",
    "print(\"Val rows:\",   val_ds.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Pixel diffusion LightningModule  \n",
    "A minimal **de-noising diffusion** policy:  \n",
    "* Input = noisy image + scalar timestep (packed as a 4-channel tensor)  \n",
    "* Output = predicted noise œµ  \n",
    "Log per-epoch losses and save them so every worker can later plot global curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 08. Pixel De-noising Diffusion Model  ‚Äî final, logging via Lightning/Ray\n",
    "\n",
    "class PixelDiffusion(pl.LightningModule):\n",
    "    \"\"\"Tiny CNN that predicts noise œµ given noisy image + timestep.\"\"\"\n",
    "\n",
    "    def __init__(self, max_t=1000):\n",
    "        super().__init__()\n",
    "        self.max_t = max_t\n",
    "\n",
    "        # Network: (3+1)-channel input ‚Üí 3-channel noise prediction\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(4, 32, 3, padding=1), nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, 3, padding=1), nn.ReLU(),\n",
    "            nn.Conv2d(32, 3, 3, padding=1),\n",
    "        )\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "\n",
    "    # ---------- forward ----------\n",
    "    def forward(self, noisy_img, t):\n",
    "        \"\"\"noisy_img: Bx3xHxW,  t: B (int) or Bx1 scalar\"\"\"\n",
    "        b, _, h, w = noisy_img.shape\n",
    "        t_scaled = (t / self.max_t).view(-1, 1, 1, 1).float().to(noisy_img.device)\n",
    "        t_img = t_scaled.expand(-1, 1, h, w)\n",
    "        x = torch.cat([noisy_img, t_img], dim=1)  # 4 channels\n",
    "        return self.net(x)\n",
    "    \n",
    "    # ---------- shared loss ----------\n",
    "    def _shared_step(self, batch):\n",
    "        clean = batch[\"image\"].to(self.device)             # Bx3xHxW, -1‚Ä¶1\n",
    "        noise = torch.randn_like(clean)                    # œµ ~ N(0, 1)\n",
    "        t = torch.randint(0, self.max_t, (clean.size(0),), device=self.device)\n",
    "        noisy = clean + noise                              # x_t = x_0 + œµ\n",
    "        pred_noise = self(noisy, t)\n",
    "        return self.loss_fn(pred_noise, noise)\n",
    "\n",
    "    # ---------- training / validation ----------\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss = self._shared_step(batch)\n",
    "        # Let Lightning aggregate + Ray callback report\n",
    "        self.log(\"train_loss\", loss, on_epoch=True, prog_bar=False, sync_dist=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss = self._shared_step(batch)\n",
    "        self.log(\"val_loss\", loss, on_epoch=True, prog_bar=False, sync_dist=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=2e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Ray Train `train_loop` (Lightning + Ray integration)  \n",
    "Define the core training logic that runs **once per Ray worker**.  \n",
    "This version uses **PyTorch Lightning with Ray Train** for automatic distributed setup, checkpointing, and metric reporting.  \n",
    "`RayDDPStrategy` and `RayTrainReportCallback` handle synchronization, while `prepare_trainer()` wires up Ray‚Äôs environment.  \n",
    "Lightning manages all checkpoints (`checkpoint.ckpt`) and metrics transparently, ensuring fully **fault-tolerant, resumable multi-GPU training**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 09. Train loop for Ray TorchTrainer (RayDDP + Lightning-native, NEW API aligned)\n",
    "\n",
    "def train_loop(config):\n",
    "    \"\"\"\n",
    "    Lightning-owned loop with Ray integration:\n",
    "      - RayDDPStrategy for multi-worker DDP\n",
    "      - RayLightningEnvironment for ranks/addrs\n",
    "      - RayTrainReportCallback to forward metrics + checkpoints to Ray\n",
    "      - Resume from the Ray-provided Lightning checkpoint (\"checkpoint.ckpt\")\n",
    "    \"\"\"\n",
    "    import warnings\n",
    "    warnings.filterwarnings(\n",
    "        \"ignore\",\n",
    "        message=\"barrier.*using the device under current context\",\n",
    "    )\n",
    "    import os\n",
    "    import torch\n",
    "    import lightning.pytorch as pl\n",
    "    from ray.train import get_checkpoint, get_context\n",
    "    from ray.train.lightning import (\n",
    "        RayLightningEnvironment,\n",
    "        RayDDPStrategy,\n",
    "        RayTrainReportCallback,\n",
    "        prepare_trainer,\n",
    "    )\n",
    "\n",
    "    # ---- Data shards from Ray Data ‚Üí iterable loaders ----\n",
    "    train_ds = ray.train.get_dataset_shard(\"train\")\n",
    "    val_ds   = ray.train.get_dataset_shard(\"val\")\n",
    "    train_loader = train_ds.iter_torch_batches(batch_size=config.get(\"batch_size\", 32))\n",
    "    val_loader   = val_ds.iter_torch_batches(batch_size=config.get(\"batch_size\", 32))\n",
    "\n",
    "    # ---- Model ----\n",
    "    model = PixelDiffusion()\n",
    "\n",
    "    # ---- Lightning Trainer configured for Ray ----\n",
    "    CKPT_ROOT = os.path.join(tempfile.gettempdir(), \"ray_pl_ckpts\")\n",
    "    os.makedirs(CKPT_ROOT, exist_ok=True)\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=config.get(\"epochs\", 10),\n",
    "        devices=\"auto\",\n",
    "        accelerator=\"auto\",\n",
    "        strategy=RayDDPStrategy(),\n",
    "        plugins=[RayLightningEnvironment()],\n",
    "        callbacks=[\n",
    "            RayTrainReportCallback(),\n",
    "            pl.callbacks.ModelCheckpoint(\n",
    "                dirpath=CKPT_ROOT,         # local scratch is fine (or leave None to use default)\n",
    "                filename=\"epoch-{epoch:03d}\",\n",
    "                every_n_epochs=1,\n",
    "                save_top_k=-1,\n",
    "                save_last=True,\n",
    "            ),\n",
    "        ],\n",
    "        default_root_dir=CKPT_ROOT,        # also local\n",
    "        enable_progress_bar=False,\n",
    "        check_val_every_n_epoch=1,\n",
    "    )\n",
    "\n",
    "    # Wire up ranks/world size with Ray\n",
    "    trainer = prepare_trainer(trainer)\n",
    "\n",
    "    # ---- Resume from latest Ray-provided Lightning checkpoint (if any) ----\n",
    "    ckpt_path = None\n",
    "    ckpt = get_checkpoint()\n",
    "    if ckpt:\n",
    "        with ckpt.as_directory() as d:\n",
    "            candidate = os.path.join(d, \"checkpoint.ckpt\")\n",
    "            if os.path.exists(candidate):\n",
    "                ckpt_path = candidate\n",
    "                if get_context().get_world_rank() == 0:\n",
    "                    print(f\"‚úÖ Resuming from Lightning checkpoint: {ckpt_path}\")\n",
    "\n",
    "    # ---- Let Lightning own the loop ----\n",
    "    trainer.fit(\n",
    "        model,\n",
    "        train_dataloaders=train_loader,\n",
    "        val_dataloaders=val_loader,\n",
    "        ckpt_path=ckpt_path,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Launch distributed Training with TorchTrainer  \n",
    "Ask for **eight GPU workers**, keep the five most-recent checkpoints, and allow up to one automatic retry.  \n",
    "`result.checkpoint` captures the checkpoint from the highest epoch (because you used `epoch` as the score attribute, you can change this to other metrics such as validation loss or training loss)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. Launch distributed training (same API, now Lightning-native inside)\n",
    "\n",
    "trainer = TorchTrainer(\n",
    "    train_loop_per_worker=train_loop,\n",
    "    scaling_config=ScalingConfig(num_workers=8, use_gpu=True),\n",
    "    datasets={\"train\": train_ds, \"val\": val_ds},\n",
    "    run_config=RunConfig(\n",
    "        name=\"food101_diffusion_ft\",\n",
    "        storage_path=\"/mnt/cluster_storage/generative_cv/food101_diffusion_results\",\n",
    "        checkpoint_config=CheckpointConfig(\n",
    "            num_to_keep=5,\n",
    "            checkpoint_score_attribute=\"epoch\",\n",
    "            checkpoint_score_order=\"max\",\n",
    "        ),\n",
    "        failure_config=FailureConfig(max_failures=1),\n",
    "    ),\n",
    ")\n",
    "\n",
    "result = trainer.fit()\n",
    "print(\"Training complete ‚Üí\", result.metrics)\n",
    "best_ckpt = result.checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Plot loss curves  \n",
    "Visualize training and validation loss directly from Ray‚Äôs tracked metrics.  \n",
    "`result.metrics_dataframe` automatically aggregates values logged by Lightning during training.  \n",
    "Plotting these losses provides a quick health check to confirm steady convergence and model generalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11. Plot train/val loss curves (Ray + Lightning integration)\n",
    "\n",
    "# Ray stores all metrics emitted by Lightning in a dataframe\n",
    "df = result.metrics_dataframe\n",
    "\n",
    "# Display first few rows (optional sanity check)\n",
    "print(df.head())\n",
    "\n",
    "# Convert and clean up\n",
    "if \"train_loss\" not in df.columns or \"val_loss\" not in df.columns:\n",
    "    raise ValueError(\"Expected train_loss and val_loss in metrics. \"\n",
    "                     \"Did you call self.log('train_loss') / self.log('val_loss') in PixelDiffusion?\")\n",
    "\n",
    "plt.figure(figsize=(7, 4))\n",
    "plt.plot(df[\"epoch\"], df[\"train_loss\"], marker=\"o\", label=\"Train\")\n",
    "plt.plot(df[\"epoch\"], df[\"val_loss\"], marker=\"o\", label=\"Val\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"MSE Loss\")\n",
    "plt.title(\"Pixel Diffusion ‚Äì Loss per Epoch (Ray Train + Lightning)\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Resume from latest checkpoint  \n",
    "Calling `trainer.fit()` again detects the run snapshot, loads the latest checkpoint, and (because `epochs=10`) exits immediately, proving that the resume path works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12. Run the trainer again to demonstrate resuming from latest checkpoint  \n",
    "\n",
    "result = trainer.fit()\n",
    "print(\"Training complete ‚Üí\", result.metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Reverse diffusion sampler  \n",
    "A simple Euler-style loop that starts from Gaussian noise and iteratively subtracts the model‚Äôs predicted noise.  \n",
    "This isn't production-grade sampling, but it's suitable for illustrating inference after training. Use **Ray Data** when performing inference at scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13. Reverse diffusion sampling\n",
    "\n",
    "def sample_image(model, steps=50, device=\"cpu\"):\n",
    "    \"\"\"Generate an image by iteratively de-noising random noise.\"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        img = torch.randn(1, 3, 224, 224, device=device)\n",
    "        for step in reversed(range(steps)):\n",
    "            t = torch.tensor([step], device=device)\n",
    "            pred_noise = model(img, t)\n",
    "            img = img - pred_noise * 0.1                      # simple Euler update\n",
    "        # Rescale back to [0,1]\n",
    "        img = torch.clamp((img * 0.5 + 0.5), 0.0, 1.0)\n",
    "        return img.squeeze(0).cpu().permute(1,2,0).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Generate and display samples from the best checkpoint \n",
    "Load the model weights from `best_ckpt`, move to GPU if available, generate three images, and show them side-by-side.  \n",
    "Remember that when using a tiny CNN and only 10 epochs, these samples look noise-like. If you replace the backbone or train longer, you will see better quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 14. Generate and display samples\n",
    "\n",
    "import glob\n",
    "from ray.train import Checkpoint\n",
    "\n",
    "assert best_ckpt is not None, \"Checkpoint is missing. Did training run and complete?\"\n",
    "\n",
    "# Restore model weights from Ray Train checkpoint (Lightning-first)\n",
    "model = PixelDiffusion()\n",
    "\n",
    "with best_ckpt.as_directory() as ckpt_dir:\n",
    "    # Prefer Lightning checkpoints (*.ckpt) saved by ModelCheckpoint\n",
    "    ckpt_files = glob.glob(os.path.join(ckpt_dir, \"*.ckpt\"))\n",
    "    if ckpt_files:\n",
    "        pl_ckpt = torch.load(ckpt_files[0], map_location=\"cpu\")\n",
    "        state = pl_ckpt.get(\"state_dict\", pl_ckpt)\n",
    "        model.load_state_dict(state, strict=False)\n",
    "    elif os.path.exists(os.path.join(ckpt_dir, \"model.pt\")):\n",
    "        # Fallback for older/manual checkpoints\n",
    "        state = torch.load(os.path.join(ckpt_dir, \"model.pt\"), map_location=\"cpu\")\n",
    "        model.load_state_dict(state, strict=False)\n",
    "    else:\n",
    "        raise FileNotFoundError(\n",
    "            f\"No Lightning .ckpt or model.pt found in: {ckpt_dir}\"\n",
    "        )\n",
    "\n",
    "# Move to device and sample\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = model.to(device)\n",
    "\n",
    "# Generate three images\n",
    "samples = [sample_image(model, steps=50, device=device) for _ in range(3)]\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=(9, 3))\n",
    "for ax, img in zip(axs, samples):\n",
    "    ax.imshow(img)\n",
    "    ax.axis(\"off\")\n",
    "plt.suptitle(\"Food-101 Diffusion Samples (unconditional)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Clean up shared storage  \n",
    "Reclaim cluster disk space by deleting the entire tutorial output directory.  \n",
    "Run this only when you‚Äôre **sure** you don‚Äôt need the checkpoints or metrics anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15. Cleanup -- delete checkpoints and metrics from model training\n",
    "\n",
    "TARGET_PATH = \"/mnt/cluster_storage/generative_cv\"\n",
    "\n",
    "if os.path.exists(TARGET_PATH):\n",
    "    shutil.rmtree(TARGET_PATH)\n",
    "    print(f\"‚úÖ Deleted everything under {TARGET_PATH}\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è Path does not exist: {TARGET_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrap up and next steps\n",
    "\n",
    "In this tutorial, you used **Ray Train and Ray Data on Anyscale** to scale a compact diffusion-policy workload, from raw JPEG bytes to distributed training and sampling, without changing the core PyTorch logic. This tutorial demonstrates:\n",
    "\n",
    "* Using **Ray Data** to decode, normalize, and shard large image datasets in parallel.  \n",
    "* Scaling training across multiple GPUs using **TorchTrainer** and a Ray-native `train_loop`.  \n",
    "* Managing distributed training state with **Ray Checkpoints** and automatic resume.  \n",
    "* Running fault-tolerant multi-node jobs on Anyscale without orchestration scripts.  \n",
    "\n",
    "---\n",
    "\n",
    "## Next steps\n",
    "\n",
    "Below are a few directions you might explore to adapt or extend the pattern:\n",
    "\n",
    "1. **Backbones and architecture upgrades**  \n",
    "   * Swap in a larger ResNet or another vision model for much better generative performance.  \n",
    "   * Try pre-trained encoders and fine-tune only the diffusion-specific layers.\n",
    "\n",
    "2. **Conditional diffusion**  \n",
    "   * Use the `label` column to condition the model (for example, class-conditioning).  \n",
    "   * Compare unconditional versus conditional generation side by side.\n",
    "\n",
    "3. **Sampling improvements**  \n",
    "   * Replace naive reverse diffusion with De-noising Diffusion Implicit Models (DDIM), Pseudo Numerical Methods for Diffusion Models (PNDM), or learned de-noisers.  \n",
    "   * Add timestep embeddings or noise schedules to increase model expressiveness.\n",
    "\n",
    "4. **Longer training and mixed precision**  \n",
    "   * Increase the `max_epochs` and enable Automatic Mixed Precision (AMP) for faster training with less memory.  \n",
    "   * Visualize convergence and training stability across longer runs.\n",
    "\n",
    "5. **Hyperparameter sweeps**  \n",
    "   * Use **Ray Tune** to search over learning rates, model size, or sampling steps.  \n",
    "   * Leverage Tune's reporting to schedule early stopping or checkpoint pruning.\n",
    "\n",
    "6. **Data handling and scaling**  \n",
    "   * Shard the dataset into multiple Parquet files and distribute across more workers.  \n",
    "   * Store and load datasets from S3 or other cloud storage.\n",
    "\n",
    "7. **Image quality evaluation**  \n",
    "   * Log Fr√©chet Inception Distance (FID) scores, perceptual similarity, or diffusion-specific metrics.  \n",
    "   * Compare generated samples from different checkpoints or backbones.\n",
    "\n",
    "8. **Model serving**  \n",
    "   * Package the reverse sampler into a Ray task or **Ray Serve** endpoint.  \n",
    "   * Run a demo app that generates images on demand from a class name or random seed.\n",
    "\n",
    "9. **End-to-end MLOps**  \n",
    "   * Register the best checkpoint with MLflow or Weights & Biases.  \n",
    "   * Wrap the training loop in a Ray Job and run it on a schedule with Anyscale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}