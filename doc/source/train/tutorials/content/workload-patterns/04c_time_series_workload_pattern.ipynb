{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Time series workload pattern\n\n<div align=\"left\">\n<a target=\"_blank\" href=\"https://console.anyscale.com/template-preview/ray_train_workloads?file=%252Ffiles%252Fworkload-patterns%252F04c_time_series_workload_pattern.ipynb\"><img src=\"https://img.shields.io/badge/ðŸš€ Run_on-Anyscale-9hf\"></a>&nbsp;\n<a href=\"https://github.com/ray-project/ray/tree/master/doc/source/train/tutorials/content/workload-patterns/04c_time_series_workload_pattern.ipynb\" role=\"button\"><img src=\"https://img.shields.io/static/v1?label=&amp;message=View%20On%20GitHub&amp;color=586069&amp;logo=github&amp;labelColor=2f363d\"></a>&nbsp;\n</div>\n\nIn this notebook you tackle **New York City (NYC) taxi-demand forecasting** (2014 half-hourly counts) and scale a *sequence-to-sequence Transformer* across an Anyscale cluster using **Ray Train**.\n\n## Learning objectives  \n- **Ray Train distributed loops**: wrap a PyTorch Transformer in `TorchTrainer` and run it across eight GPUs with a *single* `ScalingConfig` line.  \n- **Fault-tolerant checkpointing on Anyscale**: recover seamlessly from pre-emptions or node failures with automatic epoch-level checkpoints.  \n- **Inference from checkpoints**: use **Ray Data** with stateful GPU actors to perform scalable batch forecasts directly from saved checkpoints.  \nBy the end, you know how to take a single-node notebook forecast and scale data loading, training, and inference seamlessly across an Anyscale cluster."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What problem are you solving? (NYC taxi demand forecasting with a Transformer)\n",
    "\n",
    "You want to predict the **next 24 hours (48 half-hour slots)** of taxi pickups in NYC, given one week of historical demand.  \n",
    "Accurate short-term forecasts help ride-hailing fleets, traffic planners, and dynamic pricing engines allocate resources efficiently.\n",
    "\n",
    "---\n",
    "\n",
    "## What's a sequence-to-sequence Transformer?\n",
    "\n",
    "A **Transformer** models the joint distribution of a sequence by stacking self-attention layers that capture long-range dependencies without recurrence.  \n",
    "Your architecture learns a function  \n",
    "\n",
    "$$\n",
    "f_\\theta : \\underbrace{\\mathbb{R}^{T\\times 1}}_{\\text{past}} \\;\\longrightarrow\\; \\underbrace{\\mathbb{R}^{F}}_{\\text{future}}\n",
    "$$\n",
    "\n",
    "where $T=168$ half-hours (one week) and $F=48$.  \n",
    "During training you use **teacher forcing** (a design choice), feeding the shifted ground truth to the decoder, so the model can focus on learning residual patterns rather than inventing an initial context.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to migrate this time-series workload to a distributed multi-node setup using Ray on Anyscale\n",
    "This tutorial walks through the end-to-end process of **migrating a single-GPU PyTorch forecasting pipeline to a distributed Ray cluster running on Anyscale**.\n",
    "\n",
    "Follow these steps to make the transition:\n",
    "\n",
    "1. **Migrate local CSV data to shared Parquet**  \n",
    "   Download the NYC taxi dataset as a CSV, resample it to 30-minute intervals, normalize the values, and save it as **Parquet shards** in a shared filesystem (`/mnt/cluster_storage`)â€”the default storage for Anyscale clusters.\n",
    "\n",
    "2. **Create sliding window generation for Distributed Data Parallel (DDP)**  \n",
    "   Create overlapping input and output windows (past to future) to train a forecasting model. While this preprocessing is local and sequential in this tutorial, it mirrors pipelines that parallelize with **Ray Data** in large-scale settings. \n",
    "\n",
    "3. **Define a vanilla PyTorch function to use distributed Ray Train**  \n",
    "   Define a `train_loop_per_worker()` function and use **Ray Train** to launch **eight GPU workers** across the cluster. Each worker loads its own Parquet shard, trains independently under Distributed Data Parallel (DDP), and reports live metrics.\n",
    "\n",
    "4. **Configure Ray for scalable cluster orchestration**  \n",
    "   Instead of managing GPUs or process groups manually, configure `ScalingConfig`, `RunConfig`, and `FailureConfig`. **Ray and Anyscale handle fault-tolerant execution across nodes.**\n",
    "\n",
    "5. **Perform distributed batch inference with Ray Data**  \n",
    "   Use **Ray Data** with stateful GPU actors to load the trained checkpoint once per worker and run scalable, parallel forecasts on the latest data windows.  \n",
    "   This enables **efficient, reusable, and fault-tolerant** inference across the cluster.\n",
    "\n",
    "This pattern takes a local academic-style time-series workflow and scales it into a **cluster-resilient, fault-tolerant forecasting pipeline**, all while preserving your native PyTorch modeling code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports  \n",
    "This cell loads all required libraries for the tutorial: PyData tools for data processing, PyTorch for model building and training, and Ray Train for distributed orchestration. `TorchTrainer` is the main training engine, while `prepare_model` and `prepare_data_loader` help convert vanilla PyTorch code into Ray-aware components that scale seamlessly across nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 00. Runtime setup \n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "# Non-secret env var \n",
    "os.environ[\"RAY_TRAIN_V2_ENABLED\"] = \"1\"\n",
    "\n",
    "# Install Python dependencies \n",
    "subprocess.check_call([\n",
    "    sys.executable, \"-m\", \"pip\", \"install\", \"--no-cache-dir\",\n",
    "    \"torch==2.8.0\",\n",
    "    \"matplotlib==3.10.6\",\n",
    "    \"pyarrow==14.0.2\",\n",
    "    \"datasets==2.19.2\",\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 01. Imports\n",
    "import os\n",
    "import io\n",
    "import math\n",
    "import uuid\n",
    "import shutil\n",
    "import random\n",
    "import requests\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "from datasets import load_dataset   \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "import ray\n",
    "import ray.data as rdata\n",
    "import ray.train as train\n",
    "from ray.train import (\n",
    "    ScalingConfig, RunConfig, FailureConfig,\n",
    "    CheckpointConfig, Checkpoint, get_checkpoint, get_context\n",
    ")\n",
    "from ray.train.torch import prepare_model, prepare_data_loader, TorchTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load NYC taxi passenger counts (30-min)  \n",
    "Download and cache a lightweight NYC taxi demand dataset from GitHub. Store the file under the shared `/mnt/cluster_storage` directory so that all Ray workers can read it without duplication. Parse the timestamps and use them as the DataFrame index, making the data time-series ready."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 02. Load NYC taxi passenger counts (30-min) from GitHub raw â€“ no auth, ~1 MB\n",
    "\n",
    "DATA_DIR = \"/mnt/cluster_storage/nyc_taxi_ts\"\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/numenta/NAB/master/data/realKnownCause/nyc_taxi.csv\"\n",
    "csv_path = os.path.join(DATA_DIR, \"nyc_taxi.csv\")\n",
    "\n",
    "if not os.path.exists(csv_path):\n",
    "    print(\"Downloading nyc_taxi.csv â€¦\")\n",
    "    df = pd.read_csv(url)\n",
    "    df.to_csv(csv_path, index=False)\n",
    "else:\n",
    "    print(\"File already present.\")\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "# Parse timestamp and tidy\n",
    "df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"])\n",
    "df = df.set_index(\"timestamp\").rename(columns={\"value\": \"passengers\"})\n",
    "\n",
    "print(\"Rows:\", len(df), \"| Time span:\", df.index.min(), \"â†’\", df.index.max())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Resample to hourly, then normalize  \n",
    "Resample the dataset to 30-minute intervals (if it wasnâ€™t already), then z-score the `passengers` column to get a standardized signal. This helps with training stability, gradient scale, and ensures the model doesnâ€™t learn absolute magnitudes too early. You reverse the normalization after inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 03. Resample to hourly, then normalize\n",
    "hourly = df.resample(\"30min\").mean()\n",
    "\n",
    "mean, std = hourly[\"passengers\"].mean(), hourly[\"passengers\"].std()\n",
    "hourly[\"norm\"] = (hourly[\"passengers\"] - mean) / std\n",
    "\n",
    "print(f\"Half-Hourly rows: {len(hourly)}  |  mean={mean:.1f}, std={std:.1f}\")\n",
    "hourly.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Quick visual sanity-check  \n",
    "Before moving to training, itâ€™s good practice to visualise the raw data. Plot the first two weeks of half-hourly taxi demand. This helps confirm that the series exhibits strong seasonality and contains no unexpected gaps or noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 04. Quick visual sanity-check â€” first two weeks\n",
    "plt.figure(figsize=(10, 4))\n",
    "hourly[\"passengers\"].iloc[:24*14].plot()\n",
    "plt.title(\"NYC-Taxi passengers - first 2 weeks of 2014\")\n",
    "plt.ylabel(\"# trips in hour\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Sliding-window dataset to Parquet  \n",
    "Convert the time-series into a supervised learning format using sliding windows. Each sample consists of a fixed-length input sequence (1 week of past data) and a prediction target (next 24 hours). Write these to columnar Parquet files on shared storage to enable efficient streaming in distributed training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 05. Build sliding-window dataset and write to Parquet\n",
    "# ----------------------------------------------------\n",
    "INPUT_WINDOW = 24 * 7   # 1/2 week history (in 30-min steps = 168)\n",
    "HORIZON      = 48       # predict next 24 h\n",
    "STRIDE       = 12       # slide 6 hours at a time\n",
    "\n",
    "values = hourly[\"norm\"].to_numpy(dtype=\"float32\")  # already normalised\n",
    "\n",
    "# ---- Time-aware split to avoid leakage between train and val ----\n",
    "cut = int(0.9 * len(values))  # split by time index on the original series\n",
    "train_records, val_records = [], []\n",
    "\n",
    "for s in range(0, len(values) - INPUT_WINDOW - HORIZON + 1, STRIDE):\n",
    "    past   = values[s : s + INPUT_WINDOW]\n",
    "    future = values[s + INPUT_WINDOW : s + INPUT_WINDOW + HORIZON]\n",
    "    end    = s + INPUT_WINDOW + HORIZON  # last index consumed by this window\n",
    "\n",
    "    rec = {\n",
    "        \"series_id\": 0,\n",
    "        \"past\":  past.tolist(),\n",
    "        \"future\": future.tolist(),\n",
    "    }\n",
    "\n",
    "    if end <= cut:         # Entire window ends before the cut to train\n",
    "        train_records.append(rec)\n",
    "    elif s >= cut:         # Window starts after the cut to val\n",
    "        val_records.append(rec)\n",
    "    # else: window crosses the cut to drop to prevent leakage\n",
    "\n",
    "print(f\"Windows â†’ train: {len(train_records)}, val: {len(val_records)}\")\n",
    "\n",
    "# Write to Parquet\n",
    "DATA_DIR     = \"/mnt/cluster_storage/nyc_taxi_ts\"\n",
    "PARQUET_DIR  = os.path.join(DATA_DIR, \"parquet\")\n",
    "os.makedirs(PARQUET_DIR, exist_ok=True)\n",
    "\n",
    "schema = pa.schema([\n",
    "    (\"series_id\", pa.int32()),\n",
    "    (\"past\",  pa.list_(pa.float32())),\n",
    "    (\"future\", pa.list_(pa.float32()))\n",
    "])\n",
    "\n",
    "def write_parquet(records, fname):\n",
    "    pq.write_table(pa.Table.from_pylist(records, schema=schema), fname, version=\"2.6\")\n",
    "\n",
    "write_parquet(train_records, os.path.join(PARQUET_DIR, \"train.parquet\"))\n",
    "write_parquet(val_records,   os.path.join(PARQUET_DIR, \"val.parquet\"))\n",
    "print(\"Parquet shards written â†’\", PARQUET_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. PyTorch Dataset over Parquet  \n",
    "Define a lightweight PyTorch `Dataset` class that reads each window from the Parquet shard. This makes the model training logic agnostic to how you store the data. Your DataLoader receives standard PyTorch tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 06. PyTorch Dataset that reads the Parquet shards\n",
    "\n",
    "class TaxiWindowDataset(Dataset):\n",
    "    def __init__(self, parquet_path):\n",
    "        self.table  = pq.read_table(parquet_path)\n",
    "        self.past   = self.table.column(\"past\").to_pylist()\n",
    "        self.future = self.table.column(\"future\").to_pylist()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.past)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        past   = torch.tensor(self.past[idx],   dtype=torch.float32).unsqueeze(-1)   # (T, 1)\n",
    "        future = torch.tensor(self.future[idx], dtype=torch.float32)                 # (H,)\n",
    "        return past, future"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Inspect one random batch  \n",
    "Always verify shapes before diving into training. This cell uses a basic `DataLoader` to fetch one random batch and prints the dimensions of the input and target tensors. This ensures the encoder and decoder receive tensors of the correct size and shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 07. Inspect one random batch\n",
    "loader = DataLoader(TaxiWindowDataset(os.path.join(PARQUET_DIR, \"train.parquet\")),\n",
    "                    batch_size=4, shuffle=True)\n",
    "xb, yb = next(iter(loader))\n",
    "print(\"Past:\", xb.shape, \"Future:\", yb.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Ray-prepared DataLoader  \n",
    "Ray Train provides a helper to wrap your `DataLoader` so that it integrates seamlessly with distributed training. `prepare_data_loader` takes care of sharding and worker setup, ensuring each process only loads a subset of the data and communicates correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 08. Helper to build Ray-prepared DataLoader\n",
    "from ray.train.torch import prepare_data_loader\n",
    "\n",
    "def build_dataloader(parquet_path, batch_size, shuffle=True):\n",
    "    ds = TaxiWindowDataset(parquet_path)\n",
    "    loader = DataLoader(\n",
    "        ds, batch_size=batch_size, shuffle=shuffle, num_workers=2, drop_last=False,\n",
    "    )\n",
    "    return prepare_data_loader(loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. PositionalEncoding and Transformer model  \n",
    "This is the neural architecture that you train. It consists of a standard PyTorch Transformer with encoder-decoder structure and sinusoidal positional encodings. The model accepts a sequence of past observations (and optionally decoder inputs during training) and returns predictions for the future window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 09. PositionalEncoding and Transformer model (univariate)\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.2, max_len=1024):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float32).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2, dtype=torch.float32) * -(math.log(10000.0) / d_model)\n",
    "        )\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer(\"pe\", pe.unsqueeze(0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.dropout(x + self.pe[:, : x.size(1)])\n",
    "\n",
    "class TimeSeriesTransformer(nn.Module):\n",
    "    def __init__(self, input_window, horizon, d_model=64, nhead=8, num_layers=2):\n",
    "        super().__init__()\n",
    "        self.horizon  = horizon\n",
    "        self.d_model  = d_model\n",
    "\n",
    "        self.in_proj  = nn.Linear(1, d_model)\n",
    "        self.pos_enc  = PositionalEncoding(d_model)\n",
    "        self.tr_model = nn.Transformer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            num_encoder_layers=num_layers,\n",
    "            num_decoder_layers=num_layers,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.out_proj = nn.Linear(d_model, 1)\n",
    "\n",
    "    def forward(self, past, decoder_input=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            past           : (B, T, 1)    â€” encoder input\n",
    "            decoder_input  : (B, F, 1)    â€” optional decoder input (teacher forcing)\n",
    "        Returns:\n",
    "            preds          : (B, F)       â€” predicted future values\n",
    "        \"\"\"\n",
    "        B = past.size(0)\n",
    "\n",
    "        # Encoder input\n",
    "        src = self.in_proj(past) * math.sqrt(self.d_model)\n",
    "        src = self.pos_enc(src)\n",
    "\n",
    "        # Decoder input\n",
    "        if decoder_input is None:\n",
    "            decoder_input = past[:, -1:, :].repeat(1, self.horizon, 1)\n",
    "\n",
    "        tgt = self.in_proj(decoder_input) * math.sqrt(self.d_model)\n",
    "        tgt = self.pos_enc(tgt)\n",
    "\n",
    "        # Transformer forward\n",
    "        output = self.tr_model(src, tgt)  # shape: (B, F, d_model)\n",
    "        return self.out_proj(output).squeeze(-1)  # shape: (B, F)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Ray Train training loop (with teacher forcing)  \n",
    "This is the heart of Ray Train. Each worker executes this loop independently, but Ray orchestrates everything from checkpointing to failure recovery. Include teacher forcing, feeding the shifted ground-truth to the decoder, which allows the model to learn more quickly than starting from zero. Also log training and validation loss per epoch and save checkpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. Ray Train train_loop_per_worker with checkpointing, teacher forcing, and clean structure\n",
    "\n",
    "def train_loop_per_worker(config):\n",
    "    import tempfile\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.optim as optim\n",
    "    from ray import train\n",
    "    from ray.train import Checkpoint, get_context\n",
    "    from ray.train import get_checkpoint\n",
    "\n",
    "    torch.manual_seed(0)\n",
    "\n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    # 1) Model (DDP-prepared)\n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    model = TimeSeriesTransformer(\n",
    "        input_window=INPUT_WINDOW,\n",
    "        horizon=HORIZON,\n",
    "        d_model=config[\"d_model\"],\n",
    "        nhead=config[\"nhead\"],\n",
    "        num_layers=config[\"num_layers\"],\n",
    "    )\n",
    "    model = train.torch.prepare_model(model)\n",
    "\n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    # 2) Optimizer / Loss\n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config[\"lr\"])\n",
    "    loss_fn = nn.SmoothL1Loss()\n",
    "\n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    # 3) Resume from checkpoint (if provided by Ray)\n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    rank = get_context().get_world_rank()\n",
    "    start_epoch = 0\n",
    "    ckpt = get_checkpoint()\n",
    "    if ckpt:\n",
    "        with ckpt.as_directory() as ckpt_dir:\n",
    "            # Safe CPU load in case of device mismatch on resume\n",
    "            model.load_state_dict(torch.load(os.path.join(ckpt_dir, \"model.pt\"), map_location=\"cpu\"))\n",
    "            opt_state_path = os.path.join(ckpt_dir, \"optim.pt\")\n",
    "            if os.path.exists(opt_state_path):\n",
    "                optimizer.load_state_dict(torch.load(opt_state_path, map_location=\"cpu\"))\n",
    "            meta = torch.load(os.path.join(ckpt_dir, \"meta.pt\"))\n",
    "            start_epoch = int(meta.get(\"epoch\", -1)) + 1\n",
    "        if rank == 0:\n",
    "            print(f\"[Rank {rank}] âœ… Resumed from checkpoint at epoch {start_epoch}\")\n",
    "\n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    # 4) Dataloaders for this worker\n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    train_loader = build_dataloader(\n",
    "        os.path.join(PARQUET_DIR, \"train.parquet\"),\n",
    "        batch_size=config[\"bs\"],\n",
    "        shuffle=True,\n",
    "    )\n",
    "    val_loader = build_dataloader(\n",
    "        os.path.join(PARQUET_DIR, \"val.parquet\"),\n",
    "        batch_size=config[\"bs\"],\n",
    "        shuffle=False,\n",
    "    )\n",
    "\n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    # 5) Epoch loop\n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    for epoch in range(start_epoch, config[\"epochs\"]):\n",
    "        # ---- Train ----\n",
    "        model.train()\n",
    "        train_loss_sum = 0.0\n",
    "        for past, future in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Teacher forcing: shift future targets to use as decoder input\n",
    "            future = future.unsqueeze(-1)                                # (B, F, 1)\n",
    "            start_token = torch.zeros_like(future[:, :1])                # (B, 1, 1)\n",
    "            decoder_input = torch.cat([start_token, future[:, :-1]], 1)  # (B, F, 1)\n",
    "\n",
    "            pred = model(past, decoder_input)                            # (B, F)\n",
    "            loss = loss_fn(pred, future.squeeze(-1))                     # (B, F) vs (B, F)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss_sum += float(loss.item())\n",
    "\n",
    "        avg_train_loss = train_loss_sum / max(1, len(train_loader))\n",
    "\n",
    "        # ---- Validate ----\n",
    "        model.eval()\n",
    "        val_loss_sum = 0.0\n",
    "        with torch.no_grad():\n",
    "            for past, future in val_loader:\n",
    "                pred = model(past)                                       # zeros-as-decoder-input path\n",
    "                loss = loss_fn(pred, future)\n",
    "                val_loss_sum += float(loss.item())\n",
    "        avg_val_loss = val_loss_sum / max(1, len(val_loader))\n",
    "\n",
    "        if rank == 0:\n",
    "            print({\"epoch\": epoch, \"train_loss\": avg_train_loss, \"val_loss\": avg_val_loss})\n",
    "\n",
    "        metrics = {\n",
    "            \"epoch\": epoch,\n",
    "            \"train_loss\": avg_train_loss,\n",
    "            \"val_loss\": avg_val_loss,\n",
    "        }\n",
    "\n",
    "        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "        # 6) Report + temp checkpoint (rank 0 attaches; others metrics-only)\n",
    "        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "        if rank == 0:\n",
    "            with tempfile.TemporaryDirectory() as tmpdir:\n",
    "                torch.save(model.state_dict(), os.path.join(tmpdir, \"model.pt\"))\n",
    "                torch.save(optimizer.state_dict(), os.path.join(tmpdir, \"optim.pt\"))\n",
    "                torch.save({\"epoch\": epoch}, os.path.join(tmpdir, \"meta.pt\"))\n",
    "                ckpt_out = Checkpoint.from_directory(tmpdir)\n",
    "                train.report(metrics, checkpoint=ckpt_out)\n",
    "        else:\n",
    "            train.report(metrics, checkpoint=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Launch training on 8 GPUs  \n",
    "Construct a `TorchTrainer` and run it. Ray automatically distributes the model across eight GPUs, prepares the datasets for each worker, and starts training. Also configure checkpointing to retain the top-performing models and set failure recovery to three attempts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11. Launch training\n",
    "\n",
    "trainer = TorchTrainer(\n",
    "    train_loop_per_worker=train_loop_per_worker,\n",
    "    train_loop_config={\"lr\": 1e-3, \"bs\": 4, \"epochs\": 20,\n",
    "                       \"d_model\": 128, \"nhead\": 4, \"num_layers\": 3},\n",
    "    scaling_config=ScalingConfig(num_workers=8, use_gpu=True),\n",
    "    run_config=RunConfig(\n",
    "        name=\"nyc_taxi_transformer\",\n",
    "        storage_path=os.path.join(DATA_DIR, \"results\"),  \n",
    "        checkpoint_config=CheckpointConfig(\n",
    "            num_to_keep=20,\n",
    "            # Let your loop decide when to checkpoint (each epoch). Scoring still applies.\n",
    "            checkpoint_score_attribute=\"val_loss\",\n",
    "            checkpoint_score_order=\"min\",\n",
    "            # (Optional) If you want the last epochâ€™s checkpoint regardless of score:\n",
    "            # checkpoint_at_end=True,\n",
    "        ),\n",
    "        failure_config=FailureConfig(max_failures=3),\n",
    "    ),\n",
    ")\n",
    "\n",
    "result = trainer.fit()\n",
    "print(\"Final metrics:\", result.metrics)\n",
    "\n",
    "# Best checkpoint (by val_loss) thanks to checkpoint_score_* above:\n",
    "best_ckpt = result.checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Plot training and validation loss  \n",
    "After training, visualize the saved metrics to assess whether the model is over-fitting, under-fitting, or improving steadily. A healthy curve shows decreasing train and validation loss, with convergence over time. This diagnostic is especially useful when comparing different model configurations. In this tutorial, you aren't using substantial amounts of data, so you see the validation curve remains primarily stagnant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12. Plot train/val loss curves (from Ray Train results)\n",
    "\n",
    "# Pull full metrics history Ray stored for this run\n",
    "df = result.metrics_dataframe.copy()\n",
    "\n",
    "# Keep only relevant columns (defensive in case Ray adds extras)\n",
    "cols = [c for c in [\"epoch\", \"train_loss\", \"val_loss\"] if c in df.columns]\n",
    "df = df[cols].dropna()\n",
    "\n",
    "# If multiple reports per epoch exist, keep the latest one\n",
    "if \"epoch\" in df.columns:\n",
    "    df = df.sort_index().groupby(\"epoch\", as_index=False).last()\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(7, 4))\n",
    "if \"train_loss\" in df.columns:\n",
    "    plt.plot(df[\"epoch\"], df[\"train_loss\"], marker=\"o\", label=\"Train\")\n",
    "if \"val_loss\" in df.columns:\n",
    "    plt.plot(df[\"epoch\"], df[\"val_loss\"], marker=\"o\", label=\"Val\")\n",
    "\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"SmoothL1 Loss\")\n",
    "plt.title(\"TimeSeriesTransformer â€” Train vs. Val Loss\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Resume training from checkpoint  \n",
    "Ray automatically resumes from the latest checkpoint if one is available. This makes fault tolerance seamlessâ€”if the system interrupts a node or it fails, training can pick up where it left off with no manual intervention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13. Demonstrate fault-tolerant resume\n",
    "result = trainer.fit()\n",
    "print(\"Metrics after resume run:\", result.metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Inference helper â€” Ray Data batch predictor on GPU  \n",
    "\n",
    "Define a Ray Data-based batch predictor class that loads the trained `TimeSeriesTransformer` once per GPU actor and keeps it resident in memory for efficient inference.  \n",
    "Each actor processes batches of input windows (e.g., recent time series segments) in parallel, producing forecasts for the next horizon.  \n",
    "\n",
    "Ray Data inference enables scalable, fault-tolerant prediction pipelines that reuse loaded models across many requests, making it ideal for large-scale batch or near-real-time forecasting workloads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 14. Ray Data inference helper â€” stateful per-actor predictor\n",
    "\n",
    "class TimeSeriesBatchPredictor:\n",
    "    \"\"\"\n",
    "    Keeps the TimeSeriesTransformer in memory per actor (GPU if available).\n",
    "    Expects a Pandas batch with a 'past' column containing np.ndarray of shape (INPUT_WINDOW,).\n",
    "    Returns a batch with a 'pred' column (np.ndarray of shape (HORIZON,)).\n",
    "    \"\"\"\n",
    "    def __init__(self, checkpoint_path: str, model_kwargs: dict):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # Recreate model with the *same* hyperparams used during training\n",
    "        self.model = TimeSeriesTransformer(\n",
    "            input_window=model_kwargs[\"input_window\"],\n",
    "            horizon=model_kwargs[\"horizon\"],\n",
    "            d_model=model_kwargs[\"d_model\"],\n",
    "            nhead=model_kwargs[\"nhead\"],\n",
    "            num_layers=model_kwargs[\"num_layers\"],\n",
    "        ).to(self.device).eval()\n",
    "\n",
    "        # Load checkpoint weights once per actor\n",
    "        ckpt = Checkpoint.from_directory(checkpoint_path)\n",
    "        with ckpt.as_directory() as ckpt_dir:\n",
    "            state_dict = torch.load(os.path.join(ckpt_dir, \"model.pt\"), map_location=\"cpu\")\n",
    "            # Strip DDP prefix if present\n",
    "            state_dict = {k.replace(\"module.\", \"\", 1): v for k, v in state_dict.items()}\n",
    "            self.model.load_state_dict(state_dict)\n",
    "\n",
    "        torch.set_grad_enabled(False)\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        import pandas as pd\n",
    "\n",
    "        past_list = batch[\"past\"]  # each entry: np.ndarray shape (INPUT_WINDOW,)\n",
    "        # Stack into (B, T, 1)\n",
    "        x = np.stack([p.astype(np.float32) for p in past_list], axis=0)\n",
    "        x = torch.from_numpy(x).unsqueeze(-1).to(self.device)  # (B, INPUT_WINDOW, 1)\n",
    "\n",
    "        # Inference path uses the model's \"zeros as decoder input\" forward\n",
    "        preds = self.model(x).detach().cpu().numpy()  # (B, HORIZON)\n",
    "\n",
    "        out = batch.copy()\n",
    "        out[\"pred\"] = list(preds)  # each row: np.ndarray (HORIZON,)\n",
    "        return out[[\"pred\"]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Run distributed inference and visualize results  \n",
    "\n",
    "Use **Ray Data** to perform GPU-based batch inference with the trained model.  \n",
    "The model runs on a Ray worker, generates a forecast for the latest input window, and returns predictions to the driver.  \n",
    "De-normalize and plot the forecast against the ground truth to visually assess model performance.\n",
    "This tutorial uses a very small amount of data. As a result, you can see that the model learns a near-constant solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15. Run inference on the latest window with Ray Data and plot\n",
    "\n",
    "# 1) Prepare the latest window on the driver\n",
    "past_norm = hourly[\"norm\"].iloc[-INPUT_WINDOW:].to_numpy().astype(np.float32)\n",
    "future_true = hourly[\"passengers\"].iloc[-HORIZON:].to_numpy()  # for visualization only\n",
    "\n",
    "# 2) Get the best checkpoint directory selected by Ray\n",
    "with result.checkpoint.as_directory() as ckpt_dir:\n",
    "    best_ckpt_path = ckpt_dir  # path visible to workers\n",
    "\n",
    "# 3) Build a tiny Ray Dataset and run inference on a GPU actor\n",
    "model_kwargs = {\n",
    "    \"input_window\": INPUT_WINDOW,\n",
    "    \"horizon\": HORIZON,\n",
    "    \"d_model\": 128,\n",
    "    \"nhead\": 4,\n",
    "    \"num_layers\": 3,\n",
    "}\n",
    "\n",
    "ds = rdata.from_items([{\"past\": past_norm}])\n",
    "pred_ds = ds.map_batches(\n",
    "    TimeSeriesBatchPredictor,\n",
    "    fn_constructor_args=(best_ckpt_path, model_kwargs),\n",
    "    batch_size=1,\n",
    "    batch_format=\"pandas\",\n",
    "    concurrency=1,\n",
    "    num_gpus=1,  # force placement on a GPU worker if available\n",
    ")\n",
    "\n",
    "pred_row = pred_ds.take(1)[0]\n",
    "pred_norm = pred_row[\"pred\"]  # np.ndarray (HORIZON,)\n",
    "\n",
    "# 4) De-normalize on the driver\n",
    "mean, std = hourly[\"passengers\"].mean(), hourly[\"passengers\"].std()\n",
    "pred = pred_norm * std + mean\n",
    "past = past_norm * std + mean\n",
    "\n",
    "# 5) Plot\n",
    "\n",
    "t_past   = np.arange(-INPUT_WINDOW, 0)\n",
    "STEP_SIZE_HOURS = 0.5  # you mentioned 30-min data\n",
    "t_future = np.arange(0, HORIZON) * STEP_SIZE_HOURS\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(t_past, past, label=\"History\", marker=\"o\")\n",
    "plt.plot(t_future, future_true, \"--\", label=\"Ground Truth\")\n",
    "plt.plot(t_future, pred, \"-.\", label=\"Forecast\")\n",
    "plt.axvline(0)\n",
    "plt.xlabel(\"Hours relative\")\n",
    "plt.ylabel(\"# trips\")\n",
    "plt.title(\"NYC-Taxi Forecast (Ray Data Inference)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Cleanup: remove all training artifacts  \n",
    "Finally, tidy up by deleting temporary checkpoint folders and any intermediate result directories. Clearing out old artifacts frees disk space and leaves your workspace clean for whatever comes next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 16. Cleanup â€“ optionally remove all artifacts to free space\n",
    "if os.path.exists(DATA_DIR):\n",
    "    shutil.rmtree(DATA_DIR)\n",
    "    print(f\"Deleted {DATA_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrap up and next steps\n",
    "\n",
    "You built a robust, distributed forecasting workflow using **Ray Train on Anyscale** that:\n",
    "\n",
    "* Trains a Transformer model across **multiple GPUs** using **Ray Train with Distributed Data Parallel (DDP)**, abstracting away low-level orchestration.\n",
    "* Recovers automatically from failures with **built-in checkpointing and resume**, even across re-launches or node churn.\n",
    "* Logs and reports per-epoch metrics using **Ray Train's reporting APIs**, enabling real-time monitoring and seamless plotting.\n",
    "* Performs inference using **Ray Data**. This allows you to scale forecasting across GPUs or nodes without changing model code.\n",
    "\n",
    "---\n",
    "\n",
    "## Next steps\n",
    "\n",
    "The following are a few directions you can explore to extend or adapt this workload:\n",
    "\n",
    "1. **Hyperparameter sweeps**  \n",
    "   * Wrap the `TorchTrainer` with **Ray Tune** to search over `d_model`, `nhead`, learning rate, and window sizes.  \n",
    "\n",
    "2. **Probabilistic forecasting**  \n",
    "   * Output percentiles or fit a distribution head (for example, Gaussian) to capture prediction uncertainty.  \n",
    "\n",
    "3. **Multivariate and exogenous features**  \n",
    "   * Add weather, holidays, or ride-sharing surge multipliers as extra input channels.  \n",
    "\n",
    "4. **Early-stopping and LR scheduling**  \n",
    "   * Monitor val-loss and reduce LR on plateau, or stop when improvement < 1 %.  \n",
    "\n",
    "5. **Model compression**  \n",
    "   * Distill the large Transformer into a lightweight LSTM or Tiny-Transformer for edge deployment.  \n",
    "\n",
    "6. **Streaming and online learning**  \n",
    "   * Use **Ray Serve** to deploy the model and update weights periodically with the latest data.  \n",
    "\n",
    "7. **Interpretability**  \n",
    "   * Visualize attention maps to see which time lags the model focuses onâ€”effective for stakeholder trust.  \n",
    "\n",
    "8. **End-to-end MLOps**  \n",
    "   * Schedule nightly retraining with **Ray jobs**, log artifacts to MLflow or Weights & Biases, and automate model promotion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}