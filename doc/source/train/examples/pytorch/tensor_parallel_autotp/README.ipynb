{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get started with 2D Parallelism (Tensor + Data Parallelism) using DeepSpeed and Ray Train\n",
    "\n",
    "**Time to complete:** 20 min\n",
    "\n",
    "This template shows how to train large language models using tensor parallelism with DeepSpeed's AutoTP and Ray Train for distributed execution.\n",
    "\n",
    "**Tensor Parallelism (TP)** shards model weights across multiple GPUs, enabling training of models that are too large to fit on a single GPU. Combined with **Data Parallelism (DP)**, this creates a powerful **2D parallelism** strategy that scales efficiently to many GPUs.\n",
    "\n",
    "This tutorial provides a step-by-step guide covering:\n",
    "\n",
    "- Understanding 2D parallelism (Tensor Parallelism + Data Parallelism)\n",
    "- Setting up a data loader compatible with 2D parallelism\n",
    "- Preparing the model with DeepSpeed AutoTP and ZeRO\n",
    "- Checkpointing\n",
    "- Distributed training with Ray Train\n",
    "\n",
    "**Note:** This tutorial uses DeepSpeed's AutoTP API. DeepSpeed automatically identifies and shards linear layers for tensor parallelism."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"anyscale-note\" class=\"alert alert-block alert-warning\">\n",
    "\n",
    "  <strong>Anyscale Specific Configuration</strong>\n",
    "\n",
    "  <p><strong>Note:</strong> This tutorial is optimized for the Anyscale platform. When running on open source Ray, additional configuration is required. For example, you would need to manually:</p>\n",
    "\n",
    "  <ul>\n",
    "    <li><strong>Configure your Ray Cluster</strong>: Set up your multi-node environment and manage resource allocation without Anyscale's automation.</li>\n",
    "    <li><strong>Manage Dependencies</strong>: Manually install and manage dependencies on each node.</li>\n",
    "    <li><strong>Set Up Storage</strong>: Configure your own distributed or shared storage system for model checkpointing.</li>\n",
    "  </ul>\n",
    "</div>\n",
    "\n",
    "<style>\n",
    "  div#anyscale-note > p,\n",
    "  div#anyscale-note > ul,\n",
    "  div#anyscale-note > ul li {\n",
    "    color: black;\n",
    "  }\n",
    "\n",
    "  div#anyscale-note {\n",
    "    background-color: rgb(255, 243, 205);\n",
    "  }\n",
    "\n",
    "  div#anyscale-note {\n",
    "    border: 1px solid #ccc; \n",
    "    border-radius: 8px;\n",
    "    padding: 15px;\n",
    "  }\n",
    "\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding 2D Parallelism\n",
    "\n",
    "2D parallelism can be achieved by combining two complementary strategies:\n",
    "\n",
    "- **Tensor Parallelism (TP)**: Shards model weights across GPUs within a TP group. All GPUs in a TP group process the same input data but hold different parts of the model.\n",
    "- **Data Parallelism (DP)**: Replicates the model across DP groups. Each DP group processes different data and synchronizes gradients.\n",
    "\n",
    "With `tp_size=2` and `dp_size=2` on 4 GPUs, the device mesh looks like:\n",
    "\n",
    "```\n",
    "Device Mesh (2x2):\n",
    "        TP Dim\n",
    "      [0]  [1]\n",
    " DP   +---+---+\n",
    " Dim  | 0 | 1 |  <- TP Group 0 (same data, sharded model)\n",
    "      +---+---+\n",
    "      | 2 | 3 |  <- TP Group 1 (same data, sharded model)\n",
    "      +---+---+\n",
    "        ^   ^\n",
    "       DP Groups (different data, gradient sync)\n",
    "```\n",
    "\n",
    "- **TP Groups** (rows): GPUs 0,1 and GPUs 2,3 share the same input data but have sharded model weights\n",
    "- **DP Groups** (columns): GPUs 0,2 and GPUs 1,3 see different data and synchronize gradients\n",
    "\n",
    "The following figure illustrates how tensor parallelism partitions a linear layer across GPUs:\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"images/tp_partition.png\" alt=\"Tensor Parallelism Partitioning\" width=\"60%\">\n",
    "</p>\n",
    "\n",
    "In tensor parallelism, linear layers alternate between column-wise and row-wise partitioning:\n",
    "\n",
    "1. **Column-wise partitioned layer**: The parameter tensor is split along the output dimension. The input activation is replicated across all GPUs. Each GPU computes its portion, producing a column-wise partitioned output.\n",
    "\n",
    "2. **Row-wise partitioned layer**: The next layer's parameters are split along the input dimension. Each GPU computes locally with its partition, then an **all-reduce** operation sums the partial results across all GPUs to produce the mathematically correct output.\n",
    "\n",
    "This alternating pattern allows consecutive layers to be computed efficiently while maintaining numerical correctness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why DeepSpeed AutoTP?\n",
    "\n",
    "This tutorial uses [DeepSpeed AutoTP](https://github.com/deepspeedai/DeepSpeed/blob/master/blogs/huggingface-tp/README.md) for tensor parallelism, which provides two key benefits. For a complete overview of AutoTP, see the [DeepSpeed AutoTP docs](https://deepspeed.readthedocs.io/en/latest/training.html#automatic-tensor-parallel-training).\n",
    "\n",
    "1. **Automatic partitioning**: For supported models, AutoTP automatically identifies which parameters should be partitioned and in which dimensions. You don't need to manually define partitioning patterns. Most popular models including Llama and Qwen series are already supported out of the box.\n",
    "\n",
    "2. **Efficient combination with ZeRO**: DeepSpeed has a series of memory optimization techniques called ZeRO stage 1, 2, and 3, which drastically reduce the memory usage of data parallelism. AutoTP integrates seamlessly with DeepSpeed's ZeRO (stage 1 and 2). This combination allows you to scale to larger models while maintaining high training throughput.\n",
    "\n",
    "You can also build tensor parallelism with PyTorch DTensor and FSDP, but you must explicitly define which parameters shard on which dimensions (for example, with `parallelize_module`) and maintain that plan as your model changes. AutoTP removes that step when the model is supported. The trade-off is that AutoTP doesn't support ZeRO stage 3 parameter sharding across data parallel ranks. If you need full parameter sharding to minimize memory, FSDP + DTensor is the better fit, though it can add communication overhead at larger data parallel sizes. For details on DTensor-based tensor parallelism, see the [PyTorch DTensor TP docs](https://docs.pytorch.org/docs/stable/distributed.tensor.parallel.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When to Use Tensor Parallelism vs ZeRO stage 3\n",
    "\n",
    "DeepSpeed ZeRO stage 3 partitions model parameters across GPUs, but they have different communication patterns and overhead characteristics.\n",
    "\n",
    "**Communication patterns:**\n",
    "\n",
    "- **ZeRO Stage 3**: Gathers partitioned parameters via all-gather before each layer's computation. The communication volume is proportional to the model size (M × 2 bytes for bfloat16 with M parameters).\n",
    "- **Tensor Parallelism**: Uses all-reduce on activations (intermediate data) after each layer. The communication volume depends on: layers × hidden_size × seq_length × batch_size × 2 (dtype) × 2 (attention + FFN).\n",
    "\n",
    "**Key trade-offs:**\n",
    "\n",
    "- **TP benefits from high-bandwidth interconnects**: With NVLink or similar high-speed connections, TP's frequent but small communications complete quickly, enabling efficient layer-by-layer parallelism.\n",
    "- **ZeRO Stage 3's overhead is proportional to model size**, which can become a bottleneck for very large models even when communication overlaps with computation.\n",
    "- **TP is more efficient for large models with moderate batch sizes**, since TP's communication volume depends on activation size rather than model size. For very large models, activation-based communication can be significantly smaller than parameter-based communication.\n",
    "\n",
    "**Recommended configuration:**\n",
    "\n",
    "Given these trade-offs, a typical configuration is:\n",
    "\n",
    "- **Use TP for intra-node parallelism**: Leverage high-speed interconnects like NVLink within a node where latency is low and bandwidth is high.\n",
    "- **Use ZeRO for inter-node parallelism**: Inter-node communication has higher latency, but ZeRO's ability to overlap communication with computation makes it more suitable for this setting.\n",
    "\n",
    "This is why we combine TP with ZeRO in this tutorial—TP handles the fast GPU-to-GPU communication within a node, while ZeRO handles the data parallel synchronization that can span across nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Package and environment setup\n",
    "\n",
    "Install the required dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "%%bash\npip install torch transformers datasets \"deepspeed>=0.18.6\""
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import logging\n",
    "import tempfile\n",
    "import uuid\n",
    "\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "\n",
    "# Set up logging\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def get_mixed_precision_dtype() -> torch.dtype:\n",
    "    \"\"\"Select a mixed-precision dtype that the current GPU supports.\"\"\"\n",
    "    if not torch.cuda.is_available():\n",
    "        return torch.float32\n",
    "    return torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data loading with TP-aware sharding\n",
    "\n",
    "A critical aspect of tensor parallelism is ensuring all GPUs in a TP group receive identical input data. Standard data loaders shard by `world_rank`, giving each GPU different data. With TP, you must shard by `dp_rank` instead.\n",
    "\n",
    "**Global batch size**: Because all GPUs in a TP group see the same data, the effective (global) batch size is `batch_size_per_gpu * dp_size`, not `batch_size_per_gpu * world_size`. For example, with `batch_size_per_gpu=1`, `dp_size=2`, and `tp_size=2` (4 GPUs total), the global batch size is 2, not 4.\n",
    "\n",
    "```python\n",
    "# All TP ranks in same DP group get identical batches\n",
    "sampler = DistributedSampler(\n",
    "    dataset,\n",
    "    num_replicas=dp_size,  # NOT world_size\n",
    "    rank=dp_rank,          # NOT world_rank\n",
    ")\n",
    "```\n",
    "\n",
    "The following function creates a dataloader with proper TP-aware sharding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from datasets import DownloadConfig, load_dataset\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data.distributed import DistributedSampler\nfrom transformers import AutoTokenizer\n\nimport ray.train\n\n\ndef create_dataloader(\n    model_name: str,\n    dataset_name: str,\n    seq_length: int,\n    batch_size_per_gpu: int,\n    dp_rank: int,\n    dp_size: int,\n    seed: int = 42,\n    dataset_percentage: float = 10.0,\n) -> DataLoader:\n    \"\"\"\n    Create dataloader with TP-aware sharding.\n\n    IMPORTANT: Uses dp_rank/dp_size for sharding (NOT world_rank/world_size).\n    This ensures all TP ranks in the same DP group see identical batches.\n    \"\"\"\n    world_rank = ray.train.get_context().get_world_rank()\n\n    # Handle datasets that require a config name\n    dataset_config = \"wikitext-2-raw-v1\" if dataset_name == \"wikitext\" else None\n    dataset_percentage = float(dataset_percentage)\n    if not 0 < dataset_percentage <= 100:\n        raise ValueError(\n            f\"dataset_percentage must be in (0, 100], got {dataset_percentage}.\"\n        )\n    split_spec = f\"train[:{dataset_percentage:.15g}%]\"\n\n    # Rank 0 downloads first to avoid conflicts\n    if world_rank == 0:\n        AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n        load_dataset(\n            dataset_name,\n            dataset_config,\n            split=split_spec,\n            download_config=DownloadConfig(disable_tqdm=True),\n        )\n    dist.barrier()\n\n    # All ranks load from cache\n    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n    dataset = load_dataset(\n        dataset_name,\n        dataset_config,\n        split=split_spec,\n        download_config=DownloadConfig(disable_tqdm=True),\n    )\n\n    # Set pad token if needed\n    if tokenizer.pad_token is None:\n        if tokenizer.eos_token is not None:\n            tokenizer.pad_token = tokenizer.eos_token\n        else:\n            raise ValueError(\n                f\"Tokenizer for {model_name} has no pad_token or eos_token. \"\n                \"Please set a pad token manually.\"\n            )\n\n    # Tokenize dataset\n    def tokenize_fn(examples):\n        return tokenizer(\n            examples[\"text\"], padding=\"max_length\", max_length=seq_length, truncation=True\n        )\n\n    tokenized = dataset.map(\n        tokenize_fn,\n        batched=True,\n        num_proc=None,\n        keep_in_memory=True,\n        remove_columns=dataset.column_names,\n    )\n    tokenized = tokenized.filter(\n        lambda example: sum(example[\"attention_mask\"]) > 1,\n        keep_in_memory=True,\n    )\n\n    # Add labels (ignore padding tokens for causal LM)\n    def add_labels(examples):\n        labels = []\n        for input_ids, attention_mask in zip(\n            examples[\"input_ids\"], examples[\"attention_mask\"]\n        ):\n            labels.append(\n                [\n                    token if mask == 1 else -100\n                    for token, mask in zip(input_ids, attention_mask)\n                ]\n            )\n        examples[\"labels\"] = labels\n        return examples\n\n    tokenized = tokenized.map(add_labels, batched=True, num_proc=None, keep_in_memory=True)\n    tokenized.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n\n    # [1] Use DP rank/size for sharding (ensures TP ranks get same data)\n    sampler = DistributedSampler(\n        tokenized, num_replicas=dp_size, rank=dp_rank, shuffle=True, seed=seed\n    )\n\n    return DataLoader(tokenized, batch_size=batch_size_per_gpu, sampler=sampler, drop_last=True)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3. Model Parallelization with DeepSpeed AutoTP\n\nDeepSpeed AutoTP is config-driven: setting `\"tensor_parallel\": {\"autotp_size\": N}` in the DeepSpeed config activates automatic tensor parallelism. The sharding happens inside `deepspeed.initialize()` — no additional API calls are needed.\n\nThe Model Parallel Unit (MPU) interface tells DeepSpeed about the parallelism topology, enabling correct gradient synchronization across data parallel ranks."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import deepspeed\nfrom transformers import AutoConfig, AutoModelForCausalLM\n\nimport ray.train.torch\n\n\nclass ModelParallelUnit:\n    \"\"\"\n    Model Parallel Unit (MPU) interface for DeepSpeed.\n\n    DeepSpeed uses this to understand the parallelism topology and\n    perform gradient all-reduce only across data parallel ranks.\n    \"\"\"\n\n    def __init__(\n        self,\n        tp_group: dist.ProcessGroup,\n        dp_group: dist.ProcessGroup,\n        tp_size: int,\n        dp_size: int,\n        tp_rank: int,\n        dp_rank: int,\n    ):\n        self._tp_group = tp_group\n        self._dp_group = dp_group\n        self._tp_size = tp_size\n        self._dp_size = dp_size\n        self._tp_rank = tp_rank\n        self._dp_rank = dp_rank\n\n    def get_data_parallel_group(self) -> dist.ProcessGroup:\n        return self._dp_group\n\n    def get_model_parallel_group(self) -> dist.ProcessGroup:\n        return self._tp_group\n\n    def get_data_parallel_world_size(self) -> int:\n        return self._dp_size\n\n    def get_model_parallel_world_size(self) -> int:\n        return self._tp_size\n\n    def get_data_parallel_rank(self) -> int:\n        return self._dp_rank\n\n    def get_model_parallel_rank(self) -> int:\n        return self._tp_rank\n\n\ndef setup_model_with_autotp(\n    model_name: str,\n    tp_size: int,\n    dp_size: int,\n    world_rank: int,\n    world_size: int,\n    config: dict,\n):\n    \"\"\"\n    Set up the model with DeepSpeed AutoTP and optional data parallelism.\n\n    Returns:\n        tuple: (engine, tp_group, dp_group, tp_rank, dp_rank)\n    \"\"\"\n    dtype = get_mixed_precision_dtype()\n    use_bf16 = dtype == torch.bfloat16\n\n    # Validate configuration\n    if dp_size * tp_size != world_size:\n        raise ValueError(\n            f\"dp_size ({dp_size}) * tp_size ({tp_size}) must equal \"\n            f\"world_size ({world_size})\"\n        )\n\n    # Calculate TP and DP rank\n    tp_rank = world_rank % tp_size\n    dp_rank = world_rank // tp_size\n\n    # Load model config and validate TP compatibility\n    hf_config = AutoConfig.from_pretrained(model_name, trust_remote_code=True)\n    num_kv_heads = getattr(hf_config, \"num_key_value_heads\", None)\n    if num_kv_heads is None:\n        num_kv_heads = getattr(hf_config, \"num_attention_heads\", None)\n    if num_kv_heads is None:\n        raise ValueError(\n            \"Model config must define `num_key_value_heads` or `num_attention_heads` \"\n            \"to validate tensor parallel compatibility.\"\n        )\n    if num_kv_heads % tp_size != 0:\n        raise ValueError(f\"TP size {tp_size} must divide attention heads count {num_kv_heads}\")\n\n    if world_rank == 0:\n        logger.info(f\"Setting up 2D mesh: dp_size={dp_size}, tp_size={tp_size}\")\n\n    # [1] Create TP and DP process groups\n    tp_group = None\n    dp_group = None\n\n    # Create TP groups (processes in the same row)\n    for dp_idx in range(dp_size):\n        tp_group_ranks = list(range(dp_idx * tp_size, (dp_idx + 1) * tp_size))\n        group = dist.new_group(tp_group_ranks)\n        if world_rank in tp_group_ranks:\n            tp_group = group\n\n    # Create DP groups (processes in the same column)\n    for tp_idx in range(tp_size):\n        dp_group_ranks = [tp_idx + dp_idx * tp_size for dp_idx in range(dp_size)]\n        group = dist.new_group(dp_group_ranks)\n        if world_rank in dp_group_ranks:\n            dp_group = group\n\n    if world_rank == 0:\n        logger.info(f\"Process groups created: tp_rank={tp_rank}, dp_rank={dp_rank}\")\n\n    # [2] Initialize DeepSpeed distributed backend\n    deepspeed.init_distributed()\n\n    # [3] Load pretrained model weights and move to device\n    device = torch.device(f\"cuda:{world_rank % torch.cuda.device_count()}\")\n    model = AutoModelForCausalLM.from_pretrained(\n        model_name,\n        trust_remote_code=True,\n        torch_dtype=dtype,\n    )\n    model = model.to(device)\n\n    # [4] Build DeepSpeed config\n    # AutoTP is activated by setting \"tensor_parallel\": {\"autotp_size\": N}.\n    # Sharding happens automatically inside deepspeed.initialize().\n    batch_size_per_gpu = config.get(\"batch_size_per_gpu\", 1)\n    gradient_accumulation_steps = config.get(\"gradient_accumulation_steps\", 1)\n    zero_stage = config.get(\"zero_stage\", 1)\n    if zero_stage not in (1, 2):\n        raise ValueError(\n            f\"DeepSpeed AutoTP supports ZeRO stage 1 or 2, but got zero_stage={zero_stage}.\"\n        )\n    effective_dp = dp_size if tp_size > 1 else world_size\n\n    ds_config = {\n        \"train_batch_size\": batch_size_per_gpu * effective_dp * gradient_accumulation_steps,\n        \"train_micro_batch_size_per_gpu\": batch_size_per_gpu,\n        \"gradient_accumulation_steps\": gradient_accumulation_steps,\n        \"gradient_clipping\": config.get(\"max_grad_norm\", 1.0),\n        \"zero_optimization\": {\n            \"stage\": zero_stage,\n            \"overlap_comm\": True,\n        },\n        \"tensor_parallel\": {\n            \"autotp_size\": tp_size,\n        },\n        \"data_parallel_size\": dp_size,\n        \"zero_allow_untested_optimizer\": True,\n        \"steps_per_print\": 2000,\n        \"wall_clock_breakdown\": False,\n    }\n\n    if use_bf16:\n        ds_config[\"bf16\"] = {\n            \"enabled\": True,\n            \"bf16_master_weights_and_grads\": True,\n            \"bf16_optimizer_states\": True,\n        }\n    else:\n        ds_config[\"fp16\"] = {\n            \"enabled\": True,\n        }\n\n    # [5] Create optimizer\n    params = list(model.parameters())\n    optimizer = torch.optim.AdamW(\n        params,\n        lr=config.get(\"learning_rate\", 1e-5),\n        weight_decay=config.get(\"weight_decay\", 0.01),\n    )\n\n    # [6] Create MPU for DeepSpeed\n    mpu = ModelParallelUnit(\n        tp_group=tp_group,\n        dp_group=dp_group,\n        tp_size=tp_size,\n        dp_size=dp_size,\n        tp_rank=tp_rank,\n        dp_rank=dp_rank,\n    )\n\n    # [7] Initialize DeepSpeed engine\n    # AutoTP sharding is applied automatically during initialize()\n    engine, optimizer, _, _ = deepspeed.initialize(\n        model=model,\n        optimizer=optimizer,\n        config=ds_config,\n        mpu=mpu,\n    )\n\n    if world_rank == 0:\n        num_params = sum(p.numel() for p in params)\n        logger.info(f\"Model initialized with {num_params:,} parameters\")\n        if dp_size > 1:\n            logger.info(f\"2D parallelism: {dp_size} DP x {tp_size} TP\")\n\n    return engine, tp_group, dp_group, tp_rank, dp_rank"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Checkpointing\n",
    "\n",
    "With tensor parallelism, each worker holds a shard of the model. Checkpointing saves each shard independently, and Ray Train aggregates them into a single checkpoint.\n",
    "\n",
    "For DeepSpeed AutoTP + ZeRO, prefer DeepSpeed's native checkpoint API. `engine.save_checkpoint(...)` writes model shards, ZeRO optimizer partition state, and training state in a format that `engine.load_checkpoint(...)` can restore correctly.\n",
    "\n",
    "In this example, all workers call `engine.save_checkpoint(...)` with the same `tag` and `client_state` (`epoch`/`step`), then each reports the checkpoint directory to Ray Train via `ray.train.report(..., checkpoint=...)`.\n",
    "\n",
    "On restore, each worker calls `engine.load_checkpoint(...)` from the Ray Train checkpoint directory and resumes from `client_state[\"epoch\"] + 1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.train import Checkpoint\n",
    "\n",
    "\n",
    "def save_checkpoint(\n",
    "    engine,\n",
    "    epoch: int,\n",
    "    step: int,\n",
    "    avg_loss: float,\n",
    ") -> None:\n",
    "    \"\"\"Save checkpoint and report to Ray Train.\"\"\"\n",
    "    with tempfile.TemporaryDirectory() as checkpoint_dir:\n",
    "        ckpt_tag = f\"epoch_{epoch:04d}_step_{step:06d}\"\n",
    "        # DeepSpeed requires all ranks to call save_checkpoint with same tag.\n",
    "        engine.save_checkpoint(\n",
    "            checkpoint_dir,\n",
    "            tag=ckpt_tag,\n",
    "            client_state={\"epoch\": epoch, \"step\": step},\n",
    "            save_latest=True,\n",
    "        )\n",
    "\n",
    "        # All workers must call report() with their checkpoint\n",
    "        checkpoint = Checkpoint.from_directory(checkpoint_dir)\n",
    "        ray.train.report({\"loss\": avg_loss, \"epoch\": epoch}, checkpoint=checkpoint)\n",
    "\n",
    "\n",
    "def load_checkpoint(\n",
    "    engine,\n",
    ") -> int:\n",
    "    \"\"\"Load DeepSpeed checkpoint from Ray Train checkpoint directory.\"\"\"\n",
    "    checkpoint = ray.train.get_checkpoint()\n",
    "    if checkpoint is None:\n",
    "        return 0\n",
    "\n",
    "    with checkpoint.as_directory() as checkpoint_dir:\n",
    "        load_path, client_state = engine.load_checkpoint(\n",
    "            checkpoint_dir,\n",
    "            load_optimizer_states=True,\n",
    "            load_lr_scheduler_states=True,\n",
    "        )\n",
    "        if load_path is None:\n",
    "            raise RuntimeError(\"Failed to load DeepSpeed checkpoint.\")\n",
    "\n",
    "        start_epoch = 0\n",
    "        if client_state is not None:\n",
    "            start_epoch = client_state.get(\"epoch\", -1) + 1\n",
    "\n",
    "    return start_epoch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training loop\n",
    "\n",
    "The main training function brings together all components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_func(config):\n",
    "    \"\"\"\n",
    "    Main training loop executed by each Ray Train worker.\n",
    "\n",
    "    This function:\n",
    "    1. Sets up TP and DP process groups\n",
    "    2. Creates and shards the model with DeepSpeed AutoTP\n",
    "    3. Runs the training loop with checkpointing\n",
    "    \"\"\"\n",
    "    # Get Ray Train context\n",
    "    world_rank = ray.train.get_context().get_world_rank()\n",
    "    world_size = ray.train.get_context().get_world_size()\n",
    "    device = ray.train.torch.get_device()\n",
    "\n",
    "    tp_size = config[\"tp_size\"]\n",
    "    dp_size = config[\"dp_size\"]\n",
    "\n",
    "    if world_rank == 0:\n",
    "        logger.info(f\"Worker started: world_rank={world_rank}, world_size={world_size}\")\n",
    "\n",
    "    # Set up model with DeepSpeed AutoTP\n",
    "    engine, _, _, _, dp_rank = setup_model_with_autotp(\n",
    "        model_name=config[\"model_name\"],\n",
    "        tp_size=tp_size,\n",
    "        dp_size=dp_size,\n",
    "        world_rank=world_rank,\n",
    "        world_size=world_size,\n",
    "        config=config,\n",
    "    )\n",
    "    start_epoch = load_checkpoint(engine)\n",
    "\n",
    "    # Create dataloader with TP-aware sharding\n",
    "    dataloader = create_dataloader(\n",
    "        model_name=config[\"model_name\"],\n",
    "        dataset_name=config[\"dataset_name\"],\n",
    "        seq_length=config[\"seq_length\"],\n",
    "        batch_size_per_gpu=config[\"batch_size_per_gpu\"],\n",
    "        dp_rank=dp_rank,\n",
    "        dp_size=dp_size,\n",
    "        seed=config.get(\"seed\", 42),\n",
    "        dataset_percentage=config.get(\"dataset_percentage\", 10.0),\n",
    "    )\n",
    "\n",
    "    steps_per_epoch = len(dataloader)\n",
    "    if world_rank == 0:\n",
    "        logger.info(f\"Dataloader created: {steps_per_epoch} steps per epoch\")\n",
    "    if steps_per_epoch == 0:\n",
    "        raise ValueError(\n",
    "            \"Dataloader is empty. Increase dataset_percentage or reduce batch_size_per_gpu.\"\n",
    "        )\n",
    "    log_interval = config.get(\"log_interval\", 10)\n",
    "\n",
    "    # Training loop\n",
    "    engine.train()\n",
    "\n",
    "    for epoch in range(start_epoch, config[\"num_epochs\"]):\n",
    "        dataloader.sampler.set_epoch(epoch)\n",
    "\n",
    "        running_loss = 0.0\n",
    "        num_batches = 0\n",
    "        step = -1  # Initialize in case dataloader is empty\n",
    "\n",
    "        for step, batch in enumerate(dataloader):\n",
    "            # Move batch to device\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "            # Forward pass with labels for loss computation\n",
    "            outputs = engine(\n",
    "                input_ids=batch[\"input_ids\"],\n",
    "                attention_mask=batch[\"attention_mask\"],\n",
    "                labels=batch[\"labels\"],\n",
    "                use_cache=False,\n",
    "            )\n",
    "            loss = outputs.loss\n",
    "\n",
    "            # Backward pass (through DeepSpeed engine)\n",
    "            engine.backward(loss)\n",
    "\n",
    "            # Optimizer step (through DeepSpeed engine)\n",
    "            engine.step()\n",
    "\n",
    "            # Track loss\n",
    "            loss_value = loss.item()\n",
    "            running_loss += loss_value\n",
    "            num_batches += 1\n",
    "\n",
    "            # Log progress\n",
    "            if (\n",
    "                world_rank == 0\n",
    "                and log_interval is not None\n",
    "                and log_interval > 0\n",
    "                and step % log_interval == 0\n",
    "            ):\n",
    "                logger.info(\n",
    "                    f\"Epoch: {epoch} Step: {step + 1}/{steps_per_epoch} Loss: {loss_value:.4f}\"\n",
    "                )\n",
    "\n",
    "            # Debug mode: stop early for testing\n",
    "            if config.get(\"debug_steps\", 0) > 0 and step + 1 >= config[\"debug_steps\"]:\n",
    "                if world_rank == 0:\n",
    "                    logger.info(f\"Debug steps finished. Stopping epoch {epoch}.\")\n",
    "                break\n",
    "\n",
    "        if num_batches == 0:\n",
    "            if world_rank == 0:\n",
    "                logger.warning(\n",
    "                    f\"Epoch {epoch} processed zero batches. Skipping checkpoint and continuing.\"\n",
    "                )\n",
    "            continue\n",
    "\n",
    "        # Calculate average loss for epoch\n",
    "        avg_loss = running_loss / num_batches\n",
    "\n",
    "        # Save checkpoint at end of epoch\n",
    "        save_checkpoint(engine, epoch, step, avg_loss)\n",
    "\n",
    "        if world_rank == 0:\n",
    "            logger.info(f\"Epoch {epoch} completed. Average loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Launch the distributed training job\n",
    "\n",
    "Configure and launch the training job using Ray Train's TorchTrainer. This example uses:\n",
    "- 4 workers (GPUs)\n",
    "- 2-way tensor parallelism\n",
    "- 2-way data parallelism\n",
    "- A small model (Qwen2.5-0.5B) for demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.train import RunConfig, ScalingConfig\n",
    "from ray.train.torch import TorchTrainer\n",
    "\n",
    "# Parallelism configuration\n",
    "tp_size = 2  # Tensor parallel degree\n",
    "dp_size = 2  # Data parallel degree\n",
    "num_workers = tp_size * dp_size  # Total workers must equal tp_size * dp_size\n",
    "\n",
    "# Configure distributed training resources\n",
    "scaling_config = ScalingConfig(\n",
    "    num_workers=num_workers,\n",
    "    use_gpu=True,\n",
    ")\n",
    "\n",
    "# Training configuration\n",
    "train_loop_config = {\n",
    "    # Model and data\n",
    "    \"model_name\": \"Qwen/Qwen2.5-0.5B\",  # Small model for demo\n",
    "    \"dataset_name\": \"wikitext\",\n",
    "    \"dataset_percentage\": 5.0,  # Use 5% of dataset for faster demo\n",
    "    # Parallelism\n",
    "    \"tp_size\": tp_size,\n",
    "    \"dp_size\": dp_size,\n",
    "    # Training hyperparameters\n",
    "    \"batch_size_per_gpu\": 1,  # Global batch size = batch_size_per_gpu * dp_size\n",
    "    \"seq_length\": 512,\n",
    "    \"num_epochs\": 1,\n",
    "    \"learning_rate\": 1e-5,\n",
    "    \"weight_decay\": 0.01,\n",
    "    # Logging and debug\n",
    "    \"log_interval\": 5,\n",
    "    \"debug_steps\": 20,  # Stop after 20 steps for demo (set to 0 for full training)\n",
    "    \"seed\": 42,\n",
    "}\n",
    "\n",
    "# Create experiment name\n",
    "experiment_name = f\"tp_autotp_{uuid.uuid4().hex[:8]}\"\n",
    "storage_path = \"/tmp/ray_train_tp_autotp\"  # Use persistent/shared storage in production\n",
    "\n",
    "# Configure run settings\n",
    "run_config = RunConfig(\n",
    "    name=experiment_name,\n",
    "    storage_path=storage_path,\n",
    ")\n",
    "\n",
    "# Initialize and launch the trainer\n",
    "trainer = TorchTrainer(\n",
    "    train_loop_per_worker=train_func,\n",
    "    scaling_config=scaling_config,\n",
    "    train_loop_config=train_loop_config,\n",
    "    run_config=run_config,\n",
    ")\n",
    "\n",
    "print(f\"Starting tensor parallel training with {tp_size}-way TP and {dp_size}-way DP...\")\n",
    "result = trainer.fit()\n",
    "print(\"Training completed successfully!\")\n",
    "print(f\"Final metrics: {result.metrics}\")\n",
    "\n",
    "# Reuse the same RunConfig(name, storage_path). train_func() will receive the\n",
    "# latest checkpoint from ray.train.get_checkpoint() and continue automatically.\n",
    "RUN_RESUME_DEMO = False\n",
    "if RUN_RESUME_DEMO:\n",
    "    resume_train_loop_config = dict(train_loop_config)\n",
    "    resume_train_loop_config[\"num_epochs\"] = 2  # Continue to epoch 1\n",
    "    resume_trainer = TorchTrainer(\n",
    "        train_loop_per_worker=train_func,\n",
    "        scaling_config=scaling_config,\n",
    "        train_loop_config=resume_train_loop_config,\n",
    "        run_config=RunConfig(name=experiment_name, storage_path=storage_path),\n",
    "    )\n",
    "    resume_result = resume_trainer.fit()\n",
    "    print(f\"Resumed metrics: {resume_result.metrics}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling to larger models\n",
    "\n",
    "To train larger models like Qwen2-7B or Llama-3-8B, adjust the configuration. For example, on 8 GPUs you can use 4-way TP and 2-way DP:\n",
    "\n",
    "```python\n",
    "train_loop_config = {\n",
    "    \"model_name\": \"Qwen/Qwen2-7B\",\n",
    "    \"tp_size\": 4,\n",
    "    \"dp_size\": 2,\n",
    "    \"batch_size_per_gpu\": 1,\n",
    "    \"seq_length\": 2048,\n",
    "    ...\n",
    "}\n",
    "\n",
    "scaling_config = ScalingConfig(\n",
    "    num_workers=8,\n",
    "    use_gpu=True,\n",
    ")\n",
    "```\n",
    "\n",
    "**Tips for scaling:**\n",
    "- Increase `tp_size` to fit larger models (TP shards model weights)\n",
    "- Increase `dp_size` to improve throughput (DP processes more data in parallel)\n",
    "- Ensure `tp_size` divides the model's `num_key_value_heads`\n",
    "- Use NVLink-connected GPUs for efficient TP communication\n",
    "- DeepSpeed AutoTP works with ZeRO optimization for additional memory efficiency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this tutorial, you learned:\n",
    "\n",
    "- How 2D parallelism combines tensor parallelism and data parallelism\n",
    "- How to set up DeepSpeed AutoTP for automatic model sharding\n",
    "- The importance of TP-aware data loading for correct gradient computation\n",
    "- How to combine AutoTP with ZeRO optimization for 2D parallelism\n",
    "- How to save distributed checkpoints with Ray Train\n",
    "\n",
    "For production training of large models, consider:\n",
    "- Using larger `tp_size` for models that don't fit on a single GPU\n",
    "- Enabling activation checkpointing for memory efficiency (`model.gradient_checkpointing_enable()`)\n",
    "- Using ZeRO Stage 2 or 3 for additional memory savings with large models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  },
  "orphan": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}