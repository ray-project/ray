{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Fine-tune an LLM with Ray Train and DeepSpeed\n",
    "\n",
    "**Time to complete:** 20 min\n",
    "\n",
    "This notebook combines **Ray Train** with **DeepSpeed** to efficiently scale PyTorch training across GPUs and nodes while minimizing GPU memory usage.\n",
    "\n",
    "This hands-on example includes the following:\n",
    "-  Fine-tuning an LLM\n",
    "- Checkpoint saving and resuming with Ray Train\n",
    "- Configuring ZeRO for memory and performance (stages, mixed precision, CPU offload)\n",
    "- Launching a distributed training job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"anyscale-note\" class=\"alert alert-block alert-warning\">\n",
    "\n",
    "  <strong>Anyscale specific configuration</strong>\n",
    "\n",
    "  <p><strong>Note:</strong> This template is optimized for the Anyscale platform. On Anyscale, most configuration is automated. When running on open-source Ray, manually complete the following steps:</p>\n",
    "\n",
    "  <ul>\n",
    "    <li><strong>Configure your Ray cluster</strong>: Multi-node setup and resource allocation.</li>\n",
    "    <li><strong>Manage dependencies</strong>: Install prerequisites on each node.</li>\n",
    "    <li><strong>Set up storage</strong>: Provide shared or distributed checkpoint storage.</li>\n",
    "  </ul>\n",
    "</div>\n",
    "\n",
    "<style>\n",
    "  div#anyscale-note > p,\n",
    "  div#anyscale-note > ul,\n",
    "  div#anyscale-note > ul li {\n",
    "    color: black;\n",
    "  }\n",
    "\n",
    "  div#anyscale-note {\n",
    "    background-color: rgb(255, 243, 205);\n",
    "  }\n",
    "\n",
    "  div#anyscale-note {\n",
    "    border: 1px solid #ccc; \n",
    "    border-radius: 8px;\n",
    "    padding: 15px;\n",
    "  }\n",
    "\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Install dependencies (if needed)\n",
    "\n",
    "Run the cell below only if your environment still needs these packages installed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "pip install torch torchvision\n",
    "pip install transformers datasets==3.6.0 trl==0.23.1\n",
    "pip install deepspeed ray[train]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Configuration constants\n",
    "\n",
    "This notebook uses simple constants instead of `argparse` to simplify execution. Adjust these as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Training constants (edit these) ----\n",
    "MODEL_NAME = \"gpt2\"\n",
    "DATASET_NAME = \"ag_news\"\n",
    "BATCH_SIZE = 1\n",
    "NUM_EPOCHS = 1\n",
    "SEQ_LENGTH = 512\n",
    "LEARNING_RATE = 1e-6\n",
    "ZERO_STAGE = 3\n",
    "TUTORIAL_STEPS = 30\n",
    "\n",
    "# Ray scaling settings\n",
    "NUM_WORKERS = 2\n",
    "USE_GPU = True\n",
    "\n",
    "# Storage\n",
    "STORAGE_PATH = \"/mnt/cluster_storage/\"\n",
    "EXPERIMENT_PREFIX = \"deepspeed_sample\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Define the training function\n",
    "\n",
    "First, define the training loop function for each worker to execute. Note that Ray Train allocates a unique GPU to each worker.\n",
    "Ray Train runs this training function on every worker to orchestrate the overall training process. The training function outlines the high-level structure common to most deep learning workflows, showing how setup, data ingestion, optimization, and reporting stages come together on each worker.\n",
    "\n",
    "The training function does the following:\n",
    "\n",
    "1. Initializes the model and optimizer with DeepSpeed (`setup_model_and_optimizer`).\n",
    "1. Restores training from a checkpoint if one is available (`load_checkpoint`).\n",
    "1. Sets up the dataloader (`setup_dataloader`).\n",
    "1. Accesses the device that Ray Train assigns to this worker.\n",
    "1. Iterates through the specified number of epochs.\n",
    "1. For multi-GPU training, ensures each worker sees a unique data shard each epoch.\n",
    "1. For each batch:\n",
    "   - Moves inputs to the device.\n",
    "   - Runs the forward pass to compute loss.\n",
    "   - Logs the loss.\n",
    "1. Performs the backward pass and optimizer step with DeepSpeed.\n",
    "1. Aggregates average loss and reports metrics, saving a checkpoint at the end of each epoch. (`report_metrics_and_save_checkpoint`)\n",
    "\n",
    "Later steps define the above helper functions (`setup_model_and_optimizer`, `load_checkpoint`, `setup_dataloader`, `report_metrics_and_save_checkpoint`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Any\n",
    "\n",
    "import os\n",
    "os.environ[\"RAY_TRAIN_V2_ENABLED\"] = \"1\"  # Ensure Ray Train v2 APIs\n",
    "import ray\n",
    "\n",
    "def train_loop(config: Dict[str, Any]) -> None:\n",
    "    # (1) Initialize model and optimizer with DeepSpeed\n",
    "    ds_engine = setup_model_and_optimizer(config[\"model_name\"], config[\"learning_rate\"], config[\"ds_config\"])\n",
    "\n",
    "    # (2) Load checkpoint if it exists\n",
    "    ckpt = ray.train.get_checkpoint()\n",
    "    start_epoch = 0\n",
    "    if ckpt:\n",
    "        start_epoch = load_checkpoint(ds_engine, ckpt)\n",
    "\n",
    "    # (3) Set up dataloader\n",
    "    train_loader = setup_dataloader(config[\"model_name\"], config[\"dataset_name\"], config[\"seq_length\"], config[\"batch_size\"])\n",
    "    steps_per_epoch = len(train_loader)\n",
    "\n",
    "    # (4) Access the device for this worker\n",
    "    device = ray.train.torch.get_device()\n",
    "\n",
    "    # Set model to training mode\n",
    "    ds_engine.train()\n",
    "\n",
    "    for epoch in range(start_epoch, config[\"epochs\"]):\n",
    "        # (6) Ensure unique shard per worker when using multiple GPUs\n",
    "        if ray.train.get_context().get_world_size() > 1 and hasattr(train_loader, \"sampler\"):\n",
    "            sampler = getattr(train_loader, \"sampler\", None)\n",
    "            if sampler and hasattr(sampler, \"set_epoch\"):\n",
    "                sampler.set_epoch(epoch)\n",
    "\n",
    "        running_loss = 0.0\n",
    "        num_batches = 0\n",
    "\n",
    "        # (7) Iterate over batches\n",
    "        for step, batch in enumerate(train_loader):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = ds_engine(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=input_ids,\n",
    "                use_cache=False\n",
    "            )\n",
    "            loss = outputs.loss\n",
    "            print(f\"Epoch: {epoch} Step: {step + 1}/{steps_per_epoch} Loss: {loss.item()}\")\n",
    "\n",
    "            # Backward pass and optimizer step\n",
    "            ds_engine.backward(loss)\n",
    "            ds_engine.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            num_batches += 1\n",
    "\n",
    "            # Stop early in the tutorial so runs finish quickly\n",
    "            if step + 1 >= config[\"tutorial_steps\"]:\n",
    "                print(f\"Stopping early at {config['tutorial_steps']} steps for the tutorial\")\n",
    "                break\n",
    "\n",
    "        # (8) Report metrics and save checkpoint\n",
    "        report_metrics_and_save_checkpoint(ds_engine, {\"loss\": running_loss / num_batches, \"epoch\": epoch})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ray Train runs the `train_loop` on each worker, which naturally supports **data parallelism**. In this setup, each worker processes a unique shard of data, computes gradients locally, and participates in synchronization to keep model parameters consistent. On top of this, DeepSpeed partitions model and optimizer states across GPUs to reduce memory usage and communication overhead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Set Up the dataloader\n",
    "\n",
    "The code below demonstrates how to prepare text data so that each worker can efficiently feed batches during training.\n",
    "\n",
    "1. Downloads a tokenizer from the Hugging Face Hub (`AutoTokenizer`).  \n",
    "2. Loads the `ag_news` dataset using Hugging Face's `load_dataset`.  \n",
    "3. Applies tokenization with padding and truncation by calling `map`.  \n",
    "4. Converts the dataset into a PyTorch `DataLoader`, which handles batching and shuffling.  \n",
    "5. Finally, call `ray.train.torch.prepare_data_loader` to make the dataloader distributed-ready.\n",
    "\n",
    "When you use **data parallelism**, each GPU worker trains on a unique shard of the dataset while holding its own copy of the model; gradients are synchronized after each step.\n",
    "Ray Train's `prepare_data_loader` wraps PyTorchâ€™s `DataLoader` and ensures workers see disjoint data, balances splits, and correctly handle epoch boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray.train\n",
    "import ray.train.torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset, DownloadConfig\n",
    "\n",
    "def setup_dataloader(model_name: str, dataset_name: str, seq_length: int, batch_size: int) -> DataLoader:\n",
    "    # (1) Get tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    # Set pad token if not already set\n",
    "    if tokenizer.pad_token is None:\n",
    "        if tokenizer.eos_token is not None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "        else:\n",
    "            # Fallback for models without eos_token\n",
    "            tokenizer.pad_token = tokenizer.unk_token\n",
    "\n",
    "    # (2) Load dataset\n",
    "    # This example uses only 1% of the dataset for quick testing. Adjust as needed.\n",
    "    dataset = load_dataset(dataset_name, split=\"train[:1%]\", download_config=DownloadConfig(disable_tqdm=True))\n",
    "\n",
    "    # (3) Tokenize\n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(examples['text'], padding='max_length', max_length=seq_length, truncation=True)\n",
    "    tokenized_dataset = dataset.map(tokenize_function, batched=True, num_proc=1, keep_in_memory=True)\n",
    "    tokenized_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n",
    "\n",
    "    # (4) Create DataLoader\n",
    "    data_loader = DataLoader(tokenized_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # (5) Use prepare_data_loader for distributed training\n",
    "    return ray.train.torch.prepare_data_loader(data_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code demonstrates how to use the tokenizer to encode a sample string. \n",
    "- `AutoTokenizer.from_pretrained` downloads and configures the tokenizer for your model.\n",
    "- You can encode any text string and inspect the resulting token IDs and attention mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage of get_tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "sample_text = \"Ray Train and DeepSpeed make distributed training easy!\"\n",
    "encoded = tokenizer(sample_text)\n",
    "print(encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Initialize model and optimizer\n",
    "\n",
    "After preparing and distributing the dataset, the next step is to set up the model and optimizer for training. This function does the following:\n",
    "\n",
    "1. Loads a pretrained model from the Hugging Face Hub (`AutoModelForCausalLM`).  \n",
    "2. Defines the optimizer (`AdamW`).  \n",
    "3. Initializes DeepSpeed with ZeRO options and returns a `DeepSpeedEngine`.\n",
    "\n",
    "DeepSpeedâ€™s `initialize` always partitions **optimizer states** (ZeRO Stage 1) across the GPU memory of all workers participating in training. Depending on the chosen stage, it can also partition **gradients** (Stage 2) and **model parameters/weights** (Stage 3). This staged approach balances memory savings and communication overhead, and the tutorial covers these stages in more detail [in later steps](#deepspeed-zero-stages)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Any\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "import deepspeed\n",
    "\n",
    "def setup_model_and_optimizer(model_name: str, learning_rate: float, ds_config: Dict[str, Any]) -> deepspeed.runtime.engine.DeepSpeedEngine:\n",
    "    # (1) Load pretrained model\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "    # (2) Define optimizer\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # (3) Initialize with DeepSpeed (distributed and memory optimizations)\n",
    "    ds_engine, _, _, _ = deepspeed.initialize(model=model, optimizer=optimizer, config=ds_config)\n",
    "    return ds_engine\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Checkpoint saving and loading\n",
    "\n",
    "Checkpointing is crucial for fault tolerance and for resuming training after interruptions. This section saves and restores model and optimizer states in a distributed Ray Train with DeepSpeed setup. It demonstrates how each worker saves its own checkpoint shard, how Ray bundles them into a unified checkpoint, and how this enables seamless recovery or further fine-tuning from the saved state.\n",
    "\n",
    "### Saving checkpoints\n",
    "\n",
    "First define how Ray Train should save checkpoints during training. The code below shows how to create temporary directories, store model states, and report checkpoint information and metrics back to Ray Train for tracking and coordination. Note that DeepSpeed saves model and optimizer states in a **partitioned format**, where each worker stores only its shard.\n",
    "\n",
    "1. Create a temporary directory for storing checkpoints.\n",
    "1. Save the partitioned model and optimizer states with DeepSpeed's `save_checkpoint`.\n",
    "1. Report metrics and checkpoint location to Ray Train with `ray.train.report`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "import ray.train\n",
    "from ray.train import Checkpoint\n",
    "\n",
    "def report_metrics_and_save_checkpoint(\n",
    "    ds_engine: deepspeed.runtime.engine.DeepSpeedEngine,\n",
    "    metrics: Dict[str, Any]\n",
    ") -> None:\n",
    "    \"\"\"Save worker checkpoints and report metrics to Ray Train.\n",
    "    Each rank writes its shard to a temp directory so Ray Train bundles all of them.\n",
    "    \"\"\"\n",
    "    ctx = ray.train.get_context()\n",
    "    epoch_value = metrics[\"epoch\"]\n",
    "\n",
    "    with tempfile.TemporaryDirectory() as tmp_dir:\n",
    "        checkpoint_dir = os.path.join(tmp_dir, \"checkpoint\")\n",
    "        os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "        ds_engine.save_checkpoint(checkpoint_dir)\n",
    "\n",
    "        epoch_file = os.path.join(checkpoint_dir, \"epoch.txt\")\n",
    "        with open(epoch_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(str(epoch_value))\n",
    "\n",
    "        checkpoint = Checkpoint.from_directory(tmp_dir)\n",
    "        ray.train.report(metrics, checkpoint=checkpoint)\n",
    "\n",
    "        if ctx.get_world_rank() == 0:\n",
    "            experiment_name = ctx.get_experiment_name()\n",
    "            print(\n",
    "                f\"Checkpoint saved successfully for experiment {experiment_name} at {checkpoint_dir}. Metrics: {metrics}\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading checkpoints\n",
    "\n",
    "After saving checkpoints, the next step is being able to resume training or evaluation from a saved state.\n",
    "This ensures that progress isnâ€™t lost due to interruptions and allows long-running jobs to continue seamlessly across sessions.\n",
    "When restarting, Ray Train provides each worker with the latest checkpoint so that DeepSpeed can rebuild the model, optimizer, and training progress from where it left off.\n",
    "\n",
    "Restore a previously saved checkpoint into the DeepSpeed engine using `load_checkpoint`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(ds_engine: deepspeed.runtime.engine.DeepSpeedEngine, ckpt: ray.train.Checkpoint) -> int:\n",
    "    \"\"\"Restore DeepSpeed state and determine next epoch.\"\"\"\n",
    "    next_epoch = 0\n",
    "    try:\n",
    "        with ckpt.as_directory() as checkpoint_dir:\n",
    "            print(f\"Loading checkpoint from {checkpoint_dir}\")\n",
    "            epoch_dir = os.path.join(checkpoint_dir, \"checkpoint\")\n",
    "            if not os.path.isdir(epoch_dir):\n",
    "                epoch_dir = checkpoint_dir\n",
    "\n",
    "            ds_engine.load_checkpoint(epoch_dir)\n",
    "\n",
    "            epoch_file = os.path.join(epoch_dir, \"epoch.txt\")\n",
    "            if os.path.isfile(epoch_file):\n",
    "                with open(epoch_file, \"r\", encoding=\"utf-8\") as f:\n",
    "                    last_epoch = int(f.read().strip())\n",
    "                next_epoch = last_epoch + 1\n",
    "\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Checkpoint loading failed: {e}\") from e\n",
    "    return next_epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 5. Configure DeepSpeed\n",
    "\n",
    "Before launching distributed training, you need to define a DeepSpeed configuration dictionary (`ds_config`) that controls data type settings, batch sizes, optimizations including ZeRO (model state partitioning strategies), etc. This configuration determines how DeepSpeed manages memory, communication, and performance across GPUs.\n",
    "\n",
    "The example below shows a minimal setup that enables bfloat16 precision, gradient clipping, and ZeRO optimization. You can further customize this configuration based on your model size, hardware, and performance goals. See [Advanced Configurations](#advanced-configurations) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DeepSpeed configuration\n",
    "ds_config = {\n",
    "    \"train_micro_batch_size_per_gpu\": BATCH_SIZE,\n",
    "    \"bf16\": {\"enabled\": True},\n",
    "    \"grad_accum_dtype\": \"bf16\",\n",
    "    \"zero_optimization\": {\n",
    "        \"stage\": ZERO_STAGE,\n",
    "        \"overlap_comm\": True,\n",
    "        \"contiguous_gradients\": True,\n",
    "    },\n",
    "    \"gradient_clipping\": 1.0,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Launch distributed training\n",
    "\n",
    "The final step is to configure parameters and launch the distributed training job.\n",
    "Ray Trainâ€™s `TorchTrainer` automatically starts multiple workersâ€”one per GPUâ€”and executes the `train_loop` on each instance. The **scaling configuration** determines how many workers to launch and what resources they use, while the **run configuration** manages storage and experiment tracking.\n",
    "\n",
    "This function does the following:\n",
    "1. Parses command-line arguments for training and model settings.\n",
    "1. Defines the Ray Train `ScalingConfig`â€”for example, the number of workers and GPU usage.\n",
    "1. Prepares the training loop configuration with hyperparameters and model details.\n",
    "1. Sets up the Ray Train `RunConfig` to manage storage and experiment metadata. This example sets a random experiment name, but you can specify the name of a previous experiment to load the checkpoint.\n",
    "1. Creates a `TorchTrainer` that launches the training function on multiple GPU workers.\n",
    "1. Starts training with `trainer.fit()` and prints the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from ray.train.torch import TorchTrainer\n",
    "from ray.train import ScalingConfig, RunConfig\n",
    "\n",
    "# Ray Train scaling configuration\n",
    "scaling_config = ScalingConfig(num_workers=NUM_WORKERS, use_gpu=USE_GPU)\n",
    "\n",
    "# Training loop configuration\n",
    "train_loop_config = {\n",
    "    \"epochs\": NUM_EPOCHS,\n",
    "    \"learning_rate\": LEARNING_RATE,\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"ds_config\": ds_config,\n",
    "    \"model_name\": MODEL_NAME,\n",
    "    \"dataset_name\": DATASET_NAME,\n",
    "    \"seq_length\": SEQ_LENGTH,\n",
    "    \"tutorial_steps\": TUTORIAL_STEPS,\n",
    "}\n",
    "\n",
    "# Ray Train run configuration\n",
    "run_config = RunConfig(\n",
    "    storage_path=STORAGE_PATH,\n",
    "    # Set the name of the previous experiment when resuming from a checkpoint\n",
    "    name=f\"{EXPERIMENT_PREFIX}_{uuid.uuid4().hex[:8]}\",\n",
    ")\n",
    "\n",
    "# Create and launch the trainer\n",
    "trainer = TorchTrainer(\n",
    "    train_loop_per_worker=train_loop,\n",
    "    scaling_config=scaling_config,\n",
    "    train_loop_config=train_loop_config,\n",
    "    run_config=run_config,\n",
    ")\n",
    "\n",
    "# To actually run training, execute the following:\n",
    "result = trainer.fit()\n",
    "print(f\"Training finished. Result: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running as a standalone script\n",
    "\n",
    "While this tutorial is designed to run interactively in a Jupyter notebook, you can also launch the same training workflow as a standalone Python script.\n",
    "This is useful for running longer experiments, automating jobs, or deploying training on a cluster.\n",
    "\n",
    "The [full code](https://github.com/ray-project/ray/blob/master/doc/source/train/examples/pytorch/deepspeed_finetune/train.py) is also available.\n",
    "To start training from the command line, run:\n",
    "\n",
    "```bash\n",
    "python train.py\n",
    "```\n",
    "\n",
    "## Advanced configurations\n",
    "\n",
    "DeepSpeed has many other configuration options to tune performance and memory usage.\n",
    "This section introduces some of the most commonly used options.\n",
    "See the [DeepSpeed documentation](https://www.deepspeed.ai/docs/config-json/) for more details.\n",
    "\n",
    "### DeepSpeed ZeRO stages\n",
    "- **Stage 1**: Partitions optimizer states (always on when using ZeRO).  \n",
    "- **Stage 2**: Additionally partitions gradients.  \n",
    "- **Stage 3**: Additionally partitions model parameters or weights.\n",
    "\n",
    "The higher the stage, the more memory savings you get, but it may also introduce more communication overhead and complexity in training.\n",
    "You can select the stage through `ds_config[\"zero_optimization\"][\"stage\"]`. See the DeepSpeed docs for more details.\n",
    "\n",
    "```python\n",
    "ds_config = {\n",
    "    \"zero_optimization\": {\n",
    "        \"stage\": 2,  # or 1 or 3\n",
    "    },\n",
    "}\n",
    "```\n",
    "\n",
    "### Mixed precision\n",
    "Enable BF16 or FP16:\n",
    "```python\n",
    "ds_config = {\n",
    "    \"bf16\": {\"enabled\": True},  # or \"fp16\": {\"enabled\": True}\n",
    "}\n",
    "```\n",
    "\n",
    "### CPU offloading\n",
    "Reduce GPU memory pressure by offloading to CPU at the cost of PCIe traffic:\n",
    "```python\n",
    "ds_config = {\n",
    "    \"offload_param\": {\"device\": \"cpu\", \"pin_memory\": True},\n",
    "    # or\n",
    "    \"offload_optimizer\": {\"device\": \"cpu\", \"pin_memory\": True},\n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this tutorial, you did the following:\n",
    "\n",
    "- Fine-tuned an LLM using Ray Train and DeepSpeed ZeRO\n",
    "- Set up distributed data loading with Ray Train's `prepare_data_loader`\n",
    "- Saved and managed checkpoints with Ray Train's storage configuration\n",
    "- Configured and launched multi-GPU training with `TorchTrainer` and scaling configurations\n",
    "- Explored advanced DeepSpeed configurations (ZeRO stages, mixed precision, and CPU offloading)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "orphan": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
