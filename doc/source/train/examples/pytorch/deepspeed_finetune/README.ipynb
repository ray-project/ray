{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "\n",
    "# A Step-by-step Guide: Fine-tuning LLMs with Ray Train and DeepSpeed\n",
    "\n",
    "**Time to complete:** 20 min\n",
    "\n",
    "This notebook walks through how to combine **DeepSpeed** with **Ray Train** to efficiently scale PyTorch training across GPUs and nodes while minimizing memory usage.\n",
    "\n",
    "It includes:\n",
    "- A hands-on example of fine-tuning an LLM\n",
    "- Checkpoint saving and resuming with Ray Train\n",
    "- Configuring ZeRO for memory and performance (stages, mixed precision, CPU offload)\n",
    "- Launching a distributed training job\n",
    "\n",
    "> **Note**: This template is optimized for the Anyscale platform. When running on open-source Ray, you must configure a Ray cluster, install dependencies on all nodes, and set up storage for checkpoints.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "<div id=\"anyscale-note\" class=\"alert alert-block alert-warning\">\n",
    "\n",
    "  <strong>Anyscale Specific Configuration</strong>\n",
    "\n",
    "  <p><strong>Note:</strong> This template is optimized for the Anyscale platform. On Anyscale, most configuration is automated. When running on open-source Ray, manually complete the following steps:</p>\n",
    "\n",
    "  <ul>\n",
    "    <li><strong>Configure your Ray Cluster</strong>: Multi-node setup and resource allocation.</li>\n",
    "    <li><strong>Manage Dependencies</strong>: Install prerequisites on each node.</li>\n",
    "    <li><strong>Set Up Storage</strong>: Provide shared or distributed checkpoint storage.</li>\n",
    "  </ul>\n",
    "</div>\n",
    "\n",
    "<style>\n",
    "  div#anyscale-note > p,\n",
    "  div#anyscale-note > ul,\n",
    "  div#anyscale-note > ul li {\n",
    "    color: black;\n",
    "  }\n",
    "\n",
    "  div#anyscale-note {\n",
    "    background-color: rgb(255, 243, 205);\n",
    "  }\n",
    "\n",
    "  div#anyscale-note {\n",
    "    border: 1px solid #ccc; \n",
    "    border-radius: 8px;\n",
    "    padding: 15px;\n",
    "  }\n",
    "\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "\n",
    "## Install Dependencies (if needed)\n",
    "\n",
    "Run the cell below only if your environment still needs these packages installed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "pip install torch torchvision\n",
    "pip install transformers datasets==3.6.0 trl==0.23.1\n",
    "pip install deepspeed ray[train]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "\n",
    "## Configuration Constants\n",
    "\n",
    "This notebook uses simple constants instead of `argparse` to simplify execution. Adjust these as needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"RAY_TRAIN_V2_ENABLED\"] = \"1\"  # Ensure Ray Train v2 APIs\n",
    "\n",
    "# ---- Training constants (edit these) ----\n",
    "MODEL_NAME = \"MiniLLM/MiniPLM-Qwen-500M\"\n",
    "DATASET_NAME = \"ag_news\"\n",
    "BATCH_SIZE = 1\n",
    "NUM_EPOCHS = 1\n",
    "SEQ_LENGTH = 512\n",
    "LEARNING_RATE = 1e-6\n",
    "ZERO_STAGE = 3\n",
    "TUTORIAL_STEPS = 30\n",
    "\n",
    "# Ray scaling settings\n",
    "NUM_WORKERS = 2\n",
    "USE_GPU = True\n",
    "\n",
    "# Storage\n",
    "STORAGE_PATH = \"/mnt/cluster_storage/\"\n",
    "EXPERIMENT_PREFIX = \"deepspeed_sample\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "\n",
    "### 1. Import Packages\n",
    "This section imports **Ray Train** for distributed orchestration, **PyTorch** for modeling, **Hugging Face Transformers/Datasets** for models and data, and **DeepSpeed** for ZeRO-based optimization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "import logging\n",
    "from typing import Dict, Any\n",
    "import tempfile\n",
    "\n",
    "import ray\n",
    "import ray.train\n",
    "import ray.train.torch\n",
    "from ray.train.torch import TorchTrainer\n",
    "from ray.train import ScalingConfig, RunConfig, Checkpoint\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset, DownloadConfig\n",
    "\n",
    "import deepspeed\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "\n",
    "### 2. Set Up the Dataloader\n",
    "\n",
    "The code below:\n",
    "\n",
    "1. Downloads a tokenizer from the Hugging Face Hub (`AutoTokenizer`).  \n",
    "2. Loads the `ag_news` dataset using Hugging Face’s `load_dataset`.  \n",
    "3. Applies tokenization with padding and truncation by calling `map`.  \n",
    "4. Converts the dataset into a PyTorch `DataLoader`, which handles batching and shuffling.  \n",
    "5. Finally, call `ray.train.torch.prepare_data_loader` to make the dataloader distributed-ready.\n",
    "\n",
    "This example uses only 1% of the dataset for quick testing. Adjust as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_dataloader(model_name: str, dataset_name: str, seq_length: int, batch_size: int) -> DataLoader:\n",
    "    # (1) Get tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    # (2) Load dataset\n",
    "    dataset = load_dataset(dataset_name, split=\"train[:1%]\", download_config=DownloadConfig(disable_tqdm=True))\n",
    "\n",
    "    # (3) Tokenize\n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(examples['text'], padding='max_length', max_length=seq_length, truncation=True)\n",
    "    tokenized_dataset = dataset.map(tokenize_function, batched=True, num_proc=1, keep_in_memory=True)\n",
    "    tokenized_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n",
    "\n",
    "    # (4) Create DataLoader\n",
    "    data_loader = DataLoader(tokenized_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # (5) Use prepare_data_loader for distributed training\n",
    "    return ray.train.torch.prepare_data_loader(data_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "The following code demonstrates how to use the tokenizer to encode a sample string. \n",
    "- `AutoTokenizer.from_pretrained` downloads and configures the tokenizer for your model.\n",
    "- You can encode any text string and inspect the resulting token IDs and attention mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage of get_tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "sample_text = \"Ray Train and DeepSpeed make distributed training easy!\"\n",
    "encoded = tokenizer(sample_text, padding='max_length', max_length=32, truncation=True)\n",
    "print(encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "\n",
    "**Making the dataloader distributed-ready with Ray**  \n",
    "In **data parallelism**, each GPU worker trains on a unique shard of the dataset while holding its own copy of the model; gradients are synchronized after each step.  \n",
    "Ray’s `prepare_data_loader` wraps PyTorch’s `DataLoader` and automatically applies a `DistributedSampler`, ensuring workers see disjoint data, splits are balanced, and epoch boundaries are handled correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "\n",
    "### 3. Initialize Model and Optimizer\n",
    "\n",
    "The function below:\n",
    "\n",
    "1. Loads a pretrained model from the Hugging Face Hub (`AutoModelForCausalLM`).  \n",
    "2. Defines the optimizer (`AdamW`).  \n",
    "3. Initializes DeepSpeed with ZeRO options and returns a `DeepSpeedEngine`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def setup_model_and_optimizer(model_name: str, learning_rate: float, ds_config: Dict[str, Any]) -> deepspeed.runtime.engine.DeepSpeedEngine:\n",
    "    # (1) Load pretrained model\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "    # (2) Define optimizer\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # (3) Initialize with DeepSpeed (distributed + memory optimizations)\n",
    "    ds_engine, _, _, _ = deepspeed.initialize(model=model, optimizer=optimizer, config=ds_config)\n",
    "    return ds_engine\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "\n",
    "**Making the model distributed-ready with Ray and DeepSpeed**  \n",
    "DeepSpeed’s `initialize` always partitions **optimizer states** (ZeRO Stage 1). Depending on the chosen stage, it can also partition **gradients** (Stage 2) and **model parameters/weights** (Stage 3). This staged approach balances memory savings and communication overhead, and the tutorial covers these stages in more detail [later in the tutorial](#deepspeed-zero-stages)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "\n",
    "### 4. Checkpointing and Loading\n",
    "\n",
    "\n",
    "Checkpointing is crucial for fault tolerance and for resuming training after interruptions. The functions below:\n",
    "\n",
    "1. Create a temporary directory for storing checkpoints.\n",
    "1. Save the partitioned model and optimizer states with DeepSpeed’s `save_checkpoint`.\n",
    "1. Synchronize all workers with `torch.distributed.barrier` to ensure every process finishes saving.\n",
    "1. Report metrics and checkpoint location to Ray with `ray.train.report`.\n",
    "1. Restore a previously saved checkpoint into the DeepSpeed engine using `load_checkpoint`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def report_metrics_and_save_checkpoint(\n",
    "    ds_engine: deepspeed.runtime.engine.DeepSpeedEngine,\n",
    "    metrics: Dict[str, Any]\n",
    ") -> None:\n",
    "    \"\"\"Save worker checkpoints and report metrics to Ray.\n",
    "    Each rank writes its shard to a temp directory so Ray bundles all of them.\n",
    "    \"\"\"\n",
    "    ctx = ray.train.get_context()\n",
    "    epoch_value = metrics[\"epoch\"]\n",
    "\n",
    "    with tempfile.TemporaryDirectory() as tmp_dir:\n",
    "        checkpoint_dir = os.path.join(tmp_dir, \"checkpoint\")\n",
    "        os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "        ds_engine.save_checkpoint(checkpoint_dir)\n",
    "\n",
    "        epoch_file = os.path.join(checkpoint_dir, \"epoch.txt\")\n",
    "        with open(epoch_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(str(epoch_value))\n",
    "\n",
    "        checkpoint = Checkpoint.from_directory(tmp_dir)\n",
    "        ray.train.report(metrics, checkpoint=checkpoint)\n",
    "\n",
    "        if ctx.get_world_rank() == 0:\n",
    "            experiment_name = ctx.get_experiment_name()\n",
    "            print(\n",
    "                f\"Checkpoint saved successfully for experiment {experiment_name} at {checkpoint_dir}. Metrics: {metrics}\"\n",
    "            )\n",
    "\n",
    "\n",
    "def load_checkpoint(ds_engine: deepspeed.runtime.engine.DeepSpeedEngine, ckpt: ray.train.Checkpoint) -> int:\n",
    "    \"\"\"Restore DeepSpeed state and determine next epoch.\"\"\"\n",
    "    next_epoch = 0\n",
    "    try:\n",
    "        with ckpt.as_directory() as checkpoint_dir:\n",
    "            print(f\"Loading checkpoint from {checkpoint_dir}\")\n",
    "            epoch_dir = os.path.join(checkpoint_dir, \"checkpoint\")\n",
    "            if not os.path.isdir(epoch_dir):\n",
    "                epoch_dir = checkpoint_dir\n",
    "\n",
    "            ds_engine.load_checkpoint(epoch_dir)\n",
    "\n",
    "            epoch_file = os.path.join(epoch_dir, \"epoch.txt\")\n",
    "            if os.path.isfile(epoch_file):\n",
    "                with open(epoch_file, \"r\", encoding=\"utf-8\") as f:\n",
    "                    last_epoch = int(f.read().strip())\n",
    "                next_epoch = last_epoch + 1\n",
    "\n",
    "            if torch.distributed.is_available() and torch.distributed.is_initialized():\n",
    "                torch.distributed.barrier()\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Checkpoint loading failed: {e}\") from e\n",
    "    return next_epoch\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "\n",
    "**Making checkpoints distributed-ready with Ray and DeepSpeed**  \n",
    "DeepSpeed saves model and optimizer states in a **partitioned format**, where each worker stores only its shard. This requires synchronization across processes, so all workers must reach the same checkpointing point before proceeding. The call to `torch.distributed.barrier()` ensures that every worker finishes saving before moving on.  \n",
    "Finally, `ray.train.report` both reports training metrics and saves the checkpoint to persistent storage, making it accessible for resuming training later.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "\n",
    "### 5. Training Iteration\n",
    "\n",
    "Ray Train runs a training loop function on each GPU worker to orchestrate the entire process. The function below:\n",
    "\n",
    "1. Initializes the model and optimizer with DeepSpeed.\n",
    "1. Restores training from a checkpoint if one is available.\n",
    "1. Sets up the dataloader with `setup_dataloader`.\n",
    "1. Gets the device assigned to this worker.\n",
    "1. Iterates through the specified number of epochs.\n",
    "1. For multi-GPU training, ensures each worker sees a unique data shard each epoch.\n",
    "1. For each batch:\n",
    "   - Moves inputs to the device.\n",
    "   - Runs the forward pass to compute loss.\n",
    "   - Logs the loss.\n",
    "1. Performs backward pass and optimizer step with DeepSpeed.\n",
    "1. Aggregates average loss and reports metrics, saving a checkpoint at the end of each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_loop(config: Dict[str, Any]) -> None:\n",
    "    # (1) Initialize model + optimizer with DeepSpeed\n",
    "    ds_engine = setup_model_and_optimizer(config[\"model_name\"], config[\"learning_rate\"], config[\"ds_config\"])\n",
    "\n",
    "    # (2) Load checkpoint if exists\n",
    "    ckpt = ray.train.get_checkpoint()\n",
    "    start_epoch = 0\n",
    "    if ckpt:\n",
    "        start_epoch = load_checkpoint(ds_engine, ckpt)\n",
    "\n",
    "    # (3) Set up dataloader\n",
    "    train_loader = setup_dataloader(config[\"model_name\"], config[\"dataset_name\"], config[\"seq_length\"], config[\"batch_size\"])\n",
    "    steps_per_epoch = len(train_loader)\n",
    "\n",
    "    # (4) Get device for this worker\n",
    "    device = ray.train.torch.get_device()\n",
    "\n",
    "    for epoch in range(start_epoch, config[\"epochs\"]):\n",
    "        # (6) Ensure unique shard per worker when using multiple GPUs\n",
    "        if ray.train.get_context().get_world_size() > 1 and hasattr(train_loader, \"sampler\"):\n",
    "            sampler = getattr(train_loader, \"sampler\", None)\n",
    "            if sampler and hasattr(sampler, \"set_epoch\"):\n",
    "                sampler.set_epoch(epoch)\n",
    "\n",
    "        running_loss = 0.0\n",
    "        num_batches = 0\n",
    "\n",
    "        # (7) Iterate over batches\n",
    "        for step, batch in enumerate(train_loader):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = ds_engine(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=input_ids,\n",
    "                use_cache=False\n",
    "            )\n",
    "            loss = outputs.loss\n",
    "            print(f\"Epoch: {epoch} Step: {step + 1}/{steps_per_epoch} Loss: {loss.item()}\")\n",
    "\n",
    "            # Backward pass + optimizer step\n",
    "            ds_engine.backward(loss)\n",
    "            ds_engine.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            num_batches += 1\n",
    "\n",
    "            # Stop early in the tutorial so runs finish quickly\n",
    "            if step + 1 >= config[\"tutorial_steps\"]:\n",
    "                print(f\"Stopping early at {config['tutorial_steps']} steps for the tutorial\")\n",
    "                break\n",
    "\n",
    "        # (8) Report metrics + save checkpoint\n",
    "        report_metrics_and_save_checkpoint(ds_engine, {\"loss\": running_loss / num_batches, \"epoch\": epoch})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "\n",
    "**Coordinating distributed training with Ray and DeepSpeed**  \n",
    "Ray launches this `train_loop` on each worker, while DeepSpeed manages partitioning and memory optimizations. With **data parallelism**, each worker processes a unique shard of data, computes gradients locally, and participates in synchronization so parameters stay in sync.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "\n",
    "### 6. Configure DeepSpeed and Launch Trainer\n",
    "\n",
    "The final step is to configure parameters and launch the distributed training job with Ray’s `TorchTrainer`. The function below:\n",
    "1. Parses command-line arguments for training and model settings.\n",
    "1. Defines the Ray scaling configuration—for example, the number of workers and GPU usage.\n",
    "1. Builds the DeepSpeed configuration dictionary (`ds_config`).\n",
    "1. Prepares the training loop configuration with hyperparameters and model details.\n",
    "1. Sets up the Ray `RunConfig` to manage storage and experiment metadata. This example sets a random experiment name, but you can specify the name of a previous experiment to load the checkpoint.\n",
    "1. Creates a `TorchTrainer` that launches the training loop on multiple GPU workers.\n",
    "1. Starts training with `trainer.fit()` and prints the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Ray scaling configuration\n",
    "scaling_config = ScalingConfig(num_workers=NUM_WORKERS, use_gpu=USE_GPU)\n",
    "\n",
    "# DeepSpeed configuration\n",
    "ds_config = {\n",
    "    \"train_micro_batch_size_per_gpu\": BATCH_SIZE,\n",
    "    \"bf16\": {\"enabled\": True},\n",
    "    \"grad_accum_dtype\": \"bf16\",\n",
    "    \"zero_optimization\": {\n",
    "        \"stage\": ZERO_STAGE,\n",
    "        \"overlap_comm\": True,\n",
    "        \"contiguous_gradients\": True,\n",
    "    },\n",
    "    \"gradient_clipping\": 1.0,\n",
    "}\n",
    "\n",
    "# Training loop configuration\n",
    "train_loop_config = {\n",
    "    \"epochs\": NUM_EPOCHS,\n",
    "    \"learning_rate\": LEARNING_RATE,\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"ds_config\": ds_config,\n",
    "    \"model_name\": MODEL_NAME,\n",
    "    \"dataset_name\": DATASET_NAME,\n",
    "    \"seq_length\": SEQ_LENGTH,\n",
    "    \"tutorial_steps\": TUTORIAL_STEPS,\n",
    "}\n",
    "\n",
    "# Ray run configuration\n",
    "run_config = RunConfig(\n",
    "    storage_path=STORAGE_PATH,\n",
    "    # Set the name of the previous experiment when resuming from a checkpoint\n",
    "    name=f\"{EXPERIMENT_PREFIX}_{uuid.uuid4().hex[:8]}\",\n",
    ")\n",
    "\n",
    "# Create and launch the trainer\n",
    "trainer = TorchTrainer(\n",
    "    train_loop_per_worker=train_loop,\n",
    "    scaling_config=scaling_config,\n",
    "    train_loop_config=train_loop_config,\n",
    "    run_config=run_config,\n",
    ")\n",
    "\n",
    "# To actually run training, execute the following:\n",
    "result = trainer.fit()\n",
    "print(f\"Training finished. Result: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "**Launching distributed training with Ray and DeepSpeed**\n",
    "\n",
    "Ray’s `TorchTrainer` automatically launches multiple workers—one per GPU—and runs the `train_loop` on each instance. The scaling configuration controls how many workers to start, while the run configuration handles logging, storage, and experiment tracking.\n",
    "\n",
    "DeepSpeed’s `ds_config` ensures that the right ZeRO stage and optimizations apply inside each worker. Together, this setup makes it easy to scale from a single GPU to a multi-node cluster without changing your training loop code.\n",
    "\n",
    "`train.py` in this directory contains the full code.\n",
    "\n",
    "## Advanced Configurations\n",
    "\n",
    "DeepSpeed has many other configuration options to tune performance and memory usage.\n",
    "This section introduces some of the most commonly used options.\n",
    "Refer to the [DeepSpeed documentation](https://www.deepspeed.ai/docs/config-json/) for more details.\n",
    "\n",
    "### DeepSpeed ZeRO Stages\n",
    "- **Stage 1**: Partitions optimizer states (always on when using ZeRO).  \n",
    "- **Stage 2**: Additionally partitions gradients.  \n",
    "- **Stage 3**: Additionally partitions model parameters/weights.\n",
    "\n",
    "The higher the stage, the more memory savings you get, but it may also introduce more communication overhead and complexity in training.\n",
    "You can select the stage through `ds_config[\"zero_optimization\"][\"stage\"]`. See the DeepSpeed docs for more details.\n",
    "\n",
    "```python\n",
    "ds_config = {\n",
    "    \"zero_optimization\": {\n",
    "        \"stage\": 2,  # or 1 or 3\n",
    "    },\n",
    "}\n",
    "```\n",
    "\n",
    "### Mixed Precision\n",
    "Enable BF16 or FP16:\n",
    "```python\n",
    "ds_config = {\n",
    "    \"bf16\": {\"enabled\": True},  # or \"fp16\": {\"enabled\": True}\n",
    "}\n",
    "```\n",
    "\n",
    "### CPU Offloading\n",
    "Reduce GPU memory pressure by offloading to CPU (at the cost of PCIe traffic):\n",
    "```python\n",
    "ds_config = {\n",
    "    \"offload_param\": {\"device\": \"cpu\", \"pin_memory\": True},\n",
    "    # or\n",
    "    \"offload_optimizer\": {\"device\": \"cpu\", \"pin_memory\": True},\n",
    "}\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "orphan": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
