{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning a Pytorch Image Classifier with Ray AIR\n",
    "This example fine tunes a pre-trained ResNet model with Ray Train. \n",
    "\n",
    "For this example, the network architecture consists of the intermediate layer output of a pre-trained ResNet model, which feeds into a randomly initialized linear layer that outputs classification logits for our new task.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and preprocess finetuning dataset\n",
    "This example is adapted from Pytorch's [Finetuning Torchvision Models](https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html) tutorial.\n",
    "We will use *hymenoptera_data* as the finetuning dataset, which contains two classes (bees and ants) and 397 total images (across training and validation). This is a quite small dataset and used only for demonstration purposes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# To run full example, set SMOKE_TEST as False\n",
    "SMOKE_TEST = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is publicly available [here](https://www.kaggle.com/datasets/ajayrana/hymenoptera-data). Note that it is structured with directory names as the labels. Use `torchvision.datasets.ImageFolder()` to load the images and their corresponding labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, models, transforms\n",
    "import numpy as np\n",
    "\n",
    "# Data augmentation and normalization for training\n",
    "# Just normalization for validation\n",
    "data_transforms = {\n",
    "    \"train\": transforms.Compose(\n",
    "        [\n",
    "            transforms.RandomResizedCrop(224),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "        ]\n",
    "    ),\n",
    "    \"val\": transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize(224),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "        ]\n",
    "    ),\n",
    "}\n",
    "\n",
    "# Download and build torch datasets\n",
    "def build_datasets():\n",
    "    os.system(\n",
    "        \"wget https://download.pytorch.org/tutorial/hymenoptera_data.zip >/dev/null 2>&1\"\n",
    "    )\n",
    "    os.system(\"unzip hymenoptera_data.zip >/dev/null 2>&1\")\n",
    "\n",
    "    torch_datasets = {}\n",
    "    for split in [\"train\", \"val\"]:\n",
    "        torch_datasets[split] = datasets.ImageFolder(\n",
    "            os.path.join(\"./hymenoptera_data\", split), data_transforms[split]\n",
    "        )\n",
    "    return torch_datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "if SMOKE_TEST:\n",
    "    from torch.utils.data import Subset\n",
    "\n",
    "    def build_datasets():\n",
    "        os.system(\n",
    "            \"wget https://download.pytorch.org/tutorial/hymenoptera_data.zip >/dev/null 2>&1\"\n",
    "        )\n",
    "        os.system(\"unzip hymenoptera_data.zip >/dev/null 2>&1\")\n",
    "\n",
    "        torch_datasets = {}\n",
    "        for split in [\"train\", \"val\"]:\n",
    "            torch_datasets[split] = datasets.ImageFolder(\n",
    "                os.path.join(\"./hymenoptera_data\", split), data_transforms[split]\n",
    "            )\n",
    "            \n",
    "        # Only take a subset for smoke test\n",
    "        for split in [\"train\", \"val\"]:\n",
    "            indices = list(range(100))\n",
    "            torch_datasets[split] = Subset(torch_datasets[split], indices)\n",
    "        return torch_datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Model and Fine-tuning configs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's define the training configuration that will be passed into the training loop function later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loop_config = {\n",
    "    \"input_size\": 224,  # Input image size (224 x 224)\n",
    "    \"batch_size\": 32,  # Batch size for training\n",
    "    \"num_epochs\": 10,  # Number of epochs to train for\n",
    "    \"lr\": 0.001,  # Learning Rate\n",
    "    \"momentum\": 0.9,  # SGD optimizer momentum\n",
    "}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's define our model. You can either create a model from pre-trained weights or reload the model checkpoint from a previous run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.train.torch import TorchCheckpoint\n",
    "\n",
    "# Option 1: Initialize model with pretrained weights\n",
    "def initialize_model():\n",
    "    # Load pretrained model params\n",
    "    model = models.resnet50(pretrained=True)\n",
    "\n",
    "    # Replace the original classifier with a new Linear layer\n",
    "    num_features = model.fc.in_features\n",
    "    model.fc = nn.Linear(num_features, 2)\n",
    "\n",
    "    # Ensure all params get updated during finetuning\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = True\n",
    "    return model\n",
    "\n",
    "\n",
    "# Option 2: Initialize model with an AIR checkpoint\n",
    "# Replace this with your own uri\n",
    "CHECKPOINT_URI = \"s3://air-example-data/finetune-resnet-checkpoint/TorchTrainer_4f69f_00000_0_2023-02-14_14-04-09/checkpoint_000000/\"\n",
    "\n",
    "\n",
    "def initialize_model_from_uri(checkpoint_uri):\n",
    "    checkpoint = TorchCheckpoint.from_uri(checkpoint_uri)\n",
    "    resnet50 = initialize_model()\n",
    "    return checkpoint.get_model(model=resnet50)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Training Loop\n",
    "\n",
    "The `train_loop_per_worker` function defines the fine-tuning procedure for each worker.\n",
    "\n",
    "**1. Prepare dataloaders for each worker**:\n",
    "- This tutorial assumes you are using PyTorch's native `torch.utils.data.Dataset` for data input. {meth}`train.torch.prepare_data_loader() <ray.train.torch.prepare_data_loader>`  prepares your dataLoader for distributed execution. You can also use Ray Data for more efficient preprocessing(see this {doc}`example </ray-air/examples/torch_image_example>`).\n",
    "\n",
    "**2. Prepare your model**:\n",
    "- {meth}`train.torch.prepare_model() <ray.train.torch.prepare_model>` prepares the model for distributed training. Under the hood, it converts your torch model to `DistributedDataParallel` model, which synchronize its weights across all workers.\n",
    "\n",
    "**3. Report metrics and checkpoint**:\n",
    "- {meth}`session.report() <ray.air.session.report>` will report metrics and checkpoints to Ray AIR.\n",
    "- Saving checkpoints through {meth}`session.report(metrics, checkpoint=...) <ray.air.session.report>` will automatically [upload checkpoints to cloud storage](tune-cloud-checkpointing) (if configured), and allow you to easily enable Ray AIR worker fault tolerance in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray.train as train\n",
    "from ray.air import session\n",
    "from ray.train.torch import TorchCheckpoint\n",
    "\n",
    "\n",
    "def evaluate(logits, labels):\n",
    "    _, preds = torch.max(logits, 1)\n",
    "    corrects = torch.sum(preds == labels).item()\n",
    "    return corrects\n",
    "\n",
    "\n",
    "def train_loop_per_worker(configs):\n",
    "    import warnings\n",
    "\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "    # Calculate the batch size for a single worker\n",
    "    worker_batch_size = configs[\"batch_size\"] // session.get_world_size()\n",
    "\n",
    "    # Build datasets on each worker\n",
    "    torch_datasets = build_datasets()\n",
    "\n",
    "    # Prepare dataloader for each worker\n",
    "    dataloaders = dict()\n",
    "    dataloaders[\"train\"] = DataLoader(\n",
    "        torch_datasets[\"train\"], batch_size=worker_batch_size, shuffle=True\n",
    "    )\n",
    "    dataloaders[\"val\"] = DataLoader(\n",
    "        torch_datasets[\"val\"], batch_size=worker_batch_size, shuffle=False\n",
    "    )\n",
    "\n",
    "    # Distribute\n",
    "    dataloaders[\"train\"] = train.torch.prepare_data_loader(dataloaders[\"train\"])\n",
    "    dataloaders[\"val\"] = train.torch.prepare_data_loader(dataloaders[\"val\"])\n",
    "\n",
    "    device = train.torch.get_device()\n",
    "\n",
    "    # Prepare DDP Model, optimizer, and loss function\n",
    "    model = initialize_model()\n",
    "    model = train.torch.prepare_model(model)\n",
    "\n",
    "    optimizer = optim.SGD(\n",
    "        model.parameters(), lr=configs[\"lr\"], momentum=configs[\"momentum\"]\n",
    "    )\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Start training loops\n",
    "    for epoch in range(configs[\"num_epochs\"]):\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in [\"train\", \"val\"]:\n",
    "            if phase == \"train\":\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()  # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                with torch.set_grad_enabled(phase == \"train\"):\n",
    "                    # Get model outputs and calculate loss\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == \"train\":\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # calculate statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += evaluate(outputs, labels)\n",
    "\n",
    "            size = len(torch_datasets[phase]) // session.get_world_size()\n",
    "            epoch_loss = running_loss / size\n",
    "            epoch_acc = running_corrects / size\n",
    "\n",
    "            if session.get_world_rank() == 0:\n",
    "                print(\n",
    "                    \"Epoch {}-{} Loss: {:.4f} Acc: {:.4f}\".format(\n",
    "                        epoch, phase, epoch_loss, epoch_acc\n",
    "                    )\n",
    "                )\n",
    "\n",
    "            # Report metrics and checkpoint every epoch\n",
    "            if phase == \"val\":\n",
    "                checkpoint = TorchCheckpoint.from_dict(\n",
    "                    {\n",
    "                        \"epoch\": epoch,\n",
    "                        \"model\": model.module.state_dict(),\n",
    "                        \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                    }\n",
    "                )\n",
    "                session.report(\n",
    "                    metrics={\"loss\": epoch_loss, \"acc\": epoch_acc},\n",
    "                    checkpoint=checkpoint,\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, setup the TorchTrainer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.train.torch import TorchTrainer, TorchCheckpoint\n",
    "from ray.air.config import ScalingConfig, RunConfig, CheckpointConfig\n",
    "\n",
    "# Scale out model training across 4 GPUs.\n",
    "scaling_config = ScalingConfig(\n",
    "    num_workers=4, use_gpu=True, resources_per_worker={\"CPU\": 1, \"GPU\": 1}\n",
    ")\n",
    "\n",
    "# Save the latest checkpoint\n",
    "checkpoint_config = CheckpointConfig(num_to_keep=1)\n",
    "\n",
    "# Set experiment name and checkpoint configs\n",
    "run_config = RunConfig(\n",
    "    name=\"finetune-resnet\",\n",
    "    storage_path=\"/tmp/ray_results\",\n",
    "    checkpoint_config=checkpoint_config,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "if SMOKE_TEST:\n",
    "    scaling_config = ScalingConfig(\n",
    "        num_workers=8, use_gpu=False, resources_per_worker={\"CPU\": 1}\n",
    "    )\n",
    "    train_loop_config[\"num_epochs\"] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-01 12:40:15,468\tINFO worker.py:1360 -- Connecting to existing Ray cluster at address: 10.0.53.212:6379...\n",
      "2023-03-01 12:40:15,520\tINFO worker.py:1548 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32mhttps://console.anyscale-staging.com/api/v2/sessions/ses_49hwcjc1pzcddc2nf6cg9itj6b/services?redirect_to=dashboard \u001b[39m\u001b[22m\n",
      "2023-03-01 12:40:16,841\tINFO packaging.py:330 -- Pushing file package 'gcs://_ray_pkg_d6a92d7fa9e73b7fc2276251a1203373.zip' (451.72MiB) to Ray cluster...\n",
      "2023-03-01 12:40:26,413\tINFO packaging.py:343 -- Successfully pushed file package 'gcs://_ray_pkg_d6a92d7fa9e73b7fc2276251a1203373.zip'.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2023-03-01 12:41:31</td></tr>\n",
       "<tr><td>Running for: </td><td>00:01:05.01        </td></tr>\n",
       "<tr><td>Memory:      </td><td>8.3/62.0 GiB       </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using FIFO scheduling algorithm.<br>Resources requested: 0/64 CPUs, 0/4 GPUs, 0.0/163.97 GiB heap, 0.0/72.85 GiB objects (0.0/4.0 accelerator_type:T4)\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th>status    </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    loss</th><th style=\"text-align: right;\">     acc</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>TorchTrainer_4c393_00000</td><td>TERMINATED</td><td>10.0.62.120:1395</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         51.9574</td><td style=\"text-align: right;\">0.143938</td><td style=\"text-align: right;\">0.973684</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=1478, ip=10.0.62.120) 2023-03-01 12:40:37,398\tINFO config.py:86 -- Setting up process group for: env:// [rank=0, world_size=4]\n",
      "(RayTrainWorker pid=89742) 2023-03-01 12:40:39,344\tINFO train_loop_utils.py:307 -- Moving model to device: cuda:0\n",
      "(RayTrainWorker pid=1478, ip=10.0.62.120) 2023-03-01 12:40:39,474\tINFO train_loop_utils.py:307 -- Moving model to device: cuda:0\n",
      "(RayTrainWorker pid=1902, ip=10.0.24.64) 2023-03-01 12:40:39,495\tINFO train_loop_utils.py:307 -- Moving model to device: cuda:0\n",
      "(RayTrainWorker pid=3689, ip=10.0.52.145) 2023-03-01 12:40:39,588\tINFO train_loop_utils.py:307 -- Moving model to device: cuda:0\n",
      "(RayTrainWorker pid=89742) 2023-03-01 12:40:40,888\tINFO train_loop_utils.py:367 -- Wrapping provided model in DistributedDataParallel.\n",
      "(RayTrainWorker pid=1478, ip=10.0.62.120) 2023-03-01 12:40:41,001\tINFO train_loop_utils.py:367 -- Wrapping provided model in DistributedDataParallel.\n",
      "(RayTrainWorker pid=1902, ip=10.0.24.64) 2023-03-01 12:40:41,019\tINFO train_loop_utils.py:367 -- Wrapping provided model in DistributedDataParallel.\n",
      "(RayTrainWorker pid=3689, ip=10.0.52.145) 2023-03-01 12:40:41,123\tINFO train_loop_utils.py:367 -- Wrapping provided model in DistributedDataParallel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=1478, ip=10.0.62.120) Epoch 0-train Loss: 0.7398 Acc: 0.4426\n",
      "(RayTrainWorker pid=1478, ip=10.0.62.120) Epoch 0-val Loss: 0.5739 Acc: 0.7105\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"trialProgress\">\n",
       "  <h3>Trial Progress</h3>\n",
       "  <table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th style=\"text-align: right;\">     acc</th><th>date               </th><th>done  </th><th style=\"text-align: right;\">  experiment_tag</th><th>hostname      </th><th style=\"text-align: right;\">  iterations_since_restore</th><th style=\"text-align: right;\">    loss</th><th>node_ip    </th><th style=\"text-align: right;\">  pid</th><th>should_checkpoint  </th><th style=\"text-align: right;\">  time_since_restore</th><th style=\"text-align: right;\">  time_this_iter_s</th><th style=\"text-align: right;\">  time_total_s</th><th style=\"text-align: right;\">  timestamp</th><th style=\"text-align: right;\">  training_iteration</th><th>trial_id   </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>TorchTrainer_4c393_00000</td><td style=\"text-align: right;\">0.973684</td><td>2023-03-01_12-41-26</td><td>True  </td><td style=\"text-align: right;\">               0</td><td>ip-10-0-62-120</td><td style=\"text-align: right;\">                        10</td><td style=\"text-align: right;\">0.143938</td><td>10.0.62.120</td><td style=\"text-align: right;\"> 1395</td><td>True               </td><td style=\"text-align: right;\">             51.9574</td><td style=\"text-align: right;\">            4.0961</td><td style=\"text-align: right;\">       51.9574</td><td style=\"text-align: right;\"> 1677703285</td><td style=\"text-align: right;\">                  10</td><td>4c393_00000</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".trialProgress {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".trialProgress h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".trialProgress td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=1478, ip=10.0.62.120) Epoch 1-train Loss: 0.5130 Acc: 0.8197\n",
      "(RayTrainWorker pid=1478, ip=10.0.62.120) Epoch 1-val Loss: 0.3553 Acc: 0.9737\n",
      "(RayTrainWorker pid=1478, ip=10.0.62.120) Epoch 2-train Loss: 0.4676 Acc: 0.7705\n",
      "(RayTrainWorker pid=1478, ip=10.0.62.120) Epoch 2-val Loss: 0.2600 Acc: 0.9737\n",
      "(RayTrainWorker pid=1478, ip=10.0.62.120) Epoch 3-train Loss: 0.3940 Acc: 0.8525\n",
      "(RayTrainWorker pid=1478, ip=10.0.62.120) Epoch 3-val Loss: 0.2136 Acc: 0.9737\n",
      "(RayTrainWorker pid=1478, ip=10.0.62.120) Epoch 4-train Loss: 0.3602 Acc: 0.8852\n",
      "(RayTrainWorker pid=1478, ip=10.0.62.120) Epoch 4-val Loss: 0.1854 Acc: 1.0000\n",
      "(RayTrainWorker pid=1478, ip=10.0.62.120) Epoch 5-train Loss: 0.2871 Acc: 0.8689\n",
      "(RayTrainWorker pid=1478, ip=10.0.62.120) Epoch 5-val Loss: 0.1691 Acc: 1.0000\n",
      "(RayTrainWorker pid=1478, ip=10.0.62.120) Epoch 6-train Loss: 0.2858 Acc: 0.9344\n",
      "(RayTrainWorker pid=1478, ip=10.0.62.120) Epoch 6-val Loss: 0.1459 Acc: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-01 12:41:13,026\tWARNING util.py:244 -- The `process_trial_save` operation took 2.925 s, which may be a performance bottleneck.\n",
      "2023-03-01 12:41:13,027\tWARNING trial_runner.py:678 -- Consider turning off forced head-worker trial checkpoint syncs by setting sync_on_checkpoint=False. Note that this may result in faulty trial restoration if a failure occurs while the checkpoint is being synced from the worker to the head node.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=1478, ip=10.0.62.120) Epoch 7-train Loss: 0.1965 Acc: 0.9344\n",
      "(RayTrainWorker pid=1478, ip=10.0.62.120) Epoch 7-val Loss: 0.1387 Acc: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-01 12:41:17,101\tWARNING util.py:244 -- The `process_trial_save` operation took 2.925 s, which may be a performance bottleneck.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=1478, ip=10.0.62.120) Epoch 8-train Loss: 0.2277 Acc: 0.9344\n",
      "(RayTrainWorker pid=1478, ip=10.0.62.120) Epoch 8-val Loss: 0.1500 Acc: 0.9737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-01 12:41:21,195\tWARNING util.py:244 -- The `process_trial_save` operation took 2.936 s, which may be a performance bottleneck.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=1478, ip=10.0.62.120) Epoch 9-train Loss: 0.1884 Acc: 0.9344\n",
      "(RayTrainWorker pid=1478, ip=10.0.62.120) Epoch 9-val Loss: 0.1439 Acc: 0.9737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-01 12:41:25,360\tWARNING util.py:244 -- The `process_trial_save` operation took 2.947 s, which may be a performance bottleneck.\n",
      "2023-03-01 12:41:29,205\tWARNING util.py:244 -- The `process_trial_save` operation took 2.696 s, which may be a performance bottleneck.\n",
      "2023-03-01 12:41:33,757\tINFO tune.py:825 -- Total run time: 66.96 seconds (65.01 seconds for the tuning loop).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result(\n",
      "  metrics={'loss': 0.14393797124686994, 'acc': 0.9736842105263158, 'should_checkpoint': True, 'done': True, 'trial_id': '4c393_00000', 'experiment_tag': '0'},\n",
      "  log_dir=PosixPath('/tmp/ray_results/finetune-resnet/TorchTrainer_4c393_00000_0_2023-03-01_12-40-31'),\n",
      "  checkpoint=TorchCheckpoint(local_path=/tmp/ray_results/finetune-resnet/TorchTrainer_4c393_00000_0_2023-03-01_12-40-31/checkpoint_000009)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "trainer = TorchTrainer(\n",
    "    train_loop_per_worker=train_loop_per_worker,\n",
    "    train_loop_config=train_loop_config,\n",
    "    scaling_config=scaling_config,\n",
    "    run_config=run_config,\n",
    ")\n",
    "\n",
    "result = trainer.fit()\n",
    "print(result)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the checkpoint for prediction:\n",
    "\n",
    " \n",
    " The metadata and checkpoints have already been saved in the `storage_path` specified in TorchTrainer:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to load the trained model and evaluate it on test data. The best model parameters have been saved in `log_dir`. We can load the resulting checkpoint from our fine-tuning run using the previously defined `initialize_model_from_uri()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ray/anaconda3/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/ray/anaconda3/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model = initialize_model_from_uri(result.checkpoint.uri)\n",
    "device = torch.device(\"cuda\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "if SMOKE_TEST:\n",
    "    device = torch.device(\"cpu\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, define a simple evaluation loop and check the performance of the checkpoint model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.934640522875817\n"
     ]
    }
   ],
   "source": [
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "torch_datasets = build_datasets()\n",
    "dataloader = DataLoader(torch_datasets[\"val\"], batch_size=32, num_workers=4)\n",
    "corrects = 0\n",
    "for inputs, labels in dataloader:\n",
    "    inputs = inputs.to(device)\n",
    "    labels = labels.to(device)\n",
    "    preds = model(inputs)\n",
    "    corrects += evaluate(preds, labels)\n",
    "\n",
    "print(\"Accuracy: \", corrects / len(dataloader.dataset))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ray",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "a8c1140d108077f4faeb76b2438f85e4ed675f93d004359552883616a1acd54c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
