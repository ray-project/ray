{
    "cells": [
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Finetuning a Pytorch Image Classifier with Ray AIR\n",
                "This example fine tunes a pre-trained ResNet model with Ray Train. \n",
                "\n",
                "For this example, the network architecture consists of the intermediate layer output of a pre-trained ResNet model, which feeds into a randomly initialized linear layer that outputs classification logits for our new task.\n",
                "\n",
                "\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Load and preprocess finetuning dataset\n",
                "This example is adapted from Pytorch's [Finetuning Torchvision Models](https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html) tutorial.\n",
                "We will use *hymenoptera_data* as the finetuning dataset, which contains two classes (bees and ants) and 397 total images (across training and validation). This is a quite small dataset and used only for demonstration purposes. "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {
                "tags": [
                    "remove-cell"
                ]
            },
            "outputs": [],
            "source": [
                "# To run full example, set SMOKE_TEST as False\n",
                "SMOKE_TEST = True\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The dataset is publicly available [here](https://www.kaggle.com/datasets/ajayrana/hymenoptera-data). Note that it is structured with directory names as the labels. Use `torchvision.datasets.ImageFolder()` to load the images and their corresponding labels."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.optim as optim\n",
                "from torch.utils.data import DataLoader\n",
                "from torchvision import datasets, models, transforms\n",
                "import numpy as np\n",
                "\n",
                "# Data augmentation and normalization for training\n",
                "# Just normalization for validation\n",
                "data_transforms = {\n",
                "    \"train\": transforms.Compose(\n",
                "        [\n",
                "            transforms.RandomResizedCrop(224),\n",
                "            transforms.RandomHorizontalFlip(),\n",
                "            transforms.ToTensor(),\n",
                "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
                "        ]\n",
                "    ),\n",
                "    \"val\": transforms.Compose(\n",
                "        [\n",
                "            transforms.Resize(224),\n",
                "            transforms.CenterCrop(224),\n",
                "            transforms.ToTensor(),\n",
                "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
                "        ]\n",
                "    ),\n",
                "}\n",
                "\n",
                "def download_datasets():\n",
                "    os.system(\n",
                "        \"wget https://download.pytorch.org/tutorial/hymenoptera_data.zip >/dev/null 2>&1\"\n",
                "    )\n",
                "    os.system(\"unzip hymenoptera_data.zip >/dev/null 2>&1\")\n",
                "\n",
                "# Download and build torch datasets\n",
                "def build_datasets():\n",
                "    torch_datasets = {}\n",
                "    for split in [\"train\", \"val\"]:\n",
                "        torch_datasets[split] = datasets.ImageFolder(\n",
                "            os.path.join(\"./hymenoptera_data\", split), data_transforms[split]\n",
                "        )\n",
                "    return torch_datasets\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {
                "tags": [
                    "remove-cell"
                ]
            },
            "outputs": [],
            "source": [
                "if SMOKE_TEST:\n",
                "    from torch.utils.data import Subset\n",
                "\n",
                "    def build_datasets():\n",
                "        torch_datasets = {}\n",
                "        for split in [\"train\", \"val\"]:\n",
                "            torch_datasets[split] = datasets.ImageFolder(\n",
                "                os.path.join(\"./hymenoptera_data\", split), data_transforms[split]\n",
                "            )\n",
                "            \n",
                "        # Only take a subset for smoke test\n",
                "        for split in [\"train\", \"val\"]:\n",
                "            indices = list(range(100))\n",
                "            torch_datasets[split] = Subset(torch_datasets[split], indices)\n",
                "        return torch_datasets\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Initialize Model and Fine-tuning configs"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Next, let's define the training configuration that will be passed into the training loop function later."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": [
                "train_loop_config = {\n",
                "    \"input_size\": 224,  # Input image size (224 x 224)\n",
                "    \"batch_size\": 32,  # Batch size for training\n",
                "    \"num_epochs\": 10,  # Number of epochs to train for\n",
                "    \"lr\": 0.001,  # Learning Rate\n",
                "    \"momentum\": 0.9,  # SGD optimizer momentum\n",
                "}\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Next, let's define our model. You can either create a model from pre-trained weights or reload the model checkpoint from a previous run."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import torch\n",
                "from ray.train import Checkpoint\n",
                "\n",
                "# Option 1: Initialize model with pretrained weights\n",
                "def initialize_model():\n",
                "    # Load pretrained model params\n",
                "    model = models.resnet50(pretrained=True)\n",
                "\n",
                "    # Replace the original classifier with a new Linear layer\n",
                "    num_features = model.fc.in_features\n",
                "    model.fc = nn.Linear(num_features, 2)\n",
                "\n",
                "    # Ensure all params get updated during finetuning\n",
                "    for param in model.parameters():\n",
                "        param.requires_grad = True\n",
                "    return model\n",
                "\n",
                "\n",
                "# Option 2: Initialize model with an AIR checkpoint\n",
                "# Replace this with your own uri\n",
                "CHECKPOINT_FROM_S3 = Checkpoint(\n",
                "    path=\"s3://air-example-data/finetune-resnet-checkpoint/TorchTrainer_4f69f_00000_0_2023-02-14_14-04-09/checkpoint_000001/\"\n",
                ")\n",
                "\n",
                "\n",
                "def initialize_model_from_checkpoint(checkpoint: Checkpoint):\n",
                "    with checkpoint.as_directory() as tmpdir:\n",
                "        state_dict = torch.load(os.path.join(tmpdir, \"checkpoint.pt\"))\n",
                "    resnet50 = initialize_model()\n",
                "    resnet50.load_state_dict(state_dict[\"model\"])\n",
                "    return resnet50\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Define the Training Loop\n",
                "\n",
                "The `train_loop_per_worker` function defines the fine-tuning procedure for each worker.\n",
                "\n",
                "**1. Prepare dataloaders for each worker**:\n",
                "- This tutorial assumes you are using PyTorch's native `torch.utils.data.Dataset` for data input. {meth}`train.torch.prepare_data_loader() <ray.train.torch.prepare_data_loader>`  prepares your dataLoader for distributed execution. You can also use Ray Data for more efficient preprocessing. For more details on using Ray Data for for images, see the {doc}`Working with Images </data/working-with-images>` Ray Data user guide.\n",
                "\n",
                "**2. Prepare your model**:\n",
                "- {meth}`train.torch.prepare_model() <ray.train.torch.prepare_model>` prepares the model for distributed training. Under the hood, it converts your torch model to `DistributedDataParallel` model, which synchronize its weights across all workers.\n",
                "\n",
                "**3. Report metrics and checkpoint**:\n",
                "- {meth}`train.report() <ray.train.report>` will report metrics and checkpoints to Ray AIR.\n",
                "- Saving checkpoints through {meth}`train.report(metrics, checkpoint=...) <ray.train.report>` will automatically [upload checkpoints to cloud storage](tune-cloud-checkpointing) (if configured), and allow you to easily enable Ray AIR worker fault tolerance in the future."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "from tempfile import TemporaryDirectory\n",
                "\n",
                "import ray.train as train\n",
                "from ray.train import Checkpoint\n",
                "\n",
                "\n",
                "\n",
                "def evaluate(logits, labels):\n",
                "    _, preds = torch.max(logits, 1)\n",
                "    corrects = torch.sum(preds == labels).item()\n",
                "    return corrects\n",
                "\n",
                "\n",
                "def train_loop_per_worker(configs):\n",
                "    import warnings\n",
                "\n",
                "    warnings.filterwarnings(\"ignore\")\n",
                "\n",
                "    # Calculate the batch size for a single worker\n",
                "    worker_batch_size = configs[\"batch_size\"] // train.get_context().get_world_size()\n",
                "\n",
                "    # Download dataset once on local rank 0 worker\n",
                "    if train.get_context().get_local_rank() == 0:\n",
                "        download_datasets()\n",
                "    torch.distributed.barrier()\n",
                "\n",
                "    # Build datasets on each worker\n",
                "    torch_datasets = build_datasets()\n",
                "\n",
                "    # Prepare dataloader for each worker\n",
                "    dataloaders = dict()\n",
                "    dataloaders[\"train\"] = DataLoader(\n",
                "        torch_datasets[\"train\"], batch_size=worker_batch_size, shuffle=True\n",
                "    )\n",
                "    dataloaders[\"val\"] = DataLoader(\n",
                "        torch_datasets[\"val\"], batch_size=worker_batch_size, shuffle=False\n",
                "    )\n",
                "\n",
                "    # Distribute\n",
                "    dataloaders[\"train\"] = train.torch.prepare_data_loader(dataloaders[\"train\"])\n",
                "    dataloaders[\"val\"] = train.torch.prepare_data_loader(dataloaders[\"val\"])\n",
                "\n",
                "    device = train.torch.get_device()\n",
                "\n",
                "    # Prepare DDP Model, optimizer, and loss function\n",
                "    model = initialize_model()\n",
                "    model = train.torch.prepare_model(model)\n",
                "\n",
                "    optimizer = optim.SGD(\n",
                "        model.parameters(), lr=configs[\"lr\"], momentum=configs[\"momentum\"]\n",
                "    )\n",
                "    criterion = nn.CrossEntropyLoss()\n",
                "\n",
                "    # Start training loops\n",
                "    for epoch in range(configs[\"num_epochs\"]):\n",
                "        # Each epoch has a training and validation phase\n",
                "        for phase in [\"train\", \"val\"]:\n",
                "            if phase == \"train\":\n",
                "                model.train()  # Set model to training mode\n",
                "            else:\n",
                "                model.eval()  # Set model to evaluate mode\n",
                "\n",
                "            running_loss = 0.0\n",
                "            running_corrects = 0\n",
                "\n",
                "            for inputs, labels in dataloaders[phase]:\n",
                "                inputs = inputs.to(device)\n",
                "                labels = labels.to(device)\n",
                "\n",
                "                # zero the parameter gradients\n",
                "                optimizer.zero_grad()\n",
                "\n",
                "                # forward\n",
                "                with torch.set_grad_enabled(phase == \"train\"):\n",
                "                    # Get model outputs and calculate loss\n",
                "                    outputs = model(inputs)\n",
                "                    loss = criterion(outputs, labels)\n",
                "\n",
                "                    # backward + optimize only if in training phase\n",
                "                    if phase == \"train\":\n",
                "                        loss.backward()\n",
                "                        optimizer.step()\n",
                "\n",
                "                # calculate statistics\n",
                "                running_loss += loss.item() * inputs.size(0)\n",
                "                running_corrects += evaluate(outputs, labels)\n",
                "\n",
                "            size = len(torch_datasets[phase]) // train.get_context().get_world_size()\n",
                "            epoch_loss = running_loss / size\n",
                "            epoch_acc = running_corrects / size\n",
                "\n",
                "            if train.get_context().get_world_rank() == 0:\n",
                "                print(\n",
                "                    \"Epoch {}-{} Loss: {:.4f} Acc: {:.4f}\".format(\n",
                "                        epoch, phase, epoch_loss, epoch_acc\n",
                "                    )\n",
                "                )\n",
                "\n",
                "            # Report metrics and checkpoint every epoch\n",
                "            if phase == \"val\":\n",
                "                with TemporaryDirectory() as tmpdir:\n",
                "                    state_dict = {\n",
                "                        \"epoch\": epoch,\n",
                "                        \"model\": model.module.state_dict(),\n",
                "                        \"optimizer_state_dict\": optimizer.state_dict(),\n",
                "                    }\n",
                "                    torch.save(state_dict, os.path.join(tmpdir, \"checkpoint.pt\"))\n",
                "                    train.report(\n",
                "                        metrics={\"loss\": epoch_loss, \"acc\": epoch_acc},\n",
                "                        checkpoint=Checkpoint.from_directory(tmpdir),\n",
                "                    )\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Next, setup the TorchTrainer:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [],
            "source": [
                "from ray.train.torch import TorchTrainer, LegacyTorchCheckpoint\n",
                "from ray.train import ScalingConfig, RunConfig, CheckpointConfig\n",
                "\n",
                "# Scale out model training across 4 GPUs.\n",
                "scaling_config = ScalingConfig(\n",
                "    num_workers=4, use_gpu=True, resources_per_worker={\"CPU\": 1, \"GPU\": 1}\n",
                ")\n",
                "\n",
                "# Save the latest checkpoint\n",
                "checkpoint_config = CheckpointConfig(num_to_keep=1)\n",
                "\n",
                "# Set experiment name and checkpoint configs\n",
                "run_config = RunConfig(\n",
                "    name=\"finetune-resnet\",\n",
                "    storage_path=\"/tmp/ray_results\",\n",
                "    checkpoint_config=checkpoint_config,\n",
                ")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {
                "tags": [
                    "remove-cell"
                ]
            },
            "outputs": [],
            "source": [
                "if SMOKE_TEST:\n",
                "    scaling_config = ScalingConfig(\n",
                "        num_workers=8, use_gpu=False, resources_per_worker={\"CPU\": 1}\n",
                "    )\n",
                "    train_loop_config[\"num_epochs\"] = 1\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2023-03-01 12:40:15,468\tINFO worker.py:1360 -- Connecting to existing Ray cluster at address: 10.0.53.212:6379...\n",
                        "2023-03-01 12:40:15,520\tINFO worker.py:1548 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32mhttps://console.anyscale-staging.com/api/v2/sessions/ses_49hwcjc1pzcddc2nf6cg9itj6b/services?redirect_to=dashboard \u001b[39m\u001b[22m\n",
                        "2023-03-01 12:40:16,841\tINFO packaging.py:330 -- Pushing file package 'gcs://_ray_pkg_d6a92d7fa9e73b7fc2276251a1203373.zip' (451.72MiB) to Ray cluster...\n",
                        "2023-03-01 12:40:26,413\tINFO packaging.py:343 -- Successfully pushed file package 'gcs://_ray_pkg_d6a92d7fa9e73b7fc2276251a1203373.zip'.\n"
                    ]
                },
                {
                    "data": {
                        "text/html": [
                            "<div class=\"tuneStatus\">\n",
                            "  <div style=\"display: flex;flex-direction: row\">\n",
                            "    <div style=\"display: flex;flex-direction: column;\">\n",
                            "      <h3>Tune Status</h3>\n",
                            "      <table>\n",
                            "<tbody>\n",
                            "<tr><td>Current time:</td><td>2023-03-01 12:41:31</td></tr>\n",
                            "<tr><td>Running for: </td><td>00:01:05.01        </td></tr>\n",
                            "<tr><td>Memory:      </td><td>8.3/62.0 GiB       </td></tr>\n",
                            "</tbody>\n",
                            "</table>\n",
                            "    </div>\n",
                            "    <div class=\"vDivider\"></div>\n",
                            "    <div class=\"systemInfo\">\n",
                            "      <h3>System Info</h3>\n",
                            "      Using FIFO scheduling algorithm.<br>Resources requested: 0/64 CPUs, 0/4 GPUs, 0.0/163.97 GiB heap, 0.0/72.85 GiB objects (0.0/4.0 accelerator_type:T4)\n",
                            "    </div>\n",
                            "    \n",
                            "  </div>\n",
                            "  <div class=\"hDivider\"></div>\n",
                            "  <div class=\"trialStatus\">\n",
                            "    <h3>Trial Status</h3>\n",
                            "    <table>\n",
                            "<thead>\n",
                            "<tr><th>Trial name              </th><th>status    </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    loss</th><th style=\"text-align: right;\">     acc</th></tr>\n",
                            "</thead>\n",
                            "<tbody>\n",
                            "<tr><td>TorchTrainer_4c393_00000</td><td>TERMINATED</td><td>10.0.62.120:1395</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         51.9574</td><td style=\"text-align: right;\">0.143938</td><td style=\"text-align: right;\">0.973684</td></tr>\n",
                            "</tbody>\n",
                            "</table>\n",
                            "  </div>\n",
                            "</div>\n",
                            "<style>\n",
                            ".tuneStatus {\n",
                            "  color: var(--jp-ui-font-color1);\n",
                            "}\n",
                            ".tuneStatus .systemInfo {\n",
                            "  display: flex;\n",
                            "  flex-direction: column;\n",
                            "}\n",
                            ".tuneStatus td {\n",
                            "  white-space: nowrap;\n",
                            "}\n",
                            ".tuneStatus .trialStatus {\n",
                            "  display: flex;\n",
                            "  flex-direction: column;\n",
                            "}\n",
                            ".tuneStatus h3 {\n",
                            "  font-weight: bold;\n",
                            "}\n",
                            ".tuneStatus .hDivider {\n",
                            "  border-bottom-width: var(--jp-border-width);\n",
                            "  border-bottom-color: var(--jp-border-color0);\n",
                            "  border-bottom-style: solid;\n",
                            "}\n",
                            ".tuneStatus .vDivider {\n",
                            "  border-left-width: var(--jp-border-width);\n",
                            "  border-left-color: var(--jp-border-color0);\n",
                            "  border-left-style: solid;\n",
                            "  margin: 0.5em 1em 0.5em 1em;\n",
                            "}\n",
                            "</style>\n"
                        ],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "(RayTrainWorker pid=1478, ip=10.0.62.120) 2023-03-01 12:40:37,398\tINFO config.py:86 -- Setting up process group for: env:// [rank=0, world_size=4]\n",
                        "(RayTrainWorker pid=89742) 2023-03-01 12:40:39,344\tINFO train_loop_utils.py:307 -- Moving model to device: cuda:0\n",
                        "(RayTrainWorker pid=1478, ip=10.0.62.120) 2023-03-01 12:40:39,474\tINFO train_loop_utils.py:307 -- Moving model to device: cuda:0\n",
                        "(RayTrainWorker pid=1902, ip=10.0.24.64) 2023-03-01 12:40:39,495\tINFO train_loop_utils.py:307 -- Moving model to device: cuda:0\n",
                        "(RayTrainWorker pid=3689, ip=10.0.52.145) 2023-03-01 12:40:39,588\tINFO train_loop_utils.py:307 -- Moving model to device: cuda:0\n",
                        "(RayTrainWorker pid=89742) 2023-03-01 12:40:40,888\tINFO train_loop_utils.py:367 -- Wrapping provided model in DistributedDataParallel.\n",
                        "(RayTrainWorker pid=1478, ip=10.0.62.120) 2023-03-01 12:40:41,001\tINFO train_loop_utils.py:367 -- Wrapping provided model in DistributedDataParallel.\n",
                        "(RayTrainWorker pid=1902, ip=10.0.24.64) 2023-03-01 12:40:41,019\tINFO train_loop_utils.py:367 -- Wrapping provided model in DistributedDataParallel.\n",
                        "(RayTrainWorker pid=3689, ip=10.0.52.145) 2023-03-01 12:40:41,123\tINFO train_loop_utils.py:367 -- Wrapping provided model in DistributedDataParallel.\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "(RayTrainWorker pid=1478, ip=10.0.62.120) Epoch 0-train Loss: 0.7398 Acc: 0.4426\n",
                        "(RayTrainWorker pid=1478, ip=10.0.62.120) Epoch 0-val Loss: 0.5739 Acc: 0.7105\n"
                    ]
                },
                {
                    "data": {
                        "text/html": [
                            "<div class=\"trialProgress\">\n",
                            "  <h3>Trial Progress</h3>\n",
                            "  <table>\n",
                            "<thead>\n",
                            "<tr><th>Trial name              </th><th style=\"text-align: right;\">     acc</th><th>date               </th><th>done  </th><th style=\"text-align: right;\">  experiment_tag</th><th>hostname      </th><th style=\"text-align: right;\">  iterations_since_restore</th><th style=\"text-align: right;\">    loss</th><th>node_ip    </th><th style=\"text-align: right;\">  pid</th><th>should_checkpoint  </th><th style=\"text-align: right;\">  time_since_restore</th><th style=\"text-align: right;\">  time_this_iter_s</th><th style=\"text-align: right;\">  time_total_s</th><th style=\"text-align: right;\">  timestamp</th><th style=\"text-align: right;\">  training_iteration</th><th>trial_id   </th></tr>\n",
                            "</thead>\n",
                            "<tbody>\n",
                            "<tr><td>TorchTrainer_4c393_00000</td><td style=\"text-align: right;\">0.973684</td><td>2023-03-01_12-41-26</td><td>True  </td><td style=\"text-align: right;\">               0</td><td>ip-10-0-62-120</td><td style=\"text-align: right;\">                        10</td><td style=\"text-align: right;\">0.143938</td><td>10.0.62.120</td><td style=\"text-align: right;\"> 1395</td><td>True               </td><td style=\"text-align: right;\">             51.9574</td><td style=\"text-align: right;\">            4.0961</td><td style=\"text-align: right;\">       51.9574</td><td style=\"text-align: right;\"> 1677703285</td><td style=\"text-align: right;\">                  10</td><td>4c393_00000</td></tr>\n",
                            "</tbody>\n",
                            "</table>\n",
                            "</div>\n",
                            "<style>\n",
                            ".trialProgress {\n",
                            "  display: flex;\n",
                            "  flex-direction: column;\n",
                            "  color: var(--jp-ui-font-color1);\n",
                            "}\n",
                            ".trialProgress h3 {\n",
                            "  font-weight: bold;\n",
                            "}\n",
                            ".trialProgress td {\n",
                            "  white-space: nowrap;\n",
                            "}\n",
                            "</style>\n"
                        ],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "(RayTrainWorker pid=1478, ip=10.0.62.120) Epoch 1-train Loss: 0.5130 Acc: 0.8197\n",
                        "(RayTrainWorker pid=1478, ip=10.0.62.120) Epoch 1-val Loss: 0.3553 Acc: 0.9737\n",
                        "(RayTrainWorker pid=1478, ip=10.0.62.120) Epoch 2-train Loss: 0.4676 Acc: 0.7705\n",
                        "(RayTrainWorker pid=1478, ip=10.0.62.120) Epoch 2-val Loss: 0.2600 Acc: 0.9737\n",
                        "(RayTrainWorker pid=1478, ip=10.0.62.120) Epoch 3-train Loss: 0.3940 Acc: 0.8525\n",
                        "(RayTrainWorker pid=1478, ip=10.0.62.120) Epoch 3-val Loss: 0.2136 Acc: 0.9737\n",
                        "(RayTrainWorker pid=1478, ip=10.0.62.120) Epoch 4-train Loss: 0.3602 Acc: 0.8852\n",
                        "(RayTrainWorker pid=1478, ip=10.0.62.120) Epoch 4-val Loss: 0.1854 Acc: 1.0000\n",
                        "(RayTrainWorker pid=1478, ip=10.0.62.120) Epoch 5-train Loss: 0.2871 Acc: 0.8689\n",
                        "(RayTrainWorker pid=1478, ip=10.0.62.120) Epoch 5-val Loss: 0.1691 Acc: 1.0000\n",
                        "(RayTrainWorker pid=1478, ip=10.0.62.120) Epoch 6-train Loss: 0.2858 Acc: 0.9344\n",
                        "(RayTrainWorker pid=1478, ip=10.0.62.120) Epoch 6-val Loss: 0.1459 Acc: 1.0000\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2023-03-01 12:41:13,026\tWARNING util.py:244 -- The `process_trial_save` operation took 2.925 s, which may be a performance bottleneck.\n",
                        "2023-03-01 12:41:13,027\tWARNING trial_runner.py:678 -- Consider turning off forced head-worker trial checkpoint syncs by setting sync_on_checkpoint=False. Note that this may result in faulty trial restoration if a failure occurs while the checkpoint is being synced from the worker to the head node.\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "(RayTrainWorker pid=1478, ip=10.0.62.120) Epoch 7-train Loss: 0.1965 Acc: 0.9344\n",
                        "(RayTrainWorker pid=1478, ip=10.0.62.120) Epoch 7-val Loss: 0.1387 Acc: 1.0000\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2023-03-01 12:41:17,101\tWARNING util.py:244 -- The `process_trial_save` operation took 2.925 s, which may be a performance bottleneck.\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "(RayTrainWorker pid=1478, ip=10.0.62.120) Epoch 8-train Loss: 0.2277 Acc: 0.9344\n",
                        "(RayTrainWorker pid=1478, ip=10.0.62.120) Epoch 8-val Loss: 0.1500 Acc: 0.9737\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2023-03-01 12:41:21,195\tWARNING util.py:244 -- The `process_trial_save` operation took 2.936 s, which may be a performance bottleneck.\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "(RayTrainWorker pid=1478, ip=10.0.62.120) Epoch 9-train Loss: 0.1884 Acc: 0.9344\n",
                        "(RayTrainWorker pid=1478, ip=10.0.62.120) Epoch 9-val Loss: 0.1439 Acc: 0.9737\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2023-03-01 12:41:25,360\tWARNING util.py:244 -- The `process_trial_save` operation took 2.947 s, which may be a performance bottleneck.\n",
                        "2023-03-01 12:41:29,205\tWARNING util.py:244 -- The `process_trial_save` operation took 2.696 s, which may be a performance bottleneck.\n",
                        "2023-03-01 12:41:33,757\tINFO tune.py:825 -- Total run time: 66.96 seconds (65.01 seconds for the tuning loop).\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Result(\n",
                        "  metrics={'loss': 0.14393797124686994, 'acc': 0.9736842105263158, 'should_checkpoint': True, 'done': True, 'trial_id': '4c393_00000', 'experiment_tag': '0'},\n",
                        "  log_dir=PosixPath('/tmp/ray_results/finetune-resnet/TorchTrainer_4c393_00000_0_2023-03-01_12-40-31'),\n",
                        "  checkpoint=LegacyTorchCheckpoint(local_path=/tmp/ray_results/finetune-resnet/TorchTrainer_4c393_00000_0_2023-03-01_12-40-31/checkpoint_000009)\n",
                        ")\n"
                    ]
                }
            ],
            "source": [
                "trainer = TorchTrainer(\n",
                "    train_loop_per_worker=train_loop_per_worker,\n",
                "    train_loop_config=train_loop_config,\n",
                "    scaling_config=scaling_config,\n",
                "    run_config=run_config,\n",
                ")\n",
                "\n",
                "result = trainer.fit()\n",
                "print(result)\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Load the checkpoint for prediction:\n",
                "\n",
                " \n",
                " The metadata and checkpoints have already been saved in the `storage_path` specified in TorchTrainer:"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We now need to load the trained model and evaluate it on test data. The best model parameters have been saved in `log_dir`. We can load the resulting checkpoint from our fine-tuning run using the previously defined `initialize_model_from_checkpoint()` function."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/home/ray/anaconda3/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
                        "  warnings.warn(\n",
                        "/home/ray/anaconda3/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
                        "  warnings.warn(msg)\n"
                    ]
                }
            ],
            "source": [
                "model = initialize_model_from_checkpoint(result.checkpoint)\n",
                "device = torch.device(\"cuda\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {
                "tags": [
                    "remove-cell"
                ]
            },
            "outputs": [],
            "source": [
                "if SMOKE_TEST:\n",
                "    device = torch.device(\"cpu\")\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Finally, define a simple evaluation loop and check the performance of the checkpoint model."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Accuracy:  0.934640522875817\n"
                    ]
                }
            ],
            "source": [
                "model = model.to(device)\n",
                "model.eval()\n",
                "\n",
                "download_datasets()\n",
                "torch_datasets = build_datasets()\n",
                "dataloader = DataLoader(torch_datasets[\"val\"], batch_size=32, num_workers=4)\n",
                "corrects = 0\n",
                "for inputs, labels in dataloader:\n",
                "    inputs = inputs.to(device)\n",
                "    labels = labels.to(device)\n",
                "    preds = model(inputs)\n",
                "    corrects += evaluate(preds, labels)\n",
                "\n",
                "print(\"Accuracy: \", corrects / len(dataloader.dataset))\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "ray",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.11"
        },
        "vscode": {
            "interpreter": {
                "hash": "a8c1140d108077f4faeb76b2438f85e4ed675f93d004359552883616a1acd54c"
            }
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
