{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get started with 2D Parallelism (Tensor + Data Parallelism) using FSDP2 and Ray Train\n",
    "\n",
    "**Time to complete:** 20 min\n",
    "\n",
    "This template shows how to train large language models using tensor parallelism with PyTorch's native [DTensor (Distributed Tensor) API](https://docs.pytorch.org/docs/stable/distributed.tensor.html) and Ray Train for distributed execution.\n",
    "\n",
    "**Tensor Parallelism (TP)** shards model weights across multiple GPUs, enabling training of models that are too large to fit on a single GPU. Combined with **Data Parallelism (DP)**, this creates a powerful **2D parallelism** strategy that scales efficiently to many GPUs.\n",
    "\n",
    "This tutorial provides a step-by-step guide covering:\n",
    "\n",
    "- Understanding 2D parallelism (Tensor Parallelism + Data Parallelism) and 2D PyTorch `DeviceMesh`\n",
    "- Setting up a data loader compatible with 2D parallelism\n",
    "- Preparing the model with DTensor and FSDP2 APIs\n",
    "- Checkpointing\n",
    "- Distributed training with Ray Train\n",
    "\n",
    "**Note:** This tutorial uses PyTorch's native `DTensor` and `fully_shard` APIs. These require PyTorch 2.4 or later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"anyscale-note\" class=\"alert alert-block alert-warning\">\n",
    "\n",
    "  <strong>Anyscale Specific Configuration</strong>\n",
    "\n",
    "  <p><strong>Note:</strong> This tutorial is optimized for the Anyscale platform. When running on open source Ray, additional configuration is required. For example, you would need to manually:</p>\n",
    "\n",
    "  <ul>\n",
    "    <li><strong>Configure your Ray Cluster</strong>: Set up your multi-node environment and manage resource allocation without Anyscale's automation.</li>\n",
    "    <li><strong>Manage Dependencies</strong>: Manually install and manage dependencies on each node.</li>\n",
    "    <li><strong>Set Up Storage</strong>: Configure your own distributed or shared storage system for model checkpointing.</li>\n",
    "  </ul>\n",
    "</div>\n",
    "\n",
    "<style>\n",
    "  div#anyscale-note > p,\n",
    "  div#anyscale-note > ul,\n",
    "  div#anyscale-note > ul li {\n",
    "    color: black;\n",
    "  }\n",
    "\n",
    "  div#anyscale-note {\n",
    "    background-color: rgb(255, 243, 205);\n",
    "  }\n",
    "\n",
    "  div#anyscale-note {\n",
    "    border: 1px solid #ccc; \n",
    "    border-radius: 8px;\n",
    "    padding: 15px;\n",
    "  }\n",
    "\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding 2D Parallelism (Tensor Parallelism + Data Parallelism)\n",
    "\n",
    "We can combine two complementary parallelization strategies:\n",
    "\n",
    "- **Tensor Parallelism (TP)**: Shards model weights across GPUs within a TP group. All GPUs in a TP group process the same input data but hold different parts of the model.\n",
    "- **Data Parallelism (DP)**: Replicates the model across DP groups. Each DP group processes different data and synchronizes gradients.\n",
    "\n",
    "This tutorial uses **FSDP2** for data parallelism instead of the older `DistributedDataParallel` (DDP). While DDP replicates the entire model on each GPU, FSDP2 shards model parameters across the data parallel dimension, significantly reducing memory usage.\n",
    "\n",
    "Additionally, FSDP2 is built on the same DTensor primitives as tensor parallelism, which makes them naturally composable for 2D parallelism.\n",
    "\n",
    "The following figure shows the dataflow of a forward pass using tensor parallelism and FSDP2. Assume we split a parameter tensor row-wise in a tensor-parallel layer. The input tensor (activations) must be partitioned column-wise. After multiplying the local shards, we run an all-reduce to produce the layer output.\n",
    "\n",
    "When combining FSDP2 with tensor parallelism, the parameter shard created for tensor parallelism is sharded again along the data-parallel dimension. The shards are first concatenated across the data-parallel dimension via all-gather communication. Then we multiply the local shards and run an all-reduce.\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"images/tensor_parallel.png\" alt=\"2D Parallelism: Tensor Parallelism + FSDP2\" width=\"50%\">\n",
    "</p>\n",
    "\n",
    "Similarly, during the backward pass we need an all-reduce (and an all-gather when combined with FSDP2). Note that optimizer parameter updates (e.g., Adam) do not require communication.\n",
    "\n",
    "You can define a `DeviceMesh` to map multiple GPU devices to multiple dimensions for partitioning. With `tp_size=2` and `dp_size=2` on 4 GPUs, the device mesh looks like:\n",
    "\n",
    "```\n",
    "Device Mesh (2x2):\n",
    "        TP Dim\n",
    "      [0]  [1]\n",
    " DP   +---+---+\n",
    " Dim  | 0 | 1 |  <- TP Group 0 (same data, sharded model)\n",
    "      +---+---+\n",
    "      | 2 | 3 |  <- TP Group 1 (same data, sharded model)\n",
    "      +---+---+\n",
    "        ^   ^\n",
    "       DP Groups (different data, gradient sync)\n",
    "```\n",
    "\n",
    "- **TP Groups** (rows): GPUs 0,1 and GPUs 2,3 share the same input data but have sharded model weights\n",
    "- **DP Groups** (columns): GPUs 0,2 and GPUs 1,3 see different data and synchronize gradients\n",
    "\n",
    "**When to use Tensor Parallelism vs FSDP:** The communication overhead of each parallelism strategy is determined by communication volume and network bandwidth.\n",
    "\n",
    "- **TP communication volume**: layers × hidden × seq_len × batch_size × 2 (dtype) × 2 (attention + FFN)\n",
    "- **FSDP communication volume**: M × 2 bytes (bfloat16) for M parameters\n",
    "\n",
    "Based on these formulas, TP's overhead grows with batch size and sequence length, while FSDP's overhead remains constant for a given model. Additionally, TP's communication is on the critical path, whereas FSDP's communication can overlap with computation.\n",
    "\n",
    "TP's communication is also more frequent (after each layer), making it more sensitive to network latency. This is particularly impactful in multi-node settings where inter-node latency is higher.\n",
    "\n",
    "Considering these factors, a typical configuration is to use **TP for intra-node parallelism** (leveraging high-speed NVLink) and **FSDP for inter-node parallelism** (where communication can be overlapped with computation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Package and environment setup\n",
    "\n",
    "Install the required dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "pip install torch transformers datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "import os\n",
    "import tempfile\n",
    "import uuid\n",
    "\n",
    "import torch\n",
    "\n",
    "# Set up logging\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def get_mixed_precision_dtype() -> torch.dtype:\n",
    "    \"\"\"Select a mixed-precision dtype that the current GPU supports.\"\"\"\n",
    "    return torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data loading with TP-aware sharding\n",
    "\n",
    "A critical aspect of tensor parallelism is ensuring all GPUs in a TP group receive identical input data. Standard data loaders shard by `world_rank`, giving each GPU different data. With TP, you must shard by `dp_rank` instead.\n",
    "\n",
    "**Global batch size**: Because all GPUs in a TP group see the same data, the effective (global) batch size is `batch_size_per_gpu * dp_size`, not `batch_size_per_gpu * world_size`. For example, with `batch_size_per_gpu=1`, `dp_size=2`, and `tp_size=2` (4 GPUs total), the global batch size is 2, not 4.\n",
    "\n",
    "```python\n",
    "# All TP ranks in same DP group get identical batches\n",
    "sampler = DistributedSampler(\n",
    "    dataset,\n",
    "    num_replicas=dp_size,  # NOT world_size\n",
    "    rank=dp_rank,          # NOT world_rank\n",
    ")\n",
    "```\n",
    "\n",
    "The following function creates a dataloader with proper TP-aware sharding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DownloadConfig, load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "import ray.train\n",
    "\n",
    "\n",
    "def create_dataloader(\n",
    "    model_name: str,\n",
    "    dataset_name: str,\n",
    "    seq_length: int,\n",
    "    batch_size_per_gpu: int,\n",
    "    dp_rank: int,\n",
    "    dp_size: int,\n",
    "    seed: int = 42,\n",
    "    dataset_percentage: float = 10.0,\n",
    ") -> DataLoader:\n",
    "    \"\"\"\n",
    "    Create dataloader with TP-aware sharding.\n",
    "\n",
    "    IMPORTANT: Uses dp_rank/dp_size for sharding (NOT world_rank/world_size).\n",
    "    This ensures all TP ranks in the same DP group see identical batches.\n",
    "    \"\"\"\n",
    "    # Handle datasets that require a config name\n",
    "    dataset_config = \"wikitext-2-raw-v1\" if dataset_name == \"wikitext\" else None\n",
    "    dataset_percentage = float(dataset_percentage)\n",
    "    if not 0 < dataset_percentage <= 100:\n",
    "        raise ValueError(\n",
    "            f\"dataset_percentage must be in (0, 100], got {dataset_percentage}.\"\n",
    "        )\n",
    "    split_spec = f\"train[:{dataset_percentage:.15g}%]\"\n",
    "\n",
    "    # HF datasets/tokenizers handle process-safe caching and downloads.\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "    dataset = load_dataset(\n",
    "        dataset_name,\n",
    "        dataset_config,\n",
    "        split=split_spec,\n",
    "        download_config=DownloadConfig(disable_tqdm=True),\n",
    "    )\n",
    "\n",
    "    # Set pad token if needed\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    # Tokenize dataset\n",
    "    def tokenize_fn(examples):\n",
    "        return tokenizer(\n",
    "            examples[\"text\"], padding=\"max_length\", max_length=seq_length, truncation=True\n",
    "        )\n",
    "\n",
    "    tokenized = dataset.map(\n",
    "        tokenize_fn, batched=True, num_proc=1, keep_in_memory=True,\n",
    "        remove_columns=dataset.column_names,\n",
    "    )\n",
    "    tokenized = tokenized.filter(\n",
    "        lambda example: sum(example[\"attention_mask\"]) > 1,\n",
    "        keep_in_memory=True,\n",
    "    )\n",
    "\n",
    "    # Add labels (ignore padding tokens for causal LM)\n",
    "    def add_labels(examples):\n",
    "        labels = []\n",
    "        for input_ids, attention_mask in zip(\n",
    "            examples[\"input_ids\"], examples[\"attention_mask\"]\n",
    "        ):\n",
    "            labels.append(\n",
    "                [\n",
    "                    token if mask == 1 else -100\n",
    "                    for token, mask in zip(input_ids, attention_mask)\n",
    "                ]\n",
    "            )\n",
    "        examples[\"labels\"] = labels\n",
    "        return examples\n",
    "\n",
    "    tokenized = tokenized.map(add_labels, batched=True, num_proc=1, keep_in_memory=True)\n",
    "    tokenized.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "    # [1] Use DP rank/size for sharding (ensures TP ranks get same data)\n",
    "    sampler = DistributedSampler(\n",
    "        tokenized, num_replicas=dp_size, rank=dp_rank, shuffle=True, seed=seed\n",
    "    )\n",
    "\n",
    "    return DataLoader(tokenized, batch_size=batch_size_per_gpu, sampler=sampler, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model parallelization with the DTensor API\n",
    "\n",
    "PyTorch's DTensor API provides native tensor parallelism through the `parallelize_module` API. For transformer models, you apply:\n",
    "\n",
    "- **ColwiseParallel**: Splits output features across TP ranks (used for q, k, v projections and MLP up projections)\n",
    "- **RowwiseParallel**: Splits input features across TP ranks (used for output projections and MLP down projections)\n",
    "\n",
    "The following code sets up the 2D device mesh and applies tensor parallelism:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributed._composable.fsdp import MixedPrecisionPolicy, fully_shard\n",
    "from torch.distributed.device_mesh import init_device_mesh\n",
    "from torch.distributed.tensor.parallel import (\n",
    "    ColwiseParallel,\n",
    "    RowwiseParallel,\n",
    "    parallelize_module,\n",
    ")\n",
    "from transformers import AutoConfig, AutoModelForCausalLM\n",
    "\n",
    "import ray.train.torch\n",
    "\n",
    "\n",
    "def setup_model_with_tp(\n",
    "    model_name: str,\n",
    "    tp_size: int,\n",
    "    dp_size: int,\n",
    "    world_rank: int,\n",
    "    world_size: int,\n",
    "    device: torch.device,\n",
    "    seed: int,\n",
    "):\n",
    "    \"\"\"\n",
    "    Set up the model with tensor parallelism (DTensor) and data parallelism (FSDP2).\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (model, tp_mesh, dp_mesh, tp_rank, dp_rank)\n",
    "    \"\"\"\n",
    "    # Validate configuration\n",
    "    if tp_size <= 0 or dp_size <= 0:\n",
    "        raise ValueError(\n",
    "            f\"tp_size and dp_size must be positive, got tp_size={tp_size}, dp_size={dp_size}.\"\n",
    "        )\n",
    "    if dp_size * tp_size != world_size:\n",
    "        raise ValueError(\n",
    "            f\"dp_size ({dp_size}) * tp_size ({tp_size}) must equal \"\n",
    "            f\"world_size ({world_size})\"\n",
    "        )\n",
    "\n",
    "    # Calculate TP and DP rank\n",
    "    tp_rank = world_rank % tp_size\n",
    "    dp_rank = world_rank // tp_size\n",
    "\n",
    "    # Load model config and validate TP compatibility\n",
    "    hf_config = AutoConfig.from_pretrained(model_name, trust_remote_code=True)\n",
    "    num_kv_heads = getattr(hf_config, \"num_key_value_heads\", None)\n",
    "    if num_kv_heads is None:\n",
    "        num_kv_heads = getattr(hf_config, \"num_attention_heads\", None)\n",
    "    if num_kv_heads is None:\n",
    "        raise ValueError(\n",
    "            \"Model config must define `num_key_value_heads` or `num_attention_heads` \"\n",
    "            \"to validate tensor parallel compatibility.\"\n",
    "        )\n",
    "    if num_kv_heads % tp_size != 0:\n",
    "        raise ValueError(f\"TP size {tp_size} must divide attention heads count {num_kv_heads}\")\n",
    "\n",
    "    if world_rank == 0:\n",
    "        logger.info(f\"Setting up 2D mesh: dp_size={dp_size}, tp_size={tp_size}\")\n",
    "\n",
    "    # [1] Create 2D device mesh: (dp, tp)\n",
    "    device_mesh = init_device_mesh(\n",
    "        \"cuda\", (dp_size, tp_size), mesh_dim_names=(\"dp\", \"tp\")\n",
    "    )\n",
    "    tp_mesh = device_mesh[\"tp\"]\n",
    "    dp_mesh = device_mesh[\"dp\"]\n",
    "\n",
    "    if world_rank == 0:\n",
    "        logger.info(f\"Device mesh created: {device_mesh}\")\n",
    "\n",
    "    # [2] Load pretrained model weights on the target device\n",
    "    dtype = get_mixed_precision_dtype()\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        trust_remote_code=True,\n",
    "        torch_dtype=dtype,\n",
    "    ).to(device)\n",
    "\n",
    "    # Get transformer layers (Qwen/Llama-style models)\n",
    "    layers = model.model.layers\n",
    "\n",
    "    # [3] Define TP mapping for transformer layers\n",
    "    # ColwiseParallel: splits output features across TP ranks\n",
    "    # RowwiseParallel: splits input features across TP ranks\n",
    "    tp_mapping = {\n",
    "        # Attention projections\n",
    "        \"self_attn.q_proj\": ColwiseParallel(),\n",
    "        \"self_attn.k_proj\": ColwiseParallel(),\n",
    "        \"self_attn.v_proj\": ColwiseParallel(),\n",
    "        \"self_attn.o_proj\": RowwiseParallel(),\n",
    "        # MLP projections\n",
    "        \"mlp.gate_proj\": ColwiseParallel(),\n",
    "        \"mlp.up_proj\": ColwiseParallel(),\n",
    "        \"mlp.down_proj\": RowwiseParallel(),\n",
    "    }\n",
    "\n",
    "    if world_rank == 0:\n",
    "        logger.info(f\"Applying tensor parallelism to {len(layers)} layers\")\n",
    "\n",
    "    # [4] Apply DTensor TP to transformer layers\n",
    "    for layer in layers:\n",
    "        parallelize_module(layer, tp_mesh, tp_mapping)\n",
    "\n",
    "    # [5] Apply FSDP2 for data parallelism (if dp_size > 1)\n",
    "    mp_policy = MixedPrecisionPolicy(param_dtype=dtype, reduce_dtype=dtype)\n",
    "\n",
    "    if dp_size > 1:\n",
    "        if world_rank == 0:\n",
    "            logger.info(\"Applying FSDP2 to transformer layers\")\n",
    "\n",
    "        for layer in layers:\n",
    "            fully_shard(layer, mesh=dp_mesh, mp_policy=mp_policy)\n",
    "\n",
    "        # Apply to the whole model\n",
    "        fully_shard(model, mesh=dp_mesh, mp_policy=mp_policy)\n",
    "    else:\n",
    "        if world_rank == 0:\n",
    "            logger.info(\"dp_size=1, skipping FSDP sharding (TP only)\")\n",
    "\n",
    "    if world_rank == 0:\n",
    "        num_params = sum(p.numel() for p in model.parameters())\n",
    "        logger.info(f\"Model initialized with {num_params:,} parameters\")\n",
    "\n",
    "    return model, tp_mesh, dp_mesh, tp_rank, dp_rank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Checkpointing\n",
    "\n",
    "With tensor parallelism and FSDP2, each worker holds a shard of the model and optimizer state. For this setup, use PyTorch [Distributed Checkpoint (DCP)](https://pytorch.org/docs/stable/distributed.checkpoint.html) APIs so sharded state dicts are saved and restored correctly.\n",
    "\n",
    "In this example, `dcp.save` and `dcp.load` handle the distributed state dicts, and rank 0 additionally writes `metadata.json` for `epoch`/`step`. Calling `ray.train.report(..., checkpoint=...)` on all workers lets Ray package the distributed checkpoint into one logical artifact.\n",
    "\n",
    "On restore, each worker loads its local shard through DCP and resumes from `metadata.json`. This avoids materializing a full model copy on each worker and aligns naturally with DTensor/FSDP sharding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.distributed.checkpoint as dcp\n",
    "from torch.distributed.checkpoint.state_dict import get_state_dict, set_state_dict\n",
    "from ray.train import Checkpoint\n",
    "\n",
    "\n",
    "def save_checkpoint(\n",
    "    model: torch.nn.Module,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    world_rank: int,\n",
    "    epoch: int,\n",
    "    step: int,\n",
    "    avg_loss: float,\n",
    ") -> None:\n",
    "    \"\"\"Save checkpoint and report to Ray Train.\"\"\"\n",
    "    with tempfile.TemporaryDirectory() as checkpoint_dir:\n",
    "        model_state_dict, optimizer_state_dict = get_state_dict(model, optimizer)\n",
    "        state_dict = {\n",
    "            \"model\": model_state_dict,\n",
    "            \"optimizer\": optimizer_state_dict,\n",
    "        }\n",
    "\n",
    "        # Save distributed model/optimizer shards.\n",
    "        dcp.save(state_dict=state_dict, checkpoint_id=checkpoint_dir)\n",
    "\n",
    "        # Save metadata (from rank 0 only)\n",
    "        if world_rank == 0:\n",
    "            with open(os.path.join(checkpoint_dir, \"metadata.json\"), \"w\") as f:\n",
    "                json.dump({\"epoch\": epoch, \"step\": step}, f)\n",
    "\n",
    "        # All workers must call report() with their checkpoint\n",
    "        checkpoint = Checkpoint.from_directory(checkpoint_dir)\n",
    "        ray.train.report({\"loss\": avg_loss, \"epoch\": epoch}, checkpoint=checkpoint)\n",
    "\n",
    "def load_checkpoint(\n",
    "    model: torch.nn.Module,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    ") -> int:\n",
    "    \"\"\"Load model/optimizer shards from the latest Ray Train checkpoint.\"\"\"\n",
    "    checkpoint = ray.train.get_checkpoint()\n",
    "    if checkpoint is None:\n",
    "        return 0\n",
    "\n",
    "    with checkpoint.as_directory() as checkpoint_dir:\n",
    "        metadata_path = os.path.join(checkpoint_dir, \"metadata.json\")\n",
    "\n",
    "        model_state_dict, optimizer_state_dict = get_state_dict(model, optimizer)\n",
    "        state_dict = {\n",
    "            \"model\": model_state_dict,\n",
    "            \"optimizer\": optimizer_state_dict,\n",
    "        }\n",
    "        dcp.load(state_dict=state_dict, checkpoint_id=checkpoint_dir)\n",
    "        set_state_dict(\n",
    "            model,\n",
    "            optimizer,\n",
    "            model_state_dict=state_dict[\"model\"],\n",
    "            optim_state_dict=state_dict[\"optimizer\"],\n",
    "        )\n",
    "\n",
    "        start_epoch = 0\n",
    "        if os.path.exists(metadata_path):\n",
    "            with open(metadata_path, \"r\") as f:\n",
    "                metadata = json.load(f)\n",
    "            start_epoch = metadata.get(\"epoch\", -1) + 1\n",
    "\n",
    "    return start_epoch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training loop\n",
    "\n",
    "The main training function brings together all components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_func(config):\n",
    "    \"\"\"\n",
    "    Main training loop executed by each Ray Train worker.\n",
    "\n",
    "    This function:\n",
    "    1. Sets up the 2D device mesh for TP + DP\n",
    "    2. Creates and shards the model with DTensor (TP) and FSDP2 (DP)\n",
    "    3. Runs the training loop with checkpointing\n",
    "    \"\"\"\n",
    "    # Get Ray Train context\n",
    "    world_rank = ray.train.get_context().get_world_rank()\n",
    "    world_size = ray.train.get_context().get_world_size()\n",
    "    device = ray.train.torch.get_device()\n",
    "\n",
    "    tp_size = config[\"tp_size\"]\n",
    "    dp_size = config[\"dp_size\"]\n",
    "\n",
    "    if world_rank == 0:\n",
    "        logger.info(f\"Worker started: world_rank={world_rank}, world_size={world_size}\")\n",
    "\n",
    "    # Set up model with 2D parallelism\n",
    "    model, _, _, _, dp_rank = setup_model_with_tp(\n",
    "        model_name=config[\"model_name\"],\n",
    "        tp_size=tp_size,\n",
    "        dp_size=dp_size,\n",
    "        world_rank=world_rank,\n",
    "        world_size=world_size,\n",
    "        device=device,\n",
    "        seed=config.get(\"seed\", 42),\n",
    "    )\n",
    "\n",
    "    # Create optimizer\n",
    "    # Note: Use foreach=False because DTensor doesn't support fused optimizer ops\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=config.get(\"learning_rate\", 1e-5),\n",
    "        weight_decay=config.get(\"weight_decay\", 0.01),\n",
    "        foreach=False,\n",
    "    )\n",
    "    start_epoch = load_checkpoint(model, optimizer)\n",
    "\n",
    "    dtype = get_mixed_precision_dtype()\n",
    "\n",
    "    if world_rank == 0:\n",
    "        if dp_size > 1:\n",
    "            logger.info(f\"2D parallelism: {dp_size} DP x {tp_size} TP\")\n",
    "        logger.info(f\"torch.autocast enabled with dtype={dtype}\")\n",
    "\n",
    "    # Create dataloader with TP-aware sharding\n",
    "    dataloader = create_dataloader(\n",
    "        model_name=config[\"model_name\"],\n",
    "        dataset_name=config[\"dataset_name\"],\n",
    "        seq_length=config[\"seq_length\"],\n",
    "        batch_size_per_gpu=config[\"batch_size_per_gpu\"],\n",
    "        dp_rank=dp_rank,\n",
    "        dp_size=dp_size,\n",
    "        seed=config.get(\"seed\", 42),\n",
    "        dataset_percentage=config.get(\"dataset_percentage\", 10.0),\n",
    "    )\n",
    "\n",
    "    steps_per_epoch = len(dataloader)\n",
    "    if world_rank == 0:\n",
    "        logger.info(f\"Dataloader created: {steps_per_epoch} steps per epoch\")\n",
    "    if steps_per_epoch == 0:\n",
    "        raise ValueError(\n",
    "            \"Dataloader is empty. Increase dataset_percentage or reduce batch_size_per_gpu.\"\n",
    "        )\n",
    "    log_interval = config.get(\"log_interval\", 10)\n",
    "\n",
    "    # Training loop\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(start_epoch, config[\"num_epochs\"]):\n",
    "        dataloader.sampler.set_epoch(epoch)\n",
    "\n",
    "        running_loss = 0.0\n",
    "        num_batches = 0\n",
    "\n",
    "        last_step = -1\n",
    "        for step, batch in enumerate(dataloader):\n",
    "            last_step = step\n",
    "            # Move batch to device\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "            # Zero gradients\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "            # Forward pass with autocast\n",
    "            with torch.autocast(device_type=\"cuda\", dtype=dtype):\n",
    "                outputs = model(\n",
    "                    input_ids=batch[\"input_ids\"],\n",
    "                    attention_mask=batch[\"attention_mask\"],\n",
    "                    labels=batch[\"labels\"],\n",
    "                    use_cache=False,\n",
    "                )\n",
    "                loss = outputs.loss\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "\n",
    "            # Optimizer step\n",
    "            optimizer.step()\n",
    "\n",
    "            # Track loss\n",
    "            loss_value = loss.item()\n",
    "            running_loss += loss_value\n",
    "            num_batches += 1\n",
    "\n",
    "            # Log progress\n",
    "            if (\n",
    "                world_rank == 0\n",
    "                and log_interval is not None\n",
    "                and log_interval > 0\n",
    "                and step % log_interval == 0\n",
    "            ):\n",
    "                logger.info(\n",
    "                    f\"Epoch: {epoch} Step: {step + 1}/{steps_per_epoch} Loss: {loss_value:.4f}\"\n",
    "                )\n",
    "\n",
    "            # Debug mode: stop early for testing\n",
    "            if config.get(\"debug_steps\", 0) > 0 and step + 1 >= config[\"debug_steps\"]:\n",
    "                if world_rank == 0:\n",
    "                    logger.info(f\"Debug steps finished. Stopping epoch {epoch}.\")\n",
    "                break\n",
    "\n",
    "        if num_batches == 0:\n",
    "            if world_rank == 0:\n",
    "                logger.warning(\n",
    "                    f\"Epoch {epoch} processed zero batches. Skipping checkpoint and continuing.\"\n",
    "                )\n",
    "            continue\n",
    "\n",
    "        # Calculate average loss for epoch\n",
    "        avg_loss = running_loss / num_batches\n",
    "\n",
    "        # Save checkpoint at end of epoch\n",
    "        save_checkpoint(model, optimizer, world_rank, epoch, last_step, avg_loss)\n",
    "\n",
    "        if world_rank == 0:\n",
    "            logger.info(f\"Epoch {epoch} completed. Average loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Launch the distributed training job\n",
    "\n",
    "Configure and launch the training job using Ray Train's TorchTrainer. This example uses:\n",
    "- 4 workers (GPUs)\n",
    "- 2-way tensor parallelism\n",
    "- 2-way data parallelism\n",
    "- A small model (Qwen2.5-0.5B) for demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.train import RunConfig, ScalingConfig\n",
    "from ray.train.torch import TorchTrainer\n",
    "\n",
    "# Parallelism configuration\n",
    "tp_size = 2  # Tensor parallel degree\n",
    "dp_size = 2  # Data parallel degree\n",
    "num_workers = tp_size * dp_size  # Total workers must equal tp_size * dp_size\n",
    "\n",
    "# Configure distributed training resources\n",
    "scaling_config = ScalingConfig(\n",
    "    num_workers=num_workers,\n",
    "    use_gpu=True,\n",
    ")\n",
    "\n",
    "# Training configuration\n",
    "train_loop_config = {\n",
    "    # Model and data\n",
    "    \"model_name\": \"Qwen/Qwen2.5-0.5B\",  # Small model for demo\n",
    "    \"dataset_name\": \"wikitext\",\n",
    "    \"dataset_percentage\": 5.0,  # Use 5% of dataset for faster demo\n",
    "    # Parallelism\n",
    "    \"tp_size\": tp_size,\n",
    "    \"dp_size\": dp_size,\n",
    "    # Training hyperparameters\n",
    "    \"batch_size_per_gpu\": 1,  # Global batch size = batch_size_per_gpu * dp_size\n",
    "    \"seq_length\": 512,\n",
    "    \"num_epochs\": 1,\n",
    "    \"learning_rate\": 1e-5,\n",
    "    \"weight_decay\": 0.01,\n",
    "    # Logging and debug\n",
    "    \"log_interval\": 5,\n",
    "    \"debug_steps\": 20,  # Stop after 20 steps for demo (set to 0 for full training)\n",
    "    \"seed\": 42,\n",
    "}\n",
    "\n",
    "# Create experiment name\n",
    "experiment_name = f\"tp_dtensor_{uuid.uuid4().hex[:8]}\"\n",
    "storage_path = \"/tmp/ray_train_tp_dtensor\"  # Use persistent/shared storage in production\n",
    "\n",
    "# Configure run settings\n",
    "run_config = RunConfig(\n",
    "    name=experiment_name,\n",
    "    storage_path=storage_path,\n",
    ")\n",
    "\n",
    "# Initialize and launch the trainer\n",
    "trainer = TorchTrainer(\n",
    "    train_loop_per_worker=train_func,\n",
    "    scaling_config=scaling_config,\n",
    "    train_loop_config=train_loop_config,\n",
    "    run_config=run_config,\n",
    ")\n",
    "\n",
    "print(f\"Starting tensor parallel training with {tp_size}-way TP and {dp_size}-way DP...\")\n",
    "result = trainer.fit()\n",
    "print(\"Training completed successfully!\")\n",
    "print(f\"Final metrics: {result.metrics}\")\n",
    "\n",
    "# Reuse the same RunConfig(name, storage_path). train_func() will receive the\n",
    "# latest checkpoint from ray.train.get_checkpoint() and continue automatically.\n",
    "RUN_RESUME_DEMO = False\n",
    "if RUN_RESUME_DEMO:\n",
    "    resume_train_loop_config = dict(train_loop_config)\n",
    "    resume_train_loop_config[\"num_epochs\"] = 2  # Continue to epoch 1\n",
    "    resume_trainer = TorchTrainer(\n",
    "        train_loop_per_worker=train_func,\n",
    "        scaling_config=scaling_config,\n",
    "        train_loop_config=resume_train_loop_config,\n",
    "        run_config=RunConfig(name=experiment_name, storage_path=storage_path),\n",
    "    )\n",
    "    resume_result = resume_trainer.fit()\n",
    "    print(f\"Resumed metrics: {resume_result.metrics}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling to larger models\n",
    "\n",
    "To train larger models like Qwen2-7B or Llama-3-8B, adjust the configuration. For example, on 8 GPUs you can use 4-way TP and 2-way DP:\n",
    "\n",
    "```python\n",
    "train_loop_config = {\n",
    "    \"model_name\": \"Qwen/Qwen2-7B\",\n",
    "    \"tp_size\": 4,\n",
    "    \"dp_size\": 2,\n",
    "    \"batch_size_per_gpu\": 1,\n",
    "    \"seq_length\": 2048,\n",
    "    ...\n",
    "}\n",
    "\n",
    "scaling_config = ScalingConfig(\n",
    "    num_workers=8,\n",
    "    use_gpu=True,\n",
    ")\n",
    "```\n",
    "\n",
    "**Tips for scaling:**\n",
    "- Increase `tp_size` to fit larger models (TP shards model weights)\n",
    "- Increase `dp_size` to improve throughput (DP processes more data in parallel)\n",
    "- Ensure `tp_size` divides the model's attention heads count\n",
    "- Use NVLink-connected GPUs for efficient TP communication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this tutorial, you learned:\n",
    "\n",
    "- How to combine tensor parallelism and data parallelism for 2D parallelism\n",
    "- How to set up a 2D device mesh with PyTorch\n",
    "- How to apply tensor parallelism with the DTensor API to transformer layers\n",
    "- The importance of TP-aware data loading for correct gradient computation\n",
    "- How to combine the DTensor API with FSDP2 for 2D parallelism\n",
    "- How to save distributed checkpoints with Ray Train\n",
    "\n",
    "For production training of large models, consider:\n",
    "- Using larger `tp_size` for models that don't fit on a single GPU\n",
    "- Enabling gradient checkpointing for memory efficiency\n",
    "- Using PyTorch's memory profiler to optimize memory usage"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "orphan": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
