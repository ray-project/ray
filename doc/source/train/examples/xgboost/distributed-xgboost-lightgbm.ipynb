{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "612b6a05",
   "metadata": {},
   "source": [
    "# Get Started with Distributed Training using XGBoost\n",
    "\n",
    "Ray Train has built-in support for XGBoost.\n",
    "\n",
    "## Quickstart\n",
    "\n",
    "### XGBoost Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb401ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "from ray.train import ScalingConfig\n",
    "from ray.train.xgboost import XGBoostTrainer\n",
    "\n",
    "# Load data.\n",
    "dataset = ray.data.read_csv(\"s3://anonymous@air-example-data/breast_cancer.csv\")\n",
    "\n",
    "# Split data into train and validation.\n",
    "train_dataset, valid_dataset = dataset.train_test_split(test_size=0.3)\n",
    "\n",
    "trainer = XGBoostTrainer(\n",
    "    scaling_config=ScalingConfig(\n",
    "        # Number of workers to use for data parallelism.\n",
    "        num_workers=2,\n",
    "        # Whether to use GPU acceleration. Set to True to schedule GPU workers.\n",
    "        use_gpu=False,\n",
    "    ),\n",
    "    label_column=\"target\",\n",
    "    num_boost_round=20,\n",
    "    params={\n",
    "        # XGBoost specific params (see the `xgboost.train` API reference)\n",
    "        \"objective\": \"binary:logistic\",\n",
    "        # uncomment this and set `use_gpu=True` to use GPU for training\n",
    "        # \"tree_method\": \"gpu_hist\",\n",
    "        \"eval_metric\": [\"logloss\", \"error\"],\n",
    "    },\n",
    "    datasets={\"train\": train_dataset, \"valid\": valid_dataset},\n",
    "    # If running in a multi-node cluster, this is where you\n",
    "    # should configure the run's persistent storage that is accessible\n",
    "    # across all worker nodes.\n",
    "    # run_config=ray.train.RunConfig(storage_path=\"s3://...\"),\n",
    ")\n",
    "result = trainer.fit()\n",
    "print(result.metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b74cfa",
   "metadata": {},
   "source": [
    "Trainer constructors pass Ray-specific parameters.\n",
    "\n",
    "## Save and load XGBoost and LightGBM checkpoints\n",
    "\n",
    "When you train a new tree on every boosting round, you can save a checkpoint to snapshot the training progress so far.\n",
    "[`XGBoostTrainer`](https://docs.ray.io/en/latest/train/api/doc/ray.train.xgboost.XGBoostTrainer.html#ray.train.xgboost.XGBoostTrainer) and [`LightGBMTrainer`](https://docs.ray.io/en/latest/train/api/doc/ray.train.lightgbm.LightGBMTrainer.html#ray.train.lightgbm.LightGBMTrainer) both implement checkpointing out of the box. These checkpoints can be loaded into memory\n",
    "using static methods [`XGBoostTrainer.get_model`](https://docs.ray.io/en/latest/train/api/doc/ray.train.xgboost.XGBoostTrainer.get_model.html#ray.train.xgboost.XGBoostTrainer.get_model) and [`LightGBMTrainer.get_model`](https://docs.ray.io/en/latest/train/api/doc/ray.train.lightgbm.LightGBMTrainer.get_model.html#ray.train.lightgbm.LightGBMTrainer.get_model).\n",
    "\n",
    "The only required change is to configure [`CheckpointConfig`](https://docs.ray.io/en/latest/train/api/doc/ray.train.CheckpointConfig.html#ray.train.CheckpointConfig) to set the checkpointing frequency. For example, the following configuration\n",
    "saves a checkpoint on every boosting round and only keeps the latest checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb522c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.train import RunConfig, CheckpointConfig\n",
    "\n",
    "run_config = RunConfig(\n",
    "    checkpoint_config=CheckpointConfig(\n",
    "        # Checkpoint every iteration.\n",
    "        checkpoint_frequency=1,\n",
    "        # Only keep the latest checkpoint and delete the others.\n",
    "        num_to_keep=1,\n",
    "    )\n",
    ")\n",
    "\n",
    "# from ray.train.xgboost import XGBoostTrainer\n",
    "# trainer = XGBoostTrainer(..., run_config=run_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b18221b",
   "metadata": {},
   "source": [
    ":::{tip} Once you enable checkpointing, you can follow [this guide](https://docs.ray.io/en/latest/train/user-guides/fault-tolerance.html#train-fault-tolerance) to enable fault tolerance. :::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164d1bde",
   "metadata": {},
   "source": [
    "## Basic training with tree-based models in Train\n",
    "\n",
    "Just as in the original [`xgboost.train()`](https://xgboost.readthedocs.io/en/stable/parameter.html) and [`lightgbm.train()`](https://lightgbm.readthedocs.io/en/latest/Parameters.html) functions, the\n",
    "training parameters are passed as the `params` dictionary.\n",
    "\n",
    "### XGBoost Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3424c4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "from ray.train import ScalingConfig\n",
    "from ray.train.xgboost import XGBoostTrainer\n",
    "\n",
    "# Load data.\n",
    "dataset = ray.data.read_csv(\"s3://anonymous@air-example-data/breast_cancer.csv\")\n",
    "\n",
    "# Split data into train and validation.\n",
    "train_dataset, valid_dataset = dataset.train_test_split(test_size=0.3)\n",
    "\n",
    "trainer = XGBoostTrainer(\n",
    "    scaling_config=ScalingConfig(\n",
    "        num_workers=2,\n",
    "        use_gpu=False,\n",
    "    ),\n",
    "    label_column=\"target\",\n",
    "    num_boost_round=20,\n",
    "    params={\n",
    "        \"objective\": \"binary:logistic\",\n",
    "        \"eval_metric\": [\"logloss\", \"error\"],\n",
    "    },\n",
    "    datasets={\"train\": train_dataset, \"valid\": valid_dataset},\n",
    ")\n",
    "result = trainer.fit()\n",
    "print(result.metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f64200b",
   "metadata": {},
   "source": [
    "## How to scale out training?\n",
    "\n",
    "The benefit of using Ray Train is that you can seamlessly scale up your training by\n",
    "adjusting the [`ScalingConfig`](https://docs.ray.io/en/latest/train/api/doc/ray.train.ScalingConfig.html#ray.train.ScalingConfig).\n",
    "\n",
    ":::{note}\n",
    "Ray Train doesnâ€™t modify or otherwise alter the working of the underlying XGBoost or LightGBM distributed training algorithms. Ray only provides orchestration, data ingest and fault tolerance. For more information on GBDT distributed training, refer to [XGBoost documentation](https://xgboost.readthedocs.io/en/stable/) and [LightGBM documentation](https://lightgbm.readthedocs.io/en/latest/).\n",
    ":::\n",
    "\n",
    "### Multi-node CPU Example\n",
    "\n",
    "Setup: 4 nodes with 8 CPUs each.\n",
    "\n",
    "Use-case: To utilize all resources in multi-node training.\n",
    "\n",
    "```python\n",
    "scaling_config = ScalingConfig(\n",
    "    num_workers=4,\n",
    "    resources_per_worker={\"CPU\": 8},\n",
    ")\n",
    "```\n",
    "\n",
    "### Single-node multi-GPU Example\n",
    "\n",
    "Setup: 1 node with 8 CPUs and 4 GPUs.\n",
    "\n",
    "Use-case: If you have a single node with multiple GPUs, you need to use\n",
    "distributed training to leverage all GPUs.\n",
    "\n",
    "```python\n",
    "scaling_config = ScalingConfig(\n",
    "    num_workers=4,\n",
    "    use_gpu=True,\n",
    ")\n",
    "```\n",
    "\n",
    "### Multi-node multi-GPU Example\n",
    "\n",
    "Setup: 4 nodes with 8 CPUs and 4 GPUs each.\n",
    "\n",
    "Use-case: If you have multiple nodes with multiple GPUs, you need to\n",
    "schedule one worker per GPU.\n",
    "\n",
    "```python\n",
    "scaling_config = ScalingConfig(\n",
    "    num_workers=16,\n",
    "    use_gpu=True,\n",
    ")\n",
    "```\n",
    "\n",
    "::: {warning}\n",
    "Specifying a *shared storage location* (such as cloud storage or NFS) is *optional* for single-node clusters, but it is **required for multi-node clusters**. Using a local path will [raise an error](https://docs.ray.io/en/latest/train/user-guides/persistent-storage.html#multinode-local-storage-warning) during checkpointing for multi-node clusters.\n",
    "\n",
    "```python\n",
    "trainer = XGBoostTrainer(\n",
    "    ..., run_config=ray.train.RunConfig(storage_path=\"s3://...\")\n",
    ")\n",
    "```\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f00d4d7",
   "metadata": {},
   "source": [
    "## How to preprocess data for training?\n",
    "\n",
    "Particularly for tabular data, Ray Data comes with out-of-the-box preprocessors that implement common feature preprocessing operations.\n",
    "You can use this with Ray Train Trainers by applying them on the dataset before passing the dataset into a Trainer. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26377530",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "from ray.data.preprocessors import MinMaxScaler\n",
    "from ray.train.xgboost import XGBoostTrainer\n",
    "from ray.train import ScalingConfig\n",
    "\n",
    "train_dataset = ray.data.from_items([{\"x\": x, \"y\": 2 * x} for x in range(0, 32, 3)])\n",
    "valid_dataset = ray.data.from_items([{\"x\": x, \"y\": 2 * x} for x in range(1, 32, 3)])\n",
    "\n",
    "preprocessor = MinMaxScaler([\"x\"])\n",
    "preprocessor.fit(train_dataset)\n",
    "train_dataset = preprocessor.transform(train_dataset)\n",
    "valid_dataset = preprocessor.transform(valid_dataset)\n",
    "\n",
    "trainer = XGBoostTrainer(\n",
    "    label_column=\"y\",\n",
    "    params={\"objective\": \"reg:squarederror\"},\n",
    "    scaling_config=ScalingConfig(num_workers=2),\n",
    "    datasets={\"train\": train_dataset, \"valid\": valid_dataset},\n",
    ")\n",
    "result = trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9acf41b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.train import CheckpointConfig\n",
    "\n",
    "checkpoint_config = CheckpointConfig(\n",
    "    checkpoint_frequency=1,\n",
    "    num_to_keep=1,\n",
    ")\n",
    "\n",
    "trainer = XGBoostTrainer(\n",
    "    scaling_config=ScalingConfig(num_workers=2),\n",
    "    label_column=\"target\",\n",
    "    num_boost_round=20,\n",
    "    params={\"objective\": \"binary:logistic\"},\n",
    "    datasets={\"train\": train_dataset, \"valid\": valid_dataset},\n",
    "    checkpoint_config=checkpoint_config,\n",
    ")\n",
    "result = trainer.fit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
