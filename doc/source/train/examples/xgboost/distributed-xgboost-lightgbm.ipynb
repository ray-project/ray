{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "612b6a05",
   "metadata": {},
   "source": [
    "# Get Started with Distributed Training using XGBoost\n",
    "\n",
    "Ray Train has built-in support for XGBoost.\n",
    "\n",
    "## Quickstart\n",
    "\n",
    "### XGBoost Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb401ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "from ray.train import ScalingConfig\n",
    "from ray.train.xgboost import XGBoostTrainer\n",
    "\n",
    "# Load data.\n",
    "dataset = ray.data.read_csv(\"s3://anonymous@air-example-data/breast_cancer.csv\")\n",
    "\n",
    "# Split data into train and validation.\n",
    "train_dataset, valid_dataset = dataset.train_test_split(test_size=0.3)\n",
    "\n",
    "trainer = XGBoostTrainer(\n",
    "    scaling_config=ScalingConfig(\n",
    "        # Number of workers to use for data parallelism.\n",
    "        num_workers=2,\n",
    "        # Whether to use GPU acceleration. Set to True to schedule GPU workers.\n",
    "        use_gpu=False,\n",
    "    ),\n",
    "    label_column=\"target\",\n",
    "    num_boost_round=20,\n",
    "    params={\n",
    "        # XGBoost specific params (see the `xgboost.train` API reference)\n",
    "        \"objective\": \"binary:logistic\",\n",
    "        # uncomment this and set `use_gpu=True` to use GPU for training\n",
    "        # \"tree_method\": \"gpu_hist\",\n",
    "        \"eval_metric\": [\"logloss\", \"error\"],\n",
    "    },\n",
    "    datasets={\"train\": train_dataset, \"valid\": valid_dataset},\n",
    "    # If running in a multi-node cluster, this is where you\n",
    "    # should configure the run's persistent storage that is accessible\n",
    "    # across all worker nodes.\n",
    "    # run_config=ray.train.RunConfig(storage_path=\"s3://...\"),\n",
    ")\n",
    "result = trainer.fit()\n",
    "print(result.metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f64200b",
   "metadata": {},
   "source": [
    "## How to scale out training?\n",
    "\n",
    "The benefit of using Ray Train is that you can seamlessly scale up your training by\n",
    "adjusting the `ScalingConfig`.\n",
    "\n",
    "### Multi-node CPU Example\n",
    "\n",
    "Setup: 4 nodes with 8 CPUs each.\n",
    "\n",
    "Use-case: To utilize all resources in multi-node training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51eb59ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaling_config = ScalingConfig(\n",
    "    num_workers=4,\n",
    "    resources_per_worker={\"CPU\": 8},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df402d29",
   "metadata": {},
   "source": [
    "### Single-node multi-GPU Example\n",
    "\n",
    "Setup: 1 node with 8 CPUs and 4 GPUs.\n",
    "\n",
    "Use-case: If you have a single node with multiple GPUs, you need to use\n",
    "distributed training to leverage all GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12025e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaling_config = ScalingConfig(\n",
    "    num_workers=4,\n",
    "    use_gpu=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52b3bd8",
   "metadata": {},
   "source": [
    "### Multi-node multi-GPU Example\n",
    "\n",
    "Setup: 4 nodes with 8 CPUs and 4 GPUs each.\n",
    "\n",
    "Use-case: If you have multiple nodes with multiple GPUs, you need to\n",
    "schedule one worker per GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5080f0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaling_config = ScalingConfig(\n",
    "    num_workers=16,\n",
    "    use_gpu=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f00d4d7",
   "metadata": {},
   "source": [
    "## How to preprocess data for training?\n",
    "\n",
    "Particularly for tabular data, Ray Data comes with out-of-the-box preprocessors that implement common feature preprocessing operations.\n",
    "You can use this with Ray Train Trainers by applying them on the dataset before passing the dataset into a Trainer. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26377530",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "from ray.data.preprocessors import MinMaxScaler\n",
    "from ray.train.xgboost import XGBoostTrainer\n",
    "from ray.train import ScalingConfig\n",
    "\n",
    "train_dataset = ray.data.from_items([{\"x\": x, \"y\": 2 * x} for x in range(0, 32, 3)])\n",
    "valid_dataset = ray.data.from_items([{\"x\": x, \"y\": 2 * x} for x in range(1, 32, 3)])\n",
    "\n",
    "preprocessor = MinMaxScaler([\"x\"])\n",
    "preprocessor.fit(train_dataset)\n",
    "train_dataset = preprocessor.transform(train_dataset)\n",
    "valid_dataset = preprocessor.transform(valid_dataset)\n",
    "\n",
    "trainer = XGBoostTrainer(\n",
    "    label_column=\"y\",\n",
    "    params={\"objective\": \"reg:squarederror\"},\n",
    "    scaling_config=ScalingConfig(num_workers=2),\n",
    "    datasets={\"train\": train_dataset, \"valid\": valid_dataset},\n",
    ")\n",
    "result = trainer.fit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
