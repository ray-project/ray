{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(lightning-trainer-exp-tracking)=\n",
    "\n",
    "# Using W&B, CometML, and MLFlow Logger in LightningTrainer\n",
    "\n",
    "W&B, CometML, and MLFlow are all popular tools in the field of machine learning for managing, visualizing, and tracking experiments. \n",
    "\n",
    "PyTorch Lightning provides built-in support for logging metrics to W&B and CometML during training, allowing users to easily track their model performance over time. MLFlow can also be used to track experiments and store artifacts such as trained models and hyperparameters. With AIR LightningTrainer, you can still leverage the built-in supports with the PyTorch Lightning's `logger`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define your model and dataloader\n",
    "\n",
    "No need for any code change here. We simply create a dummy model with dummy datasets for demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# create dummy data\n",
    "X = torch.randn(128, 3)  # 128 samples, 3 features\n",
    "y = torch.randint(0, 2, (128,))  # 128 binary labels\n",
    "\n",
    "# create a TensorDataset to wrap the data\n",
    "dataset = TensorDataset(X, y)\n",
    "\n",
    "# create a DataLoader to iterate over the dataset\n",
    "batch_size = 8\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a dummy model\n",
    "class DummyModel(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer = torch.nn.Linear(3, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layer(x)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = F.binary_cross_entropy_with_logits(y_hat.flatten(), y.float())\n",
    "\n",
    "        # The metrics below will be reported to Loggers\n",
    "        self.log('train_loss', loss)\n",
    "        self.log_dict({\"metric_1\": batch_idx * 10, \"metric_2\": batch_idx * 100})\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define your loggers\n",
    "\n",
    "You don't need to make any changes to your original code for offline logging. If you would like to upload your log online to W&B or CometML, make sure to set up the API key environment variables on the head node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CometLogger will be initialized in online mode\n"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning.loggers.wandb import WandbLogger\n",
    "from pytorch_lightning.loggers.comet import CometLogger\n",
    "from pytorch_lightning.loggers.mlflow import MLFlowLogger\n",
    "from pytorch_lightning.loggers.tensorboard import TensorBoardLogger\n",
    "from pytorch_lightning.utilities.rank_zero import rank_zero_only\n",
    "import wandb\n",
    "\n",
    "# A callback to login wandb in each worker\n",
    "class WandbLoginCallback(pl.Callback):\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def setup(self, trainer, pl_module, stage) -> None:\n",
    "        wandb.login(key=self.key)\n",
    "\n",
    "\n",
    "def create_loggers(name, project_name, save_dir=\"./logs\", offline=False):\n",
    "    # Wandb\n",
    "    wandb_api_key = os.environ.get(\"WANDB_API_KEY\", None)\n",
    "\n",
    "    # Avoid creating a new experiment run in the driver.\n",
    "    rank_zero_only.rank = None\n",
    "    wandb_logger = WandbLogger(\n",
    "        name=name, project=project_name, save_dir=f\"{save_dir}/wandb\", offline=offline\n",
    "    )\n",
    "    callbacks = [] if offline else [WandbLoginCallback(key=wandb_api_key)]\n",
    "\n",
    "    # CometML\n",
    "    comet_api_key = os.environ.get(\"COMET_API_KEY\", None)\n",
    "    comet_logger = CometLogger(\n",
    "        api_key=comet_api_key,\n",
    "        experiment_name=name,\n",
    "        project_name=project_name,\n",
    "        save_dir=f\"{save_dir}/comet\",\n",
    "        offline=offline,\n",
    "    )\n",
    "\n",
    "    # MLFlow\n",
    "    mlflow_logger = MLFlowLogger(\n",
    "        run_name=name, experiment_name=project_name, save_dir=f\"{save_dir}/mlflow\"\n",
    "    )\n",
    "\n",
    "    # Tensorboard\n",
    "    tensorboard_logger = TensorBoardLogger(\n",
    "        name=name, save_dir=f\"{save_dir}/tensorboard\"\n",
    "    )\n",
    "\n",
    "    return [wandb_logger, comet_logger, mlflow_logger, tensorboard_logger], callbacks\n",
    "\n",
    "\n",
    "loggers, callbacks = create_loggers(\n",
    "    name=\"demo-run\", project_name=\"demo-project\", offline=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# # FOR SMOKE TESTS\n",
    "loggers, callbacks = create_loggers(\n",
    "    name=\"demo-run\", project_name=\"demo-project\", offline=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model and check out your logs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.train.lightning import LightningConfigBuilder, LightningTrainer\n",
    "\n",
    "builder = LightningConfigBuilder()\n",
    "builder.module(cls=DummyModel)\n",
    "builder.trainer(max_epochs=5, accelerator=\"cpu\", logger=loggers, callbacks=callbacks, log_every_n_steps=1)\n",
    "builder.fit_params(train_dataloaders=dataloader)\n",
    "lightning_config = builder.build()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2023-04-27 19:14:06</td></tr>\n",
       "<tr><td>Running for: </td><td>00:01:02.45        </td></tr>\n",
       "<tr><td>Memory:      </td><td>6.2/62.0 GiB       </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using FIFO scheduling algorithm.<br>Logical resource usage: 0/16 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:T4)\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status    </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  train_loss</th><th style=\"text-align: right;\">  metric_1</th><th style=\"text-align: right;\">  metric_2</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>LightningTrainer_35598_00000</td><td>TERMINATED</td><td>10.0.74.175:78857</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         34.5741</td><td style=\"text-align: right;\">    0.792705</td><td style=\"text-align: right;\">        30</td><td style=\"text-align: right;\">       300</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-27 19:13:04,277\tINFO data_parallel_trainer.py:357 -- GPUs are detected in your Ray cluster, but GPU training is not enabled for this trainer. To enable GPU training, make sure to set `use_gpu` to True in your scaling config.\n",
      "(pid=78857) /mnt/cluster_storage/pypi/lib/python3.9/site-packages/neptune/common/warnings.py:62: NeptuneDeprecationWarning: You're importing the Neptune client library via the deprecated `neptune.new` module, which will be removed in a future release. Import directly from `neptune` instead.\n",
      "(pid=78857)   warnings.warn(\n",
      "(TrainTrainable pid=78857) 2023-04-27 19:13:13,689\tINFO data_parallel_trainer.py:357 -- GPUs are detected in your Ray cluster, but GPU training is not enabled for this trainer. To enable GPU training, make sure to set `use_gpu` to True in your scaling config.\n",
      "(LightningTrainer pid=78857) 2023-04-27 19:13:13,695\tINFO data_parallel_trainer.py:357 -- GPUs are detected in your Ray cluster, but GPU training is not enabled for this trainer. To enable GPU training, make sure to set `use_gpu` to True in your scaling config.\n",
      "(LightningTrainer pid=78857) 2023-04-27 19:13:16,455\tINFO backend_executor.py:128 -- Starting distributed worker processes: ['79231 (10.0.74.175)', '79232 (10.0.74.175)', '79233 (10.0.74.175)', '79234 (10.0.74.175)']\n",
      "(RayTrainWorker pid=79231) 2023-04-27 19:13:17,511\tINFO config.py:86 -- Setting up process group for: env:// [rank=0, world_size=4]\n",
      "(RayTrainWorker pid=79232)   warnings.warn(\n",
      "(RayTrainWorker pid=79232)   warnings.warn(\n",
      "(RayTrainWorker pid=79231) /mnt/cluster_storage/pypi/lib/python3.9/site-packages/neptune/common/warnings.py:62: NeptuneDeprecationWarning: You're importing the Neptune client library via the deprecated `neptune.new` module, which will be removed in a future release. Import directly from `neptune` instead.\n",
      "(RayTrainWorker pid=79231)   warnings.warn(\n",
      "(RayTrainWorker pid=79231) GPU available: False, used: False\n",
      "(RayTrainWorker pid=79231) TPU available: False, using: 0 TPU cores\n",
      "(RayTrainWorker pid=79231) IPU available: False, using: 0 IPUs\n",
      "(RayTrainWorker pid=79231) HPU available: False, using: 0 HPUs\n",
      "(RayTrainWorker pid=79231) wandb: Currently logged in as: yunxuan. Use `wandb login --relogin` to force relogin\n",
      "(RayTrainWorker pid=79231) wandb: Appending key for api.wandb.ai to your netrc file: /home/ray/.netrc\n",
      "(RayTrainWorker pid=79231) Experiment with name demo-project not found. Creating it.\n",
      "(RayTrainWorker pid=79231) Missing logger folder: ./logs/tensorboard/demo-run\n",
      "(RayTrainWorker pid=79231) \n",
      "(RayTrainWorker pid=79231)   | Name  | Type   | Params\n",
      "(RayTrainWorker pid=79231) ---------------------------------\n",
      "(RayTrainWorker pid=79231) 0 | layer | Linear | 4     \n",
      "(RayTrainWorker pid=79231) ---------------------------------\n",
      "(RayTrainWorker pid=79231) 4         Trainable params\n",
      "(RayTrainWorker pid=79231) 0         Non-trainable params\n",
      "(RayTrainWorker pid=79231) 4         Total params\n",
      "(RayTrainWorker pid=79231) 0.000     Total estimated model params size (MB)\n",
      "(RayTrainWorker pid=79231) /home/ray/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "(RayTrainWorker pid=79231)   rank_zero_warn(\n",
      "(RayTrainWorker pid=79231) /home/ray/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:229: UserWarning: You called `self.log('metric_1', ...)` in your `training_step` but the value needs to be floating point. Converting it to torch.float32.\n",
      "(RayTrainWorker pid=79231)   warning_cache.warn(\n",
      "(RayTrainWorker pid=79231) /home/ray/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:229: UserWarning: You called `self.log('metric_2', ...)` in your `training_step` but the value needs to be floating point. Converting it to torch.float32.\n",
      "(RayTrainWorker pid=79231)   warning_cache.warn(\n",
      "(RayTrainWorker pid=79231) [W reducer.cpp:1298] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "(RayTrainWorker pid=79231) wandb: WARNING Path ./logs/wandb/wandb/ wasn't writable, using system temp directory.\n",
      "(RayTrainWorker pid=79231) wandb: WARNING Path ./logs/wandb/wandb/ wasn't writable, using system temp directory\n",
      "(RayTrainWorker pid=79231) wandb: Tracking run with wandb version 0.15.0\n",
      "(RayTrainWorker pid=79231) wandb: Run data is saved locally in /tmp/wandb/run-20230427_191327-mwc335jd\n",
      "(RayTrainWorker pid=79231) wandb: Run `wandb offline` to turn off syncing.\n",
      "(RayTrainWorker pid=79231) wandb: Syncing run demo-run\n",
      "(RayTrainWorker pid=79231) wandb: ⭐️ View project at https://wandb.ai/yunxuan/demo-project\n",
      "(RayTrainWorker pid=79231) wandb: 🚀 View run at https://wandb.ai/yunxuan/demo-project/runs/mwc335jd\n",
      "(RayTrainWorker pid=79231) COMET WARNING: Comet has disabled auto-logging functionality as it has been imported after the following ML modules: xgboost, tensorboard, sklearn, mlflow, torch. Metrics and hyperparameters can still be logged using comet_ml.log_metrics() and comet_ml.log_parameters()\n",
      "(RayTrainWorker pid=79234) /mnt/cluster_storage/pypi/lib/python3.9/site-packages/neptune/common/warnings.py:62: NeptuneDeprecationWarning: You're importing the Neptune client library via the deprecated `neptune.new` module, which will be removed in a future release. Import directly from `neptune` instead. [repeated 2x across cluster]\n",
      "(RayTrainWorker pid=79234)   warnings.warn( [repeated 2x across cluster]\n",
      "(RayTrainWorker pid=79234) wandb: Currently logged in as: yunxuan. Use `wandb login --relogin` to force relogin [repeated 3x across cluster]\n",
      "(RayTrainWorker pid=79234) wandb: Appending key for api.wandb.ai to your netrc file: /home/ray/.netrc [repeated 3x across cluster]\n",
      "(RayTrainWorker pid=79234) Missing logger folder: ./logs/tensorboard/demo-run [repeated 3x across cluster]\n",
      "(RayTrainWorker pid=79234) [W reducer.cpp:1298] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator()) [repeated 3x across cluster]\n",
      "(RayTrainWorker pid=79231) COMET INFO: Couldn't find a Git repository in '/tmp/ray_results/ptl-exp-tracking/LightningTrainer_35598_00000_0_2023-04-27_19-13-04/rank_0' nor in any parent directory. You can override where Comet is looking for a Git Patch by setting the configuration `COMET_GIT_DIRECTORY`\n",
      "(RayTrainWorker pid=79231) COMET INFO: Experiment is live on comet.ml https://www.comet.com/woshiyyya/demo-project/a98ac061d68d438f8ae605cd4e6ea357\n",
      "(RayTrainWorker pid=79231) \n",
      "(RayTrainWorker pid=79231) 2023-04-27 19:13:45.064501: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "(RayTrainWorker pid=79231) To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "(RayTrainWorker pid=79231) 2023-04-27 19:13:45.238997: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "(RayTrainWorker pid=79231) 2023-04-27 19:13:46.190069: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "(RayTrainWorker pid=79231) 2023-04-27 19:13:46.190157: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "(RayTrainWorker pid=79231) 2023-04-27 19:13:46.190162: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"trialProgress\">\n",
       "  <h3>Trial Progress</h3>\n",
       "  <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>_report_on     </th><th>date               </th><th>done  </th><th style=\"text-align: right;\">  epoch</th><th style=\"text-align: right;\">  experiment_tag</th><th>hostname      </th><th style=\"text-align: right;\">  iterations_since_restore</th><th style=\"text-align: right;\">  metric_1</th><th style=\"text-align: right;\">  metric_2</th><th>node_ip    </th><th style=\"text-align: right;\">  pid</th><th>should_checkpoint  </th><th style=\"text-align: right;\">  step</th><th style=\"text-align: right;\">  time_since_restore</th><th style=\"text-align: right;\">  time_this_iter_s</th><th style=\"text-align: right;\">  time_total_s</th><th style=\"text-align: right;\">  timestamp</th><th style=\"text-align: right;\">  train_loss</th><th style=\"text-align: right;\">  training_iteration</th><th style=\"text-align: right;\">   trial_id</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>LightningTrainer_35598_00000</td><td>train_epoch_end</td><td>2023-04-27_19-13-48</td><td>True  </td><td style=\"text-align: right;\">      4</td><td style=\"text-align: right;\">               0</td><td>ip-10-0-74-175</td><td style=\"text-align: right;\">                         5</td><td style=\"text-align: right;\">        30</td><td style=\"text-align: right;\">       300</td><td>10.0.74.175</td><td style=\"text-align: right;\">78857</td><td>True               </td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">             34.5741</td><td style=\"text-align: right;\">          0.210718</td><td style=\"text-align: right;\">       34.5741</td><td style=\"text-align: right;\"> 1682648028</td><td style=\"text-align: right;\">    0.792705</td><td style=\"text-align: right;\">                   5</td><td style=\"text-align: right;\">35598_00000</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".trialProgress {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".trialProgress h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".trialProgress td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=79231) COMET INFO: ---------------------------\n",
      "(RayTrainWorker pid=79231) COMET INFO: Comet.ml Experiment Summary\n",
      "(RayTrainWorker pid=79231) COMET INFO: ---------------------------\n",
      "(RayTrainWorker pid=79231) COMET INFO:   Data:\n",
      "(RayTrainWorker pid=79231) COMET INFO:     display_summary_level : 1\n",
      "(RayTrainWorker pid=79231) COMET INFO:     url                   : https://www.comet.com/woshiyyya/demo-project/a98ac061d68d438f8ae605cd4e6ea357\n",
      "(RayTrainWorker pid=79231) COMET INFO:   Metrics [count] (min, max):\n",
      "(RayTrainWorker pid=79231) COMET INFO:     metric_1 [20]   : (0.0, 30.0)\n",
      "(RayTrainWorker pid=79231) COMET INFO:     metric_2 [20]   : (0.0, 300.0)\n",
      "(RayTrainWorker pid=79231) COMET INFO:     train_loss [20] : (0.6525752544403076, 0.8741906881332397)\n",
      "(RayTrainWorker pid=79231) COMET INFO:   Others:\n",
      "(RayTrainWorker pid=79231) COMET INFO:     Name : demo-run\n",
      "(RayTrainWorker pid=79231) COMET INFO:   Uploads:\n",
      "(RayTrainWorker pid=79231) COMET INFO:     conda-environment-definition : 1\n",
      "(RayTrainWorker pid=79231) COMET INFO:     conda-info                   : 1\n",
      "(RayTrainWorker pid=79231) COMET INFO:     conda-specification          : 1\n",
      "(RayTrainWorker pid=79231) COMET INFO:     environment details          : 1\n",
      "(RayTrainWorker pid=79231) COMET INFO:     filename                     : 1\n",
      "(RayTrainWorker pid=79231) COMET INFO:     installed packages           : 1\n",
      "(RayTrainWorker pid=79231) COMET INFO:     os packages                  : 1\n",
      "(RayTrainWorker pid=79231) COMET INFO:     source_code                  : 2 (30.69 KB)\n",
      "(RayTrainWorker pid=79231) COMET INFO: ---------------------------\n",
      "(RayTrainWorker pid=79231) COMET WARNING: Comet has disabled auto-logging functionality as it has been imported after the following ML modules: xgboost, tensorboard, sklearn, mlflow, torch. Metrics and hyperparameters can still be logged using comet_ml.log_metrics() and comet_ml.log_parameters()\n",
      "(RayTrainWorker pid=79231) COMET INFO: Uploading metrics, params, and assets to Comet before program termination (may take several seconds)\n",
      "(RayTrainWorker pid=79231) COMET INFO: The Python SDK has 3600 seconds to finish before aborting...\n",
      "2023-04-27 19:14:06,725\tINFO tune.py:1010 -- Total run time: 62.46 seconds (62.44 seconds for the tuning loop).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Result(\n",
       "  metrics={'_report_on': 'train_epoch_end', 'train_loss': 0.7927054166793823, 'metric_1': 30.0, 'metric_2': 300.0, 'epoch': 4, 'step': 20, 'should_checkpoint': True, 'done': True, 'trial_id': '35598_00000', 'experiment_tag': '0'},\n",
       "  path='/tmp/ray_results/ptl-exp-tracking/LightningTrainer_35598_00000_0_2023-04-27_19-13-04',\n",
       "  checkpoint=LightningCheckpoint(local_path=/tmp/ray_results/ptl-exp-tracking/LightningTrainer_35598_00000_0_2023-04-27_19-13-04/checkpoint_000004)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ray.air.config import RunConfig, ScalingConfig, CheckpointConfig\n",
    "\n",
    "scaling_config = ScalingConfig(num_workers=4, use_gpu=False)\n",
    "\n",
    "run_config = RunConfig(\n",
    "    name=\"ptl-exp-tracking\",\n",
    "    storage_path=\"/tmp/ray_results\",\n",
    ")\n",
    "\n",
    "trainer = LightningTrainer(\n",
    "    lightning_config=lightning_config,\n",
    "    scaling_config=scaling_config,\n",
    "    run_config=run_config,\n",
    ")\n",
    "\n",
    "trainer.fit()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
