{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning a Pytorch Lightning Image Classifier\n",
    "\n",
    "This example introduces how to train a Pytorch Lightning Module using AIR {class}`LightningTrainer <ray.train.lightning.LightningTrainer>`. We will demonstrate how to train a basic neural network on the MNIST dataset with distributed data parallelism.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "SMOKE_TEST = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f45609f9670>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, random_split, Subset\n",
    "from torchmetrics import Accuracy\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import trainer\n",
    "from pytorch_lightning.core import datamodule\n",
    "from pytorch_lightning.loggers.csv_logs import CSVLogger\n",
    "\n",
    "seed = 420\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Dataset and Module\n",
    "\n",
    "The Pytorch Lightning Trainer takes either `torch.utils.data.DataLoader` or `pl.LightningDataModule` as data inputs. You can keep using them without any changes for the Ray AIR LightningTrainer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, batch_size=100):\n",
    "        super().__init__()\n",
    "        self.data_dir = os.getcwd()\n",
    "        self.batch_size = batch_size\n",
    "        self.transform = transforms.Compose(\n",
    "            [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n",
    "        )\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        # split data into train and val sets\n",
    "        mnist = MNIST(\n",
    "            self.data_dir, train=True, download=True, transform=self.transform\n",
    "        )\n",
    "        self.mnist_train, self.mnist_val = random_split(mnist, [55000, 5000])\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.mnist_train, batch_size=self.batch_size, num_workers=4)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.mnist_val, batch_size=self.batch_size, num_workers=4)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        self.mnist_test = MNIST(\n",
    "            self.data_dir, train=False, download=True, transform=self.transform\n",
    "        )\n",
    "        return DataLoader(self.mnist_test, batch_size=self.batch_size, num_workers=4)\n",
    "\n",
    "datamodule = MNISTDataModule(batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, define a simple multi-layer perception as the subclass of `pl.LightningModule`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTClassifier(pl.LightningModule):\n",
    "    def __init__(self, lr=1e-3):\n",
    "        super(MNISTClassifier, self).__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28 * 28, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 10),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.lr = lr\n",
    "        self.accuracy = Accuracy()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28 * 28)\n",
    "        x = self.linear_relu_stack(x)\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = torch.nn.functional.cross_entropy(y_hat, y)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, val_batch, batch_idx):\n",
    "        loss, acc = self._shared_eval(val_batch)\n",
    "        self.log(\"val_accuracy\", acc)\n",
    "        return {\"val_loss\": loss, \"val_accuracy\": acc}\n",
    "    \n",
    "    def test_step(self, test_batch, batch_idx):\n",
    "        loss, acc = self._shared_eval(test_batch)\n",
    "        self.log(\"test_accuracy\", acc)\n",
    "        return {\"test_loss\": loss, \"test_accuracy\": acc}\n",
    "    \n",
    "    def _shared_eval(self, batch):\n",
    "        x, y = batch\n",
    "        logits = self.forward(x)\n",
    "        loss = F.nll_loss(logits, y)\n",
    "        acc = self.accuracy(logits, y)\n",
    "        return loss, acc\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([x[\"val_loss\"] for x in outputs]).mean()\n",
    "        avg_acc = torch.stack([x[\"val_accuracy\"] for x in outputs]).mean()\n",
    "        self.log(\"val_loss\", avg_loss, sync_dist=True)\n",
    "        self.log(\"val_accuracy\", avg_acc, sync_dist=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You don't need to make any change to the definition of PyTorch Lightning model and datamodule."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Cofigurations for AIR LightningTrainer\n",
    "\n",
    "The {meth}`LightningConfigBuilder <ray.train.lightning.LightningConfigBuilder>` class stores all the parameters involved in training a PyTorch Lightning module. It takes the same parameter lists as those in PyTorch Lightning.\n",
    "\n",
    "- The `.module()` method takes a subclass of `pl.LightningModule` and its initialization parameters. `LightningTrainer` will instantiate a model instance internally in the workers' training loop.\n",
    "- The `.trainer()` method takes the initialization parameters of `pl.Trainer`. You can specify training configurations, loggers, and callbacks here.\n",
    "- The `.fit_params()` method stores all the parameters that will be passed into `pl.Trainer.fit()`, including train/val dataloaders, datamodules, and checkpoint paths.\n",
    "- The `.checkpointing()` method saves the configurations for a `ModelCheckpoint` callback. Note that the `LightningTrainer` reports the latest metrics to the AIR session when a new checkpoint is saved.\n",
    "- The `.build()` method generates a dictionary that contains all the configurations in the builder. This dictionary will be passed to `LightningTrainer` later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.air.config import RunConfig, ScalingConfig, CheckpointConfig\n",
    "from ray.train.lightning import LightningTrainer, LightningConfigBuilder, LightningCheckpoint\n",
    "\n",
    "lightning_config = (\n",
    "    LightningConfigBuilder()\n",
    "    .module(\n",
    "        MNISTClassifier, lr=1e-3\n",
    "    )\n",
    "    .trainer(max_epochs=10, accelerator=\"cpu\", log_every_n_steps=100, logger=CSVLogger(\"logs\"))\n",
    "    .fit_params(datamodule=datamodule)\n",
    "    .checkpointing(monitor=\"val_accuracy\", mode=\"max\", save_top_k=3)\n",
    "    .build()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaling_config = ScalingConfig(\n",
    "    num_workers=4, use_gpu=True, resources_per_worker={\"CPU\": 1, \"GPU\": 1}\n",
    ")\n",
    "\n",
    "run_config = RunConfig(\n",
    "    name=\"ptl-mnist-example\",\n",
    "    local_dir=\"/tmp/ray_results\",\n",
    "    checkpoint_config=CheckpointConfig(num_to_keep=3, checkpoint_score_attribute=\"val_accuracy\", checkpoint_score_order=\"max\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SMOKE_TEST:\n",
    "    scaling_config = ScalingConfig(\n",
    "        num_workers=4, use_gpu=False, resources_per_worker={\"CPU\": 1}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = LightningTrainer(\n",
    "    lightning_config=lightning_config,\n",
    "    scaling_config=scaling_config,\n",
    "    run_config=run_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now fit your trainer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(pid=233131) /home/ray/anaconda3/lib/python3.8/site-packages/xgboost/compat.py:31: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "(pid=233131)   from pandas import MultiIndex, Int64Index\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2023-03-23 16:23:21 (running for 00:00:07.95)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 5.0/16 CPUs, 0/0 GPUs\n",
      "Result logdir: /tmp/ray_results/ptl-mnist-example\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------+----------+--------------------+\n",
      "| Trial name                   | status   | loc                |\n",
      "|------------------------------+----------+--------------------|\n",
      "| LightningTrainer_aef1f_00000 | RUNNING  | 10.0.61.115:233131 |\n",
      "+------------------------------+----------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=233458) 2023-03-23 16:23:25,524\tINFO config.py:86 -- Setting up process group for: env:// [rank=0, world_size=4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2023-03-23 16:23:26 (running for 00:00:12.95)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 5.0/16 CPUs, 0/0 GPUs\n",
      "Result logdir: /tmp/ray_results/ptl-mnist-example\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------+----------+--------------------+\n",
      "| Trial name                   | status   | loc                |\n",
      "|------------------------------+----------+--------------------|\n",
      "| LightningTrainer_aef1f_00000 | RUNNING  | 10.0.61.115:233131 |\n",
      "+------------------------------+----------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=233460) /home/ray/anaconda3/lib/python3.8/site-packages/xgboost/compat.py:31: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "(RayTrainWorker pid=233460)   from pandas import MultiIndex, Int64Index\n",
      "(RayTrainWorker pid=233458) /home/ray/anaconda3/lib/python3.8/site-packages/xgboost/compat.py:31: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "(RayTrainWorker pid=233458)   from pandas import MultiIndex, Int64Index\n",
      "(RayTrainWorker pid=233461) /home/ray/anaconda3/lib/python3.8/site-packages/xgboost/compat.py:31: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "(RayTrainWorker pid=233461)   from pandas import MultiIndex, Int64Index\n",
      "(RayTrainWorker pid=233459) /home/ray/anaconda3/lib/python3.8/site-packages/xgboost/compat.py:31: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "(RayTrainWorker pid=233459)   from pandas import MultiIndex, Int64Index\n",
      "(RayTrainWorker pid=233458) GPU available: False, used: False\n",
      "(RayTrainWorker pid=233458) TPU available: False, using: 0 TPU cores\n",
      "(RayTrainWorker pid=233458) IPU available: False, using: 0 IPUs\n",
      "(RayTrainWorker pid=233458) HPU available: False, using: 0 HPUs\n",
      "(RayTrainWorker pid=233458) Missing logger folder: logs/lightning_logs\n",
      "(RayTrainWorker pid=233458) \n",
      "(RayTrainWorker pid=233458)   | Name              | Type       | Params\n",
      "(RayTrainWorker pid=233458) -------------------------------------------------\n",
      "(RayTrainWorker pid=233458) 0 | linear_relu_stack | Sequential | 101 K \n",
      "(RayTrainWorker pid=233458) 1 | accuracy          | Accuracy   | 0     \n",
      "(RayTrainWorker pid=233458) -------------------------------------------------\n",
      "(RayTrainWorker pid=233458) 101 K     Trainable params\n",
      "(RayTrainWorker pid=233458) 0         Non-trainable params\n",
      "(RayTrainWorker pid=233458) 101 K     Total params\n",
      "(RayTrainWorker pid=233458) 0.407     Total estimated model params size (MB)\n",
      "(RayTrainWorker pid=233459) Missing logger folder: logs/lightning_logs\n",
      "(RayTrainWorker pid=233461) Missing logger folder: logs/lightning_logs\n",
      "(RayTrainWorker pid=233460) Missing logger folder: logs/lightning_logs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2023-03-23 16:23:31 (running for 00:00:17.95)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 5.0/16 CPUs, 0/0 GPUs\n",
      "Result logdir: /tmp/ray_results/ptl-mnist-example\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------+----------+--------------------+\n",
      "| Trial name                   | status   | loc                |\n",
      "|------------------------------+----------+--------------------|\n",
      "| LightningTrainer_aef1f_00000 | RUNNING  | 10.0.61.115:233131 |\n",
      "+------------------------------+----------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=233461) [W reducer.cpp:1298] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "(RayTrainWorker pid=233458) [W reducer.cpp:1298] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "(RayTrainWorker pid=233459) [W reducer.cpp:1298] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "(RayTrainWorker pid=233460) [W reducer.cpp:1298] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for LightningTrainer_aef1f_00000:\n",
      "  _report_on: train_epoch_end\n",
      "  date: 2023-03-23_16-23-34\n",
      "  done: false\n",
      "  epoch: 0\n",
      "  hostname: ip-10-0-61-115\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 10.0.61.115\n",
      "  pid: 233131\n",
      "  should_checkpoint: true\n",
      "  step: 108\n",
      "  time_since_restore: 12.394395589828491\n",
      "  time_this_iter_s: 12.394395589828491\n",
      "  time_total_s: 12.394395589828491\n",
      "  timestamp: 1679613814\n",
      "  train_loss: 0.7441539168357849\n",
      "  training_iteration: 1\n",
      "  trial_id: aef1f_00000\n",
      "  val_accuracy: 0.7299904227256775\n",
      "  val_loss: -6.072018623352051\n",
      "  \n",
      "== Status ==\n",
      "Current time: 2023-03-23 16:23:38 (running for 00:00:25.00)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 5.0/16 CPUs, 0/0 GPUs\n",
      "Result logdir: /tmp/ray_results/ptl-mnist-example\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------+----------+--------------------+--------+------------------+--------------+----------------+------------+\n",
      "| Trial name                   | status   | loc                |   iter |   total time (s) |   train_loss |   val_accuracy |   val_loss |\n",
      "|------------------------------+----------+--------------------+--------+------------------+--------------+----------------+------------|\n",
      "| LightningTrainer_aef1f_00000 | RUNNING  | 10.0.61.115:233131 |      3 |          17.0451 |     0.590514 |        0.75637 |   -7.11417 |\n",
      "+------------------------------+----------+--------------------+--------+------------------+--------------+----------------+------------+\n",
      "\n",
      "\n",
      "Result for LightningTrainer_aef1f_00000:\n",
      "  _report_on: train_epoch_end\n",
      "  date: 2023-03-23_16-23-41\n",
      "  done: false\n",
      "  epoch: 3\n",
      "  hostname: ip-10-0-61-115\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 10.0.61.115\n",
      "  pid: 233131\n",
      "  should_checkpoint: true\n",
      "  step: 432\n",
      "  time_since_restore: 19.34447956085205\n",
      "  time_this_iter_s: 2.299403190612793\n",
      "  time_total_s: 19.34447956085205\n",
      "  timestamp: 1679613821\n",
      "  train_loss: 0.5387300252914429\n",
      "  training_iteration: 4\n",
      "  trial_id: aef1f_00000\n",
      "  val_accuracy: 0.7613121271133423\n",
      "  val_loss: -7.479836463928223\n",
      "  \n",
      "== Status ==\n",
      "Current time: 2023-03-23 16:23:45 (running for 00:00:32.00)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 5.0/16 CPUs, 0/0 GPUs\n",
      "Result logdir: /tmp/ray_results/ptl-mnist-example\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------+----------+--------------------+--------+------------------+--------------+----------------+------------+\n",
      "| Trial name                   | status   | loc                |   iter |   total time (s) |   train_loss |   val_accuracy |   val_loss |\n",
      "|------------------------------+----------+--------------------+--------+------------------+--------------+----------------+------------|\n",
      "| LightningTrainer_aef1f_00000 | RUNNING  | 10.0.61.115:233131 |      6 |          24.0445 |     0.641314 |       0.770101 |   -8.32205 |\n",
      "+------------------------------+----------+--------------------+--------+------------------+--------------+----------------+------------+\n",
      "\n",
      "\n",
      "Result for LightningTrainer_aef1f_00000:\n",
      "  _report_on: train_epoch_end\n",
      "  date: 2023-03-23_16-23-48\n",
      "  done: false\n",
      "  epoch: 6\n",
      "  hostname: ip-10-0-61-115\n",
      "  iterations_since_restore: 7\n",
      "  node_ip: 10.0.61.115\n",
      "  pid: 233131\n",
      "  should_checkpoint: true\n",
      "  step: 756\n",
      "  time_since_restore: 26.389495134353638\n",
      "  time_this_iter_s: 2.345015048980713\n",
      "  time_total_s: 26.389495134353638\n",
      "  timestamp: 1679613828\n",
      "  train_loss: 0.4778764247894287\n",
      "  training_iteration: 7\n",
      "  trial_id: aef1f_00000\n",
      "  val_accuracy: 0.7711973190307617\n",
      "  val_loss: -8.63290786743164\n",
      "  \n",
      "== Status ==\n",
      "Current time: 2023-03-23 16:23:52 (running for 00:00:39.03)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 5.0/16 CPUs, 0/0 GPUs\n",
      "Result logdir: /tmp/ray_results/ptl-mnist-example\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+------------------------------+----------+--------------------+--------+------------------+--------------+----------------+------------+\n",
      "| Trial name                   | status   | loc                |   iter |   total time (s) |   train_loss |   val_accuracy |   val_loss |\n",
      "|------------------------------+----------+--------------------+--------+------------------+--------------+----------------+------------|\n",
      "| LightningTrainer_aef1f_00000 | RUNNING  | 10.0.61.115:233131 |      9 |          31.0802 |     0.438486 |         0.7727 |   -9.05633 |\n",
      "+------------------------------+----------+--------------------+--------+------------------+--------------+----------------+------------+\n",
      "\n",
      "\n",
      "Result for LightningTrainer_aef1f_00000:\n",
      "  _report_on: train_epoch_end\n",
      "  date: 2023-03-23_16-23-55\n",
      "  done: false\n",
      "  epoch: 9\n",
      "  hostname: ip-10-0-61-115\n",
      "  iterations_since_restore: 10\n",
      "  node_ip: 10.0.61.115\n",
      "  pid: 233131\n",
      "  should_checkpoint: true\n",
      "  step: 1080\n",
      "  time_since_restore: 33.41391181945801\n",
      "  time_this_iter_s: 2.333677291870117\n",
      "  time_total_s: 33.41391181945801\n",
      "  timestamp: 1679613835\n",
      "  train_loss: 0.33654841780662537\n",
      "  training_iteration: 10\n",
      "  trial_id: aef1f_00000\n",
      "  val_accuracy: 0.7734813690185547\n",
      "  val_loss: -9.297370910644531\n",
      "  \n",
      "Trial LightningTrainer_aef1f_00000 completed.\n",
      "== Status ==\n",
      "Current time: 2023-03-23 16:23:57 (running for 00:00:43.71)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 0/16 CPUs, 0/0 GPUs\n",
      "Result logdir: /tmp/ray_results/ptl-mnist-example\n",
      "Number of trials: 1/1 (1 TERMINATED)\n",
      "+------------------------------+------------+--------------------+--------+------------------+--------------+----------------+------------+\n",
      "| Trial name                   | status     | loc                |   iter |   total time (s) |   train_loss |   val_accuracy |   val_loss |\n",
      "|------------------------------+------------+--------------------+--------+------------------+--------------+----------------+------------|\n",
      "| LightningTrainer_aef1f_00000 | TERMINATED | 10.0.61.115:233131 |     10 |          33.4139 |     0.336548 |       0.773481 |   -9.29737 |\n",
      "+------------------------------+------------+--------------------+--------+------------------+--------------+----------------+------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-23 16:23:57,592\tINFO tune.py:817 -- Total run time: 43.72 seconds (43.71 seconds for the tuning loop).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy:  0.7734813690185547\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Result(\n",
       "  metrics={'_report_on': 'train_epoch_end', 'train_loss': 0.33654841780662537, 'val_accuracy': 0.7734813690185547, 'val_loss': -9.297370910644531, 'epoch': 9, 'step': 1080, 'should_checkpoint': True, 'done': True, 'trial_id': 'aef1f_00000', 'experiment_tag': '0'},\n",
       "  log_dir=PosixPath('/tmp/ray_results/ptl-mnist-example/LightningTrainer_aef1f_00000_0_2023-03-23_16-23-13'),\n",
       "  checkpoint=LightningCheckpoint(local_path=/tmp/ray_results/ptl-mnist-example/LightningTrainer_aef1f_00000_0_2023-03-23_16-23-13/checkpoint_000009)\n",
       ")"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = trainer.fit()\n",
    "print(\"Validation Accuracy: \", result.metrics[\"val_accuracy\"])\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test your network on the test data\n",
    "\n",
    "Next, we use PyTorch Lightning's native interface to evaluate the best model: To run the test loop using the ``pl.LightningModule.test_step()`` in your user-defined code, simply pass the loaded model to ``pl.Trainer.test()``. \n",
    "\n",
    "For faster inference on large datasets, you can try to use AIR's {class}`BatchPredictor <ray.train.batch_prediction.BatchPredictor>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "checkpoint: LightningCheckpoint = result.checkpoint\n",
    "best_model: pl.LightningModule = checkpoint.get_model(MNISTClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ray/anaconda3/lib/python3.8/site-packages/pytorch_lightning/loops/utilities.py:92: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "903767de122d4087a1b339aa5b59eeb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       test_accuracy       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.7709000110626221     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m      test_accuracy      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.7709000110626221    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = pl.Trainer()\n",
    "test_dataloader = datamodule.test_dataloader()\n",
    "result = trainer.test(best_model, dataloaders=test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use `LightningPredictor` for inference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.7709\n"
     ]
    }
   ],
   "source": [
    "from ray.train.lightning import LightningPredictor\n",
    "predictor = LightningPredictor.from_checkpoint(checkpoint, MNISTClassifier, use_gpu=False)\n",
    "\n",
    "def accuracy(logits, labels):\n",
    "    preds = np.argmax(logits, axis=1)\n",
    "    correct_preds = np.sum(preds == labels)\n",
    "    return correct_preds\n",
    "\n",
    "corrects = total = 0\n",
    "for batch in test_dataloader:\n",
    "    inputs, labels = batch\n",
    "    inputs, labels = inputs.numpy(), labels.numpy()\n",
    "    logits = predictor.predict(inputs)[\"predictions\"]\n",
    "    total += labels.size\n",
    "    corrects += accuracy(logits, labels)\n",
    "    \n",
    "print(\"Accuracy: \", corrects / total)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "a8c1140d108077f4faeb76b2438f85e4ed675f93d004359552883616a1acd54c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
