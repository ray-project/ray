{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(lightning_mnist_example)=\n",
    "\n",
    "# Train a Pytorch Lightning Image Classifier\n",
    "\n",
    "This example introduces how to train a Pytorch Lightning Module using AIR {class}`LightningTrainer <ray.train.lightning.LightningTrainer>`. We will demonstrate how to train a basic neural network on the MNIST dataset with distributed data parallelism.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"torchmetrics>=0.9\" \"pytorch_lightning>=1.6\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from filelock import FileLock\n",
    "from torch.utils.data import DataLoader, random_split, Subset\n",
    "from torchmetrics import Accuracy\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import trainer\n",
    "from pytorch_lightning.loggers.csv_logs import CSVLogger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Dataset and Module\n",
    "\n",
    "The Pytorch Lightning Trainer takes either `torch.utils.data.DataLoader` or `pl.LightningDataModule` as data inputs. You can keep using them without any changes for the Ray AIR LightningTrainer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, batch_size=100):\n",
    "        super().__init__()\n",
    "        self.data_dir = os.getcwd()\n",
    "        self.batch_size = batch_size\n",
    "        self.transform = transforms.Compose(\n",
    "            [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n",
    "        )\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        with FileLock(f\"{self.data_dir}.lock\"):\n",
    "            mnist = MNIST(\n",
    "                self.data_dir, train=True, download=True, transform=self.transform\n",
    "            )\n",
    "\n",
    "            # split data into train and val sets\n",
    "            self.mnist_train, self.mnist_val = random_split(mnist, [55000, 5000])\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.mnist_train, batch_size=self.batch_size, num_workers=4)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.mnist_val, batch_size=self.batch_size, num_workers=4)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        with FileLock(f\"{self.data_dir}.lock\"):\n",
    "            self.mnist_test = MNIST(\n",
    "                self.data_dir, train=False, download=True, transform=self.transform\n",
    "            )\n",
    "        return DataLoader(self.mnist_test, batch_size=self.batch_size, num_workers=4)\n",
    "\n",
    "\n",
    "datamodule = MNISTDataModule(batch_size=128)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, define a simple multi-layer perception as the subclass of `pl.LightningModule`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTClassifier(pl.LightningModule):\n",
    "    def __init__(self, lr=1e-3, feature_dim=128):\n",
    "        torch.manual_seed(421)\n",
    "        super(MNISTClassifier, self).__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28 * 28, feature_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(feature_dim, 10),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.lr = lr\n",
    "        self.accuracy = Accuracy(task=\"multiclass\", num_classes=10)\n",
    "        self.eval_loss = []\n",
    "        self.eval_accuracy = []\n",
    "        self.test_accuracy = []\n",
    "        pl.seed_everything(888)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28 * 28)\n",
    "        x = self.linear_relu_stack(x)\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = torch.nn.functional.cross_entropy(y_hat, y)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, val_batch, batch_idx):\n",
    "        loss, acc = self._shared_eval(val_batch)\n",
    "        self.log(\"val_accuracy\", acc)\n",
    "        self.eval_loss.append(loss)\n",
    "        self.eval_accuracy.append(acc)\n",
    "        return {\"val_loss\": loss, \"val_accuracy\": acc}\n",
    "\n",
    "    def test_step(self, test_batch, batch_idx):\n",
    "        loss, acc = self._shared_eval(test_batch)\n",
    "        self.test_accuracy.append(acc)\n",
    "        self.log(\"test_accuracy\", acc, sync_dist=True, on_epoch=True)\n",
    "        return {\"test_loss\": loss, \"test_accuracy\": acc}\n",
    "\n",
    "    def _shared_eval(self, batch):\n",
    "        x, y = batch\n",
    "        logits = self.forward(x)\n",
    "        loss = F.nll_loss(logits, y)\n",
    "        acc = self.accuracy(logits, y)\n",
    "        return loss, acc\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        avg_loss = torch.stack(self.eval_loss).mean()\n",
    "        avg_acc = torch.stack(self.eval_accuracy).mean()\n",
    "        self.log(\"val_loss\", avg_loss, sync_dist=True)\n",
    "        self.log(\"val_accuracy\", avg_acc, sync_dist=True)\n",
    "        self.eval_loss.clear()\n",
    "        self.eval_accuracy.clear()\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You don't need to make any change to the definition of PyTorch Lightning model and datamodule."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(lightning-config-builder-intro)=\n",
    "\n",
    "## Define the Cofigurations for AIR LightningTrainer\n",
    "\n",
    "The {meth}`LightningConfigBuilder <ray.train.lightning.LightningConfigBuilder>` class stores all the parameters involved in training a PyTorch Lightning module. It takes the same parameter lists as those in PyTorch Lightning.\n",
    "\n",
    "- The `.module()` method takes a subclass of `pl.LightningModule` and its initialization parameters. `LightningTrainer` will instantiate a model instance internally in the workers' training loop.\n",
    "- The `.trainer()` method takes the initialization parameters of `pl.Trainer`. You can specify training configurations, loggers, and callbacks here.\n",
    "- The `.fit_params()` method stores all the parameters that will be passed into `pl.Trainer.fit()`, including train/val dataloaders, datamodules, and checkpoint paths.\n",
    "- The `.checkpointing()` method saves the configurations for a `RayModelCheckpoint` callback. This callback reports the latest metrics to the AIR session along with a newly saved checkpoint.\n",
    "- The `.build()` method generates a dictionary that contains all the configurations in the builder. This dictionary will be passed to `LightningTrainer` later.\n",
    "\n",
    "Next, let's go step-by-step to see how to convert your existing PyTorch Lightning training script to a LightningTrainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from ray.air.config import RunConfig, ScalingConfig, CheckpointConfig\n",
    "from ray.train.lightning import (\n",
    "    LightningTrainer,\n",
    "    LightningConfigBuilder,\n",
    "    LightningCheckpoint,\n",
    ")\n",
    "\n",
    "\n",
    "def build_lightning_config_from_existing_code(use_gpu):\n",
    "    # Create a config builder to encapsulate all required parameters.\n",
    "    # Note that model instantiation and fitting will occur later in the LightingTrainer,\n",
    "    # rather than in the config builder.\n",
    "    config_builder = LightningConfigBuilder()\n",
    "\n",
    "    # 1. define your model\n",
    "    # model = MNISTClassifier(lr=1e-3, feature_dim=128)\n",
    "    config_builder.module(cls=MNISTClassifier, lr=1e-3, feature_dim=128)\n",
    "\n",
    "    # 2. define a ModelCheckpoint callback\n",
    "    # checkpoint_callback = ModelCheckpoint(\n",
    "    #     monitor=\"val_accuracy\", mode=\"max\", save_top_k=3\n",
    "    # )\n",
    "    config_builder.checkpointing(monitor=\"val_accuracy\", mode=\"max\", save_top_k=3)\n",
    "\n",
    "    # 3. Define a Lightning trainer\n",
    "    # trainer = pl.Trainer(\n",
    "    #     max_epochs=10,\n",
    "    #     accelerator=\"cpu\",\n",
    "    #     strategy=\"ddp\",\n",
    "    #     log_every_n_steps=100,\n",
    "    #     logger=CSVLogger(\"logs\"),\n",
    "    #     callbacks=[checkpoint_callback],\n",
    "    # )\n",
    "    config_builder.trainer(\n",
    "        max_epochs=10,\n",
    "        accelerator=\"gpu\" if use_gpu else \"cpu\",\n",
    "        log_every_n_steps=100,\n",
    "        logger=CSVLogger(\"logs\"),\n",
    "    )\n",
    "    # You do not need to provide the checkpoint callback and strategy here,\n",
    "    # since LightningTrainer configures them automatically.\n",
    "    # You can also add any other callbacks into LightningConfigBuilder.trainer().\n",
    "\n",
    "    # 4. Parameters for model fitting\n",
    "    # trainer.fit(model, datamodule=datamodule)\n",
    "    config_builder.fit_params(datamodule=datamodule)\n",
    "\n",
    "    # Finally, compile all the configs into a dictionary for LightningTrainer\n",
    "    lightning_config = config_builder.build()\n",
    "    return lightning_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now put everything together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_gpu = True # Set it to False if you want to run without GPUs\n",
    "num_workers = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lightning_config = build_lightning_config_from_existing_code(use_gpu=use_gpu)\n",
    "\n",
    "scaling_config = ScalingConfig(num_workers=num_workers, use_gpu=use_gpu)\n",
    "\n",
    "run_config = RunConfig(\n",
    "    name=\"ptl-mnist-example\",\n",
    "    storage_path=\"/tmp/ray_results\",\n",
    "    checkpoint_config=CheckpointConfig(\n",
    "        num_to_keep=3,\n",
    "        checkpoint_score_attribute=\"val_accuracy\",\n",
    "        checkpoint_score_order=\"max\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "trainer = LightningTrainer(\n",
    "    lightning_config=lightning_config,\n",
    "    scaling_config=scaling_config,\n",
    "    run_config=run_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now fit your trainer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-13 16:05:12,869\tINFO worker.py:1452 -- Connecting to existing Ray cluster at address: 10.0.28.253:6379...\n",
      "2023-06-13 16:05:12,877\tINFO worker.py:1627 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32mhttps://console.anyscale-staging.com/api/v2/sessions/ses_15dlj65vax84ljl7ayeplubryd/services?redirect_to=dashboard \u001b[39m\u001b[22m\n",
      "2023-06-13 16:05:13,036\tINFO packaging.py:347 -- Pushing file package 'gcs://_ray_pkg_488e346d50f332edaa288fdaa22b2bdc.zip' (52.65MiB) to Ray cluster...\n",
      "2023-06-13 16:05:13,221\tINFO packaging.py:360 -- Successfully pushed file package 'gcs://_ray_pkg_488e346d50f332edaa288fdaa22b2bdc.zip'.\n",
      "2023-06-13 16:05:13,314\tINFO tune.py:226 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `Trainer(...)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2023-06-13 16:05:52</td></tr>\n",
       "<tr><td>Running for: </td><td>00:00:39.29        </td></tr>\n",
       "<tr><td>Memory:      </td><td>5.5/30.9 GiB       </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using FIFO scheduling algorithm.<br>Logical resource usage: 1.0/32 CPUs, 4.0/4 GPUs\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status    </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  train_loss</th><th style=\"text-align: right;\">  val_accuracy</th><th style=\"text-align: right;\">  val_loss</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>LightningTrainer_c0d28_00000</td><td>TERMINATED</td><td>10.0.28.253:16995</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         28.5133</td><td style=\"text-align: right;\">   0.0315991</td><td style=\"text-align: right;\">      0.970002</td><td style=\"text-align: right;\">  -12.3467</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=16995)\u001b[0m /home/ray/anaconda3/lib/python3.9/site-packages/xgboost/compat.py:31: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "\u001b[2m\u001b[36m(pid=16995)\u001b[0m   from pandas import MultiIndex, Int64Index\n",
      "\u001b[2m\u001b[36m(LightningTrainer pid=16995)\u001b[0m 2023-06-13 16:05:24,007\tINFO backend_executor.py:137 -- Starting distributed worker processes: ['17232 (10.0.28.253)', '6371 (10.0.1.80)', '7319 (10.0.58.90)', '6493 (10.0.26.229)']\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=17232)\u001b[0m 2023-06-13 16:05:24,966\tINFO config.py:86 -- Setting up process group for: env:// [rank=0, world_size=4]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=17232)\u001b[0m   from pandas import MultiIndex, Int64Index\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=17232)\u001b[0m   from pandas import MultiIndex, Int64Index\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=7319, ip=10.0.58.90)\u001b[0m /home/ray/anaconda3/lib/python3.9/site-packages/xgboost/compat.py:31: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=7319, ip=10.0.58.90)\u001b[0m   from pandas import MultiIndex, Int64Index\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=17232)\u001b[0m Global seed set to 888\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=17232)\u001b[0m GPU available: True, used: True\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=17232)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=17232)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=17232)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=6371, ip=10.0.1.80)\u001b[0m Missing logger folder: logs/lightning_logs\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=6371, ip=10.0.1.80)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=17232)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=17232)\u001b[0m   | Name              | Type       | Params\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=17232)\u001b[0m -------------------------------------------------\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=17232)\u001b[0m 0 | linear_relu_stack | Sequential | 101 K \n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=17232)\u001b[0m 1 | accuracy          | Accuracy   | 0     \n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=17232)\u001b[0m -------------------------------------------------\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=17232)\u001b[0m 101 K     Trainable params\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=17232)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=17232)\u001b[0m 101 K     Total params\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=17232)\u001b[0m 0.407     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: 0it [00:00, ?it/s]\u001b[0m \n",
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Sanity Checking DataLoader 0: 100%|██████████| 2/2 [00:00<00:00,  1.33it/s]\n",
      "Epoch 0:   0%|          | 0/118 [00:00<?, ?it/s]                           \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RayTrainWorker pid=6493, ip=10.0.26.229)\u001b[0m [W reducer.cpp:1298] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=6371, ip=10.0.1.80)\u001b[0m /home/ray/anaconda3/lib/python3.9/site-packages/xgboost/compat.py:31: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\u001b[32m [repeated 2x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=6371, ip=10.0.1.80)\u001b[0m   from pandas import MultiIndex, Int64Index\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   3%|▎         | 4/118 [00:00<00:07, 16.07it/s, loss=2.09, v_num=0]\n",
      "Epoch 0:   4%|▍         | 5/118 [00:00<00:05, 19.42it/s, loss=2.09, v_num=0]\n",
      "Epoch 0:  12%|█▏        | 14/118 [00:00<00:02, 39.49it/s, loss=1.55, v_num=0]\n",
      "Epoch 0:  12%|█▏        | 14/118 [00:00<00:02, 38.73it/s, loss=1.5, v_num=0] \n",
      "Epoch 0:  21%|██        | 25/118 [00:00<00:01, 53.89it/s, loss=0.933, v_num=0]\n",
      "Epoch 0:  30%|██▉       | 35/118 [00:00<00:01, 61.80it/s, loss=0.522, v_num=0]\n",
      "Epoch 0:  38%|███▊      | 45/118 [00:00<00:01, 67.21it/s, loss=0.425, v_num=0]\n",
      "Epoch 0:  45%|████▍     | 53/118 [00:00<00:00, 69.59it/s, loss=0.379, v_num=0]\n",
      "Epoch 0:  46%|████▌     | 54/118 [00:00<00:00, 69.65it/s, loss=0.373, v_num=0]\n",
      "Epoch 0:  54%|█████▍    | 64/118 [00:00<00:00, 73.24it/s, loss=0.364, v_num=0]\n",
      "Epoch 0:  62%|██████▏   | 73/118 [00:00<00:00, 74.68it/s, loss=0.341, v_num=0]\n",
      "Epoch 0:  63%|██████▎   | 74/118 [00:00<00:00, 75.21it/s, loss=0.341, v_num=0]\n",
      "Epoch 0:  70%|███████   | 83/118 [00:01<00:00, 76.62it/s, loss=0.335, v_num=0]\n",
      "Epoch 0:  80%|███████▉  | 94/118 [00:01<00:00, 79.16it/s, loss=0.297, v_num=0]\n",
      "Epoch 0:  90%|████████▉ | 106/118 [00:01<00:00, 82.26it/s, loss=0.281, v_num=0]\n",
      "Epoch 0:  92%|█████████▏| 108/118 [00:01<00:00, 83.04it/s, loss=0.284, v_num=0]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A2)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=17232)\u001b[0m \n",
      "Validation:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:  92%|█████████▏| 109/118 [00:01<00:00, 73.67it/s, loss=0.284, v_num=0]\n",
      "Epoch 0:  93%|█████████▎| 110/118 [00:01<00:00, 74.14it/s, loss=0.284, v_num=0]\n",
      "Epoch 0:  94%|█████████▍| 111/118 [00:01<00:00, 74.57it/s, loss=0.284, v_num=0]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=17232)\u001b[0m \n",
      "Epoch 0:  95%|█████████▍| 112/118 [00:01<00:00, 73.94it/s, loss=0.284, v_num=0]\n",
      "Epoch 0:  96%|█████████▌| 113/118 [00:01<00:00, 74.45it/s, loss=0.284, v_num=0]\n",
      "Epoch 0:  97%|█████████▋| 114/118 [00:01<00:00, 74.96it/s, loss=0.284, v_num=0]\n",
      "Epoch 0:  97%|█████████▋| 115/118 [00:01<00:00, 75.47it/s, loss=0.284, v_num=0]\n",
      "Epoch 0:  98%|█████████▊| 116/118 [00:01<00:00, 75.05it/s, loss=0.284, v_num=0]\n",
      "Epoch 0:  99%|█████████▉| 117/118 [00:01<00:00, 75.55it/s, loss=0.284, v_num=0]\n",
      "Epoch 0: 100%|██████████| 118/118 [00:01<00:00, 75.21it/s, loss=0.284, v_num=0]\n",
      "Epoch 0: 100%|██████████| 118/118 [00:01<00:00, 75.17it/s, loss=0.284, v_num=0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"trialProgress\">\n",
       "  <h3>Trial Progress</h3>\n",
       "  <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>_report_on     </th><th>date               </th><th>done  </th><th style=\"text-align: right;\">  epoch</th><th style=\"text-align: right;\">  experiment_tag</th><th>hostname      </th><th style=\"text-align: right;\">  iterations_since_restore</th><th>node_ip    </th><th style=\"text-align: right;\">  pid</th><th>should_checkpoint  </th><th style=\"text-align: right;\">  step</th><th style=\"text-align: right;\">  time_since_restore</th><th style=\"text-align: right;\">  time_this_iter_s</th><th style=\"text-align: right;\">  time_total_s</th><th style=\"text-align: right;\">  timestamp</th><th style=\"text-align: right;\">  train_loss</th><th style=\"text-align: right;\">  training_iteration</th><th>trial_id   </th><th style=\"text-align: right;\">  val_accuracy</th><th style=\"text-align: right;\">  val_loss</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>LightningTrainer_c0d28_00000</td><td>train_epoch_end</td><td>2023-06-13_16-05-50</td><td>True  </td><td style=\"text-align: right;\">      9</td><td style=\"text-align: right;\">               0</td><td>ip-10-0-28-253</td><td style=\"text-align: right;\">                        10</td><td>10.0.28.253</td><td style=\"text-align: right;\">16995</td><td>True               </td><td style=\"text-align: right;\">  1080</td><td style=\"text-align: right;\">             28.5133</td><td style=\"text-align: right;\">           1.73311</td><td style=\"text-align: right;\">       28.5133</td><td style=\"text-align: right;\"> 1686697550</td><td style=\"text-align: right;\">   0.0315991</td><td style=\"text-align: right;\">                  10</td><td>c0d28_00000</td><td style=\"text-align: right;\">      0.970002</td><td style=\"text-align: right;\">  -12.3467</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".trialProgress {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".trialProgress h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".trialProgress td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 0/118 [00:00<?, ?it/s, loss=0.284, v_num=0]          \n",
      "Epoch 1:   2%|▏         | 2/118 [00:00<00:15,  7.71it/s, loss=0.283, v_num=0]\n",
      "Epoch 1:  11%|█         | 13/118 [00:00<00:02, 35.75it/s, loss=0.268, v_num=0]\n",
      "Epoch 1:  20%|██        | 24/118 [00:00<00:01, 51.49it/s, loss=0.253, v_num=0]\n",
      "Epoch 1:  28%|██▊       | 33/118 [00:00<00:01, 57.86it/s, loss=0.252, v_num=0]\n",
      "Epoch 1:  36%|███▋      | 43/118 [00:00<00:01, 64.22it/s, loss=0.244, v_num=0]\n",
      "Epoch 1:  37%|███▋      | 44/118 [00:00<00:01, 64.96it/s, loss=0.244, v_num=0]\n",
      "Epoch 1:  37%|███▋      | 44/118 [00:00<00:01, 64.66it/s, loss=0.245, v_num=0]\n",
      "Epoch 1:  46%|████▌     | 54/118 [00:00<00:00, 69.28it/s, loss=0.241, v_num=0]\n",
      "Epoch 1:  55%|█████▌    | 65/118 [00:00<00:00, 73.79it/s, loss=0.245, v_num=0]\n",
      "Epoch 1:  64%|██████▎   | 75/118 [00:00<00:00, 75.85it/s, loss=0.22, v_num=0] \n",
      "Epoch 1:  64%|██████▎   | 75/118 [00:00<00:00, 75.83it/s, loss=0.222, v_num=0]\n",
      "Epoch 1:  72%|███████▏  | 85/118 [00:01<00:00, 78.43it/s, loss=0.203, v_num=0]\n",
      "Epoch 1:  73%|███████▎  | 86/118 [00:01<00:00, 78.66it/s, loss=0.203, v_num=0]\n",
      "Epoch 1:  81%|████████  | 95/118 [00:01<00:00, 79.71it/s, loss=0.199, v_num=0]\n",
      "Epoch 1:  92%|█████████▏| 108/118 [00:01<00:00, 83.67it/s, loss=0.206, v_num=0]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=17232)\u001b[0m \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A2)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=17232)\u001b[0m \n",
      "Validation:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 1:  92%|█████████▏| 109/118 [00:01<00:00, 73.73it/s, loss=0.206, v_num=0]\n",
      "Epoch 1:  93%|█████████▎| 110/118 [00:01<00:00, 74.00it/s, loss=0.206, v_num=0]\n",
      "Epoch 1:  94%|█████████▍| 111/118 [00:01<00:00, 74.36it/s, loss=0.206, v_num=0]\n",
      "Epoch 1:  95%|█████████▍| 112/118 [00:01<00:00, 74.72it/s, loss=0.206, v_num=0]\n",
      "Epoch 1:  96%|█████████▌| 113/118 [00:01<00:00, 75.08it/s, loss=0.206, v_num=0]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=17232)\u001b[0m \n",
      "Epoch 1:  97%|█████████▋| 114/118 [00:01<00:00, 75.42it/s, loss=0.206, v_num=0]\n",
      "Epoch 1:  97%|█████████▋| 115/118 [00:01<00:00, 75.77it/s, loss=0.206, v_num=0]\n",
      "Epoch 1:  98%|█████████▊| 116/118 [00:01<00:00, 76.08it/s, loss=0.206, v_num=0]\n",
      "Epoch 1:  99%|█████████▉| 117/118 [00:01<00:00, 76.59it/s, loss=0.206, v_num=0]\n",
      "Epoch 1: 100%|██████████| 118/118 [00:01<00:00, 76.69it/s, loss=0.206, v_num=0]\n",
      "Epoch 1: 100%|██████████| 118/118 [00:01<00:00, 76.64it/s, loss=0.206, v_num=0]\n",
      "Epoch 2:   0%|          | 0/118 [00:00<?, ?it/s, loss=0.206, v_num=0]          \n",
      "Epoch 2:   5%|▌         | 6/118 [00:00<00:05, 19.96it/s, loss=0.187, v_num=0]\n",
      "Epoch 2:   5%|▌         | 6/118 [00:00<00:05, 19.93it/s, loss=0.188, v_num=0]\n",
      "Epoch 2:  14%|█▎        | 16/118 [00:00<00:02, 39.92it/s, loss=0.176, v_num=0]\n",
      "Epoch 2:  22%|██▏       | 26/118 [00:00<00:01, 51.69it/s, loss=0.183, v_num=0]\n",
      "Epoch 2:  31%|███       | 36/118 [00:00<00:01, 59.53it/s, loss=0.18, v_num=0] \n",
      "Epoch 2:  31%|███▏      | 37/118 [00:00<00:01, 60.44it/s, loss=0.182, v_num=0]\n",
      "Epoch 2:  41%|████      | 48/118 [00:00<00:01, 67.23it/s, loss=0.178, v_num=0]\n",
      "Epoch 2:  49%|████▉     | 58/118 [00:00<00:00, 71.86it/s, loss=0.182, v_num=0]\n",
      "Epoch 2:  57%|█████▋    | 67/118 [00:00<00:00, 73.02it/s, loss=0.177, v_num=0]\n",
      "Epoch 2:  65%|██████▌   | 77/118 [00:01<00:00, 75.08it/s, loss=0.155, v_num=0]\n",
      "Epoch 2:  74%|███████▎  | 87/118 [00:01<00:00, 77.13it/s, loss=0.157, v_num=0]\n",
      "Epoch 2:  81%|████████▏ | 96/118 [00:01<00:00, 78.76it/s, loss=0.162, v_num=0]\n",
      "Epoch 2:  92%|█████████▏| 108/118 [00:01<00:00, 81.91it/s, loss=0.149, v_num=0]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=17232)\u001b[0m \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A2)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=17232)\u001b[0m \n",
      "Validation:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 2:  92%|█████████▏| 109/118 [00:01<00:00, 71.87it/s, loss=0.149, v_num=0]\n",
      "Epoch 2:  93%|█████████▎| 110/118 [00:01<00:00, 72.36it/s, loss=0.149, v_num=0]\n",
      "Epoch 2:  94%|█████████▍| 111/118 [00:01<00:00, 72.87it/s, loss=0.149, v_num=0]\n",
      "Epoch 2:  95%|█████████▍| 112/118 [00:01<00:00, 73.22it/s, loss=0.149, v_num=0]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=17232)\u001b[0m \n",
      "Epoch 2:  96%|█████████▌| 113/118 [00:01<00:00, 73.13it/s, loss=0.149, v_num=0]\n",
      "Epoch 2:  97%|█████████▋| 114/118 [00:01<00:00, 73.63it/s, loss=0.149, v_num=0]\n",
      "Epoch 2:  97%|█████████▋| 115/118 [00:01<00:00, 74.14it/s, loss=0.149, v_num=0]\n",
      "Epoch 2:  98%|█████████▊| 116/118 [00:01<00:00, 74.65it/s, loss=0.149, v_num=0]\n",
      "Epoch 2:  99%|█████████▉| 117/118 [00:01<00:00, 74.67it/s, loss=0.149, v_num=0]\n",
      "Epoch 2: 100%|██████████| 118/118 [00:01<00:00, 74.79it/s, loss=0.149, v_num=0]\n",
      "Epoch 2: 100%|██████████| 118/118 [00:01<00:00, 74.74it/s, loss=0.149, v_num=0]\n",
      "Epoch 3:   0%|          | 0/118 [00:00<?, ?it/s, loss=0.149, v_num=0]          \n",
      "Epoch 3:   1%|          | 1/118 [00:00<00:22,  5.27it/s, loss=0.149, v_num=0]\n",
      "Epoch 3:   7%|▋         | 8/118 [00:00<00:04, 27.40it/s, loss=0.144, v_num=0]\n",
      "Epoch 3:   7%|▋         | 8/118 [00:00<00:04, 26.95it/s, loss=0.143, v_num=0]\n",
      "Epoch 3:  14%|█▎        | 16/118 [00:00<00:02, 40.27it/s, loss=0.13, v_num=0] \n",
      "Epoch 3:  22%|██▏       | 26/118 [00:00<00:01, 52.98it/s, loss=0.122, v_num=0]\n",
      "Epoch 3:  30%|██▉       | 35/118 [00:00<00:01, 58.42it/s, loss=0.128, v_num=0]\n",
      "Epoch 3:  31%|███       | 36/118 [00:00<00:01, 59.33it/s, loss=0.128, v_num=0]\n",
      "Epoch 3:  39%|███▉      | 46/118 [00:00<00:01, 65.01it/s, loss=0.124, v_num=0]\n",
      "Epoch 3:  47%|████▋     | 55/118 [00:00<00:00, 67.90it/s, loss=0.138, v_num=0]\n",
      "Epoch 3:  55%|█████▌    | 65/118 [00:00<00:00, 71.30it/s, loss=0.145, v_num=0]\n",
      "Epoch 3:  55%|█████▌    | 65/118 [00:00<00:00, 70.84it/s, loss=0.152, v_num=0]\n",
      "Epoch 3:  64%|██████▍   | 76/118 [00:01<00:00, 74.81it/s, loss=0.138, v_num=0]\n",
      "Epoch 3:  71%|███████   | 84/118 [00:01<00:00, 74.74it/s, loss=0.125, v_num=0]\n",
      "Epoch 3:  80%|███████▉  | 94/118 [00:01<00:00, 76.91it/s, loss=0.124, v_num=0]\n",
      "Epoch 3:  81%|████████  | 95/118 [00:01<00:00, 77.30it/s, loss=0.124, v_num=0]\n",
      "Epoch 3:  90%|████████▉ | 106/118 [00:01<00:00, 79.61it/s, loss=0.119, v_num=0]\n",
      "Epoch 3:  92%|█████████▏| 108/118 [00:01<00:00, 80.41it/s, loss=0.123, v_num=0]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A2)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=17232)\u001b[0m \n",
      "Validation:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=17232)\u001b[0m \n",
      "Epoch 3:  92%|█████████▏| 109/118 [00:01<00:00, 70.82it/s, loss=0.123, v_num=0]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=17232)\u001b[0m \n",
      "Epoch 3:  93%|█████████▎| 110/118 [00:01<00:00, 71.26it/s, loss=0.123, v_num=0]\n",
      "Epoch 3:  94%|█████████▍| 111/118 [00:01<00:00, 71.60it/s, loss=0.123, v_num=0]\n",
      "Epoch 3:  95%|█████████▍| 112/118 [00:01<00:00, 71.92it/s, loss=0.123, v_num=0]\n",
      "Epoch 3:  96%|█████████▌| 113/118 [00:01<00:00, 72.20it/s, loss=0.123, v_num=0]\n",
      "Epoch 3:  97%|█████████▋| 114/118 [00:01<00:00, 72.64it/s, loss=0.123, v_num=0]\n",
      "Epoch 3:  97%|█████████▋| 115/118 [00:01<00:00, 73.05it/s, loss=0.123, v_num=0]\n",
      "Epoch 3:  98%|█████████▊| 116/118 [00:01<00:00, 73.55it/s, loss=0.123, v_num=0]\n",
      "Epoch 3:  99%|█████████▉| 117/118 [00:01<00:00, 74.00it/s, loss=0.123, v_num=0]\n",
      "Epoch 3: 100%|██████████| 118/118 [00:01<00:00, 73.28it/s, loss=0.123, v_num=0]\n",
      "Epoch 3: 100%|██████████| 118/118 [00:01<00:00, 73.23it/s, loss=0.123, v_num=0]\n",
      "Epoch 4:   0%|          | 0/118 [00:00<?, ?it/s, loss=0.123, v_num=0]          \n",
      "Epoch 4:   3%|▎         | 3/118 [00:00<00:10, 11.48it/s, loss=0.114, v_num=0]\n",
      "Epoch 4:  10%|█         | 12/118 [00:00<00:03, 32.93it/s, loss=0.111, v_num=0]\n",
      "Epoch 4:  18%|█▊        | 21/118 [00:00<00:02, 45.21it/s, loss=0.102, v_num=0]\n",
      "Epoch 4:  26%|██▋       | 31/118 [00:00<00:01, 54.51it/s, loss=0.11, v_num=0] \n",
      "Epoch 4:  35%|███▍      | 41/118 [00:00<00:01, 60.49it/s, loss=0.112, v_num=0]\n",
      "Epoch 4:  35%|███▍      | 41/118 [00:00<00:01, 60.40it/s, loss=0.109, v_num=0]\n",
      "Epoch 4:  43%|████▎     | 51/118 [00:00<00:01, 65.31it/s, loss=0.112, v_num=0]\n",
      "Epoch 4:  43%|████▎     | 51/118 [00:00<00:01, 64.95it/s, loss=0.11, v_num=0] \n",
      "Epoch 4:  52%|█████▏    | 61/118 [00:00<00:00, 68.96it/s, loss=0.116, v_num=0]\n",
      "Epoch 4:  53%|█████▎    | 62/118 [00:00<00:00, 69.63it/s, loss=0.116, v_num=0]\n",
      "Epoch 4:  61%|██████    | 72/118 [00:00<00:00, 72.69it/s, loss=0.12, v_num=0] \n",
      "Epoch 4:  61%|██████    | 72/118 [00:00<00:00, 72.35it/s, loss=0.119, v_num=0]\n",
      "Epoch 4:  69%|██████▊   | 81/118 [00:01<00:00, 74.46it/s, loss=0.124, v_num=0]\n",
      "Epoch 4:  69%|██████▊   | 81/118 [00:01<00:00, 73.59it/s, loss=0.121, v_num=0]\n",
      "Epoch 4:  78%|███████▊  | 92/118 [00:01<00:00, 76.35it/s, loss=0.105, v_num=0]\n",
      "Epoch 4:  78%|███████▊  | 92/118 [00:01<00:00, 76.33it/s, loss=0.108, v_num=0]\n",
      "Epoch 4:  87%|████████▋ | 103/118 [00:01<00:00, 78.93it/s, loss=0.0973, v_num=0]\n",
      "Epoch 4:  92%|█████████▏| 108/118 [00:01<00:00, 80.62it/s, loss=0.107, v_num=0] \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A2)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=17232)\u001b[0m \n",
      "Validation:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 4:  92%|█████████▏| 109/118 [00:01<00:00, 70.57it/s, loss=0.107, v_num=0]\n",
      "Epoch 4:  93%|█████████▎| 110/118 [00:01<00:00, 71.05it/s, loss=0.107, v_num=0]\n",
      "Epoch 4:  94%|█████████▍| 111/118 [00:01<00:00, 71.56it/s, loss=0.107, v_num=0]\n",
      "Epoch 4:  95%|█████████▍| 112/118 [00:01<00:00, 71.92it/s, loss=0.107, v_num=0]\n",
      "Epoch 4:  96%|█████████▌| 113/118 [00:01<00:00, 72.04it/s, loss=0.107, v_num=0]\n",
      "Epoch 4:  97%|█████████▋| 114/118 [00:01<00:00, 72.52it/s, loss=0.107, v_num=0]\n",
      "Epoch 4:  97%|█████████▋| 115/118 [00:01<00:00, 73.01it/s, loss=0.107, v_num=0]\n",
      "Epoch 4:  98%|█████████▊| 116/118 [00:01<00:00, 73.42it/s, loss=0.107, v_num=0]\n",
      "Epoch 4:  99%|█████████▉| 117/118 [00:01<00:00, 73.52it/s, loss=0.107, v_num=0]\n",
      "Epoch 4: 100%|██████████| 118/118 [00:01<00:00, 73.64it/s, loss=0.107, v_num=0]\n",
      "Epoch 4: 100%|██████████| 118/118 [00:01<00:00, 73.59it/s, loss=0.107, v_num=0]\n",
      "Epoch 5:   0%|          | 0/118 [00:00<?, ?it/s, loss=0.107, v_num=0]          \n",
      "Epoch 5:   2%|▏         | 2/118 [00:00<00:13,  8.38it/s, loss=0.103, v_num=0]\n",
      "Epoch 5:   8%|▊         | 10/118 [00:00<00:03, 29.51it/s, loss=0.101, v_num=0] \n",
      "Epoch 5:  18%|█▊        | 21/118 [00:00<00:02, 47.55it/s, loss=0.103, v_num=0] \n",
      "Epoch 5:  26%|██▋       | 31/118 [00:00<00:01, 56.79it/s, loss=0.0998, v_num=0]\n",
      "Epoch 5:  34%|███▍      | 40/118 [00:00<00:01, 61.21it/s, loss=0.104, v_num=0] \n",
      "Epoch 5:  42%|████▏     | 50/118 [00:00<00:01, 66.50it/s, loss=0.0978, v_num=0]\n",
      "Epoch 5:  43%|████▎     | 51/118 [00:00<00:00, 67.10it/s, loss=0.0978, v_num=0]\n",
      "Epoch 5:  53%|█████▎    | 62/118 [00:00<00:00, 71.68it/s, loss=0.0933, v_num=0]\n",
      "Epoch 5:  60%|██████    | 71/118 [00:00<00:00, 73.91it/s, loss=0.0864, v_num=0]\n",
      "Epoch 5:  61%|██████    | 72/118 [00:00<00:00, 74.28it/s, loss=0.0864, v_num=0]\n",
      "Epoch 5:  69%|██████▊   | 81/118 [00:01<00:00, 75.92it/s, loss=0.0845, v_num=0]\n",
      "Epoch 5:  69%|██████▉   | 82/118 [00:01<00:00, 76.33it/s, loss=0.0845, v_num=0]\n",
      "Epoch 5:  78%|███████▊  | 92/118 [00:01<00:00, 78.53it/s, loss=0.102, v_num=0] \n",
      "Epoch 5:  87%|████████▋ | 103/118 [00:01<00:00, 80.60it/s, loss=0.109, v_num=0]\n",
      "Epoch 5:  92%|█████████▏| 108/118 [00:01<00:00, 82.44it/s, loss=0.105, v_num=0]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A2)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=17232)\u001b[0m \n",
      "Validation:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 5:  92%|█████████▏| 109/118 [00:01<00:00, 72.14it/s, loss=0.105, v_num=0]\n",
      "Epoch 5:  93%|█████████▎| 110/118 [00:01<00:00, 72.45it/s, loss=0.105, v_num=0]\n",
      "Epoch 5:  94%|█████████▍| 111/118 [00:01<00:00, 72.86it/s, loss=0.105, v_num=0]\n",
      "Epoch 5:  95%|█████████▍| 112/118 [00:01<00:00, 73.21it/s, loss=0.105, v_num=0]\n",
      "Epoch 5:  96%|█████████▌| 113/118 [00:01<00:00, 73.55it/s, loss=0.105, v_num=0]\n",
      "Epoch 5:  97%|█████████▋| 114/118 [00:01<00:00, 73.75it/s, loss=0.105, v_num=0]\n",
      "Epoch 5:  97%|█████████▋| 115/118 [00:01<00:00, 74.15it/s, loss=0.105, v_num=0]\n",
      "Epoch 5:  98%|█████████▊| 116/118 [00:01<00:00, 74.65it/s, loss=0.105, v_num=0]\n",
      "Epoch 5:  99%|█████████▉| 117/118 [00:01<00:00, 75.10it/s, loss=0.105, v_num=0]\n",
      "Epoch 5: 100%|██████████| 118/118 [00:01<00:00, 74.97it/s, loss=0.105, v_num=0]\n",
      "Epoch 5: 100%|██████████| 118/118 [00:01<00:00, 74.92it/s, loss=0.105, v_num=0]\n",
      "Epoch 6:   0%|          | 0/118 [00:00<?, ?it/s, loss=0.105, v_num=0]          \n",
      "Epoch 6:   2%|▏         | 2/118 [00:00<00:13,  8.61it/s, loss=0.0952, v_num=0]\n",
      "Epoch 6:   8%|▊         | 10/118 [00:00<00:03, 29.86it/s, loss=0.0742, v_num=0]\n",
      "Epoch 6:  17%|█▋        | 20/118 [00:00<00:02, 45.73it/s, loss=0.0701, v_num=0]\n",
      "Epoch 6:  25%|██▍       | 29/118 [00:00<00:01, 53.55it/s, loss=0.0818, v_num=0]\n",
      "Epoch 6:  34%|███▍      | 40/118 [00:00<00:01, 61.80it/s, loss=0.0876, v_num=0]\n",
      "Epoch 6:  34%|███▍      | 40/118 [00:00<00:01, 61.39it/s, loss=0.0874, v_num=0]\n",
      "Epoch 6:  43%|████▎     | 51/118 [00:00<00:00, 67.65it/s, loss=0.09, v_num=0]  \n",
      "Epoch 6:  52%|█████▏    | 61/118 [00:00<00:00, 71.13it/s, loss=0.0883, v_num=0]\n",
      "Epoch 6:  60%|██████    | 71/118 [00:00<00:00, 73.82it/s, loss=0.08, v_num=0]  \n",
      "Epoch 6:  69%|██████▉   | 82/118 [00:01<00:00, 77.07it/s, loss=0.0791, v_num=0]\n",
      "Epoch 6:  69%|██████▉   | 82/118 [00:01<00:00, 76.84it/s, loss=0.0778, v_num=0]\n",
      "Epoch 6:  77%|███████▋  | 91/118 [00:01<00:00, 78.10it/s, loss=0.0764, v_num=0]\n",
      "Epoch 6:  78%|███████▊  | 92/118 [00:01<00:00, 78.52it/s, loss=0.0764, v_num=0]\n",
      "Epoch 6:  84%|████████▍ | 99/118 [00:01<00:00, 78.46it/s, loss=0.0668, v_num=0]\n",
      "Epoch 6:  92%|█████████▏| 108/118 [00:01<00:00, 81.21it/s, loss=0.0822, v_num=0]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A2)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=17232)\u001b[0m \n",
      "Validation:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 6:  92%|█████████▏| 109/118 [00:01<00:00, 70.99it/s, loss=0.0822, v_num=0]\n",
      "Epoch 6:  93%|█████████▎| 110/118 [00:01<00:00, 71.41it/s, loss=0.0822, v_num=0]\n",
      "Epoch 6:  94%|█████████▍| 111/118 [00:01<00:00, 71.74it/s, loss=0.0822, v_num=0]\n",
      "Epoch 6:  95%|█████████▍| 112/118 [00:01<00:00, 72.00it/s, loss=0.0822, v_num=0]\n",
      "Epoch 6:  96%|█████████▌| 113/118 [00:01<00:00, 72.26it/s, loss=0.0822, v_num=0]\n",
      "Epoch 6:  97%|█████████▋| 114/118 [00:01<00:00, 72.67it/s, loss=0.0822, v_num=0]\n",
      "Epoch 6:  97%|█████████▋| 115/118 [00:01<00:00, 73.08it/s, loss=0.0822, v_num=0]\n",
      "Epoch 6:  98%|█████████▊| 116/118 [00:01<00:00, 73.40it/s, loss=0.0822, v_num=0]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=17232)\u001b[0m \n",
      "Epoch 6:  99%|█████████▉| 117/118 [00:01<00:00, 73.85it/s, loss=0.0822, v_num=0]\n",
      "Epoch 6: 100%|██████████| 118/118 [00:01<00:00, 73.97it/s, loss=0.0822, v_num=0]\n",
      "Epoch 6: 100%|██████████| 118/118 [00:01<00:00, 73.91it/s, loss=0.0822, v_num=0]\n",
      "Epoch 7:   0%|          | 0/118 [00:00<?, ?it/s, loss=0.0822, v_num=0]          \n",
      "Epoch 7:   1%|          | 1/118 [00:00<00:24,  4.76it/s, loss=0.0816, v_num=0]\n",
      "Epoch 7:   8%|▊         | 10/118 [00:00<00:03, 32.68it/s, loss=0.0774, v_num=0]\n",
      "Epoch 7:  17%|█▋        | 20/118 [00:00<00:02, 48.49it/s, loss=0.0633, v_num=0]\n",
      "Epoch 7:  25%|██▍       | 29/118 [00:00<00:01, 55.60it/s, loss=0.0682, v_num=0]\n",
      "Epoch 7:  34%|███▍      | 40/118 [00:00<00:01, 63.73it/s, loss=0.0633, v_num=0]\n",
      "Epoch 7:  43%|████▎     | 51/118 [00:00<00:00, 70.11it/s, loss=0.0596, v_num=0]\n",
      "Epoch 7:  43%|████▎     | 51/118 [00:00<00:00, 69.74it/s, loss=0.0599, v_num=0]\n",
      "Epoch 7:  53%|█████▎    | 62/118 [00:00<00:00, 74.75it/s, loss=0.0601, v_num=0]\n",
      "Epoch 7:  62%|██████▏   | 73/118 [00:00<00:00, 77.99it/s, loss=0.0696, v_num=0]\n",
      "Epoch 7:  62%|██████▏   | 73/118 [00:00<00:00, 77.77it/s, loss=0.0715, v_num=0]\n",
      "Epoch 7:  70%|███████   | 83/118 [00:01<00:00, 80.11it/s, loss=0.0793, v_num=0]\n",
      "Epoch 7:  71%|███████   | 84/118 [00:01<00:00, 80.45it/s, loss=0.0793, v_num=0]\n",
      "Epoch 7:  81%|████████  | 95/118 [00:01<00:00, 82.87it/s, loss=0.0778, v_num=0]\n",
      "Epoch 7:  91%|█████████ | 107/118 [00:01<00:00, 85.72it/s, loss=0.0743, v_num=0]\n",
      "Epoch 7:  91%|█████████ | 107/118 [00:01<00:00, 85.50it/s, loss=0.0753, v_num=0]\n",
      "Epoch 7:  92%|█████████▏| 108/118 [00:01<00:00, 85.88it/s, loss=0.0742, v_num=0]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A2)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=17232)\u001b[0m \n",
      "Validation:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 7:  92%|█████████▏| 109/118 [00:01<00:00, 75.70it/s, loss=0.0742, v_num=0]\n",
      "Epoch 7:  93%|█████████▎| 110/118 [00:01<00:00, 76.11it/s, loss=0.0742, v_num=0]\n",
      "Epoch 7:  94%|█████████▍| 111/118 [00:01<00:00, 76.41it/s, loss=0.0742, v_num=0]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=17232)\u001b[0m \n",
      "Epoch 7:  95%|█████████▍| 112/118 [00:01<00:00, 76.72it/s, loss=0.0742, v_num=0]\n",
      "Epoch 7:  96%|█████████▌| 113/118 [00:01<00:00, 77.06it/s, loss=0.0742, v_num=0]\n",
      "Epoch 7:  97%|█████████▋| 114/118 [00:01<00:00, 77.34it/s, loss=0.0742, v_num=0]\n",
      "Epoch 7:  97%|█████████▋| 115/118 [00:01<00:00, 77.75it/s, loss=0.0742, v_num=0]\n",
      "Epoch 7:  98%|█████████▊| 116/118 [00:01<00:00, 77.90it/s, loss=0.0742, v_num=0]\n",
      "Epoch 7:  99%|█████████▉| 117/118 [00:01<00:00, 78.41it/s, loss=0.0742, v_num=0]\n",
      "Epoch 7: 100%|██████████| 118/118 [00:01<00:00, 77.83it/s, loss=0.0742, v_num=0]\n",
      "Epoch 7: 100%|██████████| 118/118 [00:01<00:00, 77.77it/s, loss=0.0742, v_num=0]\n",
      "Epoch 8:   0%|          | 0/118 [00:00<?, ?it/s, loss=0.0742, v_num=0]          \n",
      "Epoch 8:   5%|▌         | 6/118 [00:00<00:04, 22.78it/s, loss=0.0719, v_num=0]\n",
      "Epoch 8:  12%|█▏        | 14/118 [00:00<00:02, 38.90it/s, loss=0.0655, v_num=0]\n",
      "Epoch 8:  13%|█▎        | 15/118 [00:00<00:02, 40.47it/s, loss=0.0663, v_num=0]\n",
      "Epoch 8:  21%|██        | 25/118 [00:00<00:01, 52.81it/s, loss=0.064, v_num=0] \n",
      "Epoch 8:  29%|██▉       | 34/118 [00:00<00:01, 58.98it/s, loss=0.0592, v_num=0]\n",
      "Epoch 8:  37%|███▋      | 44/118 [00:00<00:01, 64.76it/s, loss=0.0537, v_num=0]\n",
      "Epoch 8:  46%|████▌     | 54/118 [00:00<00:00, 68.99it/s, loss=0.0569, v_num=0]\n",
      "Epoch 8:  56%|█████▌    | 66/118 [00:00<00:00, 74.51it/s, loss=0.0609, v_num=0]\n",
      "Epoch 8:  64%|██████▎   | 75/118 [00:00<00:00, 76.72it/s, loss=0.0608, v_num=0]\n",
      "Epoch 8:  72%|███████▏  | 85/118 [00:01<00:00, 78.28it/s, loss=0.0573, v_num=0]\n",
      "Epoch 8:  73%|███████▎  | 86/118 [00:01<00:00, 78.57it/s, loss=0.0573, v_num=0]\n",
      "Epoch 8:  73%|███████▎  | 86/118 [00:01<00:00, 78.45it/s, loss=0.0561, v_num=0]\n",
      "Epoch 8:  81%|████████  | 95/118 [00:01<00:00, 79.63it/s, loss=0.0485, v_num=0]\n",
      "Epoch 8:  81%|████████▏ | 96/118 [00:01<00:00, 80.00it/s, loss=0.0497, v_num=0]\n",
      "Epoch 8:  92%|█████████▏| 108/118 [00:01<00:00, 82.93it/s, loss=0.059, v_num=0] \n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=17232)\u001b[0m \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A2)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=17232)\u001b[0m \n",
      "Validation:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 8:  92%|█████████▏| 109/118 [00:01<00:00, 72.21it/s, loss=0.059, v_num=0]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=17232)\u001b[0m \n",
      "Epoch 8:  93%|█████████▎| 110/118 [00:01<00:00, 72.61it/s, loss=0.059, v_num=0]\n",
      "Epoch 8:  94%|█████████▍| 111/118 [00:01<00:00, 72.91it/s, loss=0.059, v_num=0]\n",
      "Epoch 8:  95%|█████████▍| 112/118 [00:01<00:00, 73.36it/s, loss=0.059, v_num=0]\n",
      "Epoch 8:  96%|█████████▌| 113/118 [00:01<00:00, 73.53it/s, loss=0.059, v_num=0]\n",
      "Epoch 8:  97%|█████████▋| 114/118 [00:01<00:00, 74.02it/s, loss=0.059, v_num=0]\n",
      "Epoch 8:  97%|█████████▋| 115/118 [00:01<00:00, 74.44it/s, loss=0.059, v_num=0]\n",
      "Epoch 8:  98%|█████████▊| 116/118 [00:01<00:00, 74.94it/s, loss=0.059, v_num=0]\n",
      "Epoch 8:  99%|█████████▉| 117/118 [00:01<00:00, 75.06it/s, loss=0.059, v_num=0]\n",
      "Epoch 8: 100%|██████████| 118/118 [00:01<00:00, 75.20it/s, loss=0.059, v_num=0]\n",
      "Epoch 8: 100%|██████████| 118/118 [00:01<00:00, 75.15it/s, loss=0.059, v_num=0]\n",
      "Epoch 9:   0%|          | 0/118 [00:00<?, ?it/s, loss=0.059, v_num=0]          \n",
      "Epoch 9:   4%|▍         | 5/118 [00:00<00:06, 18.72it/s, loss=0.0632, v_num=0]\n",
      "Epoch 9:   5%|▌         | 6/118 [00:00<00:05, 21.95it/s, loss=0.0632, v_num=0]\n",
      "Epoch 9:  14%|█▎        | 16/118 [00:00<00:02, 42.58it/s, loss=0.0603, v_num=0]\n",
      "Epoch 9:  21%|██        | 25/118 [00:00<00:01, 52.25it/s, loss=0.0543, v_num=0]\n",
      "Epoch 9:  31%|███       | 36/118 [00:00<00:01, 61.88it/s, loss=0.0572, v_num=0]\n",
      "Epoch 9:  31%|███       | 36/118 [00:00<00:01, 61.81it/s, loss=0.0562, v_num=0]\n",
      "Epoch 9:  39%|███▉      | 46/118 [00:00<00:01, 66.98it/s, loss=0.0504, v_num=0]\n",
      "Epoch 9:  47%|████▋     | 56/118 [00:00<00:00, 71.09it/s, loss=0.0501, v_num=0]\n",
      "Epoch 9:  57%|█████▋    | 67/118 [00:00<00:00, 75.35it/s, loss=0.0489, v_num=0]\n",
      "Epoch 9:  64%|██████▍   | 76/118 [00:00<00:00, 76.36it/s, loss=0.0412, v_num=0]\n",
      "Epoch 9:  74%|███████▎  | 87/118 [00:01<00:00, 79.08it/s, loss=0.0452, v_num=0]\n",
      "Epoch 9:  82%|████████▏ | 97/118 [00:01<00:00, 81.13it/s, loss=0.0506, v_num=0]\n",
      "Epoch 9:  83%|████████▎ | 98/118 [00:01<00:00, 81.21it/s, loss=0.0506, v_num=0]\n",
      "Epoch 9:  92%|█████████▏| 108/118 [00:01<00:00, 84.51it/s, loss=0.0563, v_num=0]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=17232)\u001b[0m \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A2)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=17232)\u001b[0m \n",
      "Validation:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 9:  92%|█████████▏| 109/118 [00:01<00:00, 74.09it/s, loss=0.0563, v_num=0]\n",
      "Epoch 9:  93%|█████████▎| 110/118 [00:01<00:00, 74.36it/s, loss=0.0563, v_num=0]\n",
      "Epoch 9:  94%|█████████▍| 111/118 [00:01<00:00, 74.67it/s, loss=0.0563, v_num=0]\n",
      "Epoch 9:  95%|█████████▍| 112/118 [00:01<00:00, 75.07it/s, loss=0.0563, v_num=0]\n",
      "Epoch 9:  96%|█████████▌| 113/118 [00:01<00:00, 75.46it/s, loss=0.0563, v_num=0]\n",
      "Epoch 9:  97%|█████████▋| 114/118 [00:01<00:00, 75.88it/s, loss=0.0563, v_num=0]\n",
      "Epoch 9:  97%|█████████▋| 115/118 [00:01<00:00, 76.36it/s, loss=0.0563, v_num=0]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=17232)\u001b[0m \n",
      "Epoch 9:  98%|█████████▊| 116/118 [00:01<00:00, 75.64it/s, loss=0.0563, v_num=0]\n",
      "Epoch 9:  99%|█████████▉| 117/118 [00:01<00:00, 76.00it/s, loss=0.0563, v_num=0]\n",
      "Epoch 9: 100%|██████████| 118/118 [00:01<00:00, 76.04it/s, loss=0.0563, v_num=0]\n",
      "Epoch 9: 100%|██████████| 118/118 [00:01<00:00, 75.99it/s, loss=0.0563, v_num=0]\n",
      "Epoch 9: 100%|██████████| 118/118 [00:01<00:00, 68.09it/s, loss=0.0563, v_num=0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-13 16:05:52,777\tINFO tune.py:1111 -- Total run time: 39.46 seconds (39.28 seconds for the tuning loop).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy:  0.9700015783309937\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Result(\n",
       "  metrics={'_report_on': 'train_epoch_end', 'train_loss': 0.03159911185503006, 'val_accuracy': 0.9700015783309937, 'val_loss': -12.346744537353516, 'epoch': 9, 'step': 1080, 'should_checkpoint': True, 'done': True, 'trial_id': 'c0d28_00000', 'experiment_tag': '0'},\n",
       "  path='/tmp/ray_results/ptl-mnist-example/LightningTrainer_c0d28_00000_0_2023-06-13_16-05-13',\n",
       "  checkpoint=LightningCheckpoint(local_path=/tmp/ray_results/ptl-mnist-example/LightningTrainer_c0d28_00000_0_2023-06-13_16-05-13/checkpoint_000009)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = trainer.fit()\n",
    "print(\"Validation Accuracy: \", result.metrics[\"val_accuracy\"])\n",
    "result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate your model on test dataset\n",
    "\n",
    "Next, let's evaluate the model's performance on the MNIST test set. We will first retrieve the best checkpoint from the fitting results and load it into the model.\n",
    "\n",
    "If you lost the in-memory result object, you can also restore the model from the checkpoint file. Here the checkpoint path is: `/tmp/ray_results/ptl-mnist-example/LightningTrainer_c0d28_00000_0_2023-06-13_16-05-13/checkpoint_000009/model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "checkpoint: LightningCheckpoint = result.checkpoint\n",
    "best_model: pl.LightningModule = checkpoint.get_model(MNISTClassifier)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single-node Testing\n",
    "\n",
    "If you have a relatively small test set, like MNIST, the easiest way is to use PyTorch Lightning's native interface to evaluate the best model. Pass the loaded model and test data loader to ``pl.Trainer.test()``, which will execute the test loop using your custom ``pl.LightningModule.test_step()`` method on your head node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ray/anaconda3/lib/python3.9/site-packages/pytorch_lightning/loops/utilities.py:92: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "GPU available: True, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/ray/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1814: PossibleUserWarning: GPU available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='gpu', devices=1)`.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cf83052456d4d9bbcce0fff8d54fd90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-13 16:05:53.932195: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-13 16:05:54.097738: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-06-13 16:05:55.022170: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2023-06-13 16:05:55.022249: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2023-06-13 16:05:55.022255: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       test_accuracy       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9740999937057495     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m      test_accuracy      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9740999937057495    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Download and setup MNIST datamodule on the head node\n",
    "datamodule.setup()\n",
    "test_dataloader = datamodule.test_dataloader()\n",
    "\n",
    "trainer = pl.Trainer()\n",
    "result = trainer.test(best_model, dataloaders=test_dataloader)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-node Testing\n",
    "\n",
    "Alternatively, if you have a large test set and want to speed up the testing process in parallel, you can create a group of Ray Actors to leverage multiple GPUs across multiple nodes for distributed inference. Here we demonstrate how to set up a process group and do evaluation using 4 GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-13 16:05:56,270\tWARNING worker.py:2019 -- Warning: The actor TestWorker is very large (53 MiB). Check that its definition is not implicitly capturing a large array or other object in scope. Tip: use ray.put() to put large objects in the Ray object store.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=17232)\u001b[0m Global seed set to 888\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=7319, ip=10.0.58.90)\u001b[0m Missing logger folder: logs/lightning_logs\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=7319, ip=10.0.58.90)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=6371, ip=10.0.1.80)\u001b[0m [W reducer.cpp:1298] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=9162, ip=10.0.26.229)\u001b[0m   from pandas import MultiIndex, Int64Index\n",
      "\u001b[2m\u001b[36m(pid=9162, ip=10.0.26.229)\u001b[0m   from pandas import MultiIndex, Int64Index\n",
      "\u001b[2m\u001b[36m(pid=9976, ip=10.0.58.90)\u001b[0m /home/ray/anaconda3/lib/python3.9/site-packages/xgboost/compat.py:31: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "\u001b[2m\u001b[36m(pid=9976, ip=10.0.58.90)\u001b[0m   from pandas import MultiIndex, Int64Index\n",
      "\u001b[2m\u001b[36m(TestWorker pid=20600)\u001b[0m /home/ray/anaconda3/lib/python3.9/site-packages/pytorch_lightning/loops/utilities.py:92: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "\u001b[2m\u001b[36m(TestWorker pid=20600)\u001b[0m   rank_zero_warn(\n",
      "\u001b[2m\u001b[36m(TestWorker pid=20600)\u001b[0m GPU available: True, used: True\n",
      "\u001b[2m\u001b[36m(TestWorker pid=20600)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(TestWorker pid=20600)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[2m\u001b[36m(TestWorker pid=20600)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[2m\u001b[36m(TestWorker pid=20600)\u001b[0m /home/ray/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:330: PossibleUserWarning: Using `DistributedSampler` with the dataloaders. During `trainer.test()`, it is recommended to use `Trainer(devices=1)` to ensure each sample/batch gets evaluated exactly once. Otherwise, multi-device settings use `DistributedSampler` that replicates some samples to make sure all devices have same batch size in case of uneven inputs.\n",
      "\u001b[2m\u001b[36m(TestWorker pid=20600)\u001b[0m   rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: 0it [00:00, ?it/s]600)\u001b[0m \n",
      "Testing DataLoader 0:   0%|          | 0/20 [00:00<?, ?it/s]\n",
      "Testing DataLoader 0:  10%|█         | 2/20 [00:00<00:13,  1.36it/s]\n",
      "Testing DataLoader 0:  75%|███████▌  | 15/20 [00:00<00:00, 22.10it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(TestWorker pid=20600)\u001b[0m 2023-06-13 16:06:07.550225: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "\u001b[2m\u001b[36m(TestWorker pid=20600)\u001b[0m To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\u001b[2m\u001b[36m(TestWorker pid=9976, ip=10.0.58.90)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=20600)\u001b[0m /home/ray/anaconda3/lib/python3.9/site-packages/xgboost/compat.py:31: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=20600)\u001b[0m   from pandas import MultiIndex, Int64Index\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(TestWorker pid=20600)\u001b[0m 2023-06-13 16:06:07.708119: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 20/20 [00:00<00:00, 22.10it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(TestWorker pid=20600)\u001b[0m 2023-06-13 16:06:08.680418: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "\u001b[2m\u001b[36m(TestWorker pid=20600)\u001b[0m 2023-06-13 16:06:08.680524: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "\u001b[2m\u001b[36m(TestWorker pid=20600)\u001b[0m 2023-06-13 16:06:08.680532: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 20/20 [00:02<00:00,  7.31it/s]\n",
      "\u001b[2m\u001b[36m(TestWorker pid=20600)\u001b[0m ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
      "\u001b[2m\u001b[36m(TestWorker pid=20600)\u001b[0m ┃        Test metric        ┃       DataLoader 0        ┃\n",
      "\u001b[2m\u001b[36m(TestWorker pid=20600)\u001b[0m ┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
      "\u001b[2m\u001b[36m(TestWorker pid=20600)\u001b[0m │       test_accuracy       │    0.9740999937057495     │\n",
      "\u001b[2m\u001b[36m(TestWorker pid=20600)\u001b[0m └───────────────────────────┴───────────────────────────┘\n"
     ]
    }
   ],
   "source": [
    "import ray\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from pytorch_lightning.plugins.environments.lightning_environment import (\n",
    "    LightningEnvironment,\n",
    ")\n",
    "from ray.air.util.torch_dist import (\n",
    "    TorchDistributedWorker,\n",
    "    init_torch_dist_process_group,\n",
    "    shutdown_torch_dist_process_group,\n",
    ")\n",
    "\n",
    "\n",
    "class RayEnvironment(LightningEnvironment):\n",
    "    \"\"\"Setup Lightning DDP training environment for Ray cluster.\"\"\"\n",
    "\n",
    "    def world_size(self) -> int:\n",
    "        return int(os.environ[\"WORLD_SIZE\"])\n",
    "\n",
    "    def global_rank(self) -> int:\n",
    "        return int(os.environ[\"RANK\"])\n",
    "\n",
    "    def local_rank(self) -> int:\n",
    "        return int(os.environ[\"LOCAL_RANK\"])\n",
    "\n",
    "    def set_world_size(self, size: int) -> None:\n",
    "        # Disable it since `world_size()` directly returns data from AIR session.\n",
    "        pass\n",
    "\n",
    "    def set_global_rank(self, rank: int) -> None:\n",
    "        # Disable it since `global_rank()` directly returns data from AIR session.\n",
    "        pass\n",
    "\n",
    "    def teardown(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "@ray.remote\n",
    "class TestWorker(TorchDistributedWorker):\n",
    "    def run(self):\n",
    "        trainer = pl.Trainer(\n",
    "            num_nodes=num_workers,\n",
    "            accelerator=\"gpu\",\n",
    "            strategy=\"ddp\",\n",
    "            plugins=[RayEnvironment()],\n",
    "        )\n",
    "        return trainer.test(best_model, dataloaders=test_dataloader)\n",
    "\n",
    "\n",
    "# Create 4 remote Ray Actors, each with 1 GPU\n",
    "workers = [TestWorker.options(num_gpus=1).remote() for _ in range(num_workers)]\n",
    "\n",
    "# Initialize the Torch distributed group among the 4 actors.\n",
    "# This will set up the required environment variables including \n",
    "# RANK, LOCAL_RANK, WORLD_SIZE, MASTER_ADDRESS, ...\n",
    "init_torch_dist_process_group(workers=workers, backend=\"nccl\")\n",
    "\n",
    "# Execute the testing run in parallel\n",
    "results = ray.get([worker.run.remote() for worker in workers])\n",
    "\n",
    "# Shutdown the process group\n",
    "shutdown_torch_dist_process_group(workers=workers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's next?\n",
    "\n",
    "- {ref}`Use LightningTrainer with Ray Data and Batch Predictor <lightning_advanced_example>`\n",
    "- {ref}`Fine-tune a Large Language Model with LightningTrainer and FSDP <dolly_lightning_fsdp_finetuning>`\n",
    "- {ref}`Hyperparameter searching with LightningTrainer + Ray Tune. <tune-pytorch-lightning-ref>`\n",
    "- {ref}`Experiment Tracking with Wandb, CometML, MLFlow, and Tensorboard in LightningTrainer <lightning_experiment_tracking>`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "a8c1140d108077f4faeb76b2438f85e4ed675f93d004359552883616a1acd54c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
