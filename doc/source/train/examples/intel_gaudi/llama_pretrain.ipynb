{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llama model pre-training on Intel Gaudi\n",
    "\n",
    "<a id=\"try-anyscale-quickstart-intel_gaudi-llama_pretrain\" href=\"https://console.anyscale.com/register/ha?render_flow=ray&utm_source=ray_docs&utm_medium=docs&utm_campaign=intel_gaudi-llama_pretrain\">\n",
    "    <img src=\"../../../_static/img/run-on-anyscale.svg\" alt=\"try-anyscale-quickstart\">\n",
    "</a>\n",
    "<br></br>\n",
    "\n",
    "In this Jupyter notebook, we will pre-train a [huggyllama/llama-7b](https://huggingface.co/huggyllama/llama-7b) model by using Intel Gaudi accelerators.\n",
    "\n",
    "We will use PyTorch for model training and Ray for distributed training.\n",
    "\n",
    "[Intel Gaudi AI Processors (HPUs)](https://habana.ai) are AI hardware accelerators designed by Habana Labs. For more information, see [Gaudi Architecture](https://docs.habana.ai/en/latest/Gaudi_Overview/index.html) and [Gaudi Developer Docs](https://developer.habana.ai/).\n",
    "\n",
    "Basic features for this pre-training example are:\n",
    "- Running on HPUs, support three execution mode: [\"lazy\", \"eager\", \"eager.compile\"](https://docs.habana.ai/en/latest/PyTorch/Reference/PyTorch_Gaudi_Theory_of_Operations.html).\n",
    "- Pre-training llama model use configuration [huggyllama/llama-7b](https://huggingface.co/huggyllama/llama-7b)\n",
    "- [`GaudiTrainer`](https://github.com/huggingface/optimum-habana/blob/main/optimum/habana/transformers/trainer.py) based training.\n",
    "- DeepSpeed based pre-training.\n",
    "- Ray based resource scheduling and management."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare environment\n",
    "This example run on single node with 4 HPUs.\n",
    "\n",
    "We recommend using a prebuilt container to run these examples. To run a container, you need Docker. See [Install Docker Engine](https://docs.docker.com/engine/install/) for installation instructions.\n",
    "\n",
    "Next, follow [Run Using Containers](https://docs.habana.ai/en/latest/Installation_Guide/Bare_Metal_Fresh_OS.html?highlight=installer#run-using-containers) to install the Habana drivers and container runtime.\n",
    "\n",
    "### Get docker image\n",
    "``` bash\n",
    "# more available docker image can be found here: https://vault.habana.ai/ui/native/gaudi-docker\n",
    "docker pull vault.habana.ai/gaudi-docker/1.22.1/ubuntu24.04/habanalabs/pytorch-installer-2.7.1:latest\n",
    "```\n",
    "### Run docker image\n",
    "``` bash\n",
    "docker run -it --runtime=habana -e HABANA_VISIBLE_DEVICES=all -e OMPI_MCA_btl_vader_single_copy_mechanism=none --cap-add=sys_nice --net=host --ipc=host vault.habana.ai/gaudi-docker/1.22.1/ubuntu24.04/habanalabs/pytorch-installer-2.7.1:latest\n",
    "# maybe should mapping your workspace volumns\n",
    "```\n",
    "### Install dependency\n",
    "``` bash\n",
    "# \"optimum-habana>1.11.1\" if execution mode \"eager\" or \"eager.compile\" \n",
    "# \"ray>=2.20.0\"\n",
    "pip install ray[train] notebook transformers datasets evaluate peft==0.17.1 accelerate scikit-learn optimum-habana\n",
    "\n",
    "# install deepspeed\n",
    "pip install git+https://github.com/HabanaAI/DeepSpeed.git@1.22.0\n",
    "\n",
    "# this notebook verfied with packages' version:\n",
    "# transformers==4.57.1\n",
    "# datasets==4.4.1\n",
    "# evaluate==0.4.6\n",
    "# peft==0.17.1\n",
    "# accelerate==1.11.0\n",
    "# scikit-learn==1.7.2\n",
    "# optimum-habana==1.19.1\n",
    "\n",
    "# deepspeed==0.16.1+hpu.synapse.v1.22.0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "import os\n",
    "from typing import Any, Dict\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import transformers\n",
    "from itertools import chain\n",
    "from datasets import load_dataset\n",
    "from transformers import default_data_collator\n",
    "from transformers.testing_utils import CaptureLogger\n",
    "from optimum.habana import GaudiConfig, GaudiTrainer, GaudiTrainingArguments\n",
    "from optimum.habana.utils import set_seed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build datasets\n",
    "\n",
    "Download and load dataset from huggingface.co"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_datasets(config):\n",
    "    dataset_name = config[\"name\"] \n",
    "    dataset_config_name = config[\"config_name\"]\n",
    "\n",
    "    # Downloading and loading a dataset from the hub.\n",
    "    raw_datasets = load_dataset(\n",
    "        dataset_name,\n",
    "        dataset_config_name,\n",
    "        cache_dir=None,\n",
    "        token=None,\n",
    "        streaming=False,\n",
    "    )\n",
    "    if \"validation\" not in raw_datasets.keys():\n",
    "        raw_datasets[\"validation\"] = load_dataset(\n",
    "            dataset_name,\n",
    "            dataset_config_name,\n",
    "            split=f\"train[:{data_args.validation_split_percentage}%]\",\n",
    "            cache_dir=None,\n",
    "            token=None,\n",
    "            streaming=False,\n",
    "        )\n",
    "        raw_datasets[\"train\"] = load_dataset(\n",
    "            dataset_name,\n",
    "            dataset_config_name,\n",
    "            split=f\"train[{data_args.validation_split_percentage}%:]\",\n",
    "            cache_dir=None,\n",
    "            token=None,\n",
    "            streaming=False,\n",
    "        )\n",
    "\n",
    "    return raw_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load tokenizer\n",
    "\n",
    "Download vocabulary from huggingface.co."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tokenizer(config):\n",
    "    name = config[\"name\"]\n",
    "    tokenizer_kwargs = {\n",
    "        \"cache_dir\": None,\n",
    "        \"use_fast\": True,\n",
    "        \"revision\": \"main\",\n",
    "        \"token\": None,\n",
    "        \"trust_remote_code\": False,\n",
    "    }\n",
    "    return transformers.AutoTokenizer.from_pretrained(name, **tokenizer_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize dataset\n",
    "\n",
    "tokenize word to token ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_dataset(datasets, tokenizer):\n",
    "    column_names = list(datasets[\"train\"].features)\n",
    "    text_column_name = \"text\" if \"text\" in column_names else column_names[0]\n",
    "\n",
    "    tok_logger = transformers.utils.logging.get_logger(\"transformers.tokenization_utils_base\")\n",
    "\n",
    "    def tokenize_function(examples):\n",
    "        with CaptureLogger(tok_logger) as cl:\n",
    "            output = tokenizer(examples[text_column_name])\n",
    "        # clm input could be much much longer than block_size\n",
    "        if \"Token indices sequence length is longer than the\" in cl.out:\n",
    "            tok_logger.warning(\n",
    "                \"^^^^^^^^^^^^^^^^ Please ignore the warning above - this long input will be chunked into smaller bits\"\n",
    "                \" before being passed to the model.\"\n",
    "            )\n",
    "        return output\n",
    "\n",
    "    tokenized_datasets = datasets.map(\n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        num_proc=None,\n",
    "        remove_columns=column_names,\n",
    "        load_from_cache_file=True,\n",
    "        desc=\"Running tokenizer on dataset\",\n",
    "    )\n",
    "\n",
    "    return tokenized_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group dataset\n",
    "\n",
    "This preprocssing will concatenate all texts from our dataset and generate chunks of block_size, and will pre-train model much faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_dataset(config, datasets, tokenizer):\n",
    "    config_name = config[\"name\"]\n",
    "    auto_config = transformers.AutoConfig.from_pretrained(config_name)\n",
    "    max_pos_embeddings = auto_config.max_position_embeddings\n",
    "    block_size = tokenizer.model_max_length\n",
    "    if block_size > max_pos_embeddings:\n",
    "        print(\n",
    "            f\"The tokenizer picked seems to have a very large `model_max_length` ({tokenizer.model_max_length}). \"\n",
    "            f\"Using block_size={min(1024, max_pos_embeddings)} instead. You can change that default value by passing --block_size xxx.\"\n",
    "        )\n",
    "        if max_pos_embeddings > 0:\n",
    "            block_size = min(1024, max_pos_embeddings)\n",
    "        else:\n",
    "            block_size = 1024\n",
    "\n",
    "    # Main data processing function that will concatenate all texts from our dataset and generate chunks of block_size.\n",
    "    def group_texts(examples):\n",
    "        # Concatenate all texts.\n",
    "        concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n",
    "        total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "        # We drop the small remainder, and if the total_length < block_size  we exclude this batch and return an empty dict.\n",
    "        # We could add padding if the model supported it instead of this drop, you can customize this part to your needs.\n",
    "        total_length = (total_length // block_size) * block_size\n",
    "        # Split by chunks of max_len.\n",
    "        result = {\n",
    "            k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "            for k, t in concatenated_examples.items()\n",
    "        }\n",
    "        result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "        return result\n",
    "\n",
    "    lm_datasets = datasets.map(\n",
    "        group_texts,\n",
    "        batched=True,\n",
    "        num_proc=None,\n",
    "        load_from_cache_file=True,\n",
    "        desc=f\"Grouping texts in chunks of {block_size}\",\n",
    "    )\n",
    "    return lm_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model\n",
    "\n",
    "Download and load pre-configed model from huggingface.co, the detail model configurations in config.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(config):\n",
    "    name = config[\"name\"]\n",
    "    model_config = config.get(\"config\", {})\n",
    "    auto_config = transformers.AutoConfig.from_pretrained(\n",
    "        pretrained_model_name_or_path=name, **model_config\n",
    "    )\n",
    "    model = transformers.AutoModelForCausalLM.from_config(auto_config, trust_remote_code=False)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare trainer\n",
    "\n",
    "Instance Trainer with `model`, `gaudi_config`, `training_args`, `tokenizer`\n",
    "\n",
    "No evaluation dataset passed, just training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trainer(training_args, datasets, tokenizer, model):\n",
    "    gaudi_config = GaudiConfig.from_pretrained(\n",
    "        training_args.gaudi_config_name, revision=\"main\",\n",
    "    )\n",
    "\n",
    "    trainer = GaudiTrainer(\n",
    "        model=model,\n",
    "        gaudi_config=gaudi_config,\n",
    "        args=training_args,\n",
    "        train_dataset=datasets[\"train\"],\n",
    "        eval_dataset=None,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=default_data_collator,\n",
    "    )\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Function\n",
    "\n",
    "This function will be executed by each worker during training, with following steps:\n",
    "- prepare GaudiTrainingArguments object.\n",
    "- load datasets from huggingface.co.\n",
    "- load pre-configed tokenizer from huggingface.co.\n",
    "- tokenize dataset with loaded model tokenizer.\n",
    "- concatenate all texts from our dataset and generate chunks of block_size.\n",
    "- instance object of `GaudiTrainer` with training_args, datasets, tokenizer, and model.\n",
    "- call `train` of trainer.\n",
    "- save model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrain_llama(config: Dict[str, Any]):\n",
    "\n",
    "    training_args = GaudiTrainingArguments(**config[\"training_args\"])\n",
    "    set_seed(training_args.seed)\n",
    "\n",
    "    raw_datasets = load_datasets(config[\"datasets\"])\n",
    "\n",
    "    tokenizer = load_tokenizer(config[\"tokenizer\"])\n",
    "\n",
    "    tokenized_datasets = tokenize_dataset(raw_datasets, tokenizer)\n",
    "\n",
    "    tokenized_datasets = group_dataset(config[\"model\"], tokenized_datasets, tokenizer)\n",
    "\n",
    "    model = load_model(config[\"model\"])\n",
    "\n",
    "    trainer = get_trainer(training_args, tokenized_datasets, tokenizer, model)\n",
    "\n",
    "    result = trainer.train()\n",
    "    trainer.save_model()\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Training Function\n",
    "\n",
    "The `main` function sets up the distributed training environment using Ray and starts the training process. To enable training using HPU, we only need to make the following changes:\n",
    "- Set the exectuion mode for training, supported execution mode are:\n",
    "\n",
    "    - \"lazy\": Deferred execution of graphs, comprising of ops delivered from script op by op similar to Eager mode. It gives the Eager mode experience with performance on Gaudi. Unlike Eager Mode with torch.compile, graph is analyzed in each iteration leading to a higher CPU usage.\n",
    "    - \"eager\": Op-by-op execution as defined in standard PyTorch Eager mode scripts.\n",
    "    - \"eager.compile\": Eager mode extended with `torch.compile` - Similar to Eager mode but extended with wrapping complete or part of model (such as a function) into a graph. Parts that are not wrapped are executed eagerly.\n",
    "\n",
    "    More detail theory can be found [here](https://docs.habana.ai/en/latest/PyTorch/Reference/PyTorch_Gaudi_Theory_of_Operations.html), and detail performance results can be found [here](https://developer.habana.ai/get-started/habana-models-performance/)\n",
    "- Require an HPU for each worker in ScalingConfig\n",
    "- Set backend to `hccl` in TorchConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(num_workers, execution_mode):\n",
    "    import ray\n",
    "    from ray.train import ScalingConfig\n",
    "    from ray.train.torch import TorchTrainer, TorchConfig\n",
    "\n",
    "    pretrain_config = {\n",
    "        \"datasets\": {\n",
    "            \"name\": \"wikitext\",\n",
    "            \"config_name\": \"wikitext-2-raw-v1\",\n",
    "        },\n",
    "        \"tokenizer\": {\n",
    "            \"name\": \"huggyllama/llama-7b\",\n",
    "            \"config\": {}\n",
    "        },\n",
    "        \"model\": {\n",
    "            \"name\": \"huggyllama/llama-7b\",\n",
    "            \"config\": {\n",
    "                \"torch_dtype\": \"bfloat16\",\n",
    "            },\n",
    "        },\n",
    "        \"training_args\": {\n",
    "            \"per_device_train_batch_size\": 1,\n",
    "            \"do_train\": True,\n",
    "            \"save_strategy\": \"no\",\n",
    "            \"output_dir\": \"/tmp/ray/pretrain-llama-2\",\n",
    "            \"logging_steps\": 1,\n",
    "            \"gaudi_config_name\": \"Habana/llama\",\n",
    "            \"use_habana\": True,\n",
    "            \"throughput_warmup_steps\": 3,\n",
    "            \"use_lazy_mode\": True,\n",
    "            \"overwrite_output_dir\": True,\n",
    "            \"seed\": 42,\n",
    "            \"bf16\": True,\n",
    "            \"report_to\":'tensorboard',\n",
    "            \"deepspeed\": {\n",
    "                \"steps_per_print\": 64,\n",
    "                \"train_batch_size\": \"auto\",\n",
    "                \"train_micro_batch_size_per_gpu\": \"auto\",\n",
    "                \"gradient_accumulation_steps\": \"auto\",\n",
    "                \"bf16\": {\n",
    "                    \"enabled\": True\n",
    "                },\n",
    "                \"gradient_clipping\": 1.0,\n",
    "                \"zero_optimization\": {\n",
    "                    \"stage\": 3,\n",
    "                    \"overlap_comm\": False,\n",
    "                    \"reduce_scatter\": False,\n",
    "                    \"contiguous_gradients\": False,\n",
    "                    \"stage3_gather_16bit_weights_on_model_save\": True\n",
    "                }\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "\n",
    "    # if execution mode is eager with compile, must spcified with a compile backend\n",
    "    if execution_mode == \"eager.compile\":\n",
    "        pretrain_config[\"training_args\"].update({\"torch_compile_backend\": \"hpu_backend\"})\n",
    "\n",
    "    scaling_config = ScalingConfig(num_workers=num_workers,\n",
    "                                   use_gpu=False,\n",
    "                                   resources_per_worker={\"CPU\": 1, \"HPU\": 1})\n",
    "\n",
    "    # Set backend to hccl in TorchConfig\n",
    "    torch_config = TorchConfig(backend=\"hccl\")\n",
    "\n",
    "    # Workaround https://github.com/ray-project/ray/issues/45302 by explictly setting HPU resource\n",
    "    ray.init(resources={\"HPU\": 8})\n",
    "\n",
    "    # Initialize a Ray TorchTrainer\n",
    "    trainer = TorchTrainer(\n",
    "        train_loop_per_worker=pretrain_llama,\n",
    "        train_loop_config=pretrain_config,\n",
    "        torch_config=torch_config,\n",
    "        scaling_config=scaling_config\n",
    "    )\n",
    "\n",
    "    result = trainer.fit()\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Training\n",
    "\n",
    "Finally, we call the `main` function to start the pre-training process.\n",
    "\n",
    "Before calling `main` function, you must set some environment variables.\n",
    "\n",
    "1. The visiable devices. Environment variable `HABANA_VISIBLE_DEVICES` and `HABANA_VISIBLE_MODULES` are used to control the HPU device visiable to application, you must set this two environment variable properly. For more detail usage of `HABANA_VISIBLE_DEVICES`, `HABANA_VISIBLE_MODULES`, please visit [here](https://docs.habana.ai/en/latest/PyTorch/Reference/PT_Multiple_Tenants_on_HPU/Multiple_Dockers_each_with_Single_Workload.html)\n",
    "\n",
    "2. The execution mode. Different execution mode has different runtime performance. The default execution mode is lazy mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set some environment variables\n",
    "os.environ[\"RAY_EXPERIMENTAL_NOSET_HABANA_VISIBLE_MODULES\"] = \"0\"\n",
    "# if using RAY_EXPERIMENTAL_NOSET_HABANA_VISIBLE_MODULES env var\n",
    "# you must set HABANA_VISIBLE_MODULES, such as\n",
    "# os.environ[\"HABANA_VISIBLE_MODULES\"] = \"0,1,2,3\"\n",
    "\n",
    "# execution_mode are [\"lazy\", \"eager\", \"eager.compile\"]\n",
    "execution_mode = \"lazy\"\n",
    "os.environ[\"PT_HPU_LAZY_MODE\"] = \"1\" if execution_mode == \"lazy\" else \"0\"\n",
    "\n",
    "main(num_workers=4, execution_mode=execution_mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Possible outputs\n",
    "\n",
    "``` text\n",
    "\n",
    "...\n",
    "\n",
    "(RayTrainWorker pid=17173) Setting up process group for: env:// [rank=0, world_size=4]\n",
    "(RayTrainWorker pid=17173) [2025-11-17 23:11:35,362] [INFO] [real_accelerator.py:225:get_accelerator] Setting ds_accelerator to hpu (auto detect)\n",
    "(RayTrainWorker pid=17176) /usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
    "(RayTrainWorker pid=17176)   warnings.warn(\n",
    "(RayTrainWorker pid=17176) Initializing conditional components...\n",
    "(RayTrainWorker pid=17176) Using HPU fused kernel for apply_rotary_pos_emb\n",
    "(RayTrainWorker pid=17176) Using HPU fused kernel for RMSNorm\n",
    "(RayTrainWorker pid=17176) Using HPU fused kernel for apply_rotary_pos_emb\n",
    "(RayTrainWorker pid=17176) Using HPU fused kernel for RMSNorm\n",
    "(TrainController pid=16640) Started training worker group of size 4: \n",
    "(TrainController pid=16640) - (ip=100.83.67.100, pid=17173) world_rank=0, local_rank=0, node_rank=0\n",
    "(TrainController pid=16640) - (ip=100.83.67.100, pid=17175) world_rank=1, local_rank=1, node_rank=0\n",
    "(TrainController pid=16640) - (ip=100.83.67.100, pid=17174) world_rank=2, local_rank=2, node_rank=0\n",
    "(TrainController pid=16640) - (ip=100.83.67.100, pid=17176) world_rank=3, local_rank=3, node_rank=0\n",
    "\n",
    "...\n",
    "\n",
    "(RayTrainWorker pid=17173) ============================= HPU PT BRIDGE CONFIGURATION ON RANK = 0 ============= \n",
    "(RayTrainWorker pid=17173)  PT_HPU_LAZY_MODE = 1\n",
    "(RayTrainWorker pid=17173)  PT_HPU_RECIPE_CACHE_CONFIG = ,false,1024,false\n",
    "(RayTrainWorker pid=17173)  PT_HPU_MAX_COMPOUND_OP_SIZE = 9223372036854775807\n",
    "(RayTrainWorker pid=17173)  PT_HPU_LAZY_ACC_PAR_MODE = 0\n",
    "(RayTrainWorker pid=17173)  PT_HPU_ENABLE_REFINE_DYNAMIC_SHAPES = 0\n",
    "(RayTrainWorker pid=17173)  PT_HPU_EAGER_PIPELINE_ENABLE = 1\n",
    "(RayTrainWorker pid=17173)  PT_HPU_EAGER_COLLECTIVE_PIPELINE_ENABLE = 1\n",
    "(RayTrainWorker pid=17173)  PT_HPU_ENABLE_LAZY_COLLECTIVES = 0\n",
    "(RayTrainWorker pid=17173) ---------------------------: System Configuration :---------------------------\n",
    "(RayTrainWorker pid=17173) Num CPU Cores : 160\n",
    "(RayTrainWorker pid=17173) CPU RAM       : 1007 GB\n",
    "(RayTrainWorker pid=17173) ------------------------------------------------------------------------------\n",
    "\n",
    "...\n",
    "\n",
    "(RayTrainWorker pid=17173) {'loss': 11.1667, 'grad_norm': 15.475007057189941, 'learning_rate': 5e-05, 'epoch': 0.0, 'memory_allocated (GB)': np.float64(28.87), 'max_memory_allocated (GB)': np.float64(55.38), 'total_memory_available (GB)': np.float64(94.62)}\n",
    "(RayTrainWorker pid=17173) {'loss': 11.0627, 'grad_norm': 15.35694694519043, 'learning_rate': 4.9951690821256045e-05, 'epoch': 0.01, 'memory_allocated (GB)': np.float64(28.87), 'max_memory_allocated (GB)': np.float64(73.24), 'total_memory_available (GB)': np.float64(94.62)}\n",
    "(RayTrainWorker pid=17173) {'loss': 10.9507, 'grad_norm': 15.467569351196289, 'learning_rate': 4.990338164251208e-05, 'epoch': 0.01, 'memory_allocated (GB)': np.float64(28.87), 'max_memory_allocated (GB)': np.float64(76.72), 'total_memory_available (GB)': np.float64(94.62)}\n",
    "(RayTrainWorker pid=17173) {'loss': 10.6487, 'grad_norm': 14.865588188171387, 'learning_rate': 4.985507246376812e-05, 'epoch': 0.01, 'memory_allocated (GB)': np.float64(28.87), 'max_memory_allocated (GB)': np.float64(76.75), 'total_memory_available (GB)': np.float64(94.62)}\n",
    "(RayTrainWorker pid=17173) {'loss': 10.1784, 'grad_norm': 13.968029022216797, 'learning_rate': 4.980676328502415e-05, 'epoch': 0.01, 'memory_allocated (GB)': np.float64(28.87), 'max_memory_allocated (GB)': np.float64(76.75), 'total_memory_available (GB)': np.float64(94.62)}\n",
    "(RayTrainWorker pid=17173) {'loss': 9.9406, 'grad_norm': 16.147335052490234, 'learning_rate': 4.9758454106280194e-05, 'epoch': 0.02, 'memory_allocated (GB)': np.float64(28.87), 'max_memory_allocated (GB)': np.float64(76.77), 'total_memory_available (GB)': np.float64(94.62)}\n",
    "(RayTrainWorker pid=17173) {'loss': 9.8046, 'grad_norm': 18.3155574798584, 'learning_rate': 4.9710144927536237e-05, 'epoch': 0.02, 'memory_allocated (GB)': np.float64(28.87), 'max_memory_allocated (GB)': np.float64(76.77), 'total_memory_available (GB)': np.float64(94.62)}\n",
    "(RayTrainWorker pid=17173) {'loss': 9.6798, 'grad_norm': 13.360419273376465, 'learning_rate': 4.966183574879227e-05, 'epoch': 0.02, 'memory_allocated (GB)': np.float64(28.87), 'max_memory_allocated (GB)': np.float64(76.77), 'total_memory_available (GB)': np.float64(94.62)}\n",
    "(RayTrainWorker pid=17173) {'loss': 9.4443, 'grad_norm': 10.956599235534668, 'learning_rate': 4.9613526570048315e-05, 'epoch': 0.03, 'memory_allocated (GB)': np.float64(28.87), 'max_memory_allocated (GB)': np.float64(76.77), 'total_memory_available (GB)': np.float64(94.62)}\n",
    "(RayTrainWorker pid=17173) {'loss': 9.3399, 'grad_norm': 10.75390911102295, 'learning_rate': 4.956521739130435e-05, 'epoch': 0.03, 'memory_allocated (GB)': np.float64(28.87), 'max_memory_allocated (GB)': np.float64(76.77), 'total_memory_available (GB)': np.float64(94.62)}\n",
    "(RayTrainWorker pid=17173) {'loss': 8.6311, 'grad_norm': 9.340704917907715, 'learning_rate': 4.9516908212560386e-05, 'epoch': 0.03, 'memory_allocated (GB)': np.float64(28.87), 'max_memory_allocated (GB)': np.float64(76.77), 'total_memory_available (GB)': np.float64(94.62)}\n",
    "(RayTrainWorker pid=17173) {'loss': 10.0624, 'grad_norm': 49.417579650878906, 'learning_rate': 4.946859903381643e-05, 'epoch': 0.03, 'memory_allocated (GB)': np.float64(28.87), 'max_memory_allocated (GB)': np.float64(76.77), 'total_memory_available (GB)': np.float64(94.62)}\n",
    "(RayTrainWorker pid=17173) {'loss': 10.9374, 'grad_norm': 66.55072021484375, 'learning_rate': 4.9420289855072464e-05, 'epoch': 0.04, 'memory_allocated (GB)': np.float64(28.87), 'max_memory_allocated (GB)': np.float64(76.77), 'total_memory_available (GB)': np.float64(94.62)}\n",
    "(RayTrainWorker pid=17173) {'loss': 8.766, 'grad_norm': 13.2837495803833, 'learning_rate': 4.9371980676328506e-05, 'epoch': 0.04, 'memory_allocated (GB)': np.float64(28.87), 'max_memory_allocated (GB)': np.float64(76.77), 'total_memory_available (GB)': np.float64(94.62)}\n",
    "(RayTrainWorker pid=17173) {'loss': 8.8053, 'grad_norm': 26.362295150756836, 'learning_rate': 4.932367149758454e-05, 'epoch': 0.04, 'memory_allocated (GB)': np.float64(28.87), 'max_memory_allocated (GB)': np.float64(76.77), 'total_memory_available (GB)': np.float64(94.62)}\n",
    "(RayTrainWorker pid=17173) {'loss': 8.6358, 'grad_norm': 7.576889514923096, 'learning_rate': 4.9275362318840584e-05, 'epoch': 0.05, 'memory_allocated (GB)': np.float64(28.87), 'max_memory_allocated (GB)': np.float64(76.77), 'total_memory_available (GB)': np.float64(94.62)}\n",
    "(RayTrainWorker pid=17173) {'loss': 8.1603, 'grad_norm': 6.979973316192627, 'learning_rate': 4.922705314009662e-05, 'epoch': 0.05, 'memory_allocated (GB)': np.float64(28.87), 'max_memory_allocated (GB)': np.float64(76.77), 'total_memory_available (GB)': np.float64(94.62)}\n",
    "(RayTrainWorker pid=17173) {'loss': 8.473, 'grad_norm': 23.650888442993164, 'learning_rate': 4.9178743961352656e-05, 'epoch': 0.05, 'memory_allocated (GB)': np.float64(28.87), 'max_memory_allocated (GB)': np.float64(76.77), 'total_memory_available (GB)': np.float64(94.62)}\n",
    "(RayTrainWorker pid=17173) {'loss': 8.3837, 'grad_norm': 20.919010162353516, 'learning_rate': 4.91304347826087e-05, 'epoch': 0.06, 'memory_allocated (GB)': np.float64(28.87), 'max_memory_allocated (GB)': np.float64(76.77), 'total_memory_available (GB)': np.float64(94.62)}\n",
    "(RayTrainWorker pid=17173) {'loss': 8.5428, 'grad_norm': 20.735576629638672, 'learning_rate': 4.9082125603864734e-05, 'epoch': 0.06, 'memory_allocated (GB)': np.float64(28.87), 'max_memory_allocated (GB)': np.float64(76.77), 'total_memory_available (GB)': np.float64(94.62)}\n",
    "(RayTrainWorker pid=17173) {'loss': 8.1541, 'grad_norm': 8.866345405578613, 'learning_rate': 4.9033816425120776e-05, 'epoch': 0.06, 'memory_allocated (GB)': np.float64(28.87), 'max_memory_allocated (GB)': np.float64(76.77), 'total_memory_available (GB)': np.float64(94.62)}\n",
    "(RayTrainWorker pid=17173) {'loss': 7.883, 'grad_norm': 5.573055267333984, 'learning_rate': 4.898550724637682e-05, 'epoch': 0.06, 'memory_allocated (GB)': np.float64(28.87), 'max_memory_allocated (GB)': np.float64(76.77), 'total_memory_available (GB)': np.float64(94.62)}\n",
    "(RayTrainWorker pid=17173) {'loss': 7.7241, 'grad_norm': 8.445459365844727, 'learning_rate': 4.893719806763285e-05, 'epoch': 0.07, 'memory_allocated (GB)': np.float64(28.87), 'max_memory_allocated (GB)': np.float64(76.77), 'total_memory_available (GB)': np.float64(94.62)}\n",
    "(RayTrainWorker pid=17173) {'loss': 7.7785, 'grad_norm': 15.916473388671875, 'learning_rate': 4.888888888888889e-05, 'epoch': 0.07, 'memory_allocated (GB)': np.float64(28.87), 'max_memory_allocated (GB)': np.float64(76.77), 'total_memory_available (GB)': np.float64(94.62)}\n",
    "(RayTrainWorker pid=17173) {'loss': 7.6401, 'grad_norm': 5.650521755218506, 'learning_rate': 4.884057971014493e-05, 'epoch': 0.07, 'memory_allocated (GB)': np.float64(28.87), 'max_memory_allocated (GB)': np.float64(76.77), 'total_memory_available (GB)': np.float64(94.62)}\n",
    "(RayTrainWorker pid=17173) {'loss': 7.6179, 'grad_norm': 7.968504428863525, 'learning_rate': 4.879227053140097e-05, 'epoch': 0.08, 'memory_allocated (GB)': np.float64(28.87), 'max_memory_allocated (GB)': np.float64(76.77), 'total_memory_available (GB)': np.float64(94.62)}\n",
    "(RayTrainWorker pid=17173) {'loss': 7.6425, 'grad_norm': 4.692033290863037, 'learning_rate': 4.874396135265701e-05, 'epoch': 0.08, 'memory_allocated (GB)': np.float64(28.87), 'max_memory_allocated (GB)': np.float64(76.77), 'total_memory_available (GB)': np.float64(94.62)}\n",
    "(RayTrainWorker pid=17173) {'loss': 7.2658, 'grad_norm': 4.123305797576904, 'learning_rate': 4.8695652173913046e-05, 'epoch': 0.08, 'memory_allocated (GB)': np.float64(28.87), 'max_memory_allocated (GB)': np.float64(76.77), 'total_memory_available (GB)': np.float64(94.62)}\n",
    "(RayTrainWorker pid=17173) {'loss': 7.2754, 'grad_norm': 5.893979072570801, 'learning_rate': 4.864734299516908e-05, 'epoch': 0.08, 'memory_allocated (GB)': np.float64(28.87), 'max_memory_allocated (GB)': np.float64(76.77), 'total_memory_available (GB)': np.float64(94.62)}\n",
    "(RayTrainWorker pid=17173) {'loss': 7.1253, 'grad_norm': 7.051833152770996, 'learning_rate': 4.8599033816425124e-05, 'epoch': 0.09, 'memory_allocated (GB)': np.float64(28.87), 'max_memory_allocated (GB)': np.float64(76.77), 'total_memory_available (GB)': np.float64(94.62)}\n",
    "(RayTrainWorker pid=17173) {'loss': 7.4429, 'grad_norm': 6.333181381225586, 'learning_rate': 4.855072463768116e-05, 'epoch': 0.09, 'memory_allocated (GB)': np.float64(28.87), 'max_memory_allocated (GB)': np.float64(76.77), 'total_memory_available (GB)': np.float64(94.62)}\n",
    "\n",
    "\n",
    "...\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orphan": true
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
