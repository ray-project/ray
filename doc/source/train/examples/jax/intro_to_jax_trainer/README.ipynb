{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "0ebcd4f7",
      "metadata": {},
      "source": [
        "# Train a GPT-2 model with Ray Train `JaxTrainer`\n",
        "\n",
        "**Time to complete**: 15 min\n",
        "\n",
        "This template shows you how to distribute a JAX/Flax training loop with [Ray Train](https://docs.ray.io/en/latest/train/train.html)'s `JaxTrainer`. You’ll train a small GPT-2-style Transformer from scratch on the [OpenWebText](https://openwebtext2.readthedocs.io/en/latest/) dataset.\n",
        "\n",
        "Ray Train lets you keep your training code in a normal Python function, then runs that function on a set of Ray workers. `JaxTrainer` handles the orchestration (starting workers, setting up the distributed context, and collecting metrics/checkpoints) so you can focus on the model and the input pipeline.\n",
        "\n",
        "This tutorial is inspired by Andrej Karpathy’s [nanoGPT](https://github.com/karpathy/nanoGPT/tree/master) and Google’s [Train a GPT-2 model with JAX on TPU for free](https://developers.googleblog.com/en/train-gpt2-model-with-jax-on-tpu/).\n",
        "\n",
        "In this tutorial, you will:\n",
        "\n",
        "1. Prepare and the `OpenWebText` dataset and wrap with Ray Data.\n",
        "2. Define a basic GPT2 model and train step in Jax/Flax.\n",
        "3. Wrap the training loop in a `train_loop_per_worker` function and scale it out using Ray Train `JaxTrainer` with GPUs or TPUs!\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a9fe11b6",
      "metadata": {},
      "source": [
        "<div id=\"anyscale-note\" class=\"alert alert-block alert-warning\">\n",
        "\n",
        "  <strong>Anyscale specific configuration</strong>\n",
        "\n",
        "  <p><strong>Note:</strong> This tutorial is optimized for the Anyscale platform. When running on open source Ray, additional configuration is required. For example, you need to manually:</p>\n",
        "\n",
        "  <ul>\n",
        "    <li><strong>Configure your Ray cluster</strong>: Set up your multi-node environment and manage resource allocation without Anyscale's automation.</li>\n",
        "    <li><strong>Manage dependencies</strong>: Manually install and manage dependencies on each node.</li>\n",
        "    <li><strong>Set up storage</strong>: Configure your own distributed or shared storage system for model checkpointing.</li>\n",
        "  </ul>\n",
        "</div>\n",
        "\n",
        "<style>\n",
        "  div#anyscale-note > p,\n",
        "  div#anyscale-note > ul,\n",
        "  div#anyscale-note > ul li {\n",
        "    color: black;\n",
        "  }\n",
        "\n",
        "  div#anyscale-note {\n",
        "    background-color: rgb(255, 243, 205);\n",
        "  }\n",
        "\n",
        "  div#anyscale-note {\n",
        "    border: 1px solid #ccc; \n",
        "    border-radius: 8px;\n",
        "    padding: 15px;\n",
        "  }\n",
        "\n",
        "</style>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bf58e5fd",
      "metadata": {},
      "source": [
        "## Step 1: Install dependencies and prepare the dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1704e5cd",
      "metadata": {},
      "source": [
        "First, install the required Python packages.\n",
        "\n",
        "If you’re running on GPUs, install `jax[cuda]`. If you’re running on Google TPUs, install `jax[tpu]`. For platform-specific requirements, see the [JAX installation guide](https://docs.jax.dev/en/latest/installation.html).\n",
        "\n",
        "This notebook uses `jax[cuda]` in the examples for simplicity.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "39b71121",
      "metadata": {},
      "source": [
        "Run below if you plan to use GPUs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8a6d4fc",
      "metadata": {},
      "outputs": [],
      "source": [
        "%%bash\n",
        "pip install pandas numpy jax[cuda] flax tiktoken datasets transformers orbax optax\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d862bb66",
      "metadata": {},
      "source": [
        "Run below if you plan to use TPUs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5bf2907",
      "metadata": {},
      "outputs": [],
      "source": [
        "# %%bash\n",
        "# pip install pandas numpy jax[tpu] flax tiktoken datasets transformers orbax optax"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "193fafe8",
      "metadata": {},
      "source": [
        "Next, prepare the data that you’ll feed into the training loop.\n",
        "\n",
        "This notebook uses [OpenWebText](https://openwebtext2.readthedocs.io/en/latest/), an open reproduction of OpenAI’s (private) WebText. The goal of this step is to tokenize the dataset once and write it to disk as two files:\n",
        "\n",
        "- `train.bin`: training tokens.\n",
        "- `val.bin`: validation tokens.\n",
        "\n",
        "You can use Karpathy’s nanoGPT prep script ([`prepare.py`](https://github.com/karpathy/nanoGPT/blob/master/data/openwebtext/prepare.py)) or download preprocessed data from Kaggle ([OpenWebText GPT-2](https://www.kaggle.com/datasets/windmaple/openwebtext-gpt2)).\n",
        "\n",
        "If running on the Anyscale workspace, the following code adapts the nanoGPT approach and writes the output to the shared storage path used in an Anyscale workspace (`/mnt/cluster_storage`). If you already have `train.bin` and `val.bin`, you can skip this step.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "90dcb1ba",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import tiktoken\n",
        "from datasets import load_dataset # huggingface datasets\n",
        "\n",
        "# number of workers in .map() call\n",
        "# good number to use is ~order number of cpu cores // 2\n",
        "num_proc = 8\n",
        "storage_path = \"/mnt/cluster_storage/openwebtext\"\n",
        "\n",
        "# number of workers in load_dataset() call\n",
        "# best number might be different from num_proc above as it also depends on NW speed.\n",
        "# it is better than 1 usually though\n",
        "num_proc_load_dataset = num_proc\n",
        "\n",
        "enc = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "dataset = load_dataset(\"openwebtext\", num_proc=num_proc_load_dataset)\n",
        "\n",
        "# owt by default only contains the 'train' split, so create a test split\n",
        "split_dataset = dataset[\"train\"].train_test_split(test_size=0.0005, seed=2357, shuffle=True)\n",
        "split_dataset['val'] = split_dataset.pop('test') # rename the test split to val\n",
        "# we now want to tokenize the dataset. first define the encoding function (gpt2 bpe)\n",
        "def process(example):\n",
        "    ids = enc.encode_ordinary(example['text']) # encode_ordinary ignores any special tokens\n",
        "    ids.append(enc.eot_token) # add the end of text token, e.g. 50256 for gpt2 bpe\n",
        "    # note: I think eot should be prepended not appended... hmm. it's called \"eot\" though...\n",
        "    out = {'ids': ids, 'len': len(ids)}\n",
        "    return out\n",
        "\n",
        "# tokenize the dataset\n",
        "tokenized = split_dataset.map(\n",
        "    process,\n",
        "    remove_columns=['text'],\n",
        "    desc=\"tokenizing the splits\",\n",
        "    num_proc=num_proc,\n",
        ")\n",
        "\n",
        "# concatenate all the ids in each dataset into one large file we can use for training\n",
        "for split, dset in tokenized.items():\n",
        "    arr_len = np.sum(dset['len'], dtype=np.uint64)\n",
        "    filename = os.path.join(storage_path, f'{split}.bin')\n",
        "    dtype = np.uint16 # (can do since enc.max_token_value == 50256 is < 2**16)\n",
        "    arr = np.memmap(filename, dtype=dtype, mode='w+', shape=(arr_len,))\n",
        "    total_batches = 1024\n",
        "\n",
        "    idx = 0\n",
        "    for batch_idx in tqdm(range(total_batches), desc=f'writing {filename}'):\n",
        "        # Batch together samples for faster write\n",
        "        batch = dset.shard(num_shards=total_batches, index=batch_idx, contiguous=True).with_format('numpy')\n",
        "        arr_batch = np.concatenate(batch['ids'])\n",
        "        # Write into mmap\n",
        "        arr[idx : idx + len(arr_batch)] = arr_batch\n",
        "        idx += len(arr_batch)\n",
        "    arr.flush()\n",
        "\n",
        "# train.bin is ~18GB, val.bin ~8.8MB\n",
        "# train has ~9B tokens \n",
        "# val has ~4M tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d89cd3f4",
      "metadata": {},
      "source": [
        "After running the script, you should have two files in shared storage:\n",
        "\n",
        "1. Training dataset: `/mnt/cluster_storage/openwebtext/train.bin`\n",
        "2. Validation dataset: `/mnt/cluster_storage/openwebtext/val.bin`\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "72f1caa8",
      "metadata": {},
      "source": [
        "## Step 2: Load the dataset with Ray Data."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "75c698e5",
      "metadata": {},
      "source": [
        "Next, let's load these files with Ray Data. Ray Data can read and preprocess data in parallel, then shard it across workers so each training process streams its own batches. This keeps the input pipeline scalable as you add more GPUs or TPU hosts.\n",
        "\n",
        "For more details about Ray Data, check out the [Ray Data documentation](https://docs.ray.io/en/latest/data/data.html).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4f234bb",
      "metadata": {},
      "outputs": [],
      "source": [
        "import ray\n",
        "import ray.data\n",
        "\n",
        "def make_bin_xy_dataset(\n",
        "    bin_path: str,\n",
        "    seqlen: int,\n",
        "    *,\n",
        "    # How many sequences to generate per epoch-like pass.\n",
        "    # You can make this very large and then re-iterate over batches in the\n",
        "    # training loop.\n",
        "    num_sequences: int,\n",
        "    seed: int = 0,\n",
        "    dtype=np.uint16,\n",
        "    concurrency: int = 64,\n",
        "):\n",
        "    \"\"\"\n",
        "    Build a Ray Dataset of (x,y) sequences sampled randomly from a .bin token file.\n",
        "\n",
        "    Produces rows:\n",
        "      - x: int32[seqlen]\n",
        "      - y: int32[seqlen]\n",
        "    \"\"\"\n",
        "    if not os.path.exists(bin_path):\n",
        "        raise FileNotFoundError(bin_path)\n",
        "\n",
        "    # Open memmap on driver just to get length.\n",
        "    data = np.memmap(bin_path, dtype=dtype, mode=\"r\")\n",
        "    n = int(len(data))\n",
        "    if n <= seqlen + 1:\n",
        "        raise ValueError(f\"{bin_path} too small: len={n}, seqlen={seqlen}\")\n",
        "\n",
        "    rng = np.random.default_rng(seed)\n",
        "    # Each start index uses [i : i+seqlen+1]\n",
        "    starts = rng.integers(0, n - (seqlen + 1), size=num_sequences, dtype=np.int64)\n",
        "\n",
        "    # Create a dataset of start indices\n",
        "    ds = ray.data.from_items([{\"i\": int(i)} for i in starts])\n",
        "\n",
        "    def read_xy(batch):\n",
        "        # Open memmap inside worker process\n",
        "        mm = np.memmap(bin_path, dtype=dtype, mode=\"r\")\n",
        "        idx = batch[\"i\"].astype(np.int64, copy=False)\n",
        "\n",
        "        # Allocate fixed arrays\n",
        "        bs = idx.shape[0]\n",
        "        x = np.empty((bs, seqlen), dtype=np.int32)\n",
        "        y = np.empty((bs, seqlen), dtype=np.int32)\n",
        "\n",
        "        # Slice per row (still Python loop, but runs in parallel across Ray workers)\n",
        "        for j, start in enumerate(idx):\n",
        "            window = mm[start : start + seqlen + 1].astype(np.int32, copy=False)\n",
        "            x[j] = window[:-1]\n",
        "            y[j] = window[1:]\n",
        "\n",
        "        return {\"x\": x, \"y\": y}\n",
        "\n",
        "    # Batch reading for efficiency\n",
        "    ds = ds.map_batches(\n",
        "        read_xy,\n",
        "        batch_format=\"numpy\",\n",
        "        batch_size=32,\n",
        "        compute=ray.data.TaskPoolStrategy(size=concurrency),\n",
        "        zero_copy_batch=True,\n",
        "    )\n",
        "\n",
        "    return ds\n",
        "\n",
        "train_ds = make_bin_xy_dataset(\n",
        "        \"/mnt/cluster_storage/openwebtext/train.bin\",\n",
        "        seqlen=1024,\n",
        "        num_sequences=5_000_000,\n",
        "        seed=2357,\n",
        "    )\n",
        "val_ds = make_bin_xy_dataset(\n",
        "    \"/mnt/cluster_storage/openwebtext/val.bin\",\n",
        "    seqlen=1024,\n",
        "    num_sequences=5_000_000,\n",
        "    seed=2357,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "87878c88",
      "metadata": {},
      "source": [
        "## Step 3: Define a JAX/Flax GPT-2-style model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d3a679f",
      "metadata": {},
      "source": [
        "Now define the model and the core training step with JAX and Flax.\n",
        "\n",
        "The JAX ecosystem is modular: JAX provides the array programming and compilation primitives, and libraries such as [Flax](https://github.com/google/flax) (neural network building blocks), [Optax](https://github.com/google-deepmind/optax) (optimizers and losses), and [Orbax](https://github.com/google/orbax) (checkpointing) provide higher-level components. You’ll use all three in this tutorial.\n",
        "\n",
        "In this section, nothing is Ray-specific yet—you’re building a normal single-process JAX/Flax training step that you’ll scale out with `JaxTrainer` in the next section.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25f1fde5",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "from dataclasses import dataclass\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax.experimental import mesh_utils\n",
        "from jax.sharding import Mesh, PartitionSpec as P, NamedSharding\n",
        "\n",
        "import flax.nnx as nnx\n",
        "import optax\n",
        "import orbax.checkpoint as orbax\n",
        "\n",
        "import tiktoken\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "267cb3bf",
      "metadata": {},
      "outputs": [],
      "source": [
        "@dataclass(frozen=True)\n",
        "class TrainingConfig:\n",
        "    # Model config (GPT-2 base model configuration).\n",
        "    tokenizer = tiktoken.get_encoding(\"gpt2\")  # We use gpt2 tokenizer for this tutorial.\n",
        "    vocab_size = tokenizer.n_vocab\n",
        "\n",
        "    num_transformer_blocks: int = 12\n",
        "    seqlen: int = 1024\n",
        "    embed_dim: int = 768\n",
        "\n",
        "    num_heads: int = 12\n",
        "    dropout_rate: float = 0.1\n",
        "\n",
        "    dtype = jnp.bfloat16 # change to jnp.float32 for older GPUs\n",
        "    param_dtype = jnp.float32\n",
        "\n",
        "    @property\n",
        "    def feed_forward_dim(self) -> int:\n",
        "        return 4 * self.embed_dim\n",
        "\n",
        "    # Optimizer config.\n",
        "    init_learning_rate: float = 5e-4\n",
        "    weight_decay: float = 1e-1\n",
        "\n",
        "    # Training loop config.\n",
        "    global_batch_size: int = 32\n",
        "    max_steps: int = 10_000\n",
        "    log_every_n_steps: int = 10\n",
        "    val_every_n_steps: int = 100\n",
        "    checkpoint_every_n_steps: int = 100\n",
        "\n",
        "    # Data/config paths.\n",
        "    openwebtext_root: str = \"/mnt/cluster_storage/openwebtext\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "df1bdea2",
      "metadata": {},
      "source": [
        "This tutorial provides a GPT-2-style Transformer model implemented with [Flax NNX](https://flax.readthedocs.io/en/v0.8.3/experimental/nnx/index.html). The model code is standard JAX/Flax—Ray Train doesn’t require any special model wrappers.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d9103cb",
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Model Definitions (Safe to keep global) ---\n",
        "# Keep dtype settings in one place so the model code doesn't rely on undefined globals.\n",
        "dtype = TrainingConfig.dtype\n",
        "param_dtype = TrainingConfig.param_dtype\n",
        "\n",
        "\n",
        "def causal_attention_mask(seq_len):\n",
        "    return jnp.tril(jnp.ones((seq_len, seq_len)))\n",
        "\n",
        "class TransformerBlock(nnx.Module):\n",
        "    def __init__(self, embed_dim: int, num_heads: int, ff_dim: int, dropout_rate: float, rngs: nnx.Rngs):\n",
        "        self.layer_norm1 = nnx.LayerNorm(epsilon=1e-6,\n",
        "                                         num_features=embed_dim,\n",
        "                                         scale_init=nnx.with_partitioning(nnx.initializers.ones_init(), ('model',)),\n",
        "                                         bias_init=nnx.with_partitioning(nnx.initializers.zeros_init(), ('model',)),\n",
        "                                         dtype=dtype,\n",
        "                                         param_dtype=param_dtype,\n",
        "                                         rngs=rngs)\n",
        "        self.mha = nnx.MultiHeadAttention(num_heads=num_heads,\n",
        "                                          in_features=embed_dim,\n",
        "                                          kernel_init=nnx.with_partitioning(nnx.initializers.xavier_uniform(), (None, 'model')),\n",
        "                                          bias_init=nnx.with_partitioning(nnx.initializers.zeros_init(), ('model',)),\n",
        "                                          dtype=dtype,\n",
        "                                          param_dtype=param_dtype,\n",
        "                                          rngs=rngs)\n",
        "        self.dropout1 = nnx.Dropout(rate=dropout_rate, rngs=rngs)\n",
        "        self.layer_norm2 = nnx.LayerNorm(epsilon=1e-6,\n",
        "                                         num_features=embed_dim,\n",
        "                                         scale_init=nnx.with_partitioning(nnx.initializers.ones_init(), ('model',)),\n",
        "                                         bias_init=nnx.with_partitioning(nnx.initializers.zeros_init(), ('model',)),\n",
        "                                         dtype=dtype,\n",
        "                                         param_dtype=param_dtype,\n",
        "                                         rngs=rngs)\n",
        "        self.linear1 = nnx.Linear(in_features=embed_dim,\n",
        "                                  out_features=ff_dim,\n",
        "                                  kernel_init=nnx.with_partitioning(nnx.initializers.xavier_uniform(), (None, 'model')),\n",
        "                                  bias_init=nnx.with_partitioning(nnx.initializers.zeros_init(), ('model',)),\n",
        "                                  dtype=dtype,\n",
        "                                  param_dtype=param_dtype,\n",
        "                                  rngs=rngs)\n",
        "        self.linear2 = nnx.Linear(in_features=ff_dim,\n",
        "                                  out_features=embed_dim,\n",
        "                                  kernel_init=nnx.with_partitioning(nnx.initializers.xavier_uniform(), (None, 'model')),\n",
        "                                  bias_init=nnx.with_partitioning(nnx.initializers.zeros_init(), ('model',)),\n",
        "                                  dtype=dtype,\n",
        "                                  param_dtype=param_dtype,\n",
        "                                  rngs=rngs)\n",
        "        self.dropout2 = nnx.Dropout(rate=dropout_rate, rngs=rngs)\n",
        "\n",
        "    def __call__(self, inputs, training: bool = False):\n",
        "        input_shape = inputs.shape\n",
        "        bs, seq_len, emb_sz = input_shape\n",
        "        attention_output = self.mha(\n",
        "            inputs_q=self.layer_norm1(inputs),\n",
        "            mask=causal_attention_mask(seq_len),\n",
        "            decode=False,\n",
        "        )\n",
        "        x = inputs + self.dropout1(attention_output, deterministic=not training)\n",
        "        mlp_output = self.linear1(self.layer_norm2(x))\n",
        "        mlp_output = nnx.gelu(mlp_output)\n",
        "        mlp_output = self.linear2(mlp_output)\n",
        "        mlp_output = self.dropout2(mlp_output, deterministic=not training)\n",
        "        return x + mlp_output\n",
        "\n",
        "class TokenAndPositionEmbedding(nnx.Module):\n",
        "    def __init__(self, seqlen: int, vocab_size: int, embed_dim: int, rngs: nnx.Rngs):\n",
        "        self.token_emb = nnx.Embed(num_embeddings=vocab_size, features=embed_dim, dtype=dtype, param_dtype=param_dtype, rngs=rngs)\n",
        "        self.pos_emb = nnx.Embed(num_embeddings=seqlen, features=embed_dim, dtype=dtype, param_dtype=param_dtype, rngs=rngs)\n",
        "\n",
        "    def __call__(self, x):\n",
        "        positions = jnp.arange(0, x.shape[1])[None, :]\n",
        "        position_embedding = self.pos_emb(positions)\n",
        "        token_embedding = self.token_emb(x)\n",
        "        return self.token_emb, token_embedding+position_embedding\n",
        "\n",
        "class GPT2(nnx.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        seqlen: int,\n",
        "        vocab_size: int,\n",
        "        embed_dim: int,\n",
        "        num_heads: int,\n",
        "        dropout_rate: float,\n",
        "        feed_forward_dim: int,\n",
        "        num_transformer_blocks: int,\n",
        "        rngs: nnx.Rngs,\n",
        "    ):\n",
        "        self.embedding_layer = TokenAndPositionEmbedding(seqlen, vocab_size, embed_dim, rngs=rngs)\n",
        "        self.dropout = nnx.Dropout(rate=dropout_rate, rngs=rngs)\n",
        "        self.transformer_blocks = nnx.List([\n",
        "            TransformerBlock(embed_dim, num_heads, feed_forward_dim, dropout_rate, rngs=rngs)\n",
        "            for _ in range(num_transformer_blocks)\n",
        "        ])\n",
        "        self.layer_norm = nnx.LayerNorm(\n",
        "            epsilon=1e-6,\n",
        "            num_features=embed_dim,\n",
        "            scale_init=nnx.with_partitioning(nnx.initializers.ones_init(), (\"model\",)),\n",
        "            bias_init=nnx.with_partitioning(nnx.initializers.zeros_init(), (\"model\",)),\n",
        "            dtype=dtype,\n",
        "            param_dtype=param_dtype,\n",
        "            rngs=rngs,\n",
        "        )\n",
        "\n",
        "    def __call__(self, inputs, training: bool = False):\n",
        "        token_embedding, x = self.embedding_layer(inputs)\n",
        "        x = self.dropout(x, deterministic=not training)\n",
        "        for transformer_block in self.transformer_blocks:\n",
        "            x = transformer_block(x, training=training)\n",
        "        x = self.layer_norm(x)\n",
        "        outputs = token_embedding.attend(x)\n",
        "        return outputs\n",
        "\n",
        "\n",
        "def create_model(*, rngs: nnx.Rngs, config: TrainingConfig):\n",
        "    return GPT2(\n",
        "        seqlen=config.seqlen,\n",
        "        vocab_size=config.vocab_size,\n",
        "        embed_dim=config.embed_dim,\n",
        "        num_heads=config.num_heads,\n",
        "        dropout_rate=config.dropout_rate,\n",
        "        feed_forward_dim=config.feed_forward_dim,\n",
        "        num_transformer_blocks=config.num_transformer_blocks,\n",
        "        rngs=rngs,\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d453706",
      "metadata": {},
      "source": [
        "Next, define the `loss_fn_train`, `loss_fn_eval`, and `train_step` functions.\n",
        "\n",
        "`train_step()` computes the loss, takes gradients, and updates model parameters through the optimizer. The training loop will call this function repeatedly.\n",
        "\n",
        "For performance, this notebook JIT-compiles these functions with `@nnx.jit`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a819d3ba",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "@nnx.jit\n",
        "def loss_fn_train(model, batch):\n",
        "    logits = model(batch[0], training=True)\n",
        "    loss = optax.softmax_cross_entropy_with_integer_labels(\n",
        "        logits=logits, labels=batch[1]\n",
        "    ).mean()\n",
        "    return loss, logits\n",
        "\n",
        "\n",
        "@nnx.jit\n",
        "def loss_fn_eval(model, batch):\n",
        "    logits = model(batch[0], training=False)\n",
        "    loss = optax.softmax_cross_entropy_with_integer_labels(\n",
        "        logits=logits, labels=batch[1]\n",
        "    ).mean()\n",
        "    return loss, logits\n",
        "\n",
        "\n",
        "@nnx.jit\n",
        "def train_step(model, optimizer, metrics, batch):\n",
        "    grad_fn = nnx.value_and_grad(loss_fn_train, has_aux=True)\n",
        "    (loss, logits), grads = grad_fn(model, batch)\n",
        "    metrics.update(loss=loss, logits=logits, labels=batch[1])\n",
        "    optimizer.update(model, grads)\n",
        "    return loss\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d9cccbed",
      "metadata": {},
      "source": [
        "## Step 4: Wrap training logic in `train_loop_per_worker`\n",
        "\n",
        "Next, let's wrap the JAX training logic in a `train_loop_per_worker` function.\n",
        "\n",
        "Each Ray Train worker runs the same Python function with a different world rank, and Ray sets device visibility per worker (for example, one GPU per worker). Inside this function, you can:\n",
        "\n",
        "- Read the distributed context (`world_rank`, `world_size`).\n",
        "- Get the per-worker dataset shard (`train.get_dataset_shard(...)`) to stream batches.\n",
        "- Report metrics and checkpoints back to the trainer with `ray.train.report(...)`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f4543c82",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "import ray\n",
        "from ray import train\n",
        "from ray.train import Checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57ea38fa",
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_loop_per_worker(config_dict: dict) -> None:\n",
        "\n",
        "    config = TrainingConfig(**config_dict)\n",
        "\n",
        "    world_rank = ray.train.get_context().get_world_rank()\n",
        "    world_size = ray.train.get_context().get_world_size()\n",
        "    print(f\"Worker rank {world_rank}/{world_size} sees devices: {jax.devices()}\")\n",
        "\n",
        "\n",
        "    # Create a mesh per worker process. \n",
        "    device_mesh = mesh_utils.create_device_mesh((jax.process_count(), 1))\n",
        "    mesh = Mesh(device_mesh, axis_names=(\"data\", \"model\"))\n",
        "    data_sharding = NamedSharding(mesh, P(\"data\", None))\n",
        "    jax.set_mesh(mesh)\n",
        "\n",
        "\n",
        "    # Initialize the model locally.\n",
        "    # Include a dropout RNG stream so nnx.Dropout can run when training=True.\n",
        "    model = create_model(rngs=nnx.Rngs(params=0, dropout=1), config=config)\n",
        "\n",
        "\n",
        "    # We use Ray data to load the training and validation datasets.\n",
        "    train_it = ray.train.get_dataset_shard(\"train\")\n",
        "    val_it = ray.train.get_dataset_shard(\"val\")\n",
        "    if train_it is None or val_it is None:\n",
        "        raise RuntimeError(\"No Ray Train datasets provided. Pass datasets={...} to JaxTrainer.\")\n",
        "    \n",
        "    local_batch_size = config.global_batch_size // jax.process_count()\n",
        "    global_input_shape = (config.global_batch_size, config.seqlen)\n",
        "    \n",
        "    train_batches = iter(train_it.iter_batches(\n",
        "        batch_size=local_batch_size,\n",
        "        batch_format=\"numpy\",\n",
        "        prefetch_batches=2,\n",
        "        drop_last=True,\n",
        "    ))\n",
        "    val_batches = iter(val_it.iter_batches(\n",
        "        batch_size=local_batch_size,\n",
        "        batch_format=\"numpy\",\n",
        "        prefetch_batches=2,\n",
        "        drop_last=True,\n",
        "    ))\n",
        "\n",
        "    def make_global_batch(local_x: np.ndarray, local_y: np.ndarray, global_shape: tuple):\n",
        "        # jax.make_array_from_process_local_data automatically handles the transfer \n",
        "        # from host memory (numpy) to device memory.\n",
        "        global_x = jax.make_array_from_process_local_data(data_sharding, local_x, global_shape)\n",
        "        global_y = jax.make_array_from_process_local_data(data_sharding, local_y, global_shape)\n",
        "        return global_x, global_y\n",
        "\n",
        "    # Initialize the optimizer.\n",
        "    schedule = optax.cosine_decay_schedule(\n",
        "        init_value=config.init_learning_rate,\n",
        "        decay_steps=config.max_steps,\n",
        "    )\n",
        "    optax_chain = optax.chain(optax.adamw(learning_rate=schedule, weight_decay=config.weight_decay))\n",
        "    optimizer = nnx.Optimizer(model, optax_chain, wrt=nnx.Param)\n",
        "\n",
        "    checkpointer = orbax.PyTreeCheckpointer()\n",
        "    start_time = time.time()\n",
        "\n",
        "    train_metrics = nnx.metrics.Average('loss')\n",
        "    val_metrics = nnx.metrics.Average('val_loss')\n",
        "\n",
        "    for step in range(config.max_steps):\n",
        "        try:\n",
        "            local_batch = next(train_batches)\n",
        "        except StopIteration:\n",
        "            train_batches = iter(train_it.iter_batches(\n",
        "                batch_size=local_batch_size,\n",
        "                batch_format=\"numpy\",\n",
        "                prefetch_batches=2,\n",
        "                drop_last=True,\n",
        "            ))\n",
        "            local_batch = next(train_batches)\n",
        "        global_x, global_y = make_global_batch(local_batch[\"x\"], local_batch[\"y\"], global_input_shape)\n",
        "\n",
        "        train_loss = train_step(model, optimizer, train_metrics, (global_x, global_y))\n",
        "\n",
        "        if (step + 1) % config.log_every_n_steps == 0:\n",
        "            elapsed = time.time() - start_time\n",
        "            # Report metrics through Ray Train.\n",
        "            ray.train.report({\"step\": step + 1, \"train_loss\": float(train_loss), \"elapsed_s\": elapsed})\n",
        "            start_time = time.time()\n",
        "        \n",
        "        if (step + 1) % config.val_every_n_steps == 0:\n",
        "            try:\n",
        "                local_validation_batch = next(val_batches)\n",
        "            except StopIteration:\n",
        "                val_batches = iter(val_it.iter_batches(\n",
        "                    batch_size=local_batch_size,\n",
        "                    batch_format=\"numpy\",\n",
        "                    prefetch_batches=2,\n",
        "                    drop_last=True,\n",
        "                ))\n",
        "                local_validation_batch = next(val_batches)\n",
        "\n",
        "            global_val_input, global_val_target = make_global_batch(\n",
        "                local_validation_batch[\"x\"], \n",
        "                local_validation_batch[\"y\"], \n",
        "                global_input_shape\n",
        "            )\n",
        "            \n",
        "            loss, logits = loss_fn_eval(model, (global_val_input, global_val_target))\n",
        "            val_metrics.update(val_loss=loss, logits=logits)\n",
        "            val_loss = float(val_metrics.compute())\n",
        "            metrics = {\"step\": step + 1, \"train_loss\": float(train_loss),\"val_loss\": float(val_loss)}\n",
        "            \n",
        "            checkpoint = None\n",
        "            if (step + 1) % config.checkpoint_every_n_steps == 0:\n",
        "                \n",
        "                # Orbax checkpointing is a barrier.\n",
        "                train_state = nnx.to_pure_dict(nnx.state(model))\n",
        "                checkpoint_path = os.path.join(\"/mnt/cluster_storage/checkpoint/jax_gpt2_ray_data\", str(step + 1))\n",
        "                checkpointer.save(checkpoint_path, train_state)\n",
        "                \n",
        "                # Save a checkpoint and report validation metrics through Ray Train.\n",
        "                # The controller persists the checkpoint to the RunConfig storage path.\n",
        "                checkpoint = Checkpoint.from_directory(checkpoint_path) \n",
        "            if world_rank == 0:\n",
        "                train.report(metrics, checkpoint=checkpoint)\n",
        "            else:\n",
        "                train.report(metrics, checkpoint=None)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "36f206e9",
      "metadata": {},
      "source": [
        "## Step 5: Define the `ScalingConfig`\n",
        "\n",
        "Let's define the `ScalingConfig` that we want to scale the training process.  \n",
        "\n",
        "`JaxTrainer` now supports both GPU training and TPU training. \n",
        "\n",
        "For a walkthrough on configuring `ScalingConfig`, see [Get Started with Distributed Training using JAX](https://docs.ray.io/en/latest/train/getting-started-jax.html).   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "260fbd24",
      "metadata": {},
      "outputs": [],
      "source": [
        "from ray.train import ScalingConfig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1a1d22d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# In this example, we use 2 GPUs.\n",
        "scaling_config = ScalingConfig(\n",
        "    use_gpu=True,\n",
        "    num_workers=2,  # Change this to match on your GPU cluster setting, by default, each worker uses one GPU.\n",
        ")\n",
        "# If you plan to use TPUs, see an example below.\n",
        "# This ScalingConfig requires a KubeRay cluster configured for a TPU v6e 4x4 slice with 4 TPU VMs.\n",
        "# For more information about TPU clusters with Ray on Kubernetes, see the\n",
        "# KubeRay TPU guide: https://docs.ray.io/en/master/cluster/kubernetes/user-guides/tpu.html#kuberay-tpu\n",
        "# scaling_config = ScalingConfig(\n",
        "#     use_tpu=True,\n",
        "#     num_workers=4,\n",
        "#     topology=\"4x4\",\n",
        "#     accelerator_type=\"TPU-V6E\",\n",
        "#     resources_per_worker={\"TPU\": 4},\n",
        "# )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dadca9ae",
      "metadata": {},
      "source": [
        "## Step 6: Launch with `JaxTrainer`\n",
        "\n",
        "To run `train_loop_per_worker` on a Ray cluster, you'll construct a `JaxTrainer` with:\n",
        "\n",
        "- `train_loop_per_worker`: the training function you've defined earlier. Each Ray Train worker runs this function.\n",
        "- `train_loop_config`: a hyperparameter dictionary passed into the function.\n",
        "- `scaling_config`: the `ScalingConfig` you've defined earlier.\n",
        "- `datasets`: the Ray Datasets to ingest for training. Datasets are keyed by name (`{name: dataset}`). Each dataset can be accessed from within the `train_loop_per_worker` by calling `ray.train.get_dataset_shard(name)`. Sharding and additional configuration can be done by passing in a `dataset_config`.\n",
        "- `run_config`: runtime configuration including where to write outputs such as checkpoints.\n",
        "\n",
        "If your workers hit CUDA or XLA library load errors, clear `LD_LIBRARY_PATH` in the runtime env to avoid picking up incompatible system libraries.\n",
        "\n",
        "`trainer.fit` spawns a controller process to orchestrate the training run and worker processes to execute the JAX training code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42b00efd",
      "metadata": {},
      "outputs": [],
      "source": [
        "from ray.train import RunConfig\n",
        "from ray.train.v2.jax import JaxTrainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc12fdfb",
      "metadata": {},
      "outputs": [],
      "source": [
        "storage_path = \"/mnt/cluster_storage\"\n",
        "\n",
        "trainer = JaxTrainer(\n",
        "    train_loop_per_worker=train_loop_per_worker,\n",
        "    train_loop_config={\n",
        "        \"global_batch_size\": 32,\n",
        "    },\n",
        "    scaling_config=scaling_config,\n",
        "    run_config=RunConfig(\n",
        "        name=\"jax_gpt2\",\n",
        "        storage_path=storage_path,\n",
        "        # Make sure to unset ``LD_LIBRARY_PATH`` if you're using CUDA devices, \n",
        "        # since ``LD_LIBRARY_PATH`` can override the CUDA libraries.\n",
        "        worker_runtime_env={\"env_vars\": {\"LD_LIBRARY_PATH\": \"\"}},\n",
        "    ),\n",
        "    datasets={\"train\": train_ds, \"val\": val_ds},\n",
        ")\n",
        "\n",
        "result = trainer.fit()\n",
        "print(result)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a0d85e1",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "In this notebook, you:\n",
        "\n",
        "1. Prepared and the `OpenWebText` dataset and wrapped with Ray Data.\n",
        "2. Defined a basic GPT2 model and train step in Jax/Flax.\n",
        "3. Wrapped the training loop in a `train_loop_per_worker` function and scaled it out using Ray Train `JaxTrainer` with GPUs or TPUs!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c37928bd",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "orphan": true
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
