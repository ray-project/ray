(profiling)=
# Profiling
Profiling is one of the most important debugging tools to diagnose performance, out of memory, hanging, or other application issues.
Here is a list of common profiling tools you may use when debugging Ray applications. 
- CPU profiling
    - py-spy
- Memory profiling
    - memray
- GPU profiling
    - PyTorch Profiler
- Ray Task / Actor profiling
    - Ray Timeline

If Ray doesn't work with certain profiling tools, try running them without Ray to debug the issues.

## CPU profiling
CPU profiling for Driver and Worker processes. This helps you understand the CPU usage by different processes and debug unexpectedly high or low usage.

### py-spy
[py-spy](https://github.com/benfred/py-spy/tree/master) is a sampling profiler for Python programs. It lets you visualize what your Python program is spending time on without restarting the program or modifying the code in any way. 

:::{note}
   If you run Ray in a Docker container, you may run into permission errors when viewing the stack traces. Follow the [solution in the py-spy documentation](https://github.com/benfred/py-spy#how-do-i-run-py-spy-in-docker) to resolve it. If you are a KubeRay user, here is the guide for how to {ref}`configure KubeRay and apply the solution <kuberay-pyspy-integration>`.
:::

Here are the {ref}`steps to use py-spy with Ray <observability-debug-hangs>`.


## Memory profiling
Memory profiling for Driver and Worker processes. This helps you analyze memory allocations in applications, trace memory leaks, and debug high/low memory or out of memory issues.

### memray
memray is a memory profiler for Python. It can track memory allocations in Python code, in native extension modules, and in the Python interpreter itself.

Here are the {ref}`steps to profile the memory usage of Ray Tasks and Actors <memray-profiling>`.


## GPU profiling
GPU and GRAM profiling for your GPU workloads like distributed training. This helps you analyze performance and debug memory issues. 
- PyTorch profiler is supported out of box when used with Ray Train
- NVIDIA Nsight System is not natively supported yet. Leave your comments in this [feature request for Nisght System support](https://github.com/ray-project/ray/issues/19631).

### PyTorch Profiler
Here are the steps to use PyTorch Profiler with Ray Train:
- Follow the [PyTorch Profiler documentation]( https://pytorch.org/tutorials/intermediate/tensorboard_profiler_tutorial.html) to record events in your PyTorch code.
- {ref}`Convert your PyTorch script to a Ray Train script <train-pytorch>`. (no change to your profiler-related code)
- Run your Ray Train script
- Collect the profiling results from all the nodes (compared to 1 node in a non-distributed setting)
    - You may want to upload results on each Node to NFS or object storage like S3 so that you don't have to fetch results from each Node.
- Visualize the results with tools like Tensorboard.

## Ray Task / Actor profiling
Profiling the execution time of Ray Tasks and Actors. This helps you analyze performance, identify the stragglers, and understand the distribution of workloads.

### Ray Timeline
Open your Ray Job in Ray Dashboard and follow the {ref}`instructions to download and visualize the trace files <dashboard-timeline>` generated by Ray Timeline.