{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Introduction to Ray Train\n\nThis guide introduces distributed training with Ray Train. It demonstrates how to scale a PyTorch training loop from a single GPU to a cluster of GPUs, using Ray Data for efficient, scalable data ingestion.\n\nFor a conceptual overview of Ray Train, see the [Ray Train overview](https://docs.ray.io/en/latest/train/overview.html) and the main [Ray Train documentation](https://docs.ray.io/en/latest/train/train.html). For a broader set of topics and how-to guides, refer to the [Ray Train user guides](https://docs.ray.io/en/latest/train/user-guides.html).\n\n**Roadmap**\n1.  **Single GPU PyTorch**: A baseline implementation.\n2.  **Migrating to Ray Train**:\n    *   Model preparation\n    *   Data ingestion with Ray Data\n    *   Metrics and Checkpointing\n    *   Updating the training loop\n3.  **Launching the Job**: Configuring and running the distributed training.\n4.  **Inspecting Results**: Accessing metrics and checkpoints.\n5.  **Observability**: Monitoring your training.\n6.  **Fault Tolerance**: Automatic retries, elastic training, and mid-epoch resumption.\n7.  **Troubleshooting**: Diagnosing common issues.\n\n## Imports\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "import csv\n",
    "import datetime\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision.models import resnet18\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor, Normalize, Compose\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import ray\n",
    "import ray.train\n",
    "import ray.data\n",
    "from ray.train import ScalingConfig, RunConfig, Checkpoint, FailureConfig\n",
    "from ray.train.torch import TorchTrainer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note on Storage\n",
    "\n",
    "Throughout this tutorial, we use `/mnt/cluster_storage` to represent a shared storage location. This is because in a multi-node cluster, Ray workers running on different nodes cannot access the head node's local file system. To ensure all workers can read datasets and write checkpoints, you must use a [shared storage solution](https://docs.anyscale.com/configuration/storage#shared) (such as NFS or cloud object storage like S3) that is accessible from every node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Single GPU PyTorch\n",
    "\n",
    "First, let's look at a standard PyTorch training setup for the MNIST dataset on a single GPU.\n",
    "\n",
    "<div align=\"center\"><img src=\"https://anyscale-public-materials.s3.us-west-2.amazonaws.com/ray-ai-libraries/diagrams/single_gpu_pytorch_v3.png\" width=\"90%\" loading=\"lazy\"></div>\n",
    "\n",
    "### Model and Data\n",
    "\n",
    "Download and preview the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MNIST(root=\"/mnt/cluster_storage/data\", train=True, download=True)\n",
    "\n",
    "fig, axs = plt.subplots(1, 10, figsize=(20, 2))\n",
    "for i in range(10):\n",
    "    img, label = dataset[i]\n",
    "    axs[i].imshow(img, cmap=\"gray\")\n",
    "    axs[i].axis(\"off\")\n",
    "    axs[i].set_title(label)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Standard Training Loop\n",
    "\n",
    "Build a ResNet18 model adapted for grayscale MNIST images (1 channel).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model() -> nn.Module:\n",
    "    model = resnet18(num_classes=10)\n",
    "    model.conv1 = nn.Conv2d(\n",
    "        in_channels=1, \n",
    "        out_channels=64,\n",
    "        kernel_size=(7, 7),\n",
    "        stride=(2, 2),\n",
    "        padding=(3, 3),\n",
    "        bias=False,\n",
    "    )\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a DataLoader for the MNIST dataset with normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_loader(batch_size: int = 128) -> DataLoader:\n",
    "    transform = Compose([ToTensor(), Normalize((0.5,), (0.5,))])\n",
    "    dataset = MNIST(root=\"/mnt/cluster_storage/data\", train=True, download=True, transform=transform)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log and print training metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_metrics_torch(loss: torch.Tensor, epoch: int) -> dict:\n",
    "    metrics = {\"loss\": loss.item(), \"epoch\": epoch}\n",
    "    print(metrics)\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save training metrics to a CSV file and the model state to a checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint_and_metrics_torch(metrics: dict, model: nn.Module, local_path: str) -> None:\n",
    "    os.makedirs(local_path, exist_ok=True)\n",
    "    \n",
    "    # Save metrics to CSV\n",
    "    with open(os.path.join(local_path, \"metrics.csv\"), \"a\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(metrics.values())\n",
    "\n",
    "    # Save model checkpoint\n",
    "    torch.save(model.state_dict(), os.path.join(local_path, \"model.pt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the training loop on a single GPU (or CPU) for the specified number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_func_single_gpu(num_epochs: int = 2, local_path: str = \"/mnt/cluster_storage/single_gpu_mnist\") -> None:\n",
    "    # Setup\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = build_model().to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-5)\n",
    "    dataloader = get_data_loader()\n",
    "    \n",
    "    # Training loop\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        for images, labels in dataloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        # Report and save\n",
    "        metrics = report_metrics_torch(loss, epoch)\n",
    "        save_checkpoint_and_metrics_torch(metrics, model, local_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run this training function on a worker node, we can use a Ray task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote(num_gpus=1, resources={\"accelerator_type:T4\": 0.0001})\n",
    "def run_single_gpu_job() -> None:\n",
    "    train_func_single_gpu()\n",
    "\n",
    "ray.init(ignore_reinit_error=True) # Initialize Ray; no-op if already initialized\n",
    "ray.get(run_single_gpu_job.remote())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Inspecting Single GPU Results\n",
    "\n",
    "After the training job finishes, we can inspect the metrics and verify the model's performance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the output files\n",
    "output_dir = \"/mnt/cluster_storage/single_gpu_mnist\"\n",
    "if os.path.exists(output_dir):\n",
    "    print(f\"Training output contents: {os.listdir(output_dir)}\")\n",
    "\n",
    "# Read and display metrics\n",
    "metrics_path = os.path.join(output_dir, \"metrics.csv\")\n",
    "if os.path.exists(metrics_path):\n",
    "    metrics_df = pd.read_csv(metrics_path, names=[\"loss\", \"epoch\"])\n",
    "    print(metrics_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load model and run inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.path.join(output_dir, \"model.pt\")\n",
    "if os.path.exists(model_path):\n",
    "    # Load the trained model\n",
    "    loaded_model = build_model()\n",
    "    # Load to CPU for inspection\n",
    "    loaded_model.load_state_dict(torch.load(model_path, map_location=\"cpu\"))\n",
    "    loaded_model.eval()\n",
    "\n",
    "    # Prepare test data\n",
    "    test_dataset = MNIST(root=\"/mnt/cluster_storage/data\", train=False, download=True)\n",
    "    transform = Compose([ToTensor(), Normalize((0.5,), (0.5,))])\n",
    "\n",
    "    # Visualize predictions\n",
    "    fig, axs = plt.subplots(1, 10, figsize=(20, 2))\n",
    "    for i in range(10):\n",
    "        img, label = test_dataset[i]\n",
    "        axs[i].imshow(img, cmap=\"gray\")\n",
    "        axs[i].axis(\"off\")\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Apply transform and add batch dimension\n",
    "            img_tensor = transform(img).unsqueeze(0)\n",
    "            pred = loaded_model(img_tensor).argmax().item()\n",
    "            \n",
    "        axs[i].set_title(f\"Pred: {pred}\\nTrue: {label}\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Migrating to Ray Train\n",
    "\n",
    "Ray Train solves common challenges in scaling deep learning:\n",
    "*   **Scale**: Move from single GPU to multiple GPUs/nodes with minimal code changes.\n",
    "*   **Infrastructure**: Abstracts away cluster management and resource provisioning.\n",
    "*   **Observability**: Provides built-in dashboards for monitoring metrics, logs, and resource usage.\n",
    "*   **Reliability**: Features automatic fault tolerance to recover from worker or node failures.\n",
    "\n",
    "To migrate our PyTorch code to Ray Train, we need to adapt the model preparation, data loading, and the training loop.\n",
    "\n",
    "The goal is to scale the single-GPU setup to a distributed data-parallel architecture:\n",
    "<div align=\"center\"><img src=\"https://anyscale-public-materials.s3.us-west-2.amazonaws.com/ray-ai-libraries/diagrams/multi_gpu_pytorch_v4.png\" width=\"90%\" loading=\"lazy\"></div>\n",
    "\n",
    "At a high level, Ray Train uses a controller (trainer) process to coordinate a group of training worker processes. The [Ray Train overview](https://docs.ray.io/en/latest/train/overview.html) introduces the core concepts: training function, workers, scaling configuration, and trainer.\n",
    "<div align=\"center\"><img src=\"https://docs.ray.io/en/latest/_images/overview.png\" width=\"80%\" loading=\"lazy\"></div>\n",
    "\n",
    "### 2.1 Migrating the Model\n",
    "\n",
    "Use [`ray.train.torch.prepare_model`](https://docs.ray.io/en/latest/train/api/doc/ray.train.torch.prepare_model.html) to automatically wrap your model in `DistributedDataParallel` and move it to the correct device.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model() -> nn.Module:\n",
    "    model = resnet18(num_classes=10)\n",
    "    # Adjust first layer for grayscale MNIST (1 channel)\n",
    "    model.conv1 = nn.Conv2d(\n",
    "        in_channels=1, \n",
    "        out_channels=64,\n",
    "        kernel_size=(7, 7),\n",
    "        stride=(2, 2),\n",
    "        padding=(3, 3),\n",
    "        bias=False,\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def train_loop_per_worker(config: dict) -> None:\n",
    "    # 1. Prepare Model\n",
    "    model = build_model()\n",
    "    model = ray.train.torch.prepare_model(model) # Instead of model = model.to(\"cuda\")\n",
    "    \n",
    "    # ... rest of the loop\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 2.2 Migrating Data Ingestion (Ray Data)\n",
    "\n",
    "Instead of a PyTorch `DataLoader`, we will use Ray Data. With a few modifications, you can scale data preprocessing and training separately. For example, you can do the former with a pool of CPU workers and the latter with a pool of GPU workers.\n",
    "\n",
    "Ray Data addresses common data pipeline needs:\n",
    "\n",
    "*   **Consistent data loading**: Standardize ingestion across formats (Parquet, CSV, images) and sources.\n",
    "*   **Scalable preprocessing**: Run on-the-fly transformations (augmentations, tokenization) on a separate pool of CPU workers so your training GPUs don't stall.\n",
    "\n",
    "For efficient data loading in distributed settings, Ray Data handles sharding, streaming, and preprocessing data across the cluster, preventing the training from being bottlenecked by data ingestion. The architecture below shows how Ray Train (controller + workers) integrates with Ray Data and your storage layer:\n",
    "\n",
    "<img src=\"https://anyscale-materials.s3.us-west-2.amazonaws.com/ray-train-deep-dive/ray_train_v2_architecture.png\" width=\"100%\" loading=\"lazy\">\n",
    "\n",
    "<div align=\"center\"><a href=\"https://anyscale-materials.s3.us-west-2.amazonaws.com/ray-train-deep-dive/ray_train_v2_architecture.png\" target=\"_blank\">View full size image</a></div>\n",
    "\n",
    "\n",
    "\n",
    "#### Preparing the Dataset\n",
    "\n",
    "First, we download the MNIST dataset and save the raw data to a local Parquet file. This simulates saving raw data to shared storage, allowing us to perform random transformations (like augmentations) on-the-fly during training.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download MNIST\n",
    "dataset = MNIST(root=\"/mnt/cluster_storage/data\", train=True, download=True)\n",
    "df = pd.DataFrame({\n",
    "    \"image\": dataset.data.numpy().tolist(),\n",
    "    \"label\": dataset.targets.numpy()\n",
    "})\n",
    "ds = ray.data.from_pandas(df)\n",
    "mnist_path = os.path.abspath(\"/mnt/cluster_storage/train_data\") # must be a shared storage location in a multi-node cluster\n",
    "ds.write_parquet(mnist_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Reading and Transforming the Dataset\n",
    "\n",
    "We create a Ray Dataset and define preprocessing using standard torchvision transforms.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ray_dataset(path: str) -> ray.data.Dataset:\n",
    "    ds = ray.data.read_parquet(path, file_extensions=[\"parquet\"])\n",
    "    \n",
    "    def transform_images(row: dict) -> dict:\n",
    "        # Define the torchvision transform.\n",
    "        transform = Compose([ToTensor(), Normalize((0.5,), (0.5,))])\n",
    "        image_arr = np.array(row[\"image\"], dtype=np.uint8)\n",
    "        row[\"image\"] = transform(Image.fromarray(image_arr))\n",
    "        return row\n",
    "\n",
    "    # Apply the transform on-the-fly\n",
    "    return ds.map(transform_images)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "For more details and performance tips, see the [Data loading and preprocessing](https://docs.ray.io/en/latest/train/user-guides/data-loading-preprocessing.html) user guide.\n",
    "\n",
    "### 2.3 Reporting Metrics and Checkpoints\n",
    "\n",
    "Ray Train uses `ray.train.report()` to log metrics and report checkpoints to the Ray Train driver.\n",
    "\n",
    "*   **Metrics**: Dictionaries of values (e.g., loss, accuracy) passed to `report()` are logged. By default, Ray Train only reports metrics from the rank 0 worker.\n",
    "*   **Checkpoints**: Model states saved to a directory and passed as a `ray.train.Checkpoint`.\n",
    "\n",
    "**Key Behaviors**:\n",
    "1.  **Synchronization**: `ray.train.report()` acts as a global barrier. All workers must call it to ensure training stays in sync.\n",
    "2.  **Efficient Checkpointing**: To avoid redundant uploads in standard DDP, only the rank 0 worker should save the checkpoint to disk. Ray Train then automatically persists it to your configured storage.\n",
    "\n",
    "The following diagram shows this checkpoint lifecycle:\n",
    "\n",
    "<div align=\"center\"><img src=\"https://docs.ray.io/en/latest/_images/checkpoint_lifecycle.png\" width=\"95%\" loading=\"lazy\"></div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint_and_report_metrics(\n",
    "    model: torch.nn.Module, metrics: dict[str, float]\n",
    ") -> None:\n",
    "    with tempfile.TemporaryDirectory() as temp_checkpoint_dir:\n",
    "        checkpoint = None\n",
    "\n",
    "        # checkpoint only from rank 0 worker\n",
    "        if ray.train.get_context().get_world_rank() == 0:\n",
    "            # Access the original model via `model.module` when wrapped in DistributedDataParallel\n",
    "            torch.save(\n",
    "                model.module.state_dict(), os.path.join(temp_checkpoint_dir, \"model.pt\")\n",
    "            )\n",
    "            checkpoint = ray.train.Checkpoint.from_directory(temp_checkpoint_dir)\n",
    "\n",
    "        # All workers must call report to synchronize\n",
    "        ray.train.report(\n",
    "            metrics,\n",
    "            checkpoint=checkpoint,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "For an in-depth guide on saving checkpoints and metrics, see the [Saving and Loading Checkpoints guide](https://docs.ray.io/en/latest/train/user-guides/checkpoints.html).\n",
    "\n",
    "### 2.4 Updating the Training Loop\n",
    "\n",
    "The following diagram shows how Ray Train and Ray Data work together: `TorchTrainer` launches a set of workers, and each worker reads its own shard of the training dataset.\n",
    "\n",
    "<div align=\"center\"><img src=\"https://anyscale-public-materials.s3.us-west-2.amazonaws.com/intro-ai-libraries/multi_gpu_pytorch_annotated.png\" width=\"90%\" loading=\"lazy\"></div>\n",
    "\n",
    "In code, each worker calls `ray.train.get_dataset_shard(\"train\")` to fetch its shard (from the `datasets` passed to `TorchTrainer`), then iterates over it with `iter_torch_batches`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop_per_worker(config: dict) -> None:\n",
    "    # 1. Setup Model\n",
    "    model = build_model()\n",
    "    model = ray.train.torch.prepare_model(model) # Instead of model = model.to(\"cuda\")\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config[\"lr\"])\n",
    "    \n",
    "    # 2. Calculate Batch Size\n",
    "    global_batch_size = config[\"batch_size\"]\n",
    "    world_size = ray.train.get_context().get_world_size()\n",
    "    per_worker_batch_size = global_batch_size // world_size\n",
    "\n",
    "    # 3. Setup Data (Ray Data)\n",
    "    # Get the data shard for this worker and create an iterator\n",
    "    dataset_shard = ray.train.get_dataset_shard(\"train\")\n",
    "    dataloader = dataset_shard.iter_torch_batches(\n",
    "        batch_size=per_worker_batch_size,\n",
    "        dtypes={\"image\": torch.float32, \"label\": torch.long},\n",
    "        device=ray.train.torch.get_device() # Auto-move to GPU\n",
    "    )\n",
    "\n",
    "    for epoch in range(config[\"epochs\"]):\n",
    "\n",
    "        # No longer need to ensure data is on the correct device\n",
    "        # dataloader.sampler.set_epoch(epoch)\n",
    "\n",
    "        for batch in dataloader:\n",
    "\n",
    "            # Note: Batches are dictionaries (from Ray Data), not tuples\n",
    "            inputs, labels = batch[\"image\"], batch[\"label\"]\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward() # gradients are now accumulated across the workers\n",
    "            optimizer.step()\n",
    "\n",
    "        # 4. Report Metrics & Checkpoint\n",
    "        metrics = {\"loss\": loss.item(), \"epoch\": epoch}\n",
    "        \n",
    "        save_checkpoint_and_report_metrics(model, metrics)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Launching the Distributed Job\n",
    "\n",
    "To launch the distributed training job, we need to configure:\n",
    "1.  [**Scaling Configuration**](https://docs.ray.io/en/latest/train/api/doc/ray.train.ScalingConfig.html): Defines the number of workers and compute resources (GPUs/CPUs) per worker.\n",
    "2.  [**Run Configuration**](https://docs.ray.io/en/latest/train/api/doc/ray.train.RunConfig.html): Specifies the storage location for checkpoints and experiment results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = get_ray_dataset(mnist_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure the `ScalingConfig`, `RunConfig`, and `TorchTrainer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaling_config = ScalingConfig(\n",
    "    num_workers=4,\n",
    "    use_gpu=True,\n",
    "    resources_per_worker={\"accelerator_type:T4\": 0.0001}\n",
    ")\n",
    "\n",
    "run_config = RunConfig(\n",
    "    name=f\"mnist_ray_train_demo_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}\",\n",
    "    storage_path=\"/mnt/cluster_storage/distributed_training\",\n",
    ")\n",
    "\n",
    "trainer = TorchTrainer(\n",
    "    train_loop_per_worker=train_loop_per_worker,\n",
    "    train_loop_config={\"lr\": 1e-5, \"batch_size\": 128, \"epochs\": 2},\n",
    "    scaling_config=scaling_config,\n",
    "    run_config=run_config,\n",
    "    datasets={\"train\": train_ds},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 4. Inspecting Results\n",
    "\n",
    "The `trainer.fit()` call returns a `Result` object containing metrics and checkpoint information. We can use this to load the trained model and generate predictions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Training finished. Result: {result}\")\n",
    "\n",
    "if result.checkpoint:\n",
    "    with result.checkpoint.as_directory() as ckpt_dir:\n",
    "        model_path = os.path.join(ckpt_dir, \"model.pt\")\n",
    "        print(f\"Checkpoint saved at: {model_path}\")\n",
    "        \n",
    "        # Load the model state dict\n",
    "        loaded_model = build_model()\n",
    "        state_dict = torch.load(model_path, map_location=\"cpu\")\n",
    "        loaded_model.load_state_dict(state_dict)\n",
    "        loaded_model.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MNIST(root=\"/mnt/cluster_storage/data\", train=False, download=True)\n",
    "transform = Compose([ToTensor(), Normalize((0.5,), (0.5,))])\n",
    "\n",
    "fig, axs = plt.subplots(1, 10, figsize=(20, 2))\n",
    "for i in range(10):\n",
    "    img, label = dataset[i]\n",
    "    axs[i].imshow(img, cmap=\"gray\")\n",
    "    axs[i].axis(\"off\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        img_tensor = transform(img).unsqueeze(0)\n",
    "        pred = loaded_model(img_tensor).argmax().item()\n",
    "    \n",
    "    axs[i].set_title(f\"Pred: {pred}\\nTrue: {label}\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "For more details on inspecting results, see [Inspecting training results](https://docs.ray.io/en/latest/train/user-guides/results.html).\n\n## 5. Observability\n\nRay provides built-in tools to monitor your training run and debug issues. See [Monitoring and Logging](https://docs.ray.io/en/latest/train/user-guides/monitoring-logging.html) for more details.\n\n### Monitor a Ray Train run in an Anyscale Workspace\nYou can monitor a Ray Train run from the workspace UI:\n\n- **Ray Train Workloads**: View worker status, inspect per-worker logs, and track training progress.\n\n  <img src=\"https://anyscale-public-materials.s3.us-west-2.amazonaws.com/intro-ai-libraries/ray-dashboard.png\" width=\"100%\" loading=\"lazy\">\n\n  <div align=\"center\"><a href=\"https://anyscale-public-materials.s3.us-west-2.amazonaws.com/intro-ai-libraries/ray-dashboard.png\" target=\"_blank\">View full size image</a></div><br>\n\n- **Metrics**: Monitor time-series charts for GPU utilization, GPU memory, network I/O, and disk I/O.\n\n  <div align=\"center\"><img src=\"https://anyscale-materials.s3.us-west-2.amazonaws.com/apple/gpu_util_and_disk.png\" width=\"60%\" loading=\"lazy\"></div><br>\n\n- **Ray Dashboard**: Debug the cluster (nodes, actors, tasks) when you need deeper system-level visibility.\n\n## 6. Fault Tolerance\n\nRay Train provides built-in fault tolerance to recover from worker failures (e.g., hardware failures, network failures, preemption).\n\n### Automatic Retries and Manual Restoration\n\nRay Train can automatically restart failed workers and resume training from the latest checkpoint.\n\n<div align=\"center\"><img src=\"https://anyscale-materials.s3.us-west-2.amazonaws.com/ray-train-deep-dive/fault_tolerance_train_v2.png\" width=\"90%\" loading=\"lazy\"></div><br>\n\nTwo things are required: enabling retries via `max_failures`, and implementing checkpoint loading in your training loop so restarts resume from saved progress rather than from scratch (see [Handling failures and node preemption](https://docs.ray.io/en/latest/train/user-guides/fault-tolerance.html)).\n\n```python\nrun_config = RunConfig(\n    name=\"mnist_ray_train_demo\",\n    ...,\n    failure_config=ray.train.FailureConfig(max_failures=3),\n)\n```\n\nIf the job is interrupted beyond `max_failures` (e.g., a driver crash), resume manually by re-executing the script with the same `RunConfig` (same `name` and `storage_path`).\n\n### Elastic Training\n\nStandard fault tolerance restarts with the same fixed worker count — which may not be possible if nodes were permanently lost or preempted. Elastic training lets the job continue with fewer workers and expand back when capacity returns, enabling spot instances and cutting cloud costs by up to 60%.\n\n```python\nelastic_scaling_config = ScalingConfig(\n    use_gpu=True,\n    num_workers=(2, 8),  # (min_workers, max_workers) — tuple instead of fixed count\n)\n```\n\nRay Train requests `max_workers` at startup, falls back in steps to `min_workers` if needed, restarts with surviving workers on node loss, and scales back up as capacity returns. Your training loop must implement [checkpoint loading](https://docs.ray.io/en/latest/train/user-guides/fault-tolerance.html) so restarts resume from progress rather than from scratch. Use gradient accumulation to keep the effective global batch size consistent across varying worker counts.\n\nFor the full walkthrough see the [Elastic Training guide](https://docs.anyscale.com/runtime/train) and [blog post](https://www.anyscale.com/blog/introducing-elastic-distributed-training).\n\n### Mid-Epoch Resumption\n\nWhen a job restarts after a mid-epoch failure, batches already processed in that epoch are replayed — some samples are seen twice, others not at all. Mid-epoch resumption checkpoints the dataset iterator position alongside model weights so training resumes from the exact batch where it stopped, ensuring each sample is seen exactly once per epoch.\n\n```python\n# Save iterator state alongside the model checkpoint\niterator_state = dataloader.state_dict()\ntorch.save(iterator_state, os.path.join(ckpt_dir, \"iterator_state.pt\"))\n\n# On resumption, restore the iterator to the saved position\ndataset_shard = ray.train.get_dataset_shard(\"train\", state_dict=iterator_state)\n```\n\nDatasets must include a unique row identifier and use only map-based transformations. Iterator state is written asynchronously with minimal overhead.\n\nFor setup requirements and the full implementation guide, see the [Mid-Epoch Resumption docs](https://docs.anyscale.com/runtime/mid-epoch-resumption).\n\n## 7. Troubleshooting\n\n### Diagnosis\n\n1.  **Persisted Logs**: When a failure occurs, check the error output in your terminal, or open the persisted worker application logs under the **Logs** tab in an Workspace, then find the traceback frame that points to your file and line number—this is usually the user-code line that caused the error.\n\n    ```text\n    File \"/home/ray/default/my_trainer.py\", line 34, in train_func\n    ```\n\n2.  **Ray observability**: Use Ray's built-in observability tools to spot resource issues and hangs:\n    - **Ray Dashboard**: Monitor GPU utilization and GPU memory.\n    - **Ray Train workload UI**: If GPU and CPU utilization stay near zero while you expect training to run, it often means one or more workers are hung.\n\n    <div align=\"center\"><img src=\"https://anyscale-public-materials.s3.us-west-2.amazonaws.com/intro-ai-libraries/low-utilization-hang.png\" width=\"95%\" loading=\"lazy\"></div>\n\n3.  **Ray Data Issues**: If you're using Ray Data with Ray Train, performance bottlenecks or OOM errors can originate in the data pipeline; See **Introduction to Ray Data** for data-related diagnosis steps.\n\n### Common Issues\n\n#### 1. Training Hangs\nTraining often hangs due to **rank-specific logic** or synchronization mismatches. Ensure that:\n*   `ray.train.report` is called by **all** workers at the same step.\n*   `ds.iter_torch_batches` is iterated by **all** workers simultaneously.\n\nIf your code is hanging, you might see this warning in the driver logs:\n```text\nStreamSplitDataIterator(epoch=1, split=1) blocked waiting on other clients for more than 30s.\n```\n\nOr a synchronization timeout from Ray Train:\n```text\n`ray.train.report` has not been called by all 4 workers in the group.\nThe workers have been waiting for 60.10 s for the following ranks to join the `ray.train.report` call: [1, 2, 3].\n```\n\nTo diagnose a Train worker hang, inspect the worker **stack trace** for `RayTrainWorker` under the Ray Train workload UI while the job is still running. The following screenshot shows an example stack trace from a hanging Train worker.\n\n<div align=\"center\"><img src=\"https://anyscale-public-materials.s3.us-west-2.amazonaws.com/intro-ai-libraries/stacktrace_from_worker.png\" width=\"95%\" loading=\"lazy\"></div>\n\n#### 2. Wrong CUDA Device\nHardcoding `torch.device(\"cuda:0\")` can cause errors when multiple workers share a node. Always use `ray.train.torch.get_device()` to assign the correct GPU.\n\n#### Other Tools\n\nFor deeper debugging, use the [**Ray Distributed Debugger**](https://docs.ray.io/en/latest/ray-observability/ray-distributed-debugger.html) to break into remote tasks and inspect failures post-mortem.\n\n## 8. Conclusion\n\nRay Train coupled with Ray Data provides a powerful stack for scaling deep learning:\n\n* **Simplicity**: Minimal code changes to migrate from single GPU.\n* **Scalability**: Seamlessly scale to many GPUs and nodes.\n* **Efficiency**: Ray Data ensures your GPUs are fed efficiently.\n* **Observability**: Built-in tools to monitor and debug distributed runs.\n\n### Next Steps\n\n- Explore more [Ray Train examples](https://docs.ray.io/en/latest/train/examples.html) for different frameworks and workloads.\n- Combine Ray Train with Ray Tune for hyperparameter optimization using the [Hyperparameter optimization](https://docs.ray.io/en/latest/train/user-guides/hyperparameter-optimization.html) guide.\n"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}