{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Ray Train and Ray Data\n",
    "\n",
    "This guide introduces distributed training with Ray Train. It demonstrates how to scale a PyTorch training loop from a single GPU to a cluster of GPUs, using Ray Data for efficient, scalable data ingestion.\n",
    "\n",
    "For a conceptual overview of Ray Train, see the [Ray Train overview](https://docs.ray.io/en/latest/train/overview.html) and the main [Ray Train documentation](https://docs.ray.io/en/latest/train/train.html). For a broader set of topics and how-to guides, refer to the [Ray Train user guides](https://docs.ray.io/en/latest/train/user-guides.html).\n",
    "\n",
    "**Roadmap**\n",
    "1.  **Single GPU PyTorch**: A baseline implementation.\n",
    "2.  **Migrating to Ray Train**:\n",
    "    *   Model preparation\n",
    "    *   Data ingestion with Ray Data\n",
    "    *   Metrics and Checkpointing\n",
    "    *   Updating the training loop\n",
    "3.  **Launching the Job**: Configuring and running the distributed training.\n",
    "4.  **Inspecting Results**: Accessing metrics and checkpoints.\n",
    "5.  **Observability**: Monitoring your training.\n",
    "6.  **Fault Tolerance**: Configuring automatic retries.\n",
    "7.  **Troubleshooting**: Diagnosing common issues.\n",
    "\n",
    "## Imports\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "import csv\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision.models import resnet18\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor, Normalize, Compose\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import ray\n",
    "import ray.train\n",
    "import ray.data\n",
    "from ray.train import ScalingConfig, RunConfig, Checkpoint, FailureConfig\n",
    "from ray.train.torch import TorchTrainer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Single GPU PyTorch\n",
    "\n",
    "First, let's look at a standard PyTorch training setup for the MNIST dataset on a single GPU.\n",
    "\n",
    "<img src=\"https://anyscale-public-materials.s3.us-west-2.amazonaws.com/ray-ai-libraries/diagrams/single_gpu_pytorch_v3.png\" width=\"800\" loading=\"lazy\">\n",
    "\n",
    "### Model and Data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and preview the dataset\n",
    "dataset = MNIST(root=\"/mnt/cluster_storage/data\", train=True, download=True)\n",
    "\n",
    "fig, axs = plt.subplots(1, 10, figsize=(20, 2))\n",
    "for i in range(10):\n",
    "    img, label = dataset[i]\n",
    "    axs[i].imshow(img, cmap=\"gray\")\n",
    "    axs[i].axis(\"off\")\n",
    "    axs[i].set_title(label)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model() -> nn.Module:\n",
    "    model = resnet18(num_classes=10)\n",
    "    # Adjust first layer for grayscale MNIST (1 channel)\n",
    "    model.conv1 = nn.Conv2d(\n",
    "        in_channels=1, \n",
    "        out_channels=64,\n",
    "        kernel_size=(7, 7),\n",
    "        stride=(2, 2),\n",
    "        padding=(3, 3),\n",
    "        bias=False,\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def get_data_loader(batch_size: int = 128) -> DataLoader:\n",
    "    transform = Compose([ToTensor(), Normalize((0.5,), (0.5,))])\n",
    "    dataset = MNIST(root=\"/mnt/cluster_storage/data\", train=True, download=True, transform=transform)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Standard Training Loop\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_metrics_torch(loss: torch.Tensor, epoch: int) -> dict:\n",
    "    metrics = {\"loss\": loss.item(), \"epoch\": epoch}\n",
    "    print(metrics)\n",
    "    return metrics\n",
    "\n",
    "def save_checkpoint_and_metrics_torch(metrics: dict, model: nn.Module, local_path: str) -> None:\n",
    "    os.makedirs(local_path, exist_ok=True)\n",
    "    \n",
    "    # Save metrics to CSV\n",
    "    with open(os.path.join(local_path, \"metrics.csv\"), \"a\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(metrics.values())\n",
    "\n",
    "    # Save model checkpoint\n",
    "    torch.save(model.state_dict(), os.path.join(local_path, \"model.pt\"))\n",
    "\n",
    "def train_func_single_gpu(num_epochs: int = 2, local_path: str = \"/mnt/cluster_storage/single_gpu_mnist\") -> None:\n",
    "    # Setup\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = build_model().to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-5)\n",
    "    dataloader = get_data_loader()\n",
    "    \n",
    "    # Training Loop\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        for images, labels in dataloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        # Report and Save\n",
    "        metrics = report_metrics_torch(loss, epoch)\n",
    "        save_checkpoint_and_metrics_torch(metrics, model, local_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To run this training function on a worker node, we can use a Ray task.\n",
    "@ray.remote(num_gpus=1, resources={\"accelerator_type:T4\": 0.0001})\n",
    "def run_single_gpu_job() -> None:\n",
    "    train_func_single_gpu()\n",
    "\n",
    "ray.init() # Ensure Ray is initialized\n",
    "ray.get(run_single_gpu_job.remote())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Inspecting Single GPU Results\n",
    "\n",
    "After the training job finishes, we can inspect the metrics and verify the model's performance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the output files\n",
    "output_dir = \"/mnt/cluster_storage/single_gpu_mnist\"\n",
    "if os.path.exists(output_dir):\n",
    "    print(f\"Training output contents: {os.listdir(output_dir)}\")\n",
    "\n",
    "# Read and display metrics\n",
    "metrics_path = os.path.join(output_dir, \"metrics.csv\")\n",
    "if os.path.exists(metrics_path):\n",
    "    metrics_df = pd.read_csv(metrics_path, names=[\"loss\", \"epoch\"])\n",
    "    print(metrics_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and run inference\n",
    "model_path = os.path.join(output_dir, \"model.pt\")\n",
    "if os.path.exists(model_path):\n",
    "    # Load the trained model\n",
    "    loaded_model = build_model()\n",
    "    # Load to CPU for inspection\n",
    "    loaded_model.load_state_dict(torch.load(model_path, map_location=\"cpu\"))\n",
    "    loaded_model.eval()\n",
    "\n",
    "    # Prepare test data\n",
    "    test_dataset = MNIST(root=\"/mnt/cluster_storage/data\", train=False, download=True)\n",
    "    transform = Compose([ToTensor(), Normalize((0.5,), (0.5,))])\n",
    "\n",
    "    # Visualize predictions\n",
    "    fig, axs = plt.subplots(1, 10, figsize=(20, 2))\n",
    "    for i in range(10):\n",
    "        img, label = test_dataset[i]\n",
    "        axs[i].imshow(img, cmap=\"gray\")\n",
    "        axs[i].axis(\"off\")\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Apply transform and add batch dimension\n",
    "            img_tensor = transform(img).unsqueeze(0)\n",
    "            pred = loaded_model(img_tensor).argmax().item()\n",
    "            \n",
    "        axs[i].set_title(f\"Pred: {pred}\\nTrue: {label}\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Migrating to Ray Train\n",
    "\n",
    "Ray Train solves common challenges in scaling deep learning:\n",
    "*   **Scale**: Move from single GPU to multiple GPUs/nodes with minimal code changes.\n",
    "*   **Infrastructure**: Abstracts away cluster management and resource provisioning.\n",
    "*   **Observability**: Provides built-in dashboards for monitoring metrics, logs, and resource usage.\n",
    "*   **Reliability**: Features automatic fault tolerance to recover from worker or node failures.\n",
    "\n",
    "To migrate our PyTorch code to Ray Train, we need to adapt the model preparation, data loading, and the training loop.\n",
    "\n",
    "The goal is to scale the single-GPU setup to a distributed data-parallel architecture:\n",
    "\n",
    "<img src=\"https://anyscale-public-materials.s3.us-west-2.amazonaws.com/ray-ai-libraries/diagrams/multi_gpu_pytorch_v4.png\" width=\"800\" loading=\"lazy\">\n",
    "\n",
    "At a high level, Ray Train uses a controller (trainer) process to coordinate a group of training worker processes. The [Ray Train overview](https://docs.ray.io/en/latest/train/overview.html) introduces the core concepts: training function, workers, scaling configuration, and trainer.\n",
    "\n",
    "<img src=\"https://docs.ray.io/en/latest/_images/overview.png\" width=\"700\" loading=\"lazy\">\n",
    "\n",
    "### 2.1 Migrating the Model\n",
    "\n",
    "Use [`ray.train.torch.prepare_model`](https://docs.ray.io/en/latest/train/api/doc/ray.train.torch.prepare_model.html) to automatically wrap your model in `DistributedDataParallel` and move it to the correct device.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop_per_worker(config: dict) -> None:\n",
    "    # 1. Prepare Model\n",
    "    model = build_model()\n",
    "    model = ray.train.torch.prepare_model(model) # Instead of model = model.to(\"cuda\")\n",
    "    \n",
    "    # ... rest of the loop\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 2.2 Migrating Data Ingestion (Ray Data)\n",
    "\n",
    "Instead of a PyTorch `DataLoader`, we will use Ray Data. With a few modifications, you can scale data preprocessing and training separately. For example, you can do the former with a pool of CPU workers and the latter with a pool of GPU workers.\n",
    "\n",
    "Use Ray Data when you face one of the following challenges:\n",
    "\n",
    "*   **Consistent Data Loading**: Standardize data ingestion across various formats (Parquet, CSV, images) and sources.\n",
    "*   **Scalable Preprocessing**: Perform on-the-fly transformations (augmentations, tokenization) on a separate pool of CPU workers to avoid stalling training GPUs.\n",
    "\n",
    "For efficient data loading in distributed settings, Ray Data handles sharding, streaming, and preprocessing data across the cluster, preventing the training from being bottlenecked by data ingestion. The architecture below shows how Ray Train (controller + workers) integrates with Ray Data and your storage layer:\n",
    "\n",
    "<img src=\"https://anyscale-materials.s3.us-west-2.amazonaws.com/ray-train-deep-dive/ray_train_v2_architecture.png\" width=\"800\" loading=\"lazy\">\n",
    "\n",
    "\n",
    "#### Preparing the Dataset\n",
    "\n",
    "First, we download the MNIST dataset and save the raw data to a local Parquet file. This simulates saving raw data to shared storage, allowing us to perform random transformations (like augmentations) on-the-fly during training.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download MNIST\n",
    "dataset = MNIST(root=\"/mnt/cluster_storage/data\", train=True, download=True)\n",
    "df = pd.DataFrame({\n",
    "    \"image\": dataset.data.numpy().tolist(),\n",
    "    \"label\": dataset.targets.numpy()\n",
    "})\n",
    "ds = ray.data.from_pandas(df)\n",
    "# NOTE: In a multi-node cluster, this path must be a shared storage location (e.g., S3, NFS) accessible by all workers.\n",
    "mnist_path = os.path.abspath(\"/mnt/cluster_storage/train_data\")\n",
    "ds.write_parquet(mnist_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Reading and Transforming the Dataset\n",
    "\n",
    "We create a Ray Dataset and define preprocessing using standard torchvision transforms.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ray_dataset(path: str) -> ray.data.Dataset:\n",
    "    ds = ray.data.read_parquet(path)\n",
    "    \n",
    "    def transform_images(row: dict) -> dict:\n",
    "        # Define the torchvision transform.\n",
    "        transform = Compose([ToTensor(), Normalize((0.5,), (0.5,))])\n",
    "        image_arr = np.array(row[\"image\"], dtype=np.uint8)\n",
    "        row[\"image\"] = transform(Image.fromarray(image_arr))\n",
    "        return row\n",
    "\n",
    "    # Apply the transform on-the-fly\n",
    "    return ds.map(transform_images)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "For more details and performance tips, see the [Data loading and preprocessing](https://docs.ray.io/en/latest/train/user-guides/data-loading-preprocessing.html) user guide.\n",
    "\n",
    "### 2.3 Reporting Metrics and Checkpoints\n",
    "\n",
    "Ray Train uses `ray.train.report()` to log metrics and report checkpoints to the Ray Train driver.\n",
    "\n",
    "*   **Metrics**: Dictionaries of values (e.g., loss, accuracy) passed to `report()` are logged. By default, Ray Train only reports metrics from the rank 0 worker.\n",
    "*   **Checkpoints**: Model states saved to a directory and passed as a `ray.train.Checkpoint`.\n",
    "\n",
    "**Key Behaviors**:\n",
    "1.  **Synchronization**: `ray.train.report()` acts as a global barrier. All workers must call it to ensure training stays in sync.\n",
    "2.  **Efficient Checkpointing**: To avoid redundant uploads in standard DDP, only the rank 0 worker should save the checkpoint to disk. Ray Train then automatically persists it to your configured storage.\n",
    "\n",
    "The following diagram shows this checkpoint lifecycle:\n",
    "\n",
    "<img src=\"https://docs.ray.io/en/latest/_images/checkpoint_lifecycle.png\" width=\"800\" loading=\"lazy\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint_and_report_metrics(\n",
    "    model: torch.nn.Module, metrics: dict[str, float]\n",
    ") -> None:\n",
    "    with tempfile.TemporaryDirectory() as temp_checkpoint_dir:\n",
    "        checkpoint = None\n",
    "\n",
    "        # checkpoint only from rank 0 worker\n",
    "        if ray.train.get_context().get_world_rank() == 0:\n",
    "            # Access the original model via `model.module` when wrapped in DistributedDataParallel\n",
    "            torch.save(\n",
    "                model.module.state_dict(), os.path.join(temp_checkpoint_dir, \"model.pt\")\n",
    "            )\n",
    "            checkpoint = ray.train.Checkpoint.from_directory(temp_checkpoint_dir)\n",
    "\n",
    "        # All workers must call report to synchronize\n",
    "        ray.train.report(\n",
    "            metrics,\n",
    "            checkpoint=checkpoint,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "For an in-depth guide on saving checkpoints and metrics, see the [Saving and Loading Checkpoints guide](https://docs.ray.io/en/latest/train/user-guides/checkpoints.html).\n",
    "\n",
    "### 2.4 Updating the Training Loop\n",
    "\n",
    "In the training loop, `ray.train.get_dataset_shard(\"train\")` automatically retrieves the data shard assigned to this worker (from the `datasets` passed to `TorchTrainer`). No manual sharding logic is required. We then iterate over the shard using `iter_torch_batches`.\n",
    "\n",
    "This pattern is also shown in the [Data loading and preprocessing](https://docs.ray.io/en/latest/train/user-guides/data-loading-preprocessing.html) guide.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop_per_worker(config: dict) -> None:\n",
    "    # 1. Setup Model\n",
    "    model = build_model()\n",
    "    model = ray.train.torch.prepare_model(model) # Instead of model = model.to(\"cuda\")\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config[\"lr\"])\n",
    "    \n",
    "    # 2. Calculate Batch Size\n",
    "    global_batch_size = config[\"batch_size\"]\n",
    "    world_size = ray.train.get_context().get_world_size()\n",
    "    per_worker_batch_size = global_batch_size // world_size\n",
    "\n",
    "    # 3. Setup Data (Ray Data)\n",
    "    # Get the data shard for this worker and create an iterator\n",
    "    dataset_shard = ray.train.get_dataset_shard(\"train\")\n",
    "    dataloader = dataset_shard.iter_torch_batches(\n",
    "        batch_size=per_worker_batch_size,\n",
    "        dtypes={\"image\": torch.float32, \"label\": torch.long},\n",
    "        device=ray.train.torch.get_device() # Auto-move to GPU\n",
    "    )\n",
    "\n",
    "    for epoch in range(config[\"epochs\"]):\n",
    "\n",
    "        # No longer need to ensure data is on the correct device\n",
    "        # dataloader.sampler.set_epoch(epoch)\n",
    "\n",
    "        for batch in dataloader:\n",
    "\n",
    "            # Note: Batches are dictionaries (from Ray Data), not tuples\n",
    "            inputs, labels = batch[\"image\"], batch[\"label\"]\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward() # gradients are now accumulated across the workers\n",
    "            optimizer.step()\n",
    "\n",
    "        # 4. Report Metrics & Checkpoint\n",
    "        metrics = {\"loss\": loss.item(), \"epoch\": epoch}\n",
    "        \n",
    "        save_checkpoint_and_report_metrics(model, metrics)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Launching the Distributed Job\n",
    "\n",
    "The following diagram illustrates the distributed training workflow. The `TorchTrainer` launches a set of workers, and each worker calls `ray.train.get_dataset_shard()` to receive its portion of the data stream from Ray Data.\n",
    "\n",
    "<img src=\"https://anyscale-public-materials.s3.us-west-2.amazonaws.com/intro-ai-libraries/multi_gpu_pytorch_annotated.png\" width=\"900\" loading=\"lazy\">\n",
    "\n",
    "To launch the distributed training job, we need to configure:\n",
    "1.  **Scaling Configuration**: Defines the number of workers and compute resources (GPUs/CPUs) per worker.\n",
    "2.  **Run Configuration**: Specifies the storage location for checkpoints and experiment results.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataset\n",
    "train_ds = get_ray_dataset(mnist_path)\n",
    "\n",
    "# Configure Scale (2 Workers)\n",
    "scaling_config = ScalingConfig(\n",
    "    num_workers=2,\n",
    "    use_gpu=True,\n",
    "    resources_per_worker={\"accelerator_type:T4\": 0.0001} # Explicitly requests a specific GPU type\n",
    ")\n",
    "\n",
    "# Configure Run (Storage path)\n",
    "run_config = RunConfig(\n",
    "    name=\"mnist_ray_train_demo\",\n",
    "    storage_path=\"/mnt/cluster_storage/distributed_training\" # must be a shared storage location in a multi-node cluster\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = TorchTrainer(\n",
    "    train_loop_per_worker=train_loop_per_worker,\n",
    "    train_loop_config={\"lr\": 1e-5, \"batch_size\": 128, \"epochs\": 2},\n",
    "    scaling_config=scaling_config,\n",
    "    run_config=run_config,\n",
    "    datasets={\"train\": train_ds}, # Inject Ray Data\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Training\n",
    "result = trainer.fit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 4. Inspecting Results\n",
    "\n",
    "The `trainer.fit()` call returns a `Result` object containing metrics and checkpoint information. We can use this to load the trained model and generate predictions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Training finished. Result: {result}\")\n",
    "\n",
    "if result.checkpoint:\n",
    "    with result.checkpoint.as_directory() as ckpt_dir:\n",
    "        model_path = os.path.join(ckpt_dir, \"model.pt\")\n",
    "        print(f\"Checkpoint saved at: {model_path}\")\n",
    "        \n",
    "        # Load the model state dict\n",
    "        loaded_model = build_model()\n",
    "        state_dict = torch.load(model_path, map_location=\"cpu\")\n",
    "        loaded_model.load_state_dict(state_dict)\n",
    "        loaded_model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions\n",
    "\n",
    "dataset = MNIST(root=\"/mnt/cluster_storage/data\", train=False, download=True)\n",
    "transform = Compose([ToTensor(), Normalize((0.5,), (0.5,))])\n",
    "\n",
    "fig, axs = plt.subplots(1, 10, figsize=(20, 2))\n",
    "for i in range(10):\n",
    "    img, label = dataset[i]\n",
    "    axs[i].imshow(img, cmap=\"gray\")\n",
    "    axs[i].axis(\"off\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        img_tensor = transform(img).unsqueeze(0)\n",
    "        pred = loaded_model(img_tensor).argmax().item()\n",
    "    \n",
    "    axs[i].set_title(f\"Pred: {pred}\\nTrue: {label}\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "For more details on inspecting results, see [Inspecting training results](https://docs.ray.io/en/latest/train/user-guides/results.html).\n",
    "\n",
    "## 5. Observability\n",
    "\n",
    "Ray provides comprehensive monitoring tools to understand your training performance. See [Monitoring and Logging](https://docs.ray.io/en/latest/train/user-guides/monitoring-logging.html) for more details.\n",
    "\n",
    "### Monitor a Ray Train run in an Anyscale Workspace\n",
    "In an Anyscale Workspace, you can monitor your Ray Train run from the workspace UI:\n",
    "\n",
    "* **ray-workload**: Open **Train** to see the running Ray Train process, the image below shows this view.\n",
    "* **Metrics**: Open **Metrics** to view time-series charts for workload and system metrics, such as GPU utilization, GPU memory, network I/O, and disk I/O.\n",
    "* **ray-dashboard**: Open **ray-dashboard** to access the Ray Dashboard for cluster-level debugging and observability.\n",
    "\n",
    "<img src=\"https://anyscale-public-materials.s3.us-west-2.amazonaws.com/intro-ai-libraries/ray-dashboard.png\" width=\"800\" loading=\"lazy\">\n",
    "\n",
    "<img src=\"https://anyscale-materials.s3.us-west-2.amazonaws.com/apple/gpu_util_and_disk.png\" width=\"400\" loading=\"lazy\">\n",
    "\n",
    "The metrics above include GPU utilization (training activity) and disk I/O (data download to worker nodes).\n",
    "\n",
    "## 6. Fault Tolerance\n",
    "\n",
    "Ray Train provides built-in fault tolerance to recover from worker failures (e.g., preemption).\n",
    "\n",
    "<img src=\"https://anyscale-materials.s3.us-west-2.amazonaws.com/ray-train-deep-dive/fault_tolerance_train_v2.png\" width=\"800\" loading=\"lazy\">\n",
    "\n",
    "### Automatic Retries\n",
    "\n",
    "Ray Train can automatically restart failed workers and resume training. To enable this, set `max_failures` in `RunConfig`:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_config = RunConfig(\n",
    "    failure_config=ray.train.FailureConfig(max_failures=3)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Manual Restoration\n",
    "\n",
    "If the job is interrupted (e.g., driver crash) or `max_failures` is exceeded, you can resume training manually by re-executing the script with the same `RunConfig` (same `name` and `storage_path`).\n",
    "\n",
    "**Prerequisite**: For either mechanism to resume progress instead of restarting from scratch, your training loop must implement logic to load the latest checkpoint upon startup. See [Handling failures and node preemption](https://docs.ray.io/en/latest/train/user-guides/fault-tolerance.html) for the implementation guide.\n",
    "\n",
    "## 7. Troubleshooting\n",
    "\n",
    "Common issues in distributed training and how to diagnose them:\n",
    "\n",
    "### 1. Trainer Hangs\n",
    "* **Symptoms**: The job is running but logs stop updating; GPU utilization drops to 0.\n",
    "* **Causes**:\n",
    "  * **Collective Ops Mismatch**: `ray.train.report` must be called on *all* workers. If one worker skips it (e.g., inside an `if` block), the others will wait indefinitely.\n",
    "  * **Data Iterator Sync**: `iter_torch_batches` acts as a synchronization barrier. All workers must iterate the dataset shard in sync.\n",
    "* **Diagnosis**: Use the Ray Dashboard to view the **Stack Trace** of the `RayTrainWorker` actors from the **Actors** or **Jobs** views.\n",
    "\n",
    "<img src=\"https://anyscale-materials.s3.us-west-2.amazonaws.com/apple/train_dashboard_stack_trace.png\" width=\"700\" loading=\"lazy\">\n",
    "\n",
    "### 2. Data Bottlenecks\n",
    "* **Symptoms**: GPU utilization is low or oscillating.\n",
    "* **Diagnosis**: Check **Iteration Blocked Time** and dataset metrics. High blocked time means the GPU is waiting for data.\n",
    "\n",
    "<img src=\"https://anyscale-materials.s3.us-west-2.amazonaws.com/pinterest/train_deep_dive/increasing-iteration-blocked-time.png\" width=\"500\" loading=\"lazy\">\n",
    "\n",
    "* **Fixes**:\n",
    "  * Increase `prefetch_batches` in `iter_torch_batches`.\n",
    "  * Scale up data processing by adding more CPU resources or optimizing the `map_batches` function.\n",
    "\n",
    "### 3. OOM (Out of Memory)\n",
    "* **Symptoms**: Worker crashes with CUDA OOM.\n",
    "* **Fixes**:\n",
    "  * Reduce `batch_size` (note: `global_batch_size` vs per-worker batch size).\n",
    "  * Ensure you are not accumulating history (tensors with gradients) in lists/dicts over epochs.\n",
    "  * If checkpoints are large, consider lighter checkpointing strategies described in [Saving and loading checkpoints](https://docs.ray.io/en/latest/train/user-guides/checkpoints.html).\n",
    "\n",
    "## 8. Conclusion\n",
    "\n",
    "Ray Train coupled with Ray Data provides a powerful stack for scaling deep learning:\n",
    "\n",
    "* **Simplicity**: Minimal code changes to migrate from single GPU.\n",
    "* **Scalability**: Seamlessly scale to many GPUs and nodes.\n",
    "* **Efficiency**: Ray Data ensures your GPUs are fed efficiently.\n",
    "* **Observability**: Built-in tools to monitor and debug distributed runs.\n",
    "\n",
    "Next steps:\n",
    "* Explore more [Ray Train examples](https://docs.ray.io/en/latest/train/examples.html) for different frameworks and workloads.\n",
    "\n",
    "* Combine Ray Train with Ray Tune for hyperparameter optimization using the [Hyperparameter optimization](https://docs.ray.io/en/latest/train/user-guides/hyperparameter-optimization.html) guide."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
