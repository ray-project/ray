{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f87b86b-8af9-4075-9107-29d4fbadbdef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from diffusers import AutoPipelineForText2Image\n",
    "from transformers import pipeline\n",
    "from transformers.utils import logging\n",
    "import numpy as np\n",
    "import random\n",
    "import ray\n",
    "import torch\n",
    "logging.set_verbosity_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f80dd2-c8e5-42c0-8b14-e614ca2815ce",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Ray Data AI Pipelines Hands On\n",
    "\n",
    "At a high level, we'll look at using Ray Data for...\n",
    "\n",
    "* Scalable data ingestion\n",
    "* Transforming data using Ray Data pipelines and operators\n",
    "* Scalable batch inference processing with accelerators\n",
    "* Joining Ray Datasets and apply data transformation to joined columns\n",
    "* Integrating scalable LLM inference and fractional resource scheduling\n",
    "\n",
    "## Batch image generation\n",
    "\n",
    "### Scenario\n",
    "\n",
    "We have a dataset of image prompts (in our example, animals) and another dataset with enhanced detail information for each record (in the demo, clothing the animal will wear).\n",
    "\n",
    "Our end goal is to combine the prompts and details, then use a LLM to enhance to prompts further, employ an image gen model to create corresponding images, and produce batch output to storage.\n",
    "\n",
    "### Ray Data motivation\n",
    "\n",
    "It's pretty easy to write a Python script to manipulate strings and use models directly from Huggingface with code like the following:\n",
    "\n",
    "```python\n",
    "pipe = AutoPipelineForText2Image.from_pretrained(\"stabilityai/sdxl-turbo\", torch_dtype=torch.float16, variant=\"fp16\").to(\"cuda\")\n",
    "\n",
    "image = pipe(\"A cinematic shot of a racoon wearing an italian priest robe.\", num_inference_steps=1, guidance_scale=0.0).images[0]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f324464-0f98-44e1-ac2c-9874df5057ea",
   "metadata": {},
   "source": [
    "But we want to build a scalable data+AI processing pipeline. To do that, we want to ...\n",
    "\n",
    "* leverage a scale-out cluster with multiple GPUs\n",
    "* read data using as much of our cluster as is useful (parallel read)\n",
    "* work with the data in chunks large enough to get benefits of scale (i.e., not suffer from excessive overhead relative to the number of records)\n",
    "    * but also small enough to allow for flexible scheduling as it flows through our pipeline -- we don't want an enormous chunk to hold up processing, require excessive disk or network I/O, etc.\n",
    "* assign work to, e.g., CPU nodes where GPU is not required; or to smaller, cheaper GPUs where large ones are not required\n",
    "* adjust batching to optimize GPU use even when ideal batch size may be different for different operations\n",
    "* handle arbitrarily large datasets by leveraging a streaming execution model\n",
    "* minimize I/O costs by, e.g., fusing operations where possible\n",
    "* produce predictable flow by managing backpressure (i.e., ensuring data doesn't \"pile up\" in between pipeline stages)\n",
    "* optimize via lazy execution and flexible logical + physical planners\n",
    "\n",
    "Ray Data is designed to address these requirements, allowing us to orchestrate at scale while still straightforward Python / Huggingface code we're used to.\n",
    "\n",
    "### Agenda and steps for incremental implementation\n",
    "\n",
    "1. Locate our datasets in shared storage\n",
    "2. Read records using Ray Data and learn how to perform basic transformations\n",
    "3. Generate images across multiple GPU nodes\n",
    "4. Lab activity: generate images and store all of our prompts and outputs as parquet data\n",
    "5. Join animal records against clothing outfit details to build a bigger prompt and generate enhanced images\n",
    "6. Lab activity: generate and export just the images as PNG files\n",
    "7. Leverage a LLM to further enhance the prompts, adding seasonal content and generate images from the full pipeline\n",
    "8. Lab activity: parameterize the LLM-based component so see how Ray Data supports separation of concerns\n",
    "9. Wrapup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf2f3c9-d304-45fa-8dfa-c1bd2212a93c",
   "metadata": {},
   "source": [
    "First, we need to get all of our data in some common location where the whole cluster can see it. This might be a blob store, NFS, database, etc.\n",
    "\n",
    "Anyscale offers `/mnt/cluster_storage` as a NFS path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ff6260-3303-4da1-9624-b399c0100c26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! cp *.csv /mnt/cluster_storage/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22045eb-ec5c-438f-a66b-a9382ec81e6f",
   "metadata": {},
   "source": [
    "Ray Data's `read_xxxx` methods (see I/O in Ray Docs for all the available formats and data sources) get us scalable, parallel reads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8dacd2-871f-4fa7-a164-1c653778d10b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "animals = ray.data.read_csv('/mnt/cluster_storage/animals.csv')\n",
    "\n",
    "animals.take_batch(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43dd7f2-39f9-4d7d-af3d-a30b3fbce1a3",
   "metadata": {},
   "source": [
    "Batches of records are represented as Python dicts where the keys correspond to the dataset column names and the values are a vectorized type -- usually a NumPy Array -- of values containing one value for each record in the batch.\n",
    "\n",
    "Ray Data contains methods for basic data transformation and allow modification of dataset schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2fc3c2-d042-4853-a815-7e73b7bb111c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "animals.rename_columns({'animal' : 'prompt'}).take_batch(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8c9812-e194-4a1e-a250-fa1e492db439",
   "metadata": {},
   "source": [
    "Stateful tranformation of datasets -- in this example, AI inference where the state is the image gen model -- is done with the following pattern.\n",
    "\n",
    "1. Define a Python class (which Ray will later instantiate across the cluster as one more actor instances to do the processing)\n",
    "1. Use Dataset's `map_batches` API to tell Ray to send batches of data to the `__call__` method in the actors instances\n",
    "    1. `map_batches` allows us to specify resource requirements, actor pool size, batch size, and more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2eed92-11c5-4d8a-b452-e4123f87ec05",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ImageGen():\n",
    "    def __init__(self):\n",
    "        self.pipe = AutoPipelineForText2Image.from_pretrained(\"stabilityai/sdxl-turbo\", torch_dtype=torch.float16, variant=\"fp16\").to(\"cuda\")\n",
    "        \n",
    "    def gen_image(self, prompts):\n",
    "        return self.pipe(prompt=list(prompts), num_inference_steps=1, guidance_scale=0.0).images\n",
    "    \n",
    "    def __call__(self, batch):\n",
    "        batch['image'] = self.gen_image(batch['prompt'])\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ba51c5-80d7-484f-a974-8602ddf9058b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "animals_images = animals.repartition(2).rename_columns({'animal' : 'prompt'}).map_batches(ImageGen, num_gpus=1, concurrency=2, batch_size=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17c3274-3975-4a24-b12a-a27225ba154c",
   "metadata": {},
   "source": [
    "Ray Datasets employ *lazy evaluation* for improved performance, so we can use APIs like `take_batch`, `take`, or `show` to trigger execution for development and testing purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37e0e5c-34c7-4743-ac35-6826b079863d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "examples = animals_images.take_batch(3)\n",
    "\n",
    "examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef0e113-70df-4a56-9975-c083a6a37aa8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "examples['image'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e024d8-31ec-49c0-a691-ff0397ac335d",
   "metadata": {},
   "source": [
    "## Lab: Generate and write all output to storage as parquet data\n",
    "\n",
    "Instructions/hints:\n",
    "\n",
    "1. Start with the Ray Dataset you'd like to write\n",
    "1. Check https://docs.ray.io/en/latest/data/api/input_output.html to find a suitable write API\n",
    "1. Remember to write to a *shared* file location, such as `/mnt/cluster_storage`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2cf51ed-64f1-4e87-aa8d-c2500b38377f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# try your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee81801-d0a7-465e-934e-4770078d6dbd",
   "metadata": {},
   "source": [
    "## Load and join details for each prompt\n",
    "\n",
    "Ray Data supports a number of high-performance JOIN APIs: https://docs.ray.io/en/latest/data/joining-data.html\n",
    "\n",
    "We can use a JOIN to connect our animal records with a detailed prompt refinement unique to that record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb868767-38e3-4a0c-a7bc-b3f294edbece",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "outfits = ray.data.read_csv('/mnt/cluster_storage/outfits.csv')\n",
    "\n",
    "outfits.take_batch(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1ec501-638f-411e-a300-c2eb00827eea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "animals_outfits = animals.join(outfits, 'inner', 1).repartition(8)\n",
    "\n",
    "animals_outfits.take_batch(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376f9e81-30fc-443c-861c-eb652a421e23",
   "metadata": {},
   "source": [
    "We can add custom logic to combine and expand the image gen prompt using another call to `map_batches`\n",
    "\n",
    "In this pattern, since the transformation is stateless and lightweight, we can define it as a Python function (which takes and returns a batch of records) and then use a simplified call to `map_batches` where Ray will autoscale the number of scheduled tasks in order to keep the best throughput for our pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3157b347-6ba4-4851-9eba-82239ed4ff61",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def expand_prompt(batch):\n",
    "    batch['prompt'] = batch['animal'] + ' wearing a ' + batch['outfit']\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3105e53e-dead-4e7d-a376-abe3c66a957a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "animals_outfits.map_batches(expand_prompt).take_batch(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9ece41-5407-4680-83f2-b9af10f9eff7",
   "metadata": {},
   "source": [
    "We can combine the prompt expansion operation with the image gen operation to produce a new set of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421f69b2-da3f-42d0-ba16-df088149b4b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dressed_animals = animals_outfits.map_batches(expand_prompt).map_batches(ImageGen, batch_size=16, concurrency=2, num_gpus=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c15171-d92d-464d-929f-e09d0758bdf0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "examples = dressed_animals.take_batch(3)\n",
    "examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f37544-d29a-43c8-a7e1-71ca0a0563e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "examples['prompt'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2837fc0-4167-4a15-9008-414096f78e96",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "examples['image'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf35723-e7ef-4fed-98a7-06e7ee405402",
   "metadata": {},
   "source": [
    "## Lab: generate images for the input prompts and write the images to a folder\n",
    "\n",
    "> Hint 1: Use `dataset.write_images(...)`\n",
    ">\n",
    "> Hint 2: To use `dataset.write_images(...)`, the images will need to be NumPy arrays (instead of PIL Image objects). You can use `np.array(my_pil_image)` to do that conversion. Use that API along with `map_batches` to convert all of your images prior to calling `write_images`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed11710-b79f-445b-b1ba-defd31d225bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# try your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c262a125-f612-4ebb-8a21-956ba431fd41",
   "metadata": {},
   "source": [
    "## Enhance pipeline with LLM-generation of prompts\n",
    "\n",
    "We can leverage a LLM to create more varied and detailed image prompts -- as well as add dynamism like a seasonal element -- by adding a LLM batch inference step to the pipeline.\n",
    "\n",
    "To implement this operation, we'll\n",
    "1. Create a Python class to encasulate the logic and data transformtion\n",
    "1. Use `map_batches` to route batches of data from our Ray Dataset through this transformation operation\n",
    "1. Demonstrate Ray's support for fractional resource allocation, so that we can schedule 4 GPU-dependent operator instances with only 2 GPUs\n",
    "1. Demonstrate the decoupling of operator batch sizes from each other (as well as from Dataset block size) to optimally use our models and GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa0791e-d241-42fc-af15-d9f27146e88c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Enhancer():\n",
    "    def __init__(self):\n",
    "        self.pipe = pipeline(\"text-generation\", model=\"Qwen/Qwen2.5-0.5B-Instruct\", device='cuda')\n",
    "        \n",
    "    def chat(self, prompts):\n",
    "        messages = []\n",
    "        for p in prompts:\n",
    "            season = random.choice(['winter', 'spring', 'summer', 'fall'])\n",
    "                                   \n",
    "            message = [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\" +\n",
    "                        \"Enhance the image description with two short elements corresponding to the \" + season + \n",
    "                        \"season. Keep animal wearing clothing and retain image medium information (like photo or painting). Return new description only, no intro.\"},\n",
    "                        {\"role\": \"user\", \"content\": p }]\n",
    "            messages.append(message)\n",
    "        return [out[0]['generated_text'][-1]['content'] for out in self.pipe(messages, max_new_tokens=200, batch_size=2)]\n",
    "    \n",
    "    def __call__(self, batch):\n",
    "        batch['prompt'] = self.chat(batch['prompt'])\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36652242-bb12-4fd9-bf62-97b60279e484",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "seasonal_images = animals_outfits.map_batches(expand_prompt) \\\n",
    "                    .map_batches(Enhancer, batch_size=4, concurrency=2, num_gpus=0.6) \\\n",
    "                    .map_batches(ImageGen, batch_size=8, concurrency=2, num_gpus=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84759af7-5e66-4daf-b7ae-1509b37d74ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "examples = seasonal_images.take_batch(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38bf38c3-f3dd-4bc9-a395-938f1a2bb6a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "examples['prompt'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce6736d-10b5-4e49-bbc3-9c14590f4a76",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "examples['image'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a0e7cf-d8d3-4733-a510-e6d0a8e0677f",
   "metadata": {},
   "source": [
    "## Lab: Modify the `Enhancer` class and `seasonal_images` pipeline for parametrization\n",
    "\n",
    "* Use variables contaioning the model name and the name of the dataset column containing the prompt as below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738809d5-8eac-456e-bd22-5bffd625ccbb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "enhancer_model = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "\n",
    "prompt_column = \"prompt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfecf82-a825-4db9-b65c-bb74308b1a45",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# try your code here: updated Enhancer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2596c5-f61e-4646-9374-9866edbf4fa4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# try your code here: updated pipelineto generate seasonal_images Ray dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae709ec6-699f-4b5b-bd81-aef9ff90cd4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "examples = seasonal_images.take_batch(4)\n",
    "examples['prompt'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b590b1bd-407a-4f3d-9107-047c8f3945f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "examples['image'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b55b8f4-dbe7-4fbe-8bc1-9fbc428bc8a5",
   "metadata": {},
   "source": [
    "## Wrapup\n",
    "\n",
    "* Q&A\n",
    "* Next Steps"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
