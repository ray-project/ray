{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a multi-agent system with A2A protocol\n",
    "\n",
    "This tutorial guides you through building and deploying a **multi-agent system** where agents communicate using the **A2A (Agent-to-Agent) protocol**. The system is built on Ray Serve for scalable deployment, LangGraph for agent orchestration, and MCP for tool integration.\n",
    "\n",
    "If you're new to Ray Serve and LangChain integration, check out this **single-agent template** first:\n",
    "-  **Anyscale Template**: [langchain-agent-ray-serve](https://console.anyscale.com/template-preview/langchain-agent-ray-serve)\n",
    "- **Ray Repository**: [`langchain_agent_ray_serve`](https://github.com/ray-project/ray/tree/master/doc/source/ray-overview/examples/langchain_agent_ray_serve/content)\n",
    "\n",
    "## 1. Architecture\n",
    "\n",
    "### Overview\n",
    "\n",
    "The multi-agent system consists of 3 agents:\n",
    "\n",
    "- **Weather Agent** answers weather-related questions using the National Weather Service API.\n",
    "\n",
    "- **Research Agent** performs web searches and fetches content using Google Custom Search.\n",
    "\n",
    "- **Travel Agent** orchestrates both agents using the A2A protocol to create comprehensive travel plans, delegating tasks to the Weather Agent and Research Agent as needed.\n",
    "\n",
    "Each agent runs as an independent, autoscaling service with two interfaces: **SSE** for human-to-agent chat and **A2A** for agent-to-agent communication.\n",
    "\n",
    "<figure>\n",
    "<img src=\"https://anyscale-public-materials.s3.us-west-2.amazonaws.com/agent-template/multi-agent/multi-agent-architecture.png\" alt=\"Multi-Agent A2A Architecture with Ray Serve &amp; Anyscale\" style=\"width: 100%; height: auto;\" />\n",
    "</figure>\n",
    "\n",
    "### Key terminology\n",
    "\n",
    "- **Agent**: The LangGraph workflow/logic that decides what actions to take and calls tools.\n",
    "- **Deployment / Service**: The Ray Serve application that hosts an agent (or MCP/LLM) behind HTTP.\n",
    "- **SSE Endpoint**: Human-to-agent chat endpoint with streaming support, such as `POST /weather-agent/chat`.\n",
    "- **A2A Endpoint**: Agent-to-agent endpoints for discovery and execution, such as `GET /.well-known/agent-card.json` and `POST /v1/message:send`.\n",
    "\n",
    "### System components\n",
    "\n",
    "| Component | Description | Interface | Resources |\n",
    "|------------------|--------------------|------------------|------------------|\n",
    "| **LLM Service** | `Qwen3-4B-Instruct-2507-FP8` with tool calling support | OpenAI-compatible API | 1x L4 GPU |\n",
    "| **Weather MCP** | National Weather Service tools | MCP protocol | 0.2 CPU |\n",
    "| **Web Search MCP** | Google search and URL fetching | MCP protocol | 0.2 CPU |\n",
    "| **Weather Agent** | Answers weather questions | SSE + A2A | 1 CPU |\n",
    "| **Research Agent** | Performs web research | SSE + A2A | 1 CPU |\n",
    "| **Travel Agent** | Orchestrates other agents | SSE + A2A | 1 CPU |\n",
    "\n",
    "### Endpoint map\n",
    "\n",
    "| Use case | Endpoint |\n",
    "|------------------------------------|------------------------------------|\n",
    "| LLM (OpenAI-compatible) | `POST /llm/v1/chat/completions` |\n",
    "| Weather agent (human chat, SSE) | `POST /weather-agent/chat` |\n",
    "| Research agent (human chat, SSE) | `POST /research-agent/chat` |\n",
    "| Travel agent (human chat, SSE) | `POST /travel-agent/chat` |\n",
    "| A2A discovery (any A2A agent) | `GET /a2a-*/.well-known/agent-card.json` |\n",
    "| A2A execute (blocking, any A2A agent) | `POST /a2a-*/v1/message:send` |\n",
    "| A2A execute (streaming, any A2A agent) | `POST /a2a-*/v1/message:stream` |\n",
    "| A2A task/status (poll + history) | `GET /a2a-*/v1/tasks/{id}` |\n",
    "\n",
    "### Data flow example: Travel planning request\n",
    "\n",
    "    ┌──────┐                                                              \n",
    "    │ User │                                                              \n",
    "    └──┬───┘                                                              \n",
    "       │ POST /travel-agent/chat                                          \n",
    "       │ \"Plan a trip to Seattle\"                                         \n",
    "       ▼                                                                  \n",
    "    ┌──────────────┐                                                      \n",
    "    │ Travel Agent │                                                      \n",
    "    └──────┬───────┘                                                      \n",
    "           │ LLM Reasoning:                                              \n",
    "           │   1. Analyze user request                                   \n",
    "           │   2. Call a2a_research(\"Seattle attractions\")               \n",
    "           │   3. Call a2a_weather(\"Seattle weather\")                    \n",
    "           │   4. Synthesize final itinerary                             \n",
    "           │                                                              \n",
    "           ├────────────────────────────────────────────────┐             \n",
    "           │                                                │             \n",
    "           ▼                                                ▼             \n",
    "    ┌───────────────────┐                        ┌───────────────────┐    \n",
    "    │  Research Agent   │                        │  Weather Agent    │    \n",
    "    │  (via A2A)        │                        │  (via A2A)        │    \n",
    "    └────────┬──────────┘                        └────────┬──────────┘    \n",
    "             │                                            │               \n",
    "             ▼                                            ▼               \n",
    "    ┌───────────────────┐                        ┌───────────────────┐    \n",
    "    │ Web Search MCP    │                        │ Weather MCP       │    \n",
    "    │ (Google + Fetch)  │                        │ (NWS API)         │    \n",
    "    └───────────────────┘                        └───────────────────┘    \n",
    "             │                                            │               \n",
    "             └──────────────────┬─────────────────────────┘               \n",
    "                                │                                         \n",
    "                                ▼                                         \n",
    "                        ┌───────────────┐                                 \n",
    "                        │ Final Result  │                                 \n",
    "                        │ (Itinerary +  │                                 \n",
    "                        │  Weather +    │                                 \n",
    "                        │  Sources)     │                                 \n",
    "                        └───────────────┘                                 \n",
    "\n",
    "### Why this architecture?\n",
    "\n",
    "**Benefits of A2A over Direct Integration:**\n",
    "\n",
    "| Aspect      | Direct Integration | A2A Protocol                        |\n",
    "|-------------|--------------------|-------------------------------------|\n",
    "| Coupling    | Tight              | Loose                               |\n",
    "| Discovery   | Hard-coded          | Dynamic (`/.well-known/agent-card.json`) |\n",
    "| Versioning  | Code changes       | Protocol-level                      |\n",
    "| Debugging   | Complex            | Traceable per-task                  |\n",
    "| Composition | Nested code        | HTTP boundaries                     |\n",
    "\n",
    "**Ray Serve capabilities:**\n",
    "\n",
    "- **Autoscaling**: Ray Serve automatically adjusts the number of replicas for each agent based on traffic demand, allowing GPUs for the LLM and CPUs for agents to scale independently while maintaining responsiveness during peak usage.\n",
    "- **Load balancing**: Ray Serve intelligently distributes incoming requests across available agent replicas, preventing any single instance from becoming overwhelmed and maintaining consistent performance.\n",
    "- **Observability**: Built-in monitoring capabilities provide visibility into your agents' performance, including request metrics, resource utilization, and system health indicators.\n",
    "- **Fault tolerance**: Ray Serve automatically detects and recovers from failures by restarting failed agents and redistributing requests to healthy replicas. If the Research Agent fails, the Weather Agent continues working, and the Travel Agent gracefully handles partial failures.\n",
    "- **Composition**: Build complex multi-agent workflows by orchestrating multiple deployments into a single pipeline. The Travel Agent orchestrates Weather and Research agents without knowing their implementation details, using A2A for agent communication and MCP for tool access.\n",
    "\n",
    "**Anyscale service additional benefits:**\n",
    "\n",
    "- **Production ready**: Anyscale provides enterprise-grade infrastructure management and automated deployments that make your multi-agent service ready for real-world production traffic.\n",
    "- **High availability**: Advanced Availability Zone aware scheduling and zero-downtime rolling updates ensure your agents maintain high availability. See [Update an Anyscale service](https://docs.anyscale.com/services/update).\n",
    "- **Logging and tracing**: Enhanced observability with comprehensive logging, distributed tracing, and real-time monitoring dashboards that provide deep insights into agent-to-agent request flows and system performance. See [Monitor a service](https://docs.anyscale.com/services/monitoring).\n",
    "- **Head node fault tolerance**: Additional resilience through managed head node redundancy, protecting against single points of failure in your Ray cluster's coordination layer. See [Head node fault tolerance](https://docs.anyscale.com/administration/resource-management/head-node-fault-tolerance).\n",
    "\n",
    "For more information, see the [Anyscale Services documentation](https://docs.anyscale.com/services).\n",
    "\n",
    "## 2. Project structure\n",
    "\n",
    "    multi-agent-a2a/\n",
    "    ├── agents/                      # Agent implementations\n",
    "    │   ├── weather_agent_with_mcp.py\n",
    "    │   ├── research_agent_with_web_search_mcp.py\n",
    "    │   └── travel_agent_with_a2a.py\n",
    "    ├── agent_runtime/               # Shared runtime utilities\n",
    "    │   ├── config.py                # Configuration management\n",
    "    │   ├── agent_builder.py         # Agent factory functions\n",
    "    │   ├── serve_deployment.py      # SSE deployment factory\n",
    "    │   └── a2a_deployment.py        # A2A deployment factory\n",
    "    ├── protocols/                   # A2A SDK helpers (cards + client)\n",
    "    │   ├── a2a_card.py\n",
    "    │   └── a2a_client.py\n",
    "    ├── mcps/                        # MCP server implementations\n",
    "    │   ├── weather_mcp_server.py\n",
    "    │   └── web_search_mcp_server.py\n",
    "    ├── llm/                         # LLM deployment\n",
    "    │   └── llm_deploy_qwen.py\n",
    "    ├── tests/                       # Test suite\n",
    "    ├── ray_serve_all_deployments.py # Unified deployment entrypoint\n",
    "    ├── serve_multi_config.yaml      # Local Ray Serve deployment config\n",
    "    ├── anyscale_service_multi_config.yaml # Anyscale production deployment config\n",
    "    └── requirements.txt             # Python dependencies\n",
    "\n",
    "## 3. Quick start with local deployment\n",
    "\n",
    "Get the system running first, then explore how it works.\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "**Docker Image:** \n",
    "- Use `anyscale/ray-llm:2.50.1-py311-cu128` for optimal compatibility.\n",
    "\n",
    "**Compute Requirements:**\n",
    "- 1x L4 GPU (g6.2xlarge, 24 GB GPU memory) for the LLM service\n",
    "- 1x m5d.xlarge (4 vCPU) for MCP servers and agents\n",
    "\n",
    "\n",
    "\n",
    "**Install Dependencies:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b289081",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install fastapi==0.115.12 langchain==1.0.5 langchain-mcp-adapters==0.1.12 langchain-openai==1.0.2 langgraph==1.0.3 openai==2.7.2 uvicorn==0.38.0 \"a2a-sdk[http-server]==0.3.22\" httpx==0.28.1 mcp==1.22.0 protego==0.5.0 readabilipy==0.3.0 markdownify==0.14.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5066c51",
   "metadata": {},
   "source": [
    "### Obtain the Google search API keys\n",
    "First, to get Google Search API keys, visit the [Google Cloud Console](https://console.cloud.google.com/) to create an API key, then set up a [Custom Search Engine](https://programmablesearchengine.google.com/) to get your Custom Search Engine (CSE) ID.\n",
    "\n",
    "Set up environment variables following the link on Anyscale Workspace: https://docs.anyscale.com/development/workspace-defaults#env-var\n",
    "\n",
    "```\n",
    "GOOGLE_API_KEY=<your-google-api-key>\n",
    "GOOGLE_CSE_ID=<your-custom-search-engine-id>\n",
    "```\n",
    "\n",
    "### Deploy all services locally\n",
    "Start Ray Serve and deploy all services with a single command in the terminal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce3fa9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!serve run serve_multi_config.yaml "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec40841",
   "metadata": {},
   "source": [
    "This command deploys all the following services:\n",
    "- **LLM Service** at `/llm`\n",
    "- **Weather MCP Server** at `/mcp-weather`\n",
    "- **Web Search MCP Server** at `/mcp-web-search`\n",
    "- **Weather Agent** at `/weather-agent` (SSE) and `/a2a-weather` (A2A)\n",
    "- **Research Agent** at `/research-agent` (SSE) and `/a2a-research` (A2A)\n",
    "- **Travel Agent** at `/travel-agent` (SSE) and `/a2a-travel` (A2A)\n",
    "\n",
    "### Confirm the deployment\n",
    "\n",
    "After all services have started, verify each layer as follows:\n",
    "\n",
    "**Test services individually (with curl):**  \n",
    "Run each of the following curl commands separately and check their responses:\n",
    "\n",
    "1) LLM (OpenAI-compatible)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9aa221",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -X POST http://127.0.0.1:8000/llm/v1/chat/completions -H \"Content-Type: application/json\" -d '{\"model\": \"Qwen/Qwen3-4B-Instruct-2507-FP8\", \"messages\": [{\"role\": \"user\", \"content\": \"Hello!\"}]}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387eacd5",
   "metadata": {},
   "source": [
    "2) Weather Agent (SSE interface)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9436d3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -X POST http://127.0.0.1:8000/weather-agent/chat -H \"Content-Type: application/json\" -d '{\"user_request\": \"What is the weather in San Francisco?\"}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea4a9c7",
   "metadata": {},
   "source": [
    "3) Research Agent (SSE interface)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02514bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -X POST http://127.0.0.1:8000/research-agent/chat -H \"Content-Type: application/json\" -d '{\"user_request\": \"What are the top attractions in Seattle? Reply with sources.\"}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0494ec8",
   "metadata": {},
   "source": [
    "4) Travel Agent (SSE interface, orchestrates other agents through A2A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc329dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -X POST http://127.0.0.1:8000/travel-agent/chat -H \"Content-Type: application/json\" -d '{\"user_request\": \"Plan a 2-day trip to Seattle next week. Include weather details and considerations.\"}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f11d1b6",
   "metadata": {},
   "source": [
    "5) A2A discovery (AgentCards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b26f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl http://127.0.0.1:8000/a2a-weather/.well-known/agent-card.json\n",
    "!curl http://127.0.0.1:8000/a2a-research/.well-known/agent-card.json\n",
    "!curl http://127.0.0.1:8000/a2a-travel/.well-known/agent-card.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068a00e3",
   "metadata": {},
   "source": [
    "### Run the full test suite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0cc1729",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run all tests\n",
    "!python tests/run_all.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4824da3e",
   "metadata": {},
   "source": [
    "## 4. Production deployment to Anyscale\n",
    "\n",
    "To deploy to production on Anyscale:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ef8e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!anyscale service deploy -f anyscale_service_multi_config.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39af9e7",
   "metadata": {},
   "source": [
    "> Note: An Anyscale service config is a superset of a Ray Serve config. For more details, see [the Anyscale service config docs](https://docs.anyscale.com/services/deploy#service-config).\n",
    "\n",
    "### Test the production services\n",
    "\n",
    "After deploying to Anyscale, your service is available at a URL like:\n",
    "\n",
    "    https://<service-name>-<id>.cld-<cluster-id>.s.anyscaleuserdata.com\n",
    "\n",
    "You also receive an authentication token for secure access.\n",
    "\n",
    "To follow the same structure as the local `serve run ...` deployment, verify production in two steps: (1) test each service directly with curl, then (2) run the full test suite.\n",
    "\n",
    "**Set up environment variables (once):**\n",
    "\n",
    "```bash\n",
    "export BASE_URL=\"https://<service-name>-<id>.cld-<cluster-id>.s.anyscaleuserdata.com\"\n",
    "export ANYSCALE_API_TOKEN=\"<your-anyscale-api-token>\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bddf194",
   "metadata": {},
   "source": [
    "> Note: Don't include a trailing `/` at the end of `BASE_URL` (after `.anyscaleuserdata.com`).\n",
    "\n",
    "**Test services individually (with curl):**  \n",
    "Run each of the following curl commands separately and check their responses:\n",
    "\n",
    "1) LLM (OpenAI-compatible)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3187fe4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -X POST \"${BASE_URL}/llm/v1/chat/completions\" -H \"Content-Type: application/json\" -H \"Authorization: Bearer ${ANYSCALE_API_TOKEN}\" -d '{\"model\": \"Qwen/Qwen3-4B-Instruct-2507-FP8\", \"messages\": [{\"role\": \"user\", \"content\": \"Hello!\"}]}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40f11c3",
   "metadata": {},
   "source": [
    "2) Weather Agent (SSE interface)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e9ddaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -X POST \"${BASE_URL}/weather-agent/chat\" -H \"Content-Type: application/json\" -H \"Authorization: Bearer ${ANYSCALE_API_TOKEN}\" -d '{\"user_request\": \"What is the weather in San Francisco?\"}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805649dc",
   "metadata": {},
   "source": [
    "3) Research Agent (SSE interface)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2e0d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -X POST \"${BASE_URL}/research-agent/chat\" -H \"Content-Type: application/json\" -H \"Authorization: Bearer ${ANYSCALE_API_TOKEN}\" -d '{\"user_request\": \"What are the top attractions in Seattle? Reply with sources.\"}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc2463d",
   "metadata": {},
   "source": [
    "4) Travel Agent (SSE interface, orchestrates other agents through A2A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbcc451",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -X POST \"${BASE_URL}/travel-agent/chat\" -H \"Content-Type: application/json\" -H \"Authorization: Bearer ${ANYSCALE_API_TOKEN}\" -d '{\"user_request\": \"Plan a 2-day trip to Seattle next week. Include weather details and considerations.\"}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02fa2c0",
   "metadata": {},
   "source": [
    "5) A2A discovery (AgentCards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa2497f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl \"${BASE_URL}/a2a-weather/.well-known/agent-card.json\" -H \"Authorization: Bearer ${ANYSCALE_API_TOKEN}\"\n",
    "!curl \"${BASE_URL}/a2a-research/.well-known/agent-card.json\" -H \"Authorization: Bearer ${ANYSCALE_API_TOKEN}\"\n",
    "!curl \"${BASE_URL}/a2a-travel/.well-known/agent-card.json\" -H \"Authorization: Bearer ${ANYSCALE_API_TOKEN}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a070100",
   "metadata": {},
   "source": [
    "### Run the test suite against production\n",
    "\n",
    "You can run the full test suite against your production deployment:\n",
    "```bash\n",
    "export BASE_URL=\"https://<service-name>-<id>.cld-<cluster-id>.s.anyscaleuserdata.com\"\n",
    "export ANYSCALE_API_TOKEN=\"<your-anyscale-api-token>\"\n",
    "export TEST_TIMEOUT_SECONDS=\"2000\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d031ad23",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python tests/run_all.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e23b6c8",
   "metadata": {},
   "source": [
    "## 5. Deep dive: Understanding each component\n",
    "\n",
    "This section explores how each service is implemented.\n",
    "\n",
    "### 5.1 The LLM service\n",
    "\n",
    "Check out the code in [`llm/llm_deploy_qwen.py`](llm/llm_deploy_qwen.py). This file deploys Qwen as an OpenAI-compatible API with tool calling support.\n",
    "\n",
    "**Key Configurations:**\n",
    "\n",
    "- **`max_model_len=65536`**: Provides a 64K token context window for complex multi-turn conversations with multiple tool calls.\n",
    "\n",
    "- **`enable_auto_tool_choice=True`**: Enables the model to automatically decide when to use tools, which is essential for agent workflows.\n",
    "\n",
    "- **`tool_call_parser=\"hermes\"`**: Parses tool calls in Hermes format, which Qwen models support natively.\n",
    "\n",
    "\n",
    "For detailed information on deploying and configuring LLM services, see the [Anyscale LLM serving documentation](https://docs.anyscale.com/llm/serving) and the [Deploy LLM template](https://console.anyscale.com/template-preview/deployment-serve-llm).\n",
    "\n",
    "### 5.2 The MCP tool services\n",
    "\n",
    "MCP (Model Context Protocol) servers expose external tools that agents can discover and use dynamically.\n",
    "\n",
    "Ray Serve only supports stateless HTTP mode in MCP. Set `stateless_http=True` to prevent \"session not found\" errors when running multiple replicas.\n",
    "\n",
    "For more information, check out the [Anyscale MCP documentation](https://docs.anyscale.com/mcp) and [MCP Ray Serve template](https://console.anyscale.com/template-preview/mcp-ray-serve).\n",
    "\n",
    "#### 5.2.1 Weather MCP server\n",
    "\n",
    "Check out [mcps/weather_mcp_server.py](mcps/weather_mcp_server.py):\n",
    "\n",
    "| Tool | Description | Parameters |\n",
    "|---------------|------------------------------|----------------------------|\n",
    "| `get_alerts` | Fetches active weather alerts | `state: str` (for example, \"CA\") |\n",
    "| `get_forecast` | Gets a 5-period forecast | `latitude: float`, `longitude: float` |\n",
    "\n",
    "#### 5.2.2 Web search MCP server\n",
    "\n",
    "Check out [mcps/web_search_mcp_server.py](mcps/web_search_mcp_server.py):\n",
    "\n",
    "| Tool | Description | Parameters |\n",
    "|---------------|------------------------------|----------------------------|\n",
    "| `google_search` | Searches using Google Custom Search Engine (CSE) | `query: str`, `num_results: int` (default: 10) |\n",
    "| `fetch_url` | Fetches and parses web pages | `url: str`, `max_length: int` (default: 5000), `start_index: int` (default: 0), `raw: bool` (default: false), `ignore_robots_txt: bool` (default: false) |\n",
    "\n",
    "\n",
    "\n",
    "### 5.3 The agent runtime\n",
    "\n",
    "The agent runtime provides a builder pattern for creating agents and deploying them with both SSE (human-to-agent) and A2A (agent-to-agent) interfaces. This shared infrastructure eliminates code duplication across agents by centralizing configuration, agent building, and deployment logic.\n",
    "\n",
    "<figure>\n",
    "<img src=\"https://anyscale-public-materials.s3.us-west-2.amazonaws.com/agent-template/multi-agent/agent-builder-pattern.png\" alt=\"Agent Runtime - Builder Pattern\" style=\"width: 65%; height: auto;\" />\n",
    "</figure>\n",
    "\n",
    "The agent runtime consists of four core modules:\n",
    "\n",
    "#### 5.3.1 Configuration management\n",
    "\n",
    "The configuration module [`agent_runtime/config.py`](agent_runtime/config.py) centralizes configuration loading for LLM and MCP settings from environment variables.\n",
    "\n",
    "- **Classes:** `LLMConfig` (LLM backend settings) and `MCPEndpoint` (MCP server configuration).\n",
    "- **Functions:** `load_llm_config()`.\n",
    "\n",
    "\n",
    "#### 5.3.2 Agent building helpers\n",
    "\n",
    "The agent builder module [`agent_runtime/agent_builder.py`](agent_runtime/agent_builder.py) provides factory functions for building LangGraph agents, centralizing LLM setup, MCP tool discovery, and agent creation to eliminate boilerplate.\n",
    "\n",
    "- **Functions:** `build_llm()`, `load_mcp_tools()`, `build_tool_agent()`, and `build_mcp_agent()`.\n",
    "- **Logic handled:** Configuration loading, LLM construction, dynamic MCP tool discovery, and agent creation with `MemorySaver` checkpointing.\n",
    "\n",
    "#### 5.3.3 SSE deployment factory\n",
    "\n",
    "The SSE deployment module [`agent_runtime/serve_deployment.py`](agent_runtime/serve_deployment.py) builds the FastAPI application and Ray Serve deployment for the human-to-agent chat interface.\n",
    "\n",
    "- **Endpoints:** Exposes `POST /chat` with SSE streaming support.\n",
    "- **Functions:** `create_chat_app()` and `create_serve_deployment()`.\n",
    "- **Features:** Real-time SSE streaming, conversation continuity through Thread IDs, and automatic LangGraph event serialization.\n",
    "\n",
    "#### 5.3.4 A2A deployment factory\n",
    "\n",
    "The A2A deployment module [`agent_runtime/a2a_deployment.py`](agent_runtime/a2a_deployment.py) enables standardized agent-to-agent communication by creating Ray Serve deployments with A2A protocol compliance.\n",
    "\n",
    "\n",
    "### 5.4 The specialized agents\n",
    "\n",
    "Each specialized agent is a LangGraph agent that combines an LLM with specific tools. The agents use the builder pattern from the agent runtime to minimize boilerplate code.\n",
    "\n",
    "#### Weather agent\n",
    "\n",
    "**File:** [`agents/weather_agent_with_mcp.py`](agents/weather_agent_with_mcp.py)\n",
    "\n",
    "This agent provides weather information using tools from the Weather MCP server. It demonstrates the MCP integration pattern where tools are dynamically discovered from the MCP server.\n",
    "\n",
    "**Implementation approach:**\n",
    "- Uses `build_mcp_agent()` from agent runtime to create the agent\n",
    "- Connects to Weather MCP server endpoint\n",
    "- Tools are auto-discovered from the MCP server (`get_alerts`, `get_forecast`)\n",
    "\n",
    "**System prompt strategy:**\n",
    "- Break tasks into sub-questions (for example, finding coordinates first)\n",
    "- Use weather tools to get current conditions and forecasts\n",
    "- Provide concise, actionable answers\n",
    "\n",
    "**Request flow example:**\n",
    "1. User sends: \"What's the weather in Palo Alto?\"\n",
    "2. Agent reasons: \"I need coordinates for Palo Alto.\"\n",
    "3. Agent calls `get_forecast(37.4419, -122.1430)`\n",
    "4. MCP server fetches data from the National Weather Service API\n",
    "5. Agent formats and returns the response\n",
    "\n",
    "**Configuration:**\n",
    "- `WEATHER_MCP_BASE_URL` - Base URL for Weather MCP server\n",
    "- `WEATHER_MCP_TOKEN` - Optional authentication token\n",
    "\n",
    "#### Research agent\n",
    "\n",
    "**File:** [`agents/research_agent_with_web_search_mcp.py`](agents/research_agent_with_web_search_mcp.py)\n",
    "\n",
    "This agent performs online research and gathers sources using the Web Search MCP server. It demonstrates how to combine multiple MCP tools (search + fetch) for comprehensive research workflows.\n",
    "\n",
    "**Implementation approach:**\n",
    "- Uses `build_mcp_agent()` from agent runtime\n",
    "- Connects to Web Search MCP server endpoint\n",
    "- Tools are auto-discovered (`google_search`, `fetch_url`)\n",
    "\n",
    "**System prompt strategy:**\n",
    "- Break research tasks into sub-questions\n",
    "- Use `google_search` first to find relevant sources\n",
    "- Use `fetch_url` to read primary sources and confirm details\n",
    "- Never fabricate information; explicitly state when unable to verify\n",
    "- Include sources as a bullet list of URLs in the final answer\n",
    "\n",
    "**Request flow example:**\n",
    "1. User sends: \"What are the top attractions in Seattle?\"\n",
    "2. Agent calls `google_search(\"Seattle top attractions\", num_results=10)`\n",
    "3. Agent calls `fetch_url(url)` for promising results\n",
    "4. Agent synthesizes information and includes source URLs\n",
    "\n",
    "**Configuration:**\n",
    "- `WEB_SEARCH_MCP_BASE_URL` - Base URL for Web Search MCP server\n",
    "- `WEB_SEARCH_MCP_TOKEN` - Optional authentication token\n",
    "\n",
    "#### Travel agent (multi-agent orchestration)\n",
    "\n",
    "**File:** [`agents/travel_agent_with_a2a.py`](agents/travel_agent_with_a2a.py)\n",
    "\n",
    "This agent demonstrates agent-to-agent communication using the A2A protocol. Instead of connecting to MCP servers directly, it orchestrates two downstream agents (Weather and Research) to create comprehensive travel plans.\n",
    "\n",
    "**Implementation approach:**\n",
    "- Uses `build_tool_agent()` from agent runtime with explicit A2A tools\n",
    "- Defines two custom tools that wrap A2A calls:\n",
    "  - `a2a_research(query)` - Calls Research Agent through A2A\n",
    "  - `a2a_weather(query)` - Calls Weather Agent through A2A\n",
    "- Uses `a2a_execute_text()` helper for simple agent-to-agent communication\n",
    "\n",
    "**System prompt strategy:**\n",
    "- Always call both tools (research and weather) at least once\n",
    "- Ask clarifying questions if missing key constraints (dates, budget, origin, travelers, pace, interests)\n",
    "- Produce structured travel plans with:\n",
    "  1. Assumptions & trip summary\n",
    "  2. Day-by-day itinerary (morning/afternoon/evening)\n",
    "  3. Weather-aware packing + timing suggestions\n",
    "  4. Budget outline (high/medium/low)\n",
    "  5. Bookings checklist + local transit notes\n",
    "  6. Sources (from research tool output)\n",
    "\n",
    "**Request flow example:**\n",
    "1. User sends: \"Plan a 3-day trip to Seattle next week\"\n",
    "2. Travel agent calls `a2a_research(\"Seattle attractions, restaurants, activities\")`\n",
    "3. Travel agent calls `a2a_weather(\"Seattle weather forecast next week\")`\n",
    "4. Travel agent synthesizes both responses into a structured itinerary\n",
    "\n",
    "**Configuration:**\n",
    "- `RESEARCH_A2A_BASE_URL` - Base URL for Research Agent A2A endpoint (default: `http://127.0.0.1:8000/a2a-research`)\n",
    "- `WEATHER_A2A_BASE_URL` - Base URL for Weather Agent A2A endpoint (default: `http://127.0.0.1:8000/a2a-weather`)\n",
    "- `A2A_TIMEOUT_S` - Timeout for downstream agent calls (default: 360 seconds)\n",
    "\n",
    "**A2A tool implementation:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f303e187",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "from protocols.a2a_client import a2a_execute_text\n",
    "\n",
    "@tool\n",
    "async def a2a_research(query: str) -> str:\n",
    "    \"\"\"Call the Research agent over A2A to gather up-to-date info and sources.\"\"\"\n",
    "    return await a2a_execute_text(RESEARCH_A2A_BASE_URL, query, timeout_s=A2A_TIMEOUT_S)\n",
    "\n",
    "@tool\n",
    "async def a2a_weather(query: str) -> str:\n",
    "    \"\"\"Call the Weather agent over A2A to get weather/forecast guidance.\"\"\"\n",
    "    return await a2a_execute_text(WEATHER_A2A_BASE_URL, query, timeout_s=A2A_TIMEOUT_S)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b361beaf",
   "metadata": {},
   "source": [
    "### 5.5 The A2A protocol and helpers\n",
    "\n",
    "The A2A (Agent-to-Agent) protocol enables standardized agent-to-agent communication. This system uses the official `a2a-sdk` with custom helper utilities.\n",
    "\n",
    "<figure>\n",
    "<img src=\"https://anyscale-public-materials.s3.us-west-2.amazonaws.com/agent-template/multi-agent/a2a-protocol.png\" alt=\"A2A Protocol Communication\" />\n",
    "</figure>\n",
    "\n",
    "**A2A components:**\n",
    "- Server endpoints: [`agent_runtime/a2a_deployment.py`](agent_runtime/a2a_deployment.py) (deployment factory)\n",
    "- AgentCard helper: [`protocols/a2a_card.py`](protocols/a2a_card.py) (discovery)\n",
    "- Client helper: [`protocols/a2a_client.py`](protocols/a2a_client.py) (execution)\n",
    "\n",
    "#### AgentCard helper\n",
    "\n",
    "**File:** [`protocols/a2a_card.py`](protocols/a2a_card.py)\n",
    "\n",
    "Provides utilities for creating A2A AgentCards using the official `a2a-sdk` types. AgentCards enable agent discovery by advertising capabilities, skills, and endpoints.\n",
    "\n",
    "**Key function:**\n",
    "- `build_agent_card()` - Creates an `a2a.types.AgentCard` for HTTP+JSON (REST) agents\n",
    "\n",
    "**Usage:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b166a0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from protocols.a2a_card import build_agent_card\n",
    "\n",
    "card = build_agent_card(\n",
    "    name=\"weather-agent\",\n",
    "    description=\"Weather agent that uses a Weather MCP server\",\n",
    "    version=\"0.1.0\",\n",
    "    skills=[\"weather\", \"forecast\", \"current_conditions\"],\n",
    "    url=\"http://127.0.0.1:8000/a2a-weather\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405f3f3d",
   "metadata": {},
   "source": [
    "**AgentCard format:**\n",
    "\n",
    "AgentCards are exposed at `GET /.well-known/agent-card.json` for A2A discovery:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"name\": \"weather-agent\",\n",
    "  \"description\": \"Weather agent that uses a Weather MCP server...\",\n",
    "  \"version\": \"0.1.0\",\n",
    "  \"url\": \"http://127.0.0.1:8000/a2a-weather\",\n",
    "  \"preferred_transport\": \"http+json\",\n",
    "  \"capabilities\": {\n",
    "    \"streaming\": true,\n",
    "    \"push_notifications\": false,\n",
    "    \"state_transition_history\": false\n",
    "  },\n",
    "  \"default_input_modes\": [\"text/plain\"],\n",
    "  \"default_output_modes\": [\"text/plain\"],\n",
    "  \"skills\": [\n",
    "    {\n",
    "      \"id\": \"weather-agent-primary\",\n",
    "      \"name\": \"weather-agent\",\n",
    "      \"description\": \"Weather agent that uses a Weather MCP server...\",\n",
    "      \"tags\": [\"weather\", \"forecast\", \"current_conditions\"]\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "#### Client helper\n",
    "\n",
    "**File:** [`protocols/a2a_client.py`](protocols/a2a_client.py)\n",
    "\n",
    "Provides a simplified interface for agent-to-agent communication using the official `a2a-sdk` REST transport. This is used by agents that orchestrate other agents.\n",
    "\n",
    "**Key function:**\n",
    "- `a2a_execute_text()` - Send a single text message to an A2A agent and get the text response\n",
    "\n",
    "**Features:**\n",
    "- Uses official `a2a-sdk` REST transport\n",
    "- Handles message creation with proper types (`Role.user`, `TextPart`)\n",
    "- Extracts text from Task/Message responses\n",
    "- Configurable timeouts for long-running agent calls\n",
    "- Optional HTTP headers for authentication\n",
    "\n",
    "**Usage:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa18dd51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from protocols.a2a_client import a2a_execute_text\n",
    "\n",
    "result = await a2a_execute_text(\n",
    "    base_url=\"http://127.0.0.1:8000/a2a-weather\",\n",
    "    input_text=\"What's the forecast for Seattle?\",\n",
    "    timeout_s=60.0,\n",
    "    headers={\"Authorization\": \"Bearer <token>\"}  # Optional\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef60a01",
   "metadata": {},
   "source": [
    "#### A2A REST endpoints\n",
    "\n",
    "**Standard A2A endpoints** (exposed by all A2A agents):\n",
    "\n",
    "| Endpoint                            | Description                         |\n",
    "|-------------------------------------|-------------------------------------|\n",
    "| `GET /.well-known/agent-card.json`  | Returns the AgentCard for discovery |\n",
    "| `POST /v1/message:send`             | Executes a message (blocking)       |\n",
    "| `POST /v1/message:stream`           | Executes a message (SSE streaming)  |\n",
    "| `GET /v1/tasks/{id}`                | Fetch / poll task state + history   |\n",
    "\n",
    "**Testing A2A endpoints:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b900329e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test A2A discovery\n",
    "!curl http://127.0.0.1:8000/a2a-weather/.well-known/agent-card.json\n",
    "\n",
    "# For execution, prefer the Python helper:\n",
    "# python -c 'import asyncio; from protocols.a2a_client import a2a_execute_text; print(asyncio.run(a2a_execute_text(\"http://127.0.0.1:8000/a2a-weather\",\"Weather in NYC?\")))'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1434054c",
   "metadata": {},
   "source": [
    "#### 5.5.4 Calling an Agent through A2A\n",
    "\n",
    "Use the `a2a_execute_text` helper for simple text-based calls:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a2b7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from protocols.a2a_client import a2a_execute_text\n",
    "\n",
    "# Call the Weather Agent via A2A\n",
    "result = await a2a_execute_text(\n",
    "    base_url=\"http://127.0.0.1:8000/a2a-weather\",\n",
    "    input_text=\"What's the forecast for Seattle?\",\n",
    "    timeout_s=60.0\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2fa70c",
   "metadata": {},
   "source": [
    "You can also test discovery directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b000dcaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl http://127.0.0.1:8000/a2a-weather/.well-known/agent-card.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84396e1",
   "metadata": {},
   "source": [
    "### 5.6 The unified deployment configuration\n",
    "\n",
    "Check out [`serve_multi_config.yaml`](serve_multi_config.yaml) for the complete deployment configuration.\n",
    "\n",
    "**Autoscaling Configuration:**\n",
    "\n",
    "The system uses Ray Serve's built-in autoscaling to handle variable load. See the configuration details in [`serve_multi_config.yaml`](serve_multi_config.yaml).\n",
    "\n",
    "\n",
    "\n",
    "## 6. Next steps\n",
    "\n",
    "1. **Build your own agents (or a multi-agent system) using the agent runtime**\n",
    "   - Start from the builder pattern in **5.3 The agent runtime** and adapt the existing agents to your use case.\n",
    "   - Add/modify tools and prompts, update your Agent Card metadata, and deploy with Ray Serve so you can test both **SSE (human-to-agent)** and **A2A (agent-to-agent)** flows.\n",
    "\n",
    "2. **Integrate Langfuse for observability, prompt management, and evals**\n",
    "   - Project: [`langfuse/langfuse`](https://github.com/langfuse/langfuse)\n",
    "   - **LLM Application Observability**: instrument your app and ingest traces to Langfuse to track LLM calls and other logic (retrieval, embedding, agent actions), inspect/debug sessions, and iterate faster.\n",
    "   - **Prompt Management**: centrally manage and version prompts; strong caching on server/client helps you iterate without adding latency.\n",
    "   - **Evaluations**: run LLM-as-a-judge, collect user feedback, support manual labeling, and build custom evaluation pipelines through APIs/SDKs.\n",
    "\n",
    "## 7. Additional resources\n",
    "\n",
    "- [Ray Serve Documentation](https://docs.ray.io/en/latest/serve/index.html)\n",
    "- [LangGraph Documentation](https://langchain-ai.github.io/langgraph/)\n",
    "- [MCP Specification](https://modelcontextprotocol.io/)\n",
    "- [A2A Protocol](https://google.github.io/A2A/)\n",
    "- [Anyscale Services Documentation](https://docs.anyscale.com/services)\n",
    "- [Anyscale LLM Serving Documentation](https://docs.anyscale.com/llm/serving) - Detailed guide for deploying and configuring LLM services\n",
    "- [Deploy LLM Template](https://console.anyscale.com/template-preview/deployment-serve-llm) - Template for deploying LLM services on Anyscale\n",
    "- [Anyscale MCP Documentation](https://docs.anyscale.com/mcp) - Guide for deploying and configuring MCP servers with Ray Serve\n",
    "- [MCP Ray Serve Template](https://console.anyscale.com/template-preview/mcp-ray-serve) - Template for deploying MCP servers on Anyscale"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
