{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Ray Data\n",
    "\n",
    "This template provides a comprehensive introduction to **Ray Data** — a scalable data processing library for AI workloads built on [Ray](https://docs.ray.io/en/latest/). You will learn what Ray Data is, why it matters for ML pipelines, and how to use its core APIs hands-on with an MNIST image classification example.\n",
    "\n",
    "**Here is the roadmap for this template:**\n",
    "\n",
    "- **Part 1:** When to Use Ray Data\n",
    "- **Part 2:** Loading Data\n",
    "- **Part 3:** Lazy Execution\n",
    "- **Part 4:** Transforming Data with `map_batches`\n",
    "- **Part 5:** Stateful Transformations and Batch Inference\n",
    "- **Part 6:** Data Preprocessing\n",
    "- **Part 7:** Data Operations — Filtering, Groupby, Aggregation, Shuffling\n",
    "- **Part 8:** Materializing Data\n",
    "- **Part 9:** Persisting Data\n",
    "- **Part 10:** Architecture Overview\n",
    "- **Part 11:** Resource Management and Autoscaling\n",
    "- **Part 12:** Observability and Performance Tuning\n",
    "- **Part 13:** Fault Tolerance and Checkpointing\n",
    "- **Part 14:** Ray Data in Production\n",
    "- **Summary and Next Steps**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize\n",
    "\n",
    "import ray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note on Storage\n",
    "\n",
    "Throughout this tutorial, we use `/mnt/cluster_storage` to represent a shared storage location. In a multi-node cluster, Ray workers on different nodes cannot access the head node's local file system. Use a [shared storage solution](https://docs.anyscale.com/configuration/storage#shared) accessible from every node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: When to Use Ray Data\n",
    "\n",
    "Use Ray Data to load and preprocess data for distributed ML workloads. Ray Data is the **last-mile bridge** from storage or ETL pipeline outputs to distributed applications and libraries in Ray.\n",
    "\n",
    "Consider using Ray Data when your project meets one or more of these criteria:\n",
    "\n",
    "| **Challenge** | **Ray Data Solution** |\n",
    "|---------------|----------------------|\n",
    "| **Operating on large datasets** (>10 TB) | Distributes data loading and processing across a Ray cluster with streaming execution |\n",
    "| **Feeding data into distributed training** | Streams data to training processes with configurable batch sizes across heterogeneous CPU/GPU resources |\n",
    "| **Running batch inference at scale** | Maximizes GPU utilization by streaming data through model inference actors |\n",
    "| **Building reliable data pipelines** | Leverages Ray Core's fault-tolerance mechanisms — retries, checkpointing, and recovery |\n",
    "\n",
    "Ray Data features a **streaming execution engine** that processes data in a pipelined fashion across a heterogeneous cluster of CPUs and GPUs. This avoids materializing the full dataset in memory and keeps all hardware utilized.\n",
    "\n",
    "<img src=\"https://docs.ray.io/en/latest/_images/dataset-loading-1.svg\" width=\"60%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Loading Data\n",
    "\n",
    "Ray Data supports a variety of IO connectors to load data from different sources:\n",
    "\n",
    "| **Source** | **API** |\n",
    "|------------|---------|\n",
    "| Parquet files (S3, GCS, local) | `ray.data.read_parquet(\"s3://...\")` |\n",
    "| Images | `ray.data.read_images(\"s3://...\")` |\n",
    "| CSV, JSON, text | `ray.data.read_csv(...)`, `ray.data.read_json(...)`, `ray.data.read_text(...)` |\n",
    "| HuggingFace Datasets | `ray.data.from_huggingface(hf_dataset)` |\n",
    "| In-memory (NumPy, Pandas, PyTorch) | `ray.data.from_numpy(...)`, `ray.data.from_pandas(...)`, `ray.data.from_torch(...)` |\n",
    "| Lakehouses (Databricks, Iceberg) | `ray.data.read_databricks_tables(...)` |\n",
    "\n",
    "See the full list in the [Input/Output docs](https://docs.ray.io/en/latest/data/api/input_output.html), and review further options in the [data loading guide](https://docs.ray.io/en/latest/data/loading-data.html).\n",
    "\n",
    "Under the hood, Ray Data uses Ray tasks to read data from remote storage. It creates read tasks proportional to the number of CPUs in your cluster, and each task produces output **blocks**:\n",
    "\n",
    "<img src=\"https://anyscale-public-materials.s3.us-west-2.amazonaws.com/ray-summit/rag-app/dataset-read-cropped-v2.svg\" width=\"90%\">\n",
    "\n",
    "Let's load MNIST image data from S3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ray.data.read_images(\n",
    "    \"s3://anyscale-public-materials/ray-ai-libraries/mnist/50_per_index/\",\n",
    "    include_paths=True,\n",
    ")\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **Dataset** is a distributed collection of **blocks** — contiguous subsets of rows stored as PyArrow tables. Blocks are distributed across the cluster and processed in parallel.\n",
    "\n",
    "<img src=\"https://docs.ray.io/en/latest/_images/dataset-arch.svg\" width=\"90%\"/>\n",
    "\n",
    "Since a Dataset is a list of Ray object references, it can be freely passed between Ray tasks, actors, and libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Lazy Execution\n",
    "\n",
    "In Ray Data, most transformations are **lazy** — they build an execution plan rather than running immediately. The plan executes only when you call a method that *consumes* or *materializes* the dataset.\n",
    "\n",
    "**Execution-triggering methods include:**\n",
    "\n",
    "| **Category** | **Methods** |\n",
    "|-------------|------------|\n",
    "| Small samples | `take_batch()`, `take()`, `show()` |\n",
    "| Full materialization | `materialize()` |\n",
    "| Write to storage | `write_parquet()`, `write_csv()`, etc. |\n",
    "| Aggregations | `count()`, `mean()`, `min()`, `max()`, `sum()`, `std()` |\n",
    "\n",
    "To materialize a small subset for inspection, use `take_batch`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = ds.take_batch(batch_size=5)\n",
    "sample.keys()  # dict_keys(['image', 'path'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This lazy execution model lets Ray Data optimize the full pipeline before running it — including operator fusion, resource allocation, and backpressure management."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Transforming Data with `map_batches`\n",
    "\n",
    "The primary transformation API in Ray Data is `map_batches()`. It applies a user-defined function to each batch of data in parallel.\n",
    "\n",
    "A batch is a `dict[str, np.ndarray]` by default. Your function receives a batch and returns a transformed batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(batch: dict[str, np.ndarray]) -> dict[str, np.ndarray]:\n",
    "    \"\"\"Normalize MNIST images to [-1, 1] range using torchvision transforms.\"\"\"\n",
    "    transform = Compose([ToTensor(), Normalize((0.5,), (0.5,))])\n",
    "    batch[\"image\"] = [transform(image) for image in batch[\"image\"]]\n",
    "    return batch\n",
    "\n",
    "\n",
    "ds_normalized = ds.map_batches(normalize)\n",
    "ds_normalized  # Lazy — not executed yet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's verify the transformation works on a small batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_batch = ds_normalized.take_batch(batch_size=10)\n",
    "\n",
    "for image in normalized_batch[\"image\"]:\n",
    "    assert image.shape == (1, 28, 28)  # channel, height, width\n",
    "    assert image.min() >= -1 and image.max() <= 1  # normalized range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key `map_batches` parameters:**\n",
    "\n",
    "| **Parameter** | **Purpose** |\n",
    "|--------------|-------------|\n",
    "| `batch_size` | Number of rows per batch (default: `None` = entire block) |\n",
    "| `batch_format` | `\"default\"`/`\"numpy\"` (dict), `\"pandas\"`, or `\"pyarrow\"` |\n",
    "| `num_cpus` / `num_gpus` | Resources per worker |\n",
    "| `fn_kwargs` | Keyword arguments to pass to your function |\n",
    "| `compute` | Execution strategy — `TaskPoolStrategy` for functions, `ActorPoolStrategy` for classes |\n",
    "\n",
    "To learn more, see the [`map_batches` API reference](https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.map_batches.html) and the [Transforming Data guide](https://docs.ray.io/en/latest/data/transforming-data.html).\n",
    "\n",
    "> **API Update (Ray 2.53+):** The `concurrency` parameter is **deprecated**. Use the `compute` argument instead:\n",
    "> - `compute=ray.data.ActorPoolStrategy(size=N)` for a fixed actor pool\n",
    "> - `compute=ray.data.ActorPoolStrategy(min_size=M, max_size=N)` for autoscaling\n",
    "> - `compute=ray.data.TaskPoolStrategy(size=N)` to limit concurrent tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Stateful Transformations and Batch Inference\n",
    "\n",
    "For operations like batch inference, you want to load a model once and reuse it across many batches. Ray Data supports this via **callable classes** passed to `map_batches`:\n",
    "\n",
    "- `__init__`: Initialize expensive state (load model, set up connections)\n",
    "- `__call__`: Process each batch using the initialized state\n",
    "\n",
    "Here's an MNIST classifier that loads a pre-trained PyTorch model and runs GPU inference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTClassifier:\n",
    "    def __init__(self, model_path: str):\n",
    "        self.model = torch.jit.load(model_path)\n",
    "        self.model.to(\"cuda\")\n",
    "        self.model.eval()\n",
    "\n",
    "    def __call__(self, batch: dict[str, np.ndarray]) -> dict[str, np.ndarray]:\n",
    "        images = torch.tensor(batch[\"image\"]).float().to(\"cuda\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = self.model(images).cpu().numpy()\n",
    "\n",
    "        batch[\"predicted_label\"] = np.argmax(logits, axis=1)\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the model to shared storage, then apply the classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp s3://anyscale-public-materials/ray-ai-libraries/mnist/model/model.pt /mnt/cluster_storage/model.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_preds = ds_normalized.map_batches(\n",
    "    MNISTClassifier,\n",
    "    fn_constructor_kwargs={\"model_path\": \"/mnt/cluster_storage/model.pt\"},\n",
    "    num_gpus=1,\n",
    "    batch_size=100,\n",
    "    compute=ray.data.ActorPoolStrategy(size=1),  # 1 GPU worker\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** We pass the class *uninitialized* — Ray Data creates the actor and passes `fn_constructor_kwargs` to `__init__`. This ensures the model is loaded once per worker, not once per batch. For more on batch inference patterns, see the [Batch Inference guide](https://docs.ray.io/en/latest/data/batch_inference.html).\n",
    "\n",
    "Let's verify predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_preds = ds_preds.take_batch(100)\n",
    "batch_preds.keys()  # dict_keys(['image', 'path', 'predicted_label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scaling up:** To use multiple GPUs, increase the pool size. For example, `compute=ray.data.ActorPoolStrategy(size=4)` creates a fixed pool of 4 GPU actors, or `compute=ray.data.ActorPoolStrategy(min_size=1, max_size=4)` creates an autoscaling pool."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Data Preprocessing\n",
    "\n",
    "Ray Data provides three approaches to data preprocessing, each suited to different use cases:\n",
    "\n",
    "| **Approach** | **When to Use** | **Example** |\n",
    "|-------------|----------------|-------------|\n",
    "| **Stateless functions** (`map_batches` with a function) | Simple transforms that don't require shared state | Normalization, feature extraction, label encoding |\n",
    "| **Stateful callable classes** (`map_batches` with a class) | Transforms requiring expensive initialization (model, connection) | Batch inference, embedding generation |\n",
    "| **Preprocessors** (fit/transform pattern) | Transforms that need a pass over the dataset to compute statistics | StandardScaler, MinMaxScaler, OneHotEncoder |\n",
    "\n",
    "### Built-in Preprocessors\n",
    "\n",
    "Ray Data includes built-in Preprocessors that follow the familiar scikit-learn `fit` / `transform` pattern. Here's a general example with tabular data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.data.preprocessors import StandardScaler, Chain, Concatenator, LabelEncoder\n",
    "\n",
    "# Example: fit a StandardScaler on a tabular dataset\n",
    "# scaler = StandardScaler(columns=[\"feature_col\"])\n",
    "# scaler = scaler.fit(ds_tabular)\n",
    "# ds_scaled = scaler.transform(ds_tabular)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chaining Preprocessors\n",
    "\n",
    "Combine multiple preprocessors into a pipeline using `Chain`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: chain multiple preprocessors for tabular data\n",
    "# preprocessor = Chain(\n",
    "#     StandardScaler(columns=[\"feature_col\"]),\n",
    "#     Concatenator(output_column_name=\"features\", exclude=[\"label\"]),\n",
    "#     LabelEncoder(label_column=\"label\"),\n",
    "# )\n",
    "# preprocessor = preprocessor.fit(ds_tabular)\n",
    "# ds_processed = preprocessor.transform(ds_tabular)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For custom preprocessing logic, you can subclass `Preprocessor` and implement `_fit()` and `_transform_pandas()` / `_transform_numpy()`. See the [Preprocessor API reference](https://docs.ray.io/en/latest/data/api/preprocessor.html) for details and the full list of built-in preprocessors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Data Operations\n",
    "\n",
    "### Adding Labels\n",
    "\n",
    "Let's add ground truth labels to our MNIST dataset by extracting them from the image paths:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_label(batch: dict[str, np.ndarray]) -> dict[str, np.ndarray]:\n",
    "    batch[\"ground_truth_label\"] = [int(path.split(\"/\")[-2]) for path in batch[\"path\"]]\n",
    "    return batch\n",
    "\n",
    "\n",
    "ds_labeled = ds_normalized.map_batches(add_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Groupby and Map Groups\n",
    "\n",
    "Use `groupby()` to group data by a key and `map_groups()` to apply per-group transformations. Here, we compute per-label accuracy using `ds_preds` from Part 5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(group: dict[str, np.ndarray]) -> dict[str, np.ndarray]:\n",
    "    return {\n",
    "        \"accuracy\": [np.mean(group[\"predicted_label\"] == group[\"ground_truth_label\"])],\n",
    "        \"ground_truth_label\": group[\"ground_truth_label\"][:1],\n",
    "    }\n",
    "\n",
    "\n",
    "accuracy_by_label = (\n",
    "    ds_preds\n",
    "    .map_batches(add_label)\n",
    "    .groupby(\"ground_truth_label\")\n",
    "    .map_groups(compute_accuracy)\n",
    "    .to_pandas()\n",
    ")\n",
    "accuracy_by_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregations\n",
    "\n",
    "Ray Data provides built-in aggregation functions: `count()`, `max()`, `mean()`, `min()`, `sum()`, `std()`. See the [aggregation API docs](https://docs.ray.io/en/latest/data/api/grouped_data.html#ray.data.aggregate.AggregateFn) for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_accuracy = ds_preds.map_batches(add_label).map_batches(compute_accuracy).mean(on=\"accuracy\")\n",
    "mean_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key takeaway:** `groupby` + `map_groups` lets you apply per-group logic, while built-in aggregation functions cover common statistical operations.\n",
    "\n",
    "### Shuffling\n",
    "\n",
    "Ray Data offers three shuffle strategies with increasing randomness and cost:\n",
    "\n",
    "| **Strategy** | **API** | **Randomness** | **Cost** |\n",
    "|-------------|---------|---------------|----------|\n",
    "| File-based shuffle on read | `read_images(..., shuffle=\"files\")` | Low | Low |\n",
    "| Block order shuffle | `ds.randomize_block_order()` | Medium | Medium |\n",
    "| Global row shuffle | `ds.random_shuffle()` | High | High |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File-based shuffle — randomize which files are read first\n",
    "ds_shuffled = ray.data.read_images(\n",
    "    \"s3://anyscale-public-materials/ray-ai-libraries/mnist/50_per_index/\",\n",
    "    shuffle=\"files\",\n",
    ")\n",
    "\n",
    "# Block order shuffle — randomize block ordering in memory\n",
    "ds_block_shuffled = ds_preds.randomize_block_order()\n",
    "\n",
    "# Global shuffle — full row-level randomization (most expensive)\n",
    "ds_row_shuffled = ds_preds.random_shuffle()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Materializing Data\n",
    "\n",
    "By default, Ray Data streams data lazily. You can **materialize** a dataset to eagerly execute the full pipeline and store results in the Ray object store (distributed across the cluster, spilling to disk if needed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_materialized = ds_preds.materialize()\n",
    "ds_materialized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**When to materialize:**\n",
    "- When you need to reuse the same dataset multiple times (avoids re-computation)\n",
    "- When downstream operations require the full dataset (e.g., `groupby`, `random_shuffle`)\n",
    "\n",
    "**When NOT to materialize:**\n",
    "- For streaming pipelines where data flows through once (training ingest, write-to-sink)\n",
    "- When the dataset is too large to fit in the object store\n",
    "\n",
    "Use `set_name()` to label your dataset in the Ray Dashboard for observability:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_preds.set_name(\"mnist_predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 9: Persisting Data\n",
    "\n",
    "Write processed data to persistent storage using any of Ray Data's write functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_preds.write_parquet(\"/mnt/cluster_storage/mnist_preds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ray Data supports writing to Parquet, CSV, JSON, TFRecords, and more. See the [Input/Output docs](https://docs.ray.io/en/latest/data/api/input_output.html) for the full list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "!rm -rf /mnt/cluster_storage/mnist_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've covered Ray Data's core APIs hands-on, let's look under the hood at how it works. The following sections are conceptual — they'll help you reason about performance, debug issues, and tune your pipelines.\n",
    "\n",
    "## Part 10: Architecture Overview\n",
    "\n",
    "### Streaming Execution\n",
    "\n",
    "Ray Data uses a **streaming execution model** rather than traditional stage-by-stage batch processing.\n",
    "\n",
    "**Traditional batch processing** completes one stage fully before starting the next, leading to idle resources:\n",
    "\n",
    "<img src=\"https://anyscale-materials.s3.us-west-2.amazonaws.com/cko-2025-q1/batch-processing.png\" width=\"800\" alt=\"Traditional Batch Processing\">\n",
    "\n",
    "**Streaming pipelining** overlaps stages, keeping all hardware (CPUs and GPUs) busy simultaneously:\n",
    "\n",
    "<img src=\"https://anyscale-materials.s3.us-west-2.amazonaws.com/cko-2025-q1/pipelining.png\" width=\"800\" alt=\"Streaming Model Pipelining\">\n",
    "\n",
    "This is critical for GPU-heavy workloads: while the GPU runs inference on one batch, the CPU can preprocess the next batch.\n",
    "\n",
    "### Blocks and the Object Store\n",
    "\n",
    "- A **Dataset** is a distributed collection of **blocks** (PyArrow tables by default)\n",
    "- Blocks live in the **Ray object store** — shared memory distributed across cluster nodes (typically 30% of node memory)\n",
    "- When the object store fills up, Ray spills blocks to disk automatically\n",
    "- Data passes between operators via zero-copy reads from the object store\n",
    "\n",
    "### Operators, Planning, and Fusion\n",
    "\n",
    "When you chain transformations (e.g., `ds.map_batches(f1).map_batches(f2).write_parquet(...)`), Ray Data builds a **logical plan** and optimizes it:\n",
    "\n",
    "1. **Logical plan** — your declared operations\n",
    "2. **Physical plan** — optimized execution graph with operator fusion\n",
    "3. **Operator fusion** — adjacent compatible operators are merged into a single task, reducing data movement\n",
    "\n",
    "Use `ds.explain()` to inspect the execution plan:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_preds.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a deeper dive into Ray Data internals, see the [Data Internals guide](https://docs.ray.io/en/latest/data/data-internals.html) and the [Key Concepts page](https://docs.ray.io/en/latest/data/key-concepts.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 11: Resource Management and Autoscaling\n",
    "\n",
    "Ray Data's **ResourceManager** tracks CPU, GPU, heap memory, and object store usage across all operators. It dynamically allocates budgets to keep the pipeline balanced.\n",
    "\n",
    "**Key concepts:**\n",
    "\n",
    "- **Per-operator resources:** Set `num_cpus`, `num_gpus`, and `memory` on `map_batches` to control what each worker gets\n",
    "- **Backpressure:** Ray Data automatically throttles upstream operators when downstream operators can't keep up, preventing OOM errors and disk spilling\n",
    "- **Autoscaling:** Ray Data can request more cluster resources when operators are bottlenecked:\n",
    "  - *Reactive autoscaling* (default): triggers when operators stall waiting for resources\n",
    "  - *Proactive autoscaling*: triggers at 75% utilization threshold, requesting whole nodes\n",
    "\n",
    "To configure resource limits, use the `DataContext`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx = ray.data.DataContext.get_current()\n",
    "\n",
    "# Example: set target block sizes to control memory usage\n",
    "ctx.target_max_block_size = 128 * 1024 * 1024  # 128 MB max block size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the [execution configurations docs](https://docs.ray.io/en/latest/data/execution-configurations.html) and [performance tips](https://docs.ray.io/en/latest/data/performance-tips.html) for tuning guidance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 12: Observability and Performance Tuning\n",
    "\n",
    "When your Ray Data pipeline isn't performing as expected, follow this systematic approach:\n",
    "\n",
    "1. **Establish a baseline** — measure single-operator throughput in isolation\n",
    "2. **Scale up** — run the full pipeline on the target cluster size\n",
    "3. **Root cause analysis** — check for:\n",
    "   - GPU under-utilization (slow CPU preprocessing starving the GPU)\n",
    "   - Disk spilling (object store memory pressure)\n",
    "   - OOM errors (batch size too large)\n",
    "4. **Iterate** — change one parameter at a time and measure impact\n",
    "\n",
    "**Useful tools:**\n",
    "- `ds.stats()` — programmatic breakdown of per-operator throughput and timing\n",
    "- `ds.explain()` — view the execution plan\n",
    "- **Ray Dashboard** — cluster utilization, per-operator metrics, object store usage\n",
    "- **Anyscale Metrics tab** — GPU utilization, memory, network, disk I/O\n",
    "\n",
    "For detailed guidance, see the [Anyscale monitoring and debugging guide](https://docs.anyscale.com/monitoring)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 13: Fault Tolerance and Checkpointing\n",
    "\n",
    "Ray Data provides fault tolerance at two levels:\n",
    "\n",
    "### Worker-Level Retry (Open Source)\n",
    "\n",
    "If a worker task fails (e.g., OOM, transient network error), Ray Data automatically retries the task. Configure retry behavior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx = ray.data.DataContext.get_current()\n",
    "ctx.retried_io_errors = [IOError, ConnectionError]  # Retry on these errors\n",
    "ctx.max_errored_blocks = 5  # Allow up to 5 failed blocks before aborting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Job-Level Checkpointing (RayTurbo / Anyscale)\n",
    "\n",
    "For recovering from driver failures, head node crashes, or job pre-emptions, RayTurbo Data provides **job-level checkpointing**:\n",
    "\n",
    "- Checkpoints are written after each block reaches the sink:\n",
    "\n",
    "<img src=\"https://anyscale-materials.s3.us-west-2.amazonaws.com/ray-data-deep-dive/ray_data_checkpointing_storing.png\" alt=\"Ray Data Checkpoint Storing Flow\" width=\"800\">\n",
    "\n",
    "- On restart, the pipeline skips already-processed rows by matching an ID column:\n",
    "\n",
    "<img src=\"https://anyscale-materials.s3.us-west-2.amazonaws.com/ray-data-deep-dive/ray_data_checkpointing_restore.png\" alt=\"Ray Data Checkpoint Restore Flow\" width=\"800\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: This import is Anyscale-specific (RayTurbo Data). Not available in open-source Ray.\n",
    "from ray.anyscale.data.checkpoint import CheckpointConfig\n",
    "\n",
    "ctx = ray.data.DataContext.get_current()\n",
    "ctx.checkpoint_config = CheckpointConfig(\n",
    "    id_column=\"row_id\",\n",
    "    checkpoint_path=\"/mnt/cluster_storage/ray_data_checkpoint/\",\n",
    "    delete_checkpoint_on_success=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note:** Job-level checkpointing is an Anyscale/RayTurbo feature. It requires a pipeline shaped as: **read** -> **map operations** -> **write**. See the [fault tolerance docs](https://docs.ray.io/en/latest/data/fault-tolerance.html) for details on both open-source and Anyscale fault tolerance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 14: Ray Data in Production\n",
    "\n",
    "Ray Data is used in production at scale by leading AI companies:\n",
    "\n",
    "- **Netflix** — multi-modal inference pipelines processing millions of items. [Watch their Ray Summit 2024 talk](https://raysummit.anyscale.com/flow/anyscale/raysummit2024/landing/page/sessioncatalog/session/1722028596844001bCg0).\n",
    "- **Pinterest** — last-mile data processing for recommendation model training with heterogeneous cluster disaggregation. [Read their engineering blog](https://medium.com/pinterest-engineering/last-mile-data-processing-with-ray-data-629affbf34ff).\n",
    "- **Runway AI** — scaling ML workloads for AI-driven filmmaking. [See this interview](https://siliconangle.com/2024/10/02/runway-transforming-ai-driven-filmmaking-innovative-tools-techniques-raysummit/).\n",
    "- **Spotify** — ML platform built on Ray Data for batch inference. [Read their engineering blog](https://engineering.atspotify.com/2023/02/unleashing-ml-innovation-at-spotify-with-ray/).\n",
    "- **ByteDance** — offline inference with multi-modal LLMs across 200 TB of data. [Read the case study](https://www.anyscale.com/blog/how-bytedance-scales-offline-inference-with-multi-modal-llms-to-200TB-data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Next Steps\n",
    "\n",
    "In this template, you learned:\n",
    "\n",
    "- **What** Ray Data is — a scalable, streaming data processing library for AI workloads\n",
    "- **Why** to use it — heterogeneous CPU/GPU clusters, batch inference, training data ingest, fault tolerance\n",
    "- **How** to use it — loading data with IO connectors, transforming with `map_batches`, stateful transforms for batch inference, preprocessing pipelines, data operations, materialization, and persistence\n",
    "- **Architecture** — streaming execution, blocks, operator fusion, resource management, backpressure, autoscaling\n",
    "- **Operations** — observability, fault tolerance, and checkpointing\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **[Ray Data User Guide](https://docs.ray.io/en/latest/data/user-guide.html)** — In-depth guides for specific use cases\n",
    "2. **[Batch Inference Guide](https://docs.ray.io/en/latest/data/batch_inference.html)** — Detailed patterns for scaling inference\n",
    "3. **[Ray Data + Ray Train Integration](https://docs.ray.io/en/latest/train/user-guides/data-loading-preprocessing.html)** — Feeding Ray Data into distributed training\n",
    "4. **[Performance Tips](https://docs.ray.io/en/latest/data/performance-tips.html)** — Tuning for production workloads\n",
    "5. **[Ray Data API Reference](https://docs.ray.io/en/latest/data/api/api.html)** — Complete API documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up any resources\n",
    "!rm -f /mnt/cluster_storage/model.pt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}