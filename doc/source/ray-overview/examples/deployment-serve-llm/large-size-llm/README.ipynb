{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8f6fcbd",
   "metadata": {},
   "source": [
    "# Deploying a large-size LLM\n",
    "\n",
    "A large-size LLM typically runs on multiple nodes with multiple GPUs, prioritizing peak quality and capability: stronger reasoning, broader knowledge, longer context windows, more robust generalization. It’s the right choice when state-of-the-art results are required and higher latency, complexity, and cost are acceptable trade-offs.\n",
    "\n",
    "This tutorial walks you through deploying a large-size LLM like DeepSeek-R1 (685&nbsp;B parameters) using Ray Serve LLM. For smaller model, see [Deploying a small-size LLM](https://docs.ray.io/en/latest/ray-overview/examples/deployment-serve-llm/small-size-llm/notebook.html) or [Deploying a medium-size LLM](https://docs.ray.io/en/latest/ray-overview/examples/deployment-serve-llm/medium-size-llm/notebook.html).\n",
    "\n",
    "---\n",
    "\n",
    "## Challenges of Large-Scale Deployment\n",
    "\n",
    "Deploying a 685&nbsp;B-parameter model like DeepSeek-R1 presents significant technical challenges. At this scale, the model can't fit on a single GPU or even a single node. It must be distributed across multiple GPUs and nodes using *tensor parallelism* (splitting tensors within each layer) and *pipeline parallelism* (spreading layers across devices).  \n",
    "\n",
    "Deploying a model of this scale normally requires manually launching and coordinating multiple nodes, unless you use a managed platform like [Anyscale](https://www.anyscale.com/), which automates cluster scaling and node orchestration. See [Production Deployment with Anyscale Service](#production-deployment-with-anyscale-service) for more details.\n",
    "\n",
    "---\n",
    "\n",
    "## Configure Ray Serve LLM\n",
    "\n",
    "A large-size LLM is typically deployed across multiple nodes with multiple GPUs. To fully utilize the hardware, set `pipeline_parallel_size` to the number of nodes and `tensor_parallel_size` to the number of GPUs per node, which distributes the model’s weights evenly.\n",
    "\n",
    "Ray Serve LLM provides multiple [Python APIs](https://docs.ray.io/en/latest/serve/api/index.html#llm-api) for defining your application. Use [`build_openai_app`](https://docs.ray.io/en/latest/serve/api/doc/ray.serve.llm.build_openai_app.html#ray.serve.llm.build_openai_app) to build a full application from your [`LLMConfig`](https://docs.ray.io/en/latest/serve/api/doc/ray.serve.llm.LLMConfig.html#ray.serve.llm.LLMConfig) object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d185d580",
   "metadata": {},
   "outputs": [],
   "source": [
    "#serve_deepseek_r1.py\n",
    "from ray.serve.llm import LLMConfig, build_openai_app\n",
    "\n",
    "llm_config = LLMConfig(\n",
    "    model_loading_config=dict(\n",
    "        model_id=\"my-deepseek-r1\",\n",
    "        model_source=\"deepseek-ai/DeepSeek-R1\",\n",
    "    ),\n",
    "    accelerator_type=\"H100\",\n",
    "    deployment_config=dict(\n",
    "        autoscaling_config=dict(\n",
    "            min_replicas=1, max_replicas=1,\n",
    "        )\n",
    "    ),\n",
    "    engine_kwargs=dict(\n",
    "        max_model_len=16384,\n",
    "        ### Uncomment if your model is gated and need your Huggingface Token to access it\n",
    "        #hf_token=os.environ.get(\"HF_TOKEN\"),\n",
    "        # Split weights among 8 GPUs in the node\n",
    "        tensor_parallel_size=8,\n",
    "        pipeline_parallel_size=2\n",
    "    ),\n",
    ")\n",
    "\n",
    "app = build_openai_app({\"llm_configs\": [llm_config]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2231a5",
   "metadata": {},
   "source": [
    "> Before moving to a production setup, it's recommended to switch to a [Serve config file](https://docs.ray.io/en/latest/serve/production-guide/config.html). This makes your deployment version-controlled, reproducible, and easier to maintain for CI/CD pipelines for example. See [Serving LLMs: Production Guide](https://docs.ray.io/en/latest/serve/llm/serving-llms.html#production-deployment) for an example.\n",
    "\n",
    "---\n",
    "\n",
    "## Local End-to-End Deployment\n",
    "\n",
    "**Prerequisites**\n",
    "\n",
    "* Access to GPU compute.\n",
    "* (Optional) A **Hugging Face token** if using gated models like Meta’s Llama. Store it in `export HF_TOKEN=<YOUR-HUGGINGFACE-TOKEN>`\n",
    "\n",
    "> Depending on the organization, you can usually request access on the model's Hugging Face page. For example, Meta’s Llama models approval can take anywhere from a few hours to several weeks.\n",
    "\n",
    "**Dependencies:**  \n",
    "```bash\n",
    "pip install \"ray[serve,llm]\"\n",
    "```\n",
    "\n",
    "**Beware**: this is an expensive deployment.\n",
    "\n",
    "---\n",
    "\n",
    "### Launch\n",
    "\n",
    "Follow the instructions at [Configure Ray Serve LLM](#configure-ray-serve-llm) to define your app in a Python module `serve_deepseek_r1.py`.  \n",
    "\n",
    "In a terminal, run:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9da12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "serve run serve_deepseek_r1:app --non-blocking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d18e22",
   "metadata": {},
   "source": [
    "Deployment typically takes a few minutes as the cluster is provisioned, the vLLM server starts, and the model is downloaded. \n",
    "\n",
    "---\n",
    "\n",
    "### Sending Requests\n",
    "\n",
    "Your endpoint is available locally at `http://localhost:8000` and you can use a placeholder authentication token for the OpenAI client, for example `\"FAKE_KEY\"`\n",
    "\n",
    "Example Curl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1dd345c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "curl -X POST http://localhost:8000/v1/chat/completions \\\n",
    "  -H \"Authorization: Bearer FAKE_KEY\" \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -d '{ \\\n",
    "        \"model\": \"my-deepseek-r1\", \\\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": \"What is 2 + 2?\"}] \\\n",
    "      }'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca5e4fd",
   "metadata": {},
   "source": [
    "Example Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584f01f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#client.py\n",
    "from urllib.parse import urljoin\n",
    "from openai import OpenAI\n",
    "\n",
    "api_key = \"FAKE_KEY\"\n",
    "base_url = \"http://localhost:8000\"\n",
    "\n",
    "client = OpenAI(base_url=urljoin(base_url, \"v1\"), api_key=api_key)\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"my-deepseek-r1\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Tell me a joke\"}],\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "for chunk in response:\n",
    "    content = chunk.choices[0].delta.content\n",
    "    if content:\n",
    "        print(content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5fd1fb",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Shutdown "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c03cdb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "serve shutdown -y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc223463",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## Production Deployment with Anyscale Service\n",
    "\n",
    "For production, it's recommended to use Anyscale Services to deploy Ray Serve apps on dedicated clusters without code changes. Anyscale provides scalability, fault tolerance, and load balancing, while also automating multi-node setup and autoscaling for large models like DeepSeek-R1.\n",
    "\n",
    "**Beware**: this is an expensive deployment. At the time of writing, the deployment cost is around \\$110 USD per hour in the `us-west-2` AWS region using on-demand instances. Because this node has a high amount of inter-node traffic, and cross-zone traffic is expensive (around \\$0.02 per GB), it's recommended to *disable cross-zone autoscaling*. This demo is pre-configured with cross-zone autoscaling disabled for your convenience.\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "The following template runs only on H100 GPUs in your self-hosted Anyscale cloud, as H100s aren't available in Anyscale’s public cloud. This example uses two nodes of type *8xH100-80&nbsp;GB:208CPU-1830&nbsp;GB* on an AWS cloud.\n",
    "\n",
    "To provision nodes with 1000 GB of disk capacity, see [Changing the default disk size for GCP clusters](https://docs.anyscale.com/configuration/compute/gcp/#changing-the-default-disk-size) for Google Cloud Platform (GCP) or [Changing the default disk size for AWS clusters](https://docs.anyscale.com/configuration/compute/aws/#changing-the-default-disk-size) for Amazon Web Services (AWS). \n",
    "\n",
    "---\n",
    "\n",
    "### Launch\n",
    "\n",
    "Write your Anyscale Service configuration, in a new `service.yaml` file, write:  \n",
    "```yaml\n",
    "#service.yaml\n",
    "name: deploy-deepseek-r1\n",
    "image_uri: anyscale/ray-llm:2.48.0-py311-cu128\n",
    "compute_config:\n",
    "  auto_select_worker_config: true \n",
    "  # Change default disk size to 1000GB\n",
    "  advanced_instance_config:\n",
    "    ## AWS ##\n",
    "    BlockDeviceMappings:\n",
    "      - Ebs:\n",
    "        - VolumeSize: 1000\n",
    "          VolumeType: gp3\n",
    "          DeleteOnTermination: true\n",
    "        DeviceName: \"/dev/sda1\"\n",
    "    #########\n",
    "    ## GCP ##\n",
    "    #instanceProperties:\n",
    "    #  disks:\n",
    "    #    - boot: true\n",
    "    #      auto_delete: true\n",
    "    #      initialize_params:\n",
    "    #        - disk_size_gb: 1000\n",
    "    #########\n",
    "  \n",
    "working_dir: .\n",
    "cloud:\n",
    "applications:\n",
    "# Point to your app in your Python module\n",
    "- import_path: serve_deepseek_r1:app\n",
    "```\n",
    "\n",
    "Deploy your Service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1c6108",
   "metadata": {
    "pygments_lexer": "bash"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "anyscale service deploy -f service.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18226fd7",
   "metadata": {},
   "source": [
    "> If your model is gated, make sure to pass your HuggingFace Token to the Service with `--env HF_TOKEN=<YOUR_HUGGINGFACE_TOKEN>`\n",
    "\n",
    "**Custom Dockerfile**\n",
    "\n",
    "You can use any image from the Anyscale registry, or build your own Dockerfile on top of an Anyscale base image. Create a new `Dockerfile` and start with this minimal setup:\n",
    "```Dockerfile\n",
    "FROM anyscale/ray:2.48.0-slim-py312-cu128\n",
    "\n",
    "# C compiler for Triton’s runtime build step (vLLM V1 engine)\n",
    "# https://github.com/vllm-project/vllm/issues/2997\n",
    "RUN sudo apt-get update && \\\n",
    "    sudo apt-get install -y --no-install-recommends build-essential\n",
    "\n",
    "RUN curl -LsSf https://astral.sh/uv/install.sh | sh\n",
    "\n",
    "RUN uv pip install --system vllm==0.9.2\n",
    "# Avoid https://github.com/vllm-project/vllm-ascend/issues/2046 with transformers >= 4.54.0\n",
    "RUN uv pip install --system transformers==4.53.3\n",
    "```\n",
    "\n",
    "In your Anyscale Service config, replace `image_uri` with `containerfile`:\n",
    "```yaml\n",
    "#service.yaml\n",
    "...\n",
    "## Replace\n",
    "#image_uri: anyscale/ray-llm:2.48.0-py311-cu128\n",
    "## With\n",
    "containerfile: ./Dockerfile # path to your dockerfile\n",
    "...\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Sending Requests \n",
    "\n",
    "Both the endpoint and authentication token are shown in the output of the `anyscale service deploy` command:\n",
    "```console\n",
    "(anyscale +3.9s) curl -H \"Authorization: Bearer <YOUR-TOKEN>\" <YOUR-ENDPOINT>\n",
    "```\n",
    "You can also retrieve both from the service page in the Anyscale Console. Just click the **Query** button at the top. See [Sending Requests](#sending-requests) for example requests, but make sure to put the correct endpoint and authentication token.  \n",
    "\n",
    "---\n",
    "\n",
    "### Serve LLM Dashboard\n",
    "\n",
    "See [Enable LLM Monitoring](#enable-llm-monitoring) for instructions on enabling LLM-specific logging. To open the Ray Serve LLM Dashboard from an Anyscale Service:\n",
    "1. In the Anyscale console, go to your **Service** or **Workspace**\n",
    "2. Navigate to the **Metrics** tab\n",
    "3. Expand **View in Grafana** and click **Serve LLM Dashboard**\n",
    "\n",
    "---\n",
    "\n",
    "### Shutdown \n",
    " \n",
    "Shutdown your Anyscale Service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211d5baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "anyscale service terminate -n deploy-deepseek-r1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8fba49",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## Enable LLM Monitoring\n",
    "\n",
    "The *Serve LLM Dashboard* offers deep visibility into model performance, latency, and system behavior, including:\n",
    "\n",
    "* Token throughput (tokens/sec)\n",
    "* Latency metrics: Time To First Token (TTFT), Time Per Output Token (TPOT)\n",
    "* KV cache utilization\n",
    "\n",
    "To enable these metrics, go to your LLM config and set `log_engine_metrics: true`. Ensure vLLM V1 is active with `VLLM_USE_V1: \"1\"`. \n",
    "> `VLLM_USE_V1: \"1\"` is the default value with `ray >= 2.48.0` and can be omitted.\n",
    "```yaml\n",
    "applications:\n",
    "- ...\n",
    "  args:\n",
    "    llm_configs:\n",
    "      - ...\n",
    "        runtime_env:\n",
    "          env_vars:\n",
    "            VLLM_USE_V1: \"1\"\n",
    "        ...\n",
    "        log_engine_metrics: true\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Improving Concurrency\n",
    "\n",
    "Ray Serve LLM uses [vLLM](https://docs.vllm.ai/en/latest/) as its backend engine, which logs the *maximum concurrency* it can support based on your configuration.  \n",
    "\n",
    "Example log:\n",
    "```console\n",
    "INFO 07-30 11:56:04 [kv_cache_utils.py:637] Maximum concurrency for 32,768 tokens per request: 29.06x\n",
    "```\n",
    "\n",
    "Here are a few ways to improve concurrency depending on your model and hardware:  \n",
    "\n",
    "**Reduce `max_model_len`**  \n",
    "Lowering `max_model_len` reduces the memory needed for KV cache.\n",
    "\n",
    "> *Example*:  \n",
    "> Running DeepSeek-R1 on 2 nodes with 8xH100-80&nbsp;GB GPUs each:\n",
    "> * `max_model_len = 32,768` → concurrency ≈ 29\n",
    "> * `max_model_len = 16,384` → concurrency ≈ 58\n",
    "\n",
    "**Use Distilled or Quantized Models**  \n",
    "Quantizing or distilling your model reduces its memory footprint, freeing up space for more KV cache and enabling more concurrent requests. For example, see [`deepseek-ai/DeepSeek-R1-Distill-Llama-70B`](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-70B) for a distilled version of DeepSeek-R1.\n",
    "\n",
    "\n",
    "**Upgrade to GPUs with more memory**  \n",
    "Some GPUs provide significantly more room for KV cache and allow for higher concurrency out of the box.\n",
    "\n",
    "**Scale with more Replicas**  \n",
    "In addition to tuning per-GPU concurrency, you can scale *horizontally* by increasing the number of replicas in your config.  \n",
    "Each replica runs on its own GPU, so raising the replica count increases the total number of concurrent requests your service can handle, especially under sustained or bursty traffic.\n",
    "```yaml\n",
    "deployment_config:\n",
    "  autoscaling_config:\n",
    "    min_replicas: 1\n",
    "    max_replicas: 4\n",
    "```\n",
    "\n",
    "*For more details on tuning strategies and hardware guidance, see this [GPU Selection Guide for LLM Serving](https://docs.anyscale.com/overview).*\n",
    "\n",
    "---\n",
    "\n",
    "## Troubleshooting\n",
    "\n",
    "**HuggingFace Auth Errors**  \n",
    "Some models, such as Llama-3.1, are gated and require prior authorization from the organization. See your model’s documentation for instructions on obtaining access.\n",
    "\n",
    "**Out-Of-Memory Errors**  \n",
    "Out‑of‑memory (OOM) errors are one of the most common failure modes when deploying LLMs, especially as model sizes, and context length increase.  \n",
    "See this [Troubleshooting Guide](https://docs.anyscale.com/overview) for common errors and how to fix them.\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this tutorial, you deployed a medium-size LLM with Ray Serve LLM, from development to production. You learned how to configure Ray Serve LLM, deploy your service on your Ray Cluster, and how to send requests. you also learned how to monitor your app and common troubleshooting issues."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "repo_ray_docs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
