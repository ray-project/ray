{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23243c2e",
   "metadata": {},
   "source": [
    "# Deploying a Vision capable LLMs\n",
    "\n",
    "This tutorial walks you through deploying a vision-capable LLM using Ray Serve LLM.  \n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "* Access to GPU compute.\n",
    "* (Optional) A **Hugging Face token** if using gated models like Meta’s Llama. Store it in `export HF_TOKEN=<YOUR-TOKEN-HERE>`\n",
    "\n",
    "> Depending on the organization, you can usually request access on the model's Hugging Face page. For example, Meta’s Llama models approval can take anywhere from a few hours to several weeks.\n",
    "\n",
    "**Dependencies:**  \n",
    "```bash\n",
    "pip install \"ray[serve,llm]\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Configure Ray Serve LLM\n",
    "\n",
    "You can configure your deployment using the Ray Serve LLM Python SDK for fast iteration, or use a Ray Serve config file for better integration and long-term maintainability in your systems.\n",
    "\n",
    "Make sure to set your Hugging Face token in the config file to run gated models.\n",
    "\n",
    "::::{tab-set}\n",
    "\n",
    ":::{tab-item} Python SDK\n",
    "\n",
    "Ray Serve LLM provides multiple [Python APIs](https://docs.ray.io/en/latest/serve/api/index.html#llm-api) for defining your application. Use [`build_openai_app`](https://docs.ray.io/en/latest/serve/api/doc/ray.serve.llm.build_openai_app.html#ray.serve.llm.build_openai_app) to build a full application from your [`LLMConfig`](https://docs.ray.io/en/latest/serve/api/doc/ray.serve.llm.LLMConfig.html#ray.serve.llm.LLMConfig) object.\n",
    "```python\n",
    "from ray import serve\n",
    "from ray.serve.llm import LLMConfig, build_openai_app\n",
    "import os\n",
    "\n",
    "llm_config = LLMConfig(\n",
    "    model_loading_config=dict(\n",
    "        model_id=\"my-qwen-VL\",\n",
    "        model_source=\"qwen/Qwen2.5-VL-7B-Instruct\",\n",
    "    ),\n",
    "    accelerator_type=\"L40S\",\n",
    "    deployment_config=dict(\n",
    "        autoscaling_config=dict(\n",
    "            min_replicas=2, max_replicas=2,\n",
    "        )\n",
    "    ),\n",
    "    engine_kwargs=dict(\n",
    "        max_model_len=8192,\n",
    "        ### Uncomment if your model is gated and need your Huggingface Token to access it\n",
    "        #hf_token=os.environ[\"HF_TOKEN\"],\n",
    "    ),\n",
    ")\n",
    "\n",
    "app = build_openai_app({\"llm_configs\": [llm_config]})\n",
    "\n",
    "serve.run(app, blocking=True)\n",
    "```\n",
    "\n",
    ":::\n",
    "\n",
    ":::{tab-item} Serve Config (YAML)\n",
    "\n",
    "In your Ray Serve config file:\n",
    "```yaml\n",
    "applications:\n",
    "- name: vision-llm-app\n",
    "  route_prefix: \"/\"\n",
    "  import_path: ray.serve.llm:build_openai_app\n",
    "  args:\n",
    "    llm_configs:\n",
    "      - model_loading_config:\n",
    "          model_id: my-qwen-VL\n",
    "          model_source: qwen/Qwen2.5-VL-7B-Instruct\n",
    "        accelerator_type: L40S\n",
    "        deployment_config:\n",
    "          autoscaling_config:\n",
    "            min_replicas: 2\n",
    "            max_replicas: 4\n",
    "        engine_kwargs:\n",
    "          max_model_len: 8192\n",
    "          ### Uncomment if your model is gated and need your Huggingface Token to access it\n",
    "          #hf_token: <YOUR-TOKEN-HERE>\n",
    "```\n",
    "\n",
    ":::\n",
    "\n",
    "::::\n",
    "\n",
    "---\n",
    "\n",
    "## Deploy\n",
    "\n",
    "There are different ways to deploy your service depending on how you defined it.  \n",
    "\n",
    "::::{tab-set}\n",
    "\n",
    ":::{tab-item} From the Python SDK\n",
    "\n",
    "Follow the instructions at [Configure Ray Serve LLM (Python SDK)](#configure-ray-serve) to define your app in a Python module `serve_my_qwen_VL.py`.  \n",
    "> Make sure your script runs your application with `serve.run(app, blocking=True)`.  \n",
    "\n",
    "In a terminal, run:  \n",
    "```bash\n",
    "python serve_my_qwen_VL.py\n",
    "```\n",
    "\n",
    ":::\n",
    "\n",
    ":::{tab-item} From a Serve Config (YAML)  \n",
    "\n",
    "Follow the instructions at [Configure Ray Serve LLM (Serve Config)](#configure-ray-serve) to define your app with a Ray Serve config file `serve_my_qwen_VL.yaml`.  \n",
    "\n",
    "In a terminal, run:\n",
    "```bash\n",
    "serve run serve_my_qwen_VL.yaml\n",
    "```\n",
    "\n",
    ":::\n",
    "\n",
    "::::\n",
    "\n",
    "---\n",
    "\n",
    "## Sending Requests with Images\n",
    "\n",
    "> Follow the [Deployment instructions](#deploy) to launch your application on your Ray Cluster.\n",
    "\n",
    "Deployment typically takes a few minutes as the cluster is provisioned, the vLLM server starts, and the model is downloaded.\n",
    "\n",
    "**Retrieve your authentication token and endpoint**  \n",
    "If running locally, your model will be available at `<YOUR-ENDPOINT-HERE> = \"http://localhost:8000\"` and you can use a placeholder authentication token: `<YOUR-TOKEN-HERE> = \"FAKE_KEY\"`\n",
    "  > **Note:** The OpenAI client requires an `api_key`, but this is **not needed** for local deployments.  \n",
    "\n",
    "Otherwise, retrieve both your endpoint and authentication token from your deployment environment’s dashboard or logs.\n",
    "\n",
    "**Send a request**  \n",
    "Use the `model_id` defined in your config (here, `my-qwen-VL`) to query your model.\n",
    "\n",
    "::::{tab-set}\n",
    "\n",
    ":::{tab-item} Example Curl\n",
    "```bash\n",
    "curl -X POST <YOUR-ENDPOINT-HERE>/v1/chat/completions \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -H \"Authorization: Bearer <YOUR-TOKEN-HERE>\" \\\n",
    "  -d '{\n",
    "        \"model\": \"my-qwen-VL\",\n",
    "        \"messages\": [\n",
    "          {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "              {\"type\": \"text\", \"text\": \"What do you see in this image?\"},\n",
    "              {\"type\": \"image_url\", \"image_url\": {\n",
    "                \"url\": \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "              }}\n",
    "            ]\n",
    "          }\n",
    "        ]\n",
    "      }'\n",
    "```\n",
    "\n",
    ":::\n",
    "\n",
    ":::{tab-item} Example Python\n",
    "```python\n",
    "from urllib.parse import urljoin\n",
    "import base64\n",
    "from openai import OpenAI\n",
    "\n",
    "api_key = <YOUR-TOKEN-HERE>\n",
    "base_url = <YOUR-ENDPOINT-HERE>\n",
    "\n",
    "client = OpenAI(base_url=urljoin(base_url, \"v1\"), api_key=api_key)\n",
    "\n",
    "### From an image locally saved as `example.jpg`\n",
    "# Load and encode image as base64\n",
    "with open(\"example.jpg\", \"rb\") as f:\n",
    "    img_base64 = base64.b64encode(f.read()).decode()\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"my-qwen-VL\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": \"What is in this image?\"},\n",
    "                {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{img_base64}\"}}\n",
    "            ]\n",
    "        }\n",
    "    ],\n",
    "    temperature=0.5,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "for chunk in response:\n",
    "    content = chunk.choices[0].delta.content\n",
    "    if content:\n",
    "        print(content, end=\"\", flush=True)\n",
    "\n",
    "### From an image's URI\n",
    "response = client.chat.completions.create(\n",
    "    model=\"my-qwen-VL\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": \"What is in this image?\"},\n",
    "                {\"type\": \"image_url\", \"image_url\": {\"url\": \"http://images.cocodataset.org/val2017/000000039769.jpg\"}}\n",
    "            ]\n",
    "        }\n",
    "    ],\n",
    "    temperature=0.5,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "for chunk in response:\n",
    "    content = chunk.choices[0].delta.content\n",
    "    if content:\n",
    "        print(content, end=\"\", flush=True)\n",
    "```\n",
    "\n",
    ":::\n",
    "\n",
    "::::\n",
    "\n",
    "---\n",
    "\n",
    "## Limiting Images per Prompt\n",
    "\n",
    "Ray Serve LLM uses [vLLM](https://docs.vllm.ai/en/latest/) as its backend engine. You can configure vLLM by passing parameters through the `engine_kwargs` section of your Serve LLM configuration. For a full list of supported options, see the [vLLM documentation](https://docs.vllm.ai/en/latest/configuration/engine_args.html#multimodalconfig).  \n",
    "\n",
    "In particular, you can limit the number of images per request by setting `limit_mm_per_prompt` in your configuration.  \n",
    "```yaml\n",
    "applications:\n",
    "- ...\n",
    "  args:\n",
    "    llm_configs:\n",
    "        ...\n",
    "        engine_kwargs:\n",
    "          ...\n",
    "          limit_mm_per_prompt: {\"image\": 3}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this tutorial, you deployed a vision-capable LLM with Ray Serve LLM, from development to production. You learned how to configure Ray Serve LLM, deploy your service on your Ray Cluster, and how to send requests with images."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
