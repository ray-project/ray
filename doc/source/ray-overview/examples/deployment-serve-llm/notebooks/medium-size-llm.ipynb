{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8f6fcbd",
   "metadata": {},
   "source": [
    "# Deploying a medium-size LLM\n",
    "\n",
    "This tutorial walks you through deploying a medium-size LLM using Ray Serve LLM.  \n",
    "\n",
    "A medium-size model typically runs on a single node with 4—8 GPUs. For smaller model, see [Deploying a small-size LLM](#), and for larger models, see [Deploying a large-size LLM](#).\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "* Access to GPU compute.\n",
    "* (Optional) A **Hugging Face token** if using gated models like Meta’s Llama. Store it in `export HF_TOKEN=<YOUR-TOKEN-HERE>`\n",
    "\n",
    "> Depending on the organization, you can usually request access on the model's Hugging Face page. For example, Meta’s Llama models approval can take anywhere from a few hours to several weeks.\n",
    "\n",
    "**Dependencies:**  \n",
    "```bash\n",
    "pip install \"ray[serve,llm]\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Configure Ray Serve\n",
    "\n",
    "You can configure your deployment using the Ray Serve LLM Python SDK for fast iteration, or use a Ray Serve config file for better integration and long-term maintainability in your systems.\n",
    "\n",
    "Make sure to set your Hugging Face token in the config file to run gated models like `Llama-3.1`.\n",
    "\n",
    "A medium-sized LLM can typically be deployed on a single node with multiple GPUs. To leverage all available GPUs, set `tensor_parallel_size` to the number of GPUs on the node, which distributes the model’s weights evenly across them.\n",
    "\n",
    "::::{tab-set}\n",
    "\n",
    ":::{tab-item} Python SDK\n",
    "\n",
    "Ray Serve LLM provides multiple [Python APIs](https://docs.ray.io/en/latest/serve/api/index.html#llm-api) for defining your application. Use [`build_openai_app`](https://docs.ray.io/en/latest/serve/api/doc/ray.serve.llm.build_openai_app.html#ray.serve.llm.build_openai_app) to build a full application from your [`LLMConfig`](https://docs.ray.io/en/latest/serve/api/doc/ray.serve.llm.LLMConfig.html#ray.serve.llm.LLMConfig) object.\n",
    "\n",
    "```python\n",
    "from ray import serve\n",
    "from ray.serve.llm import LLMConfig, build_openai_app\n",
    "import os\n",
    "\n",
    "llm_config = LLMConfig(\n",
    "    model_loading_config=dict(\n",
    "        model_id=\"my-llama-3.1-70B\",\n",
    "        # Or Qwen/Qwen2.5-72B-Instruct for an ungated model\n",
    "        model_source=\"meta-llama/Llama-3.1-70B-Instruct\",\n",
    "    ),\n",
    "    accelerator_type=\"A100-40G\",\n",
    "    deployment_config=dict(\n",
    "        autoscaling_config=dict(\n",
    "            min_replicas=2, max_replicas=4,\n",
    "        )\n",
    "    ),\n",
    "    engine_kwargs=dict(\n",
    "        max_model_len=32768,\n",
    "        ### If your model is not gated, you can skip `hf_token`\n",
    "        # Share your Hugging Face Token to the vllm engine so it can access the gated Llama 3\n",
    "        hf_token=os.environ[\"HF_TOKEN\"],\n",
    "        # Split weights among 8 GPUs in the node\n",
    "        tensor_parallel_size=8\n",
    "    ),\n",
    ")\n",
    "\n",
    "app = build_openai_app({\"llm_configs\": [llm_config]})\n",
    "\n",
    "serve.run(app, blocking=True)\n",
    "```\n",
    "\n",
    ":::\n",
    "\n",
    ":::{tab-item} Serve Config (YAML)\n",
    "\n",
    "In your Ray Serve config file:\n",
    "```yaml\n",
    "applications:\n",
    "- name: my-medium-llm-app\n",
    "  route_prefix: \"/\"\n",
    "  import_path: ray.serve.llm:build_openai_app\n",
    "  args:\n",
    "    llm_configs:\n",
    "      - model_loading_config:\n",
    "          model_id: my-llama-3.1-70B\n",
    "          # Or Qwen/Qwen2.5-72B-Instruct for an ungated model\n",
    "          model_source: meta-llama/Llama-3.1-70B-Instruct\n",
    "        accelerator_type: A100-40G\n",
    "        deployment_config:\n",
    "          autoscaling_config:\n",
    "            min_replicas: 2\n",
    "            max_replicas: 4\n",
    "        engine_kwargs:\n",
    "          max_model_len: 32768\n",
    "          # We need to share our Hugging Face Token to the workers so they can access the gated Llama 3\n",
    "          # If your model is not gated, you can skip this\n",
    "          hf_token: <YOUR-TOKEN-HERE>\n",
    "          # Split weights among 8 GPUs in the node\n",
    "          tensor_parallel_size: 8\n",
    "```\n",
    "\n",
    "Alternatively, Ray Serve LLM provides a user-friendly CLI to generate config files with `python -m ray.serve.llm.gen_config`. More info at [Serving LLM: Generate Config Files](https://docs.ray.io/en/latest/serve/llm/serving-llms.html#generate-config-files).\n",
    "\n",
    ":::\n",
    "\n",
    "::::\n",
    "\n",
    "> Before moving to a production setup, we recommend switching to a [Serve config file](https://docs.ray.io/en/latest/serve/production-guide/config.html). This makes your deployment version-controlled, reproducible, and easier to maintain for CI/CD pipelines for example. \n",
    "\n",
    "---\n",
    "\n",
    "## Deploy\n",
    "\n",
    "There are different ways to deploy your service depending on how you defined it.  \n",
    "\n",
    "::::{tab-set}\n",
    "\n",
    ":::{tab-item} From the Python SDK\n",
    "\n",
    "Follow the instructions at [Configure Ray Serve (Python SDK)](#configure-ray-serve) to define your app in a Python module.  \n",
    "> Make sure your script runs your application with `serve.run(app, blocking=True)`.  \n",
    "\n",
    "In a terminal, run:  \n",
    "```bash\n",
    "python serve_my_llama_3_1_8B.py\n",
    "```\n",
    "\n",
    ":::\n",
    "\n",
    ":::{tab-item} From a Serve Config (YAML)  \n",
    "\n",
    "Follow the instructions at [Configure Ray Serve (Serve Config)](#configure-ray-serve) to define your app with a Ray Serve config file.  \n",
    "\n",
    "In a terminal, run:\n",
    "```bash\n",
    "serve run serve_my_llama_3_1_8B.yaml\n",
    "```\n",
    "\n",
    ":::\n",
    "\n",
    "::::\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Sending Requests to your LLM\n",
    "\n",
    "> Follow the [Deployment instructions](#deploy) to launch your application on your Ray Cluster.\n",
    "\n",
    "Deployment typically takes a few minutes as the cluster is provisioned, the vLLM server starts, and the model is downloaded.\n",
    "\n",
    "**Retrieve your authentication token and endpoint**  \n",
    "If running locally, your model will be available at `<YOUR-ENDPOINT-HERE> = \"http://localhost:8000\"` and you can use a placeholder authentication token: `<YOUR-TOKEN-HERE> = \"FAKE_KEY\"`\n",
    "  > **Note:** The OpenAI client requires an `api_key`, but this is **not needed** for local deployments.  \n",
    "\n",
    "Otherwise, retrieve both your endpoint and authentication token from your deployment environment’s dashboard or logs.\n",
    "\n",
    "**Send a request**  \n",
    "Use the `model_id` defined in your config (here, `my-llama-3.1-8B`) to query your model.\n",
    "\n",
    "::::{tab-set}\n",
    "\n",
    ":::{tab-item} Example Curl\n",
    "```bash\n",
    "curl -X POST <YOUR-ENDPOINT-HERE>/v1/chat/completions \\\n",
    "  -H \"Authorization: Bearer <YOUR-TOKEN-HERE>\" \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -d '{\n",
    "        \"model\": \"my-llama-3.1-8B\",\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": \"What is 2 + 2?\"}]\n",
    "      }'\n",
    "```\n",
    "\n",
    ":::\n",
    "\n",
    ":::{tab-item} Example Python\n",
    "```python\n",
    "from urllib.parse import urljoin\n",
    "from openai import OpenAI\n",
    "\n",
    "api_key = <YOUR-TOKEN-HERE>\n",
    "base_url = <YOUR-ENDPOINT-HERE>\n",
    "\n",
    "client = OpenAI(base_url=urljoin(base_url, \"v1\"), api_key=api_key)\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"my-llama-3.1-8B\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Tell me a joke\"}],\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "for chunk in response:\n",
    "    content = chunk.choices[0].delta.content\n",
    "    if content:\n",
    "        print(content, end=\"\", flush=True)\n",
    "```\n",
    "\n",
    ":::\n",
    "\n",
    "::::\n",
    "\n",
    "---\n",
    "\n",
    "## Enable LLM Monitoring\n",
    "\n",
    "The *Serve LLM Dashboard* offers deep visibility into model performance, latency, and system behavior, including:\n",
    "\n",
    "* Token throughput (tokens/sec)\n",
    "* Latency metrics: Time To First Token (TTFT), Time Per Output Token (TPOT)\n",
    "* KV cache utilization\n",
    "\n",
    "To enable these metrics, go to your LLM config and set `log_engine_metrics: true`. Ensure vLLM V1 is active with `VLLM_USE_V1: \"1\"`. \n",
    "> `VLLM_USE_V1: \"1\"` is the default value with `ray >= 2.48.0` and can be omitted.\n",
    "```yaml\n",
    "applications:\n",
    "- ...\n",
    "  args:\n",
    "    llm_configs:\n",
    "      - ...\n",
    "        runtime_env:\n",
    "          env_vars:\n",
    "            VLLM_USE_V1: \"1\"\n",
    "        ...\n",
    "        log_engine_metrics: true\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Improving Concurrency for LLM Inference\n",
    "\n",
    "Ray Serve LLM uses [vLLM](https://docs.vllm.ai/en/latest/) as its backend engine, which logs the *maximum concurrency* it can support based on your configuration.  \n",
    "\n",
    "Example log:\n",
    "```bash\n",
    "INFO 08-06 20:15:53 [executor_base.py:118] Maximum concurrency for 8192 tokens per request: 3.53x\n",
    "```\n",
    "\n",
    "Here are a few ways to improve concurrency depending on your model and hardware:  \n",
    "\n",
    "**Reduce `max_model_len`**  \n",
    "Lowering `max_model_len` reduces the memory needed for KV cache.\n",
    "\n",
    "> *Example*:  \n",
    "> Running llama-3.1-8B On an A10G or L4 GPU:\n",
    "> * `max_model_len = 8192` → concurrency ≈ 3.5\n",
    "> * `max_model_len = 4096` → concurrency ≈ 7\n",
    "\n",
    "**Use Quantized Models**  \n",
    "Quantizing your model (for example, to FP8) reduces the model's memory footprint, freeing up memory for more KV cache and enabling more concurrent requests.\n",
    "\n",
    "**Use Pipeline Parallelism**  \n",
    "Distribute the model's layers across multiple nodes with `pipeline_parallel_size > 1`.\n",
    "\n",
    "**Upgrade to GPUs with more memory**  \n",
    "Some GPUs provide significantly more room for KV cache and allow for higher concurrency out of the box.\n",
    "\n",
    "**Scale with more Replicas**  \n",
    "In addition to tuning per-GPU concurrency, you can scale *horizontally* by increasing the number of replicas in your config.  \n",
    "Each replica runs on its own GPU, so raising the replica count increases the total number of concurrent requests your service can handle, especially under sustained or bursty traffic.\n",
    "```yaml\n",
    "deployment_config:\n",
    "  autoscaling_config:\n",
    "    min_replicas: 1\n",
    "    max_replicas: 4\n",
    "```\n",
    "\n",
    "*For more details on tuning strategies and hardware guidance, see this [GPU Selection Guide for LLM Serving](#).*\n",
    "\n",
    "---\n",
    "\n",
    "## Troubleshooting\n",
    "\n",
    "**HuggingFace Auth Errors**  \n",
    "Some models, such as Llama-3, are gated and require prior authorization from the organization. See your model’s documentation for instructions on obtaining access.\n",
    "\n",
    "**Out-Of-Memory Errors**  \n",
    "Out‑of‑memory (OOM) errors are one of the most common failure modes when deploying LLMs, especially as model sizes, and context length increase.  \n",
    "See this [Troubleshooting Guide](#) for common errors and how to fix them.\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this tutorial, you deployed a medium-size LLM with Ray Serve LLM, from development to production. You learned how to configure Ray Serve LLM, deploy your service on your Ray Cluster, and how to send requests. you also learned how to monitor your app and common troubleshooting issues."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
