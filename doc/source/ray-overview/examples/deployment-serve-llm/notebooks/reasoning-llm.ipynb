{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c105c497",
   "metadata": {},
   "source": [
    "# Deploying a Reasoning LLM\n",
    "\n",
    "This tutorial walks you through deploying a reasoning LLM using Ray Serve LLM.  \n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "* Access to GPU compute.\n",
    "* (Optional) A **Hugging Face token** if using gated models like Meta’s Llama. Store it in `export HF_TOKEN=<YOUR-TOKEN-HERE>`\n",
    "\n",
    "> Depending on the organization, you can usually request access on the model's Hugging Face page. For example, Meta’s Llama models approval can take anywhere from a few hours to several weeks.\n",
    "\n",
    "**Dependencies:**  \n",
    "```bash\n",
    "pip install \"ray[serve,llm]\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Distinction with non-reasoning models\n",
    "\n",
    "Reasoning models are designed to simulate step-by-step, structured thought processes to solve complex tasks like math, multi-hop QA, or code generation. In contrast, non-reasoning models aim for fast, direct responses and are typically trained for fluency or instruction following without explicit intermediate reasoning. The key distinction lies in whether the model attempts to \"think through\" the problem before answering.\n",
    "\n",
    "| **Model Type**          | **Core Behavior**                    | **Use Case Examples**                                    | **Limitation**                                        |\n",
    "| ----------------------- | ------------------------------------ | -------------------------------------------------------- | ----------------------------------------------------- |\n",
    "| **Reasoning Model**     | Explicit multi-step thinking process | Math, coding, logic puzzles, multi-hop QA, CoT prompting | Slower response time, more tokens used                |\n",
    "| **Non-Reasoning Model** | Direct answer generation             | Casual queries, short instructions, single-step answers  | May struggle with complex reasoning or explainability |\n",
    "\n",
    "Many reasoning-capable models structure their outputs with special markers such as `<think>` tags, or expose reasoning traces inside dedicated fields like `reasoning_content` in the OpenAI API response. Always check the model's documentation to see how thinking is structured and controlled.\n",
    "\n",
    "> Reasoning LLMs often benefit from long context windows (32K—200K tokens), high token throughput, low-temperature decoding (greedy sampling), and strong instruction tuning or scratchpad-style reasoning.\n",
    "\n",
    "---\n",
    "\n",
    "### When to use a reasoning model?\n",
    "\n",
    "Whether you should use a reasoning model depends on how much information your prompt already provides.\n",
    "\n",
    "If your input is clear and complete, a standard model is usually faster and more efficient.  \n",
    "If your input is ambiguous or complex, a reasoning model is better suited—it can work through the problem step by step and fill in gaps through intermediate reasoning.\n",
    "\n",
    "---\n",
    "\n",
    "## Parsing Reasoning Outputs\n",
    "\n",
    "Reasoning models often separate *reasoning* from the *final answer* using tags like `<think>...</think>`. Without a proper parser, this reasoning may end up in the `content` field instead of the dedicated `reasoning_content` field.\n",
    "\n",
    "To extract reasoning correctly, configure a `reasoning_parser` in your Ray Serve deployment. This tells vLLM how to isolate the model’s thought process from the rest of the output.\n",
    "> For example, *QwQ* uses the `deepseek-r1` parser. Other models may require different parsers. See the [vLLM docs](https://docs.vllm.ai/en/stable/features/reasoning_outputs.html#supported-models) or your model's documentation to find a supported parser, or [build your own](https://docs.vllm.ai/en/stable/features/reasoning_outputs.html#how-to-support-a-new-reasoning-model) if needed.\n",
    "\n",
    "```yaml\n",
    "applications:\n",
    "- name: reasoning-llm-app\n",
    "  ...\n",
    "  args:\n",
    "    llm_configs:\n",
    "      - model_loading_config:\n",
    "          model_id: my-qwq-32B\n",
    "          model_source: Qwen/QwQ-32B\n",
    "        ...\n",
    "        engine_kwargs:\n",
    "          ...\n",
    "          reasoning_parser: deepseek-r1 # <-- for QwQ models\n",
    "```\n",
    "\n",
    "See [Define your deployment](#define-your-deployment) for a complete example.\n",
    "\n",
    "**Example Response**  \n",
    "When using a reasoning parser, the response is typically structured like this:\n",
    "\n",
    "```python\n",
    "ChatCompletionMessage(\n",
    "    content=\"The temperature is...\",\n",
    "    ...,\n",
    "    reasoning_content=\"Okay, the user is asking for the temperature today and tomorrow...\"\n",
    ")\n",
    "```\n",
    "And you can extract the content and reasoning like this\n",
    "```python\n",
    "response = client.chat.completions.create(\n",
    "  ...\n",
    ")\n",
    "\n",
    "print(f\"Content: {response.choices[0].message.content}\")\n",
    "print(f\"Reasoning: {response.choices[0].message.reasoning_content}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Configure Ray Serve\n",
    "\n",
    "You can configure your deployment using the Ray Serve LLM Python SDK for fast iteration, or use a Ray Serve config file for better integration and long-term maintainability in your systems.\n",
    "\n",
    "Make sure to set your Hugging Face token in the config file to run gated models.\n",
    "\n",
    "We set `tensor_parallel_size= 8` to distribute the model's weights among 4 GPUs in the node. \n",
    "\n",
    "::::{tab-set}\n",
    "\n",
    ":::{tab-item} Python SDK\n",
    "\n",
    "Ray Serve LLM provides multiple [Python APIs](https://docs.ray.io/en/latest/serve/api/index.html#llm-api) for defining your application. Use [`build_openai_app`](https://docs.ray.io/en/latest/serve/api/doc/ray.serve.llm.build_openai_app.html#ray.serve.llm.build_openai_app) to build a full application from your [`LLMConfig`](https://docs.ray.io/en/latest/serve/api/doc/ray.serve.llm.LLMConfig.html#ray.serve.llm.LLMConfig) object.\n",
    "```python\n",
    "from ray import serve\n",
    "from ray.serve.llm import LLMConfig, LLMServer, LLMRouter, build_openai_app\n",
    "import os\n",
    "\n",
    "llm_config = LLMConfig(\n",
    "    model_loading_config=dict(\n",
    "        model_id=\"my-qwq-32B\",\n",
    "        model_source=\"Qwen/QwQ-32B\",\n",
    "    ),\n",
    "    accelerator_type=\"A100-40G\",\n",
    "    deployment_config=dict(\n",
    "        autoscaling_config=dict(\n",
    "            min_replicas=2, max_replicas=2,\n",
    "        )\n",
    "    ),\n",
    "    engine_kwargs=dict(\n",
    "        tensor_parallel_size=8,\n",
    "        max_model_len=32768\n",
    "        ### Uncomment if your model is gated and need your Huggingface Token to access it\n",
    "        #hf_token=os.environ[\"HF_TOKEN\"],\n",
    "    ),\n",
    ")\n",
    "\n",
    "app = build_openai_app({\"llm_configs\": [llm_config]})\n",
    "\n",
    "serve.run(app, blocking=True)\n",
    "```\n",
    "\n",
    ":::\n",
    "\n",
    ":::{tab-item} Serve Config (YAML)\n",
    "\n",
    "In your Ray Serve config file:\n",
    "```yaml\n",
    "applications:\n",
    "- name: reasoning-llm-app\n",
    "  route_prefix: \"/\"\n",
    "  import_path: ray.serve.llm:build_openai_app\n",
    "  args:\n",
    "    llm_configs:\n",
    "      - model_loading_config:\n",
    "          model_id: my-qwq-32B\n",
    "          model_source: Qwen/QwQ-32B\n",
    "        accelerator_type: A100-40G\n",
    "        deployment_config:\n",
    "          autoscaling_config:\n",
    "            min_replicas: 2\n",
    "            max_replicas: 2\n",
    "        engine_kwargs:\n",
    "          tensor_parallel_size: 8\n",
    "          max_model_len: 32768\n",
    "          ### Uncomment if your model is gated and need your Huggingface Token to access it\n",
    "          #hf_token: <YOUR-TOKEN-HERE>\n",
    "```\n",
    "\n",
    ":::\n",
    "\n",
    "::::\n",
    "\n",
    "---\n",
    "\n",
    "## Deploy\n",
    "\n",
    "There are different ways to deploy your service depending on how you defined it.  \n",
    "\n",
    "::::{tab-set}\n",
    "\n",
    ":::{tab-item} From the Python SDK\n",
    "\n",
    "Follow the instructions at [Configure Ray Serve (Python SDK)](#configure-ray-serve) to define your app in a Python module `serve_my_qwq_32B.py`.  \n",
    "> Make sure your script runs your application with `serve.run(app, blocking=True)`.  \n",
    "\n",
    "In a terminal, run:  \n",
    "```bash\n",
    "python serve_my_qwq_32B.py\n",
    "```\n",
    "\n",
    ":::\n",
    "\n",
    ":::{tab-item} From a Serve Config (YAML)  \n",
    "\n",
    "Follow the instructions at [Configure Ray Serve (Serve Config)](#configure-ray-serve) to define your app with a Ray Serve config file `serve_my_qwq_32B.yaml`.  \n",
    "\n",
    "In a terminal, run:\n",
    "```bash\n",
    "serve run serve_my_qwq_32B.yaml\n",
    "```\n",
    "\n",
    ":::\n",
    "\n",
    "::::\n",
    "\n",
    "---\n",
    "\n",
    "## Sending Requests\n",
    "> Follow the [Deployment instructions](#deploy) to launch your application on your Ray Cluster.\n",
    "\n",
    "Deployment typically takes a few minutes as the cluster is provisioned, the vLLM server starts, and the model is downloaded.\n",
    "\n",
    "**Retrieve your authentication token and endpoint**  \n",
    "If running locally, your model will be available at `<YOUR-ENDPOINT-HERE> = \"http://localhost:8000\"` and you can use a placeholder authentication token: `<YOUR-TOKEN-HERE> = \"FAKE_KEY\"`\n",
    "  > **Note:** The OpenAI client requires an `api_key`, but this is **not needed** for local deployments.  \n",
    "\n",
    "Otherwise, retrieve both your endpoint and authentication token from your deployment environment’s dashboard or logs.\n",
    "\n",
    "**Send a request**  \n",
    "Use the `model_id` defined in your config (here, `my-qwq-32B`) to query your model.\n",
    "\n",
    "::::{tab-set}\n",
    "\n",
    ":::{tab-item} Example Curl\n",
    "```bash\n",
    "curl -X POST http://localhost:8000/v1/chat/completions \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -H \"Authorization: Bearer FAKE_KEY\" \\\n",
    "  -d '{\n",
    "        \"model\": \"my-qwq-32B\",\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": \"Pick three random words with 3 syllables each and count the number of R'\\''s in each of them\"}]\n",
    "      }'\n",
    "```\n",
    "\n",
    ":::\n",
    "\n",
    ":::{tab-item} Example Python\n",
    "```python\n",
    "from urllib.parse import urljoin\n",
    "from openai import OpenAI\n",
    "\n",
    "api_key = <YOUR-TOKEN-HERE>\n",
    "base_url = <YOUR-ENDPOINT-HERE>\n",
    "\n",
    "client = OpenAI(base_url=urljoin(base_url, \"v1\"), api_key=api_key)\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"my-qwq-32B\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"What is the sum of all even numbers between 1 and 100?\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(f\"Reasoning: \\n{response.choices[0].message.reasoning_content}\\n\\n\")\n",
    "print(f\"Answer: \\n {response.choices[0].message.content}\")\n",
    "```\n",
    "\n",
    ":::\n",
    "\n",
    "::::\n",
    "\n",
    "If you configure the reasoning parser, the reasoning output will appear in the `reasoning_content` field of the response message. Otherwise, it may be included in the main `content` field, typically wrapped in `<think>...</think>` tags.\n",
    "\n",
    "---\n",
    "\n",
    "## Structured Outputs and Tooling call\n",
    "\n",
    "To support structured outputs and tooling calls with your reasoning model see [Structured Output with JSON Mode](#) and [Tool and Function Calling](#).  \n",
    "\n",
    "It is recommended to use an appropriate reasoning parser to ensure good formatting of your model's response, see [Parsing Reasoning Outputs](#parsing-reasoning-outputs) for more information.\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "In this tutorial, you deployed a reasoning LLM with Ray Serve LLM, from development to production. You learned how to configure Ray Serve LLM with the right reasoning parser, deploy your service on your Ray Cluster, how to send requests, and how to parse reasoning outputs in the response."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
