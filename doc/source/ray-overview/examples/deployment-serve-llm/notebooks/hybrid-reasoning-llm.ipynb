{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d96025e1",
   "metadata": {},
   "source": [
    "# Deploying a Hybrid Reasoning LLM\n",
    "\n",
    "This tutorial walks you through deploying a Hybrid Reasoning LLM using Ray Serve LLM.  \n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "* Access to GPU compute.\n",
    "* (Optional) A **Hugging Face token** if using gated models like Meta’s Llama. Store it in `export HF_TOKEN=<YOUR-TOKEN-HERE>`\n",
    "\n",
    "> Depending on the organization, you can usually request access on the model's Hugging Face page. For example, Meta’s Llama models approval can take anywhere from a few hours to several weeks.\n",
    "\n",
    "**Dependencies:**  \n",
    "```bash\n",
    "pip install \"ray[serve,llm]\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Distinction with purely reaasoning models\n",
    "\n",
    "*Hybrid reasoning models* are reasoning-capable models that allow you to toggle the thinking process on and off. This means you can enable structured, step-by-step reasoning when needed but skip it for simpler queries to reduce latency. Pure reasoning models always apply their reasoning behavior, while hybrid models give you fine-grained control over when that reasoning is used.\n",
    "\n",
    "| **Mode**         | **Core Behavior**                            | **Use Case Examples**                                               | **Limitation**                                    |\n",
    "| ---------------- | -------------------------------------------- | ------------------------------------------------------------------- | ------------------------------------------------- |\n",
    "| **Thinking ON**  | Explicit multi-step thinking process | Math, coding, logic puzzles, multi-hop QA, CoT prompting | Slower response time, more tokens used      |\n",
    "| **Thinking OFF** | Direct answer generation                   | Casual queries, short instructions, single-step answers              | May struggle with complex reasoning or explainability |\n",
    "\n",
    "> Reasoning often benefit from long context windows (32K—200K tokens), high token throughput, low-temperature decoding (greedy sampling), and strong instruction tuning or scratchpad-style reasoning.\n",
    "\n",
    "To see an example of deploying a purely reasoning model like *QwQ-32B*, see [Deploying a Reasoning LLM](#)\n",
    "\n",
    "---\n",
    "\n",
    "## Enabling or Disabling thinking\n",
    "\n",
    "---\n",
    "\n",
    "### When to Enable or Disable Thinking Mode\n",
    "\n",
    "**When to enable thinking mode:**\n",
    "- For complex, multi-step tasks that require reasoning, such as math, physics, or logic problems.\n",
    "- When handling ambiguous queries or situations with incomplete information.\n",
    "- For planning, workflow orchestration, or when the model needs to act as an \"agent\" coordinating other tools or models.\n",
    "- When analyzing intricate data, images, or charts.\n",
    "- For in-depth code reviews or evaluating outputs from other AI systems (LLM as Judge approach).\n",
    "\n",
    "**When to disable thinking mode:**\n",
    "- For simple, well-defined, or routine tasks.\n",
    "- When low latency and fast responses are the priority.\n",
    "- When the model is used for repetitive, straightforward steps within a larger automated workflow.\n",
    "\n",
    "---\n",
    "\n",
    "### How to enable or disable thinking mode\n",
    "\n",
    "Toggling the thinking mode varies by model and framework. Always check the model's documentation to see how thinking is structured and controlled.\n",
    "\n",
    "For example, to [control reasoning in Qwen-3](https://huggingface.co/Qwen/Qwen3-32B#switching-between-thinking-and-non-thinking-mode), you can:\n",
    "* Add `\"/think\"` or `\"/no_think\"` in the prompt\n",
    "* Or set `enable_thinking` in the request:\n",
    "  `extra_body={\"chat_template_kwargs\": {\"enable_thinking\": ...}}`\n",
    "\n",
    "See [Sending Requests to an Hybrid Reasoning LLM](#sending-requests-to-an-hybrid-reasoning-llm) for practical examples.\n",
    "\n",
    "---\n",
    "\n",
    "## Parsing Reasoning Outputs\n",
    "\n",
    "In thinking mode, hybrid models often separate *reasoning* from the *final answer* using tags like `<think>...</think>`. Without a proper parser, this reasoning may end up in the `content` field instead of the dedicated `reasoning_content` field.  \n",
    "\n",
    "To make sure the reasoning output is correctly parsed, configure a `reasoning_parser` in your Ray Serve LLM deployment. This tells vLLM how to isolate the model’s thought process from the rest of the output.\n",
    "> For example, *Qwen-3* uses the `qwen3` parser. Other models may require different parsers. See the [vLLM docs](https://docs.vllm.ai/en/stable/features/reasoning_outputs.html#supported-models) or your model's documentation to find a supported parser, or [build your own](https://docs.vllm.ai/en/stable/features/reasoning_outputs.html#how-to-support-a-new-reasoning-model) if needed.\n",
    "\n",
    "```yaml\n",
    "applications:\n",
    "- ...\n",
    "  args:\n",
    "    llm_configs:\n",
    "      - model_loading_config:\n",
    "          model_id: my-qwen-3-32B\n",
    "          model_source: Qwen/Qwen3-32B\n",
    "        ...\n",
    "        engine_kwargs:\n",
    "          ...\n",
    "          reasoning_parser: qwen3 # <-- for Qwen-3 models\n",
    "```\n",
    "\n",
    "See [Define your deployment](#define-your-deployment) for a complete example.\n",
    "\n",
    "**Example Response**  \n",
    "When using a reasoning parser, the response is typically structured like this:\n",
    "\n",
    "```python\n",
    "ChatCompletionMessage(\n",
    "    content=\"The temperature is...\",\n",
    "    ...,\n",
    "    reasoning_content=\"Okay, the user is asking for the temperature today and tomorrow...\"\n",
    ")\n",
    "```\n",
    "And you can extract the content and reasoning like this\n",
    "```python\n",
    "response = client.chat.completions.create(\n",
    "  ...\n",
    ")\n",
    "\n",
    "print(f\"Content: {response.choices[0].message.content}\")\n",
    "print(f\"Reasoning: {response.choices[0].message.reasoning_content}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Configure Ray Serve LLM\n",
    "\n",
    "You can configure your deployment using the Ray Serve LLM Python SDK for fast iteration, or use a Ray Serve config file for better integration and long-term maintainability in your systems.\n",
    "\n",
    "Make sure to set your Hugging Face token in the config file to access gated models.\n",
    "\n",
    "We set `tensor_parallel_size= 8` to distribute the model's weights among 4 GPUs in the node. \n",
    "\n",
    "::::{tab-set}\n",
    "\n",
    ":::{tab-item} Python SDK\n",
    "\n",
    "Ray Serve LLM provides multiple [Python APIs](https://docs.ray.io/en/latest/serve/api/index.html#llm-api) for defining your application. Use [`build_openai_app`](https://docs.ray.io/en/latest/serve/api/doc/ray.serve.llm.build_openai_app.html#ray.serve.llm.build_openai_app) to build a full application from your [`LLMConfig`](https://docs.ray.io/en/latest/serve/api/doc/ray.serve.llm.LLMConfig.html#ray.serve.llm.LLMConfig) object.\n",
    "```python\n",
    "from ray import serve\n",
    "from ray.serve.llm import LLMConfig, build_openai_app\n",
    "import os\n",
    "\n",
    "llm_config = LLMConfig(\n",
    "    model_loading_config=dict(\n",
    "        model_id=\"my-qwen-3-32B\",\n",
    "        model_source=\"Qwen/Qwen3-32B\",\n",
    "    ),\n",
    "    accelerator_type=\"A100-40G\",\n",
    "    deployment_config=dict(\n",
    "        autoscaling_config=dict(\n",
    "            min_replicas=2, max_replicas=2,\n",
    "        )\n",
    "    ),\n",
    "    engine_kwargs=dict(\n",
    "        tensor_parallel_size=8,\n",
    "        max_model_len=32768,\n",
    "        reasoning_parser='qwen3',\n",
    "        ### Uncomment if your model is gated and need your Huggingface Token to access it\n",
    "        #hf_token=os.environ[\"HF_TOKEN\"],\n",
    "    ),\n",
    ")\n",
    "app = build_openai_app({\"llm_configs\": [llm_config]})\n",
    "\n",
    "serve.run(app, blocking=True)\n",
    "```\n",
    "\n",
    ":::\n",
    "\n",
    ":::{tab-item} Serve Config (YAML)\n",
    "\n",
    "In your Ray Serve config file:\n",
    "```yaml\n",
    "applications:\n",
    "- name: my-hybrid-llm-app\n",
    "  route_prefix: \"/\"\n",
    "  import_path: ray.serve.llm:build_openai_app\n",
    "  args:\n",
    "    llm_configs:\n",
    "      - model_loading_config:\n",
    "          model_id: my-qwen-3-32B\n",
    "          model_source: Qwen/Qwen3-32B\n",
    "        accelerator_type: A100-40G\n",
    "        deployment_config:\n",
    "          autoscaling_config:\n",
    "            min_replicas: 2\n",
    "            max_replicas: 2\n",
    "        engine_kwargs:\n",
    "          tensor_parallel_size: 8\n",
    "          max_model_len: 32768\n",
    "          reasoning_parser: qwen3\n",
    "          ### Uncomment if your model is gated and need your Huggingface Token to access it\n",
    "          #hf_token: <YOUR-TOKEN-HERE>\n",
    "```\n",
    "\n",
    ":::\n",
    "\n",
    "::::\n",
    "\n",
    "---\n",
    "\n",
    "## Deploy\n",
    "\n",
    "There are different ways to deploy your service depending on how you defined it.  \n",
    "\n",
    "::::{tab-set}\n",
    "\n",
    ":::{tab-item} From the Python SDK\n",
    "\n",
    "Follow the instructions at [Configure Ray Serve (Python SDK)](#configure-ray-serve) to define your app in a Python module `serve_my_qwen_3_32B.py`.  \n",
    "> Make sure your script runs your application with `serve.run(app, blocking=True)`.  \n",
    "\n",
    "In a terminal, run:  \n",
    "```bash\n",
    "python serve_my_qwen_3_32B.py\n",
    "```\n",
    "\n",
    ":::\n",
    "\n",
    ":::{tab-item} From a Serve Config (YAML)  \n",
    "\n",
    "Follow the instructions at [Configure Ray Serve (Serve Config)](#configure-ray-serve) to define your app with a Ray Serve config file `serve_my_qwen_3_32B.yaml`.  \n",
    "\n",
    "In a terminal, run:\n",
    "```bash\n",
    "serve run serve_my_qwen_3_32B.yaml\n",
    "```\n",
    "\n",
    ":::\n",
    "\n",
    "::::\n",
    "\n",
    "Deployment typically takes a few minutes as the cluster is provisioned, the vLLM server starts, and the model is downloaded.  \n",
    "\n",
    "---\n",
    "\n",
    "## Sending Requests to an Hybrid Reasoning LLM\n",
    "\n",
    "> Follow the [Deployment instructions](#deploy) to launch your application on your Ray Cluster.\n",
    "\n",
    "**Retrieve your authentication token and endpoint**  \n",
    "If running locally, your model will be available at `<YOUR-ENDPOINT-HERE> = \"http://localhost:8000\"` and you can use a placeholder authentication token: `<YOUR-TOKEN-HERE> = \"FAKE_KEY\"`\n",
    "  > **Note:** The OpenAI client requires an `api_key`, but this is not needed for local deployments.  \n",
    "\n",
    "Otherwise, retrieve both your endpoint and authentication token from your deployment environment’s dashboard or logs.\n",
    "\n",
    "Use the `model_id` defined in your config (here, `my-qwen-3-32B`) to query your model. See [How to enable or disable thinking mode](#how-to-enable-or-disable-thinking-mode) for more information on how to enable and disable thinking in your hybrid reasoning model. Here are some examples with Qwen-3. \n",
    "\n",
    "**Send a request with thinking disabled**  \n",
    "You can disable thinking in Qwen-3 by either adding a `/no_think` tag in the prompt or by forwarding `enable_thinking: False` to the vLLM inference engine.  \n",
    "\n",
    "::::{tab-set}\n",
    "\n",
    ":::{tab-item} Example Curl with `/no_think`\n",
    "```bash\n",
    "curl -X POST http://localhost:8000/v1/chat/completions \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -H \"Authorization: Bearer FAKE_KEY\" \\\n",
    "  -d '{\n",
    "        \"model\": \"my-qwen-3-32B\",\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": \"What is greater between 7.8 and 7.11 ? /no_think\"}]\n",
    "      }'\n",
    "```\n",
    "\n",
    ":::\n",
    "\n",
    ":::{tab-item} Example Python with `enable_thinking: False`\n",
    "```python\n",
    "from urllib.parse import urljoin\n",
    "from openai import OpenAI\n",
    "\n",
    "api_key = <YOUR-TOKEN-HERE>\n",
    "base_url = <YOUR-ENDPOINT-HERE>\n",
    "\n",
    "client = OpenAI(base_url=urljoin(base_url, \"v1\"), api_key=api_key)\n",
    "\n",
    "# Example: Complex query with thinking process\n",
    "response = client.chat.completions.create(\n",
    "    model=\"my-qwen-3-32B\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"What's the capital of France ?\"}\n",
    "    ],\n",
    "    extra_body={\"chat_template_kwargs\": {\"enable_thinking\": False}}\n",
    ")\n",
    "\n",
    "print(f\"Reasoning: \\n{response.choices[0].message.reasoning_content}\\n\\n\")\n",
    "print(f\"Answer: \\n {response.choices[0].message.content}\")\n",
    "```\n",
    "\n",
    ":::\n",
    "\n",
    "::::\n",
    "\n",
    "**Send a request with thinking enabled**  \n",
    "You can enable thinking in Qwen-3 by either adding a `/think` tag in the prompt or by forwarding `enable_thinking: True` to the vLLM inference engine.  \n",
    "\n",
    "::::{tab-set}\n",
    "\n",
    ":::{tab-item} Example Curl with `/think`\n",
    "```bash\n",
    "curl -X POST http://localhost:8000/v1/chat/completions \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -H \"Authorization: Bearer FAKE_KEY\" \\\n",
    "  -d '{\n",
    "        \"model\": \"my-qwen-3-32B\",\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": \"What is greater between 7.8 and 7.11 ? /think\"}]\n",
    "      }'\n",
    "```\n",
    "\n",
    ":::\n",
    "\n",
    ":::{tab-item} Example Python with `enable_thinking: True`\n",
    "```python\n",
    "from urllib.parse import urljoin\n",
    "from openai import OpenAI\n",
    "\n",
    "api_key = <YOUR-TOKEN-HERE>\n",
    "base_url = <YOUR-ENDPOINT-HERE>\n",
    "\n",
    "client = OpenAI(base_url=urljoin(base_url, \"v1\"), api_key=api_key)\n",
    "\n",
    "# Example: Complex query with thinking process\n",
    "response = client.chat.completions.create(\n",
    "    model=\"my-qwen-3-32B\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"What's the capital of France ?\"}\n",
    "    ],\n",
    "    extra_body={\"chat_template_kwargs\": {\"enable_thinking\": True}}\n",
    ")\n",
    "\n",
    "print(f\"Reasoning: \\n{response.choices[0].message.reasoning_content}\\n\\n\")\n",
    "print(f\"Answer: \\n {response.choices[0].message.content}\")\n",
    "```\n",
    "\n",
    ":::\n",
    "\n",
    "::::\n",
    "\n",
    "When the reasoning parser is configured correctly, the reasoning output appears in the `reasoning_content` field of the response message. Otherwise, it may be included in the main `content` field, usually wrapped in `<think>...</think>` tags. See [Parsing Reasoning Outputs](#parsing-reasoning-outputs) for details.\n",
    "\n",
    "---\n",
    "\n",
    "## Streaming Reasoning Content\n",
    "\n",
    "In thinking mode, hybrid reasoning models may take longer to begin generating the main content. You can stream their intermediate reasoning output in the same way as the main content.  \n",
    "```python\n",
    "from urllib.parse import urljoin\n",
    "from openai import OpenAI\n",
    "\n",
    "api_key = <YOUR-TOKEN-HERE>\n",
    "base_url = <YOUR-ENDPOINT-HERE>\n",
    "\n",
    "client = OpenAI(base_url=urljoin(base_url, \"v1\"), api_key=api_key)\n",
    "\n",
    "# Example: Complex query with thinking process\n",
    "response = client.chat.completions.create(\n",
    "    model=\"my-qwen-3-32B\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"I need to plan a trip to Paris from Seattle. Can you help me research flight costs, create an itinerary for 3 days, and suggest restaurants based on my dietary restrictions (vegetarian)?\"}\n",
    "    ],\n",
    "    extra_body={\"chat_template_kwargs\": {\"enable_thinking\": True}},\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "# Stream \n",
    "for chunk in response:\n",
    "    # Stream reasoning content\n",
    "    if hasattr(chunk.choices[0].delta, \"reasoning_content\"):\n",
    "        data_reasoning = chunk.choices[0].delta.reasoning_content\n",
    "        if data_reasoning:\n",
    "            print(data_reasoning, end=\"\", flush=True)\n",
    "    # Later, stream the final answer\n",
    "    if hasattr(chunk.choices[0].delta, \"content\"):\n",
    "        data_content = chunk.choices[0].delta.content\n",
    "        if data_content:\n",
    "            print(data_content, end=\"\", flush=True)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this tutorial, you deployed a reasoning LLM with Ray Serve LLM, from development to production. You learned how to configure Ray Serve LLM with the right reasoning parser, deploy your service on your Ray Cluster, how to send requests, and how to parse reasoning outputs in the response."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
