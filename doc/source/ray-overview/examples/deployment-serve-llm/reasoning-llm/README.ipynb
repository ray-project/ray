{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c105c497",
   "metadata": {},
   "source": [
    "# Deploying a Reasoning LLM\n",
    "\n",
    "A reasoning LLM is built to handle tasks that require deeper analysis or step-by-step thought. It can generate intermediate reasoning before arriving at a final answer, making it better suited for situations where careful logic or structured problem-solving is more important than speed or efficiency.\n",
    "\n",
    "This tutorial walks you through deploying a reasoning LLM using Ray Serve LLM.  \n",
    "\n",
    "---\n",
    "\n",
    "## Distinction with non-reasoning models\n",
    "\n",
    "Reasoning models are designed to simulate step-by-step, structured thought processes to solve complex tasks like math, multi-hop QA, or code generation. In contrast, non-reasoning models aim for fast, direct responses and are typically trained for fluency or instruction following without explicit intermediate reasoning. The key distinction lies in whether the model attempts to \"think through\" the problem before answering.\n",
    "\n",
    "| **Model Type**          | **Core Behavior**                    | **Use Case Examples**                                    | **Limitation**                                        |\n",
    "| ----------------------- | ------------------------------------ | -------------------------------------------------------- | ----------------------------------------------------- |\n",
    "| **Reasoning Model**     | Explicit multi-step thinking process | Math, coding, logic puzzles, multi-hop QA, CoT prompting | Slower response time, more tokens used                |\n",
    "| **Non-Reasoning Model** | Direct answer generation             | Casual queries, short instructions, single-step answers  | May struggle with complex reasoning or interpretability |\n",
    "\n",
    "Many reasoning-capable models structure their outputs with special markers such as `<think>` tags, or expose reasoning traces inside dedicated fields like `reasoning_content` in the OpenAI API response. Always check the model's documentation to see how thinking is structured and controlled.\n",
    "\n",
    "> Reasoning LLMs often benefit from long context windows (32K up to +1M tokens), high token throughput, low-temperature decoding (greedy sampling), and strong instruction tuning or scratchpad-style reasoning.\n",
    "\n",
    "---\n",
    "\n",
    "### When to use a reasoning model?\n",
    "\n",
    "Whether you should use a reasoning model depends on how much information your prompt already provides.\n",
    "\n",
    "If your input is clear and complete, a standard model is usually faster and more efficient.  \n",
    "If your input is ambiguous or complex, a reasoning model is better suited, it can work through the problem step by step and fill in gaps through intermediate reasoning.\n",
    "\n",
    "---\n",
    "\n",
    "## Parsing Reasoning Outputs\n",
    "\n",
    "Reasoning models often separate *reasoning* from the *final answer* using tags like `<think>...</think>`. Without a proper parser, this reasoning may end up in the `content` field instead of the dedicated `reasoning_content` field.\n",
    "\n",
    "To extract reasoning correctly, configure a `reasoning_parser` in your Ray Serve deployment. This tells vLLM how to isolate the model’s thought process from the rest of the output.\n",
    "> For example, *QwQ* uses the `deepseek-r1` parser. Other models may require different parsers. See the [vLLM docs](https://docs.vllm.ai/en/stable/features/reasoning_outputs.html#supported-models) or your model's documentation to find a supported parser, or [build your own](https://docs.vllm.ai/en/stable/features/reasoning_outputs.html#how-to-support-a-new-reasoning-model) if needed.\n",
    "\n",
    "```yaml\n",
    "applications:\n",
    "- name: reasoning-llm-app\n",
    "  ...\n",
    "  args:\n",
    "    llm_configs:\n",
    "      - model_loading_config:\n",
    "          model_id: my-qwq-32B\n",
    "          model_source: Qwen/QwQ-32B\n",
    "        ...\n",
    "        engine_kwargs:\n",
    "          ...\n",
    "          reasoning_parser: deepseek_r1 # <-- for QwQ models\n",
    "```\n",
    "\n",
    "See [Configure Ray Serve LLM](#configure-ray-serve-llm) for a complete example.\n",
    "\n",
    "**Example Response**  \n",
    "When using a reasoning parser, the response is typically structured like this:\n",
    "\n",
    "```python\n",
    "ChatCompletionMessage(\n",
    "    content=\"The temperature is...\",\n",
    "    ...,\n",
    "    reasoning_content=\"Okay, the user is asking for the temperature today and tomorrow...\"\n",
    ")\n",
    "```\n",
    "And you can extract the content and reasoning like this\n",
    "```python\n",
    "response = client.chat.completions.create(\n",
    "  ...\n",
    ")\n",
    "\n",
    "print(f\"Content: {response.choices[0].message.content}\")\n",
    "print(f\"Reasoning: {response.choices[0].message.reasoning_content}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Configure Ray Serve LLM\n",
    "\n",
    "Make sure to set your Hugging Face token in the config file to access gated models.\n",
    "\n",
    "Ray Serve LLM provides multiple [Python APIs](https://docs.ray.io/en/latest/serve/api/index.html#llm-api) for defining your application. Use [`build_openai_app`](https://docs.ray.io/en/latest/serve/api/doc/ray.serve.llm.build_openai_app.html#ray.serve.llm.build_openai_app) to build a full application from your [`LLMConfig`](https://docs.ray.io/en/latest/serve/api/doc/ray.serve.llm.LLMConfig.html#ray.serve.llm.LLMConfig) object.\n",
    "\n",
    "Set `tensor_parallel_size= 8` to distribute the model's weights among 8 GPUs in the node. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ae0ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#serve_qwq_32b.py\n",
    "from ray.serve.llm import LLMConfig, build_openai_app\n",
    "import os\n",
    "\n",
    "llm_config = LLMConfig(\n",
    "    model_loading_config=dict(\n",
    "        model_id=\"my-qwq-32B\",\n",
    "        model_source=\"Qwen/QwQ-32B\",\n",
    "    ),\n",
    "    accelerator_type=\"A100-40G\",\n",
    "    deployment_config=dict(\n",
    "        autoscaling_config=dict(\n",
    "            min_replicas=1, max_replicas=2,\n",
    "        )\n",
    "    ),\n",
    "    engine_kwargs=dict(\n",
    "        tensor_parallel_size=8,\n",
    "        max_model_len=32768,\n",
    "        reasoning_parser='deepseek_r1',\n",
    "        ### Uncomment if your model is gated and need your Huggingface Token to access it\n",
    "        #hf_token=os.environ.get(\"HF_TOKEN\"),\n",
    "    ),\n",
    ")\n",
    "\n",
    "app = build_openai_app({\"llm_configs\": [llm_config]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d515e268",
   "metadata": {},
   "source": [
    "> Before moving to a production setup, it's recommended to switch to a [Serve config file](https://docs.ray.io/en/latest/serve/production-guide/config.html). This makes your deployment version-controlled, reproducible, and easier to maintain for CI/CD pipelines for example. See [Serving LLMs: Production Guide](https://docs.ray.io/en/latest/serve/llm/serving-llms.html#production-deployment) for an example.\n",
    "\n",
    "---\n",
    "\n",
    "## Local End-to-End Deployment\n",
    "\n",
    "**Prerequisites**\n",
    "\n",
    "* Access to GPU compute.\n",
    "* (Optional) A **Hugging Face token** if using gated models like Meta’s Llama. Store it in `export HF_TOKEN=<YOUR-TOKEN-HERE>`\n",
    "\n",
    "> Depending on the organization, you can usually request access on the model's Hugging Face page. For example, Meta’s Llama models approval can take anywhere from a few hours to several weeks.\n",
    "\n",
    "**Dependencies:**  \n",
    "```bash\n",
    "pip install \"ray[serve,llm]\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Launch\n",
    "\n",
    "Follow the instructions at [Configure Ray Serve LLM](#configure-ray-serve-llm) to define your app in a Python module `serve_qwq_32b.py`.  \n",
    "\n",
    "In a terminal, run:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d6a307",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "serve run serve_qwq_32b:app --non-blocking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646f1272",
   "metadata": {},
   "source": [
    "Deployment typically takes a few minutes as the cluster is provisioned, the vLLM server starts, and the model is downloaded. \n",
    "\n",
    "---\n",
    "\n",
    "### Sending Request\n",
    "\n",
    "Your endpoint is available locally at `http://localhost:8000` and you can use a placeholder authentication token for the OpenAI client, for example `\"FAKE_KEY\"`\n",
    "\n",
    "Example Curl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a53387",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "curl -X POST http://localhost:8000/v1/chat/completions \\\n",
    "  -H \"Authorization: Bearer FAKE_KEY\" \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -d '{ \\\n",
    "        \"model\": \"my-qwq-32B\", \\\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": \"Pick three random words with 3 syllables each and count the number of R'\\''s in each of them\"}] \\\n",
    "      }'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942e675c",
   "metadata": {},
   "source": [
    "Example Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5005dde7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#client.py\n",
    "from urllib.parse import urljoin\n",
    "from openai import OpenAI\n",
    "\n",
    "api_key = \"FAKE_KEY\"\n",
    "base_url = \"http://localhost:8000\"\n",
    "\n",
    "client = OpenAI(base_url=urljoin(base_url, \"v1\"), api_key=api_key)\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"my-qwq-32B\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"What is the sum of all even numbers between 1 and 100?\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(f\"Reasoning: \\n{response.choices[0].message.reasoning_content}\\n\\n\")\n",
    "print(f\"Answer: \\n {response.choices[0].message.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e04db4e",
   "metadata": {},
   "source": [
    "If you configure a valid reasoning parser, the reasoning output should appear in the `reasoning_content` field of the response message. Otherwise, it may be included in the main `content` field, typically wrapped in `<think>...</think>` tags. See [Parsing Reasoning Outputs](#parsing-reasoning-outputs) for more information.\n",
    "\n",
    "---\n",
    "\n",
    "### Shutdown \n",
    "\n",
    "Shutdown your LLM service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1f3edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "serve shutdown -y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc9e8eb",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Production Deployment with Anyscale Service\n",
    "\n",
    "For production, it's recommended to use Anyscale Services to deploy your Ray Serve app on a dedicated cluster without code changes. Anyscale provides scalability, fault tolerance, and load balancing, ensuring resilience against node failures, high traffic, and rolling updates. See [Deploying a medium-size LLM](https://docs.ray.io/en/latest/ray-overview/examples/deployment-serve-llm/medium-size-llm/README.html#production-deployment-with-anyscale-service) for an example with a medium-size model like the *QwQ-32&nbsp;B* used here.\n",
    "\n",
    "---\n",
    "\n",
    "## Streaming Reasoning Content\n",
    "\n",
    "Reasoning models may take longer to begin generating the main content. You can stream their intermediate reasoning output in the same way as the main content.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02472f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#client_streaming.py\n",
    "from urllib.parse import urljoin\n",
    "from openai import OpenAI\n",
    "\n",
    "api_key = <YOUR-TOKEN-HERE>\n",
    "base_url = <YOUR-ENDPOINT-HERE>\n",
    "\n",
    "client = OpenAI(base_url=urljoin(base_url, \"v1\"), api_key=api_key)\n",
    "\n",
    "# Example: Complex query with thinking process\n",
    "response = client.chat.completions.create(\n",
    "    model=\"my-qwq-32B\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"I need to plan a trip to Paris from Seattle. Can you help me research flight costs, create an itinerary for 3 days, and suggest restaurants based on my dietary restrictions (vegetarian)?\"}\n",
    "    ],\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "# Stream\n",
    "for chunk in response:\n",
    "    # Stream reasoning content\n",
    "    if hasattr(chunk.choices[0].delta, \"reasoning_content\"):\n",
    "        data_reasoning = chunk.choices[0].delta.reasoning_content\n",
    "        if data_reasoning:\n",
    "            print(data_reasoning, end=\"\", flush=True)\n",
    "    # Later, stream the final answer\n",
    "    if hasattr(chunk.choices[0].delta, \"content\"):\n",
    "        data_content = chunk.choices[0].delta.content\n",
    "        if data_content:\n",
    "            print(data_content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70455ea2",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this tutorial, you deployed a reasoning LLM with Ray Serve LLM, from development to production. You learned how to configure Ray Serve LLM with the right reasoning parser, deploy your service on your Ray Cluster, how to send requests, and how to parse reasoning outputs in the response."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "repo_ray_docs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
