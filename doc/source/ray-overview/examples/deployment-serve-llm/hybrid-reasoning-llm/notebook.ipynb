{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e926219a",
   "metadata": {},
   "source": [
    "# Deploying a Hybrid Reasoning LLM\n",
    "\n",
    "This tutorial walks you through deploying a Hybrid Reasoning LLM using Ray Serve LLM.  \n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "* Access to GPU compute.\n",
    "* (Optional) A **Hugging Face token** if using gated models like Meta’s Llama. Store it in `export HF_TOKEN=<YOUR-TOKEN-HERE>`\n",
    "\n",
    "> Depending on the organization, you can usually request access on the model's Hugging Face page. For example, Meta’s Llama models approval can take anywhere from a few hours to several weeks.\n",
    "\n",
    "**Dependencies:**  \n",
    "```bash\n",
    "pip install \"ray[serve,llm]\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Distinction with purely reasoning models\n",
    "\n",
    "*Hybrid reasoning models* are reasoning-capable models that allow you to toggle the thinking process on and off. This means you can enable structured, step-by-step reasoning when needed but skip it for simpler queries to reduce latency. Purely reasoning models always apply their reasoning behavior, while hybrid models give you fine-grained control over when that reasoning is used.\n",
    "\n",
    "| **Mode**         | **Core Behavior**                            | **Use Case Examples**                                               | **Limitation**                                    |\n",
    "| ---------------- | -------------------------------------------- | ------------------------------------------------------------------- | ------------------------------------------------- |\n",
    "| **Thinking ON**  | Explicit multi-step thinking process | Math, coding, logic puzzles, multi-hop QA, CoT prompting | Slower response time, more tokens used      |\n",
    "| **Thinking OFF** | Direct answer generation                   | Casual queries, short instructions, single-step answers              | May struggle with complex reasoning or explainability |\n",
    "\n",
    "> Reasoning often benefit from long context windows (32K up to +1M tokens), high token throughput, low-temperature decoding (greedy sampling), and strong instruction tuning or scratchpad-style reasoning.\n",
    "\n",
    "To see an example of deploying a purely reasoning model like *QwQ-32B*, see [Deploying a Reasoning LLM](https://docs.ray.io/en/latest/ray-overview/examples/deployment-serve-llm/notebooks/reasoning-llm.html)\n",
    "\n",
    "---\n",
    "\n",
    "## Enabling or Disabling thinking\n",
    "\n",
    "---\n",
    "\n",
    "### When to Enable or Disable Thinking Mode\n",
    "\n",
    "**When to enable thinking mode:**\n",
    "- For complex, multi-step tasks that require reasoning, such as math, physics, or logic problems.\n",
    "- When handling ambiguous queries or situations with incomplete information.\n",
    "- For planning, workflow orchestration, or when the model needs to act as an \"agent\" coordinating other tools or models.\n",
    "- When analyzing intricate data, images, or charts.\n",
    "- For in-depth code reviews or evaluating outputs from other AI systems (LLM as Judge approach).\n",
    "\n",
    "**When to disable thinking mode:**\n",
    "- For simple, well-defined, or routine tasks.\n",
    "- When low latency and fast responses are the priority.\n",
    "- When the model is used for repetitive, straightforward steps within a larger automated workflow.\n",
    "\n",
    "---\n",
    "\n",
    "### How to enable or disable thinking mode\n",
    "\n",
    "Toggling the thinking mode varies by model and framework. Always check the model's documentation to see how thinking is structured and controlled.\n",
    "\n",
    "For example, to [control reasoning in Qwen-3](https://huggingface.co/Qwen/Qwen3-32B#switching-between-thinking-and-non-thinking-mode), you can:\n",
    "* Add `\"/think\"` or `\"/no_think\"` in the prompt\n",
    "* Or set `enable_thinking` in the request:\n",
    "  `extra_body={\"chat_template_kwargs\": {\"enable_thinking\": ...}}`\n",
    "\n",
    "See [Sending Requests with Thinking Enabled](#sending-request-with-thinking-enabled) or [Sending Requests with Thinking Disabled](#sending-request-with-thinking-disabled) for practical examples.\n",
    "\n",
    "---\n",
    "\n",
    "## Parsing Reasoning Outputs\n",
    "\n",
    "In thinking mode, hybrid models often separate *reasoning* from the *final answer* using tags like `<think>...</think>`. Without a proper parser, this reasoning may end up in the `content` field instead of the dedicated `reasoning_content` field.  \n",
    "\n",
    "To make sure the reasoning output is correctly parsed, configure a `reasoning_parser` in your Ray Serve LLM deployment. This tells vLLM how to isolate the model’s thought process from the rest of the output.\n",
    "> For example, *Qwen-3* uses the `qwen3` parser. See the [vLLM docs](https://docs.vllm.ai/en/stable/features/reasoning_outputs.html#supported-models) or your model's documentation to find a supported parser, or [build your own](https://docs.vllm.ai/en/stable/features/reasoning_outputs.html#how-to-support-a-new-reasoning-model) if needed.\n",
    "\n",
    "```yaml\n",
    "applications:\n",
    "- ...\n",
    "  args:\n",
    "    llm_configs:\n",
    "      - model_loading_config:\n",
    "          model_id: my-qwen-3-32b\n",
    "          model_source: Qwen/Qwen3-32B\n",
    "        ...\n",
    "        engine_kwargs:\n",
    "          ...\n",
    "          reasoning_parser: qwen3 # <-- for Qwen-3 models\n",
    "```\n",
    "\n",
    "See [Configure Ray Serve LLM](#configure-ray-serve-llm) for a complete example.\n",
    "\n",
    "**Example Response**  \n",
    "When using a reasoning parser, the response is typically structured like this:\n",
    "\n",
    "```python\n",
    "ChatCompletionMessage(\n",
    "    content=\"The temperature is...\",\n",
    "    ...,\n",
    "    reasoning_content=\"Okay, the user is asking for the temperature today and tomorrow...\"\n",
    ")\n",
    "```\n",
    "And you can extract the content and reasoning like this\n",
    "```python\n",
    "response = client.chat.completions.create(\n",
    "  ...\n",
    ")\n",
    "\n",
    "print(f\"Content: {response.choices[0].message.content}\")\n",
    "print(f\"Reasoning: {response.choices[0].message.reasoning_content}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Configure Ray Serve LLM\n",
    "\n",
    "Make sure to set your Hugging Face token in the config file to access gated models.\n",
    "\n",
    "Ray Serve LLM provides multiple [Python APIs](https://docs.ray.io/en/latest/serve/api/index.html#llm-api) for defining your application. Use [`build_openai_app`](https://docs.ray.io/en/latest/serve/api/doc/ray.serve.llm.build_openai_app.html#ray.serve.llm.build_openai_app) to build a full application from your [`LLMConfig`](https://docs.ray.io/en/latest/serve/api/doc/ray.serve.llm.LLMConfig.html#ray.serve.llm.LLMConfig) object.\n",
    "\n",
    "We set `tensor_parallel_size= 8` to distribute the model's weights among 8 GPUs in the node. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1daf892",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#serve_qwen_3_32b.py\n",
    "from ray import serve\n",
    "from ray.serve.llm import LLMConfig, build_openai_app\n",
    "import os\n",
    "\n",
    "llm_config = LLMConfig(\n",
    "    model_loading_config=dict(\n",
    "        model_id=\"my-qwen-3-32b\",\n",
    "        model_source=\"Qwen/Qwen3-32B\",\n",
    "    ),\n",
    "    accelerator_type=\"A100-40G\",\n",
    "    deployment_config=dict(\n",
    "        autoscaling_config=dict(\n",
    "            min_replicas=2, max_replicas=2,\n",
    "        )\n",
    "    ),\n",
    "    engine_kwargs=dict(\n",
    "        tensor_parallel_size=8,\n",
    "        max_model_len=32768,\n",
    "        reasoning_parser='qwen3',\n",
    "        ### Uncomment if your model is gated and need your Huggingface Token to access it\n",
    "        #hf_token=os.environ[\"HF_TOKEN\"],\n",
    "    ),\n",
    ")\n",
    "app = build_openai_app({\"llm_configs\": [llm_config]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32272280",
   "metadata": {},
   "source": [
    "> Before moving to a production setup, we recommend switching to a [Serve config file](https://docs.ray.io/en/latest/serve/production-guide/config.html). This makes your deployment version-controlled, reproducible, and easier to maintain for CI/CD pipelines for example. See [Serving LLMs: Production Guide](https://docs.ray.io/en/latest/serve/llm/serving-llms.html#production-deployment) for an example.\n",
    "\n",
    "---\n",
    "\n",
    "## Local End-to-End Deployment\n",
    "\n",
    "Follow the instructions at [Configure Ray Serve LLM](#configure-ray-serve-llm) to define your app in a Python module `serve_qwen_3_32b.py`.  \n",
    "\n",
    "---\n",
    "\n",
    "### Launch\n",
    "\n",
    "In a terminal, run:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8f1b58",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!serve run serve_qwen_3_32b:app --non-blocking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24501f5",
   "metadata": {},
   "source": [
    "Deployment typically takes a few minutes as the cluster is provisioned, the vLLM server starts, and the model is downloaded. \n",
    "\n",
    "Your endpoint will be available locally at `http://localhost:8000` and you can use a placeholder authentication token for the OpenAI client, for example `\"FAKE_KEY\"`\n",
    "\n",
    "Use the `model_id` defined in your config (here, `my-qwen-3-32b`) to query your model. Here are some examples on how to send request to a Qwen-3 deployment with thinking enabled or disabled. \n",
    "\n",
    "---\n",
    "\n",
    "### Sending Request with Thinking Disabled\n",
    "\n",
    "You can disable thinking in Qwen-3 by either adding a `/no_think` tag in the prompt or by forwarding `enable_thinking: False` to the vLLM inference engine.  \n",
    "\n",
    "Example Curl with `/no_think`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77d2201",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!curl -X POST http://localhost:8000/v1/chat/completions \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -H \"Authorization: Bearer FAKE_KEY\" \\\n",
    "  -d '{ \\\n",
    "        \"model\": \"my-qwen-3-32b\", \\\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": \"What is greater between 7.8 and 7.11 ? /no_think\"}] \\\n",
    "      }'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a127ea5f",
   "metadata": {},
   "source": [
    "Example Python with `enable_thinking: False`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51e9d85",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#client_thinking_disabled.py\n",
    "from urllib.parse import urljoin\n",
    "from openai import OpenAI\n",
    "\n",
    "api_key = \"FAKE_KEY\"\n",
    "base_url = \"http://localhost:8000\"\n",
    "\n",
    "client = OpenAI(base_url=urljoin(base_url, \"v1\"), api_key=api_key)\n",
    "\n",
    "# Example: Complex query with thinking process\n",
    "response = client.chat.completions.create(\n",
    "    model=\"my-qwen-3-32b\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"What's the capital of France ?\"}\n",
    "    ],\n",
    "    extra_body={\"chat_template_kwargs\": {\"enable_thinking\": False}}\n",
    ")\n",
    "\n",
    "print(f\"Reasoning: \\n{response.choices[0].message.reasoning_content}\\n\\n\")\n",
    "print(f\"Answer: \\n {response.choices[0].message.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9765b3f8",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Sending Request with Thinking Enabled\n",
    " \n",
    "You can enable thinking in Qwen-3 by either adding a `/think` tag in the prompt or by forwarding `enable_thinking: True` to the vLLM inference engine.  \n",
    "\n",
    "Example Curl with `/think`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8702258c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!curl -X POST http://localhost:8000/v1/chat/completions \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -H \"Authorization: Bearer FAKE_KEY\" \\\n",
    "  -d '{ \\\n",
    "        \"model\": \"my-qwen-3-32b\", \\\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": \"What is greater between 7.8 and 7.11 ? /think\"}] \\\n",
    "      }'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0bad31b",
   "metadata": {},
   "source": [
    " Example Python with `enable_thinking: True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a52eb68",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#client_thinking_enabled.py\n",
    "from urllib.parse import urljoin\n",
    "from openai import OpenAI\n",
    "\n",
    "api_key = \"FAKE_KEY\"\n",
    "base_url = \"http://localhost:8000\"\n",
    "\n",
    "client = OpenAI(base_url=urljoin(base_url, \"v1\"), api_key=api_key)\n",
    "\n",
    "# Example: Complex query with thinking process\n",
    "response = client.chat.completions.create(\n",
    "    model=\"my-qwen-3-32b\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"What's the capital of France ?\"}\n",
    "    ],\n",
    "    extra_body={\"chat_template_kwargs\": {\"enable_thinking\": True}}\n",
    ")\n",
    "\n",
    "print(f\"Reasoning: \\n{response.choices[0].message.reasoning_content}\\n\\n\")\n",
    "print(f\"Answer: \\n {response.choices[0].message.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f36ba3d",
   "metadata": {},
   "source": [
    "If you configure a valid reasoning parser, the reasoning output should appear in the `reasoning_content` field of the response message. Otherwise, it may be included in the main `content` field, typically wrapped in `<think>...</think>` tags. See [Parsing Reasoning Outputs](#parsing-reasoning-outputs) for more information.\n",
    "\n",
    "---\n",
    "\n",
    "### Shutdown \n",
    "\n",
    "Shutdown your LLM service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc5cc23",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!serve shutdown -y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8009515b",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Production Deployment with Anyscale Service\n",
    "\n",
    "For production, we recommend using Anyscale Services to deploy your Ray Serve app on a dedicated cluster without code changes. Anyscale provides scalability, fault tolerance, and load balancing, ensuring resilience against node failures, high traffic, and rolling updates. See [Deploying a medium-size LLM](#) for an example with a medium-size model like the *Qwen-32b* we are using here.\n",
    "\n",
    "---\n",
    "\n",
    "## Streaming Reasoning Content\n",
    "\n",
    "In thinking mode, hybrid reasoning models may take longer to begin generating the main content. You can stream their intermediate reasoning output in the same way as the main content.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f5a877",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#client_streaming.py\n",
    "from urllib.parse import urljoin\n",
    "from openai import OpenAI\n",
    "\n",
    "api_key = \"FAKE_KEY\"\n",
    "base_url = \"http://localhost:8000\"\n",
    "\n",
    "client = OpenAI(base_url=urljoin(base_url, \"v1\"), api_key=api_key)\n",
    "\n",
    "# Example: Complex query with thinking process\n",
    "response = client.chat.completions.create(\n",
    "    model=\"my-qwen-3-32b\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"What's the capital of France ?\"}\n",
    "    ],\n",
    "    extra_body={\"chat_template_kwargs\": {\"enable_thinking\": True}}\n",
    ")\n",
    "\n",
    "print(f\"Reasoning: \\n{response.choices[0].message.reasoning_content}\\n\\n\")\n",
    "print(f\"Answer: \\n {response.choices[0].message.content}\")\n",
    "from urllib.parse import urljoin\n",
    "from openai import OpenAI\n",
    "\n",
    "api_key = \"FAKE_KEY\"\n",
    "base_url = \"http://localhost:8000\"\n",
    "\n",
    "client = OpenAI(base_url=urljoin(base_url, \"v1\"), api_key=api_key)\n",
    "\n",
    "# Example: Complex query with thinking process\n",
    "response = client.chat.completions.create(\n",
    "    model=\"my-qwen-3-32b\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"I need to plan a trip to Paris from Seattle. Can you help me research flight costs, create an itinerary for 3 days, and suggest restaurants based on my dietary restrictions (vegetarian)?\"}\n",
    "    ],\n",
    "    extra_body={\"chat_template_kwargs\": {\"enable_thinking\": True}},\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "# Stream \n",
    "for chunk in response:\n",
    "    # Stream reasoning content\n",
    "    if hasattr(chunk.choices[0].delta, \"reasoning_content\"):\n",
    "        data_reasoning = chunk.choices[0].delta.reasoning_content\n",
    "        if data_reasoning:\n",
    "            print(data_reasoning, end=\"\", flush=True)\n",
    "    # Later, stream the final answer\n",
    "    if hasattr(chunk.choices[0].delta, \"content\"):\n",
    "        data_content = chunk.choices[0].delta.content\n",
    "        if data_content:\n",
    "            print(data_content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6357c06",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this tutorial, you deployed a reasoning LLM with Ray Serve LLM, from development to production. You learned how to configure Ray Serve LLM with the right reasoning parser, deploy your service on your Ray Cluster, how to send requests, and how to parse reasoning outputs in the response."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
