{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3a1b273",
   "metadata": {},
   "source": [
    "# Quick-starts with LLM Serving\n",
    "\n",
    "These guides provide a fast path to serving LLMs using Ray Serve on Anyscale, with focused tutorials for different deployment scales, from single-GPU setups to multi-node clusters.\n",
    "\n",
    "Each tutorial includes development and production setups, tips for configuring your cluster, and guidance on monitoring and scaling with Ray Serve.\n",
    "\n",
    "## Tutorial Categories\n",
    "\n",
    "**[Small size LLM deployment](https://github.com/ray-project/ray/tree/master/doc/source/ray-overview/examples/deployment-serve-llm/notebooks/small-size-llm.ipynb)**  \n",
    "Deploy small size models on a single GPU, such as Llama 3 8&nbsp;B, Mistral 7&nbsp;B, or Phi-2.  \n",
    "\n",
    "---\n",
    "\n",
    "**[Medium size LLM deployment](https://github.com/ray-project/ray/tree/master/doc/source/ray-overview/examples/deployment-serve-llm/notebooks/medium-size-llm.ipynb)**  \n",
    "Deploy medium size models using tensor parallelism across 4â€”8 GPUs on a single node, such as Llama 3 70&nbsp;B, Qwen 14&nbsp;B, Mixtral 8x7&nbsp;B.  \n",
    "\n",
    "---\n",
    "\n",
    "**[Large size LLM deployment](https://github.com/anyscale/templates/tree/main/templates/ray_serve_llm/end-to-end-examples/gargantuan_model)**  \n",
    "Deploy massive models using pipeline parallelism across a multi-node cluster, such as Deepseek-R1 or Llama-Nemotron-253&nbsp;B.  \n",
    "\n",
    "---\n",
    "\n",
    "**[Vision LLM deployment](https://github.com/ray-project/ray/tree/master/doc/source/ray-overview/examples/deployment-serve-llm/notebooks/vision-llm.ipynb)**  \n",
    "Deploy models with image + text input such as Qwen 2.5-VL-7&nbsp;B-Instruct, MiniGPT-4, or Pixtral-12&nbsp;B.  \n",
    "\n",
    "---\n",
    "\n",
    "**[Reasoning LLM deployment](https://github.com/ray-project/ray/tree/master/doc/source/ray-overview/examples/deployment-serve-llm/notebooks/reasoning-llm.ipynb)**  \n",
    "Deploy models with reasoning capabilities designed for long-context tasks, coding, or tool use, such as QwQ-32&nbsp;B.  \n",
    "\n",
    "---\n",
    "\n",
    "**[Hybrid Thinking LLM deployment](https://github.com/ray-project/ray/tree/master/doc/source/ray-overview/examples/deployment-serve-llm/notebooks/hybrid-reasoning-llm.ipynb)**  \n",
    "Deploy models that can switch between reasoning and non-reasoning modes for flexible usage, such as Qwen-3.\n",
    "\n",
    "```{toctree}\n",
    ":hidden:\n",
    "\n",
    "small-size-llm/notebook.ipynb\n",
    "medium-size-llm/notebook.ipynb\n",
    "vision-llm/notebook.ipynb\n",
    "reasoning-llm/notebook.ipynb\n",
    "hybrid-reasoning-llm/notebook.ipynb\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
