{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b66cdd12",
   "metadata": {},
   "source": [
    "# Entity recognition with LLMs\n",
    "\n",
    "<div align=\"left\">\n",
    "<a target=\"_blank\" href=\"https://console.anyscale.com/\"><img src=\"https://img.shields.io/badge/üöÄ Run_on-Anyscale-9hf\"></a>&nbsp;\n",
    "<a href=\"https://github.com/anyscale/e2e-llm-workflows\" role=\"button\"><img src=\"https://img.shields.io/static/v1?label=&amp;message=View%20On%20GitHub&amp;color=586069&amp;logo=github&amp;labelColor=2f363d\"></a>&nbsp;\n",
    "</div>\n",
    "\n",
    "This end-to-end tutorial **fine-tunes** an LLM to perform **batch inference** and **online serving** at scale. While entity recognition (NER) is the main task in this tutorial, you can easily extend these end-to-end workflows to any use case.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/anyscale/e2e-llm-workflows/refs/heads/main/images/e2e_llm.png\" width=800>\n",
    "\n",
    "**Note**: the intent of this tutorial is to show how Ray can be use to implement end-to-end LLM workflows that can extend to any use case, including multimodal.\n",
    "\n",
    "This tutorial uses the [Ray library](https://github.com/ray-project/ray) to implement these workflows, namely the LLM APIs:\n",
    "\n",
    "[`ray.data.llm`](https://docs.ray.io/en/latest/data/working-with-llms.html):\n",
    "\n",
    "- Batch inference over distributed datasets\n",
    "- Streaming and async execution for throughput\n",
    "- Built-in metrics and tracing, including observability\n",
    "- Zero-copy GPU data transfer\n",
    "- Composable with preprocessing and postprocessing steps\n",
    "\n",
    "[`ray.serve.llm`](https://docs.ray.io/en/latest/serve/llm/serving-llms.html):\n",
    "\n",
    "- Automatic scaling and load balancing\n",
    "- Unified multi-node multi-model deployment\n",
    "- Multi-LoRA support with shared base models\n",
    "- Deep integration with inference engines, vLLM to start\n",
    "- Composable multi-model LLM pipelines\n",
    "\n",
    "And all of these workloads come with all the observability views you need to debug and tune them to **maximize throughput or latency**.\n",
    "\n",
    "# Set up\n",
    "\n",
    "## Compute\n",
    "\n",
    "This [Anyscale Workspace](https://docs.anyscale.com/platform/workspaces/) automatically provisions and autoscales the compute your workloads need. If you're not on Anyscale, then you need to provision `4xA10G:48CPU-192GB` for this tutorial.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/anyscale/foundational-ray-app/refs/heads/main/images/compute.png\" width=500>\n",
    "\n",
    "## Dependencies\n",
    "\n",
    "Start by downloading the dependencies required for this tutorial. Notice in your [`containerfile`](https://raw.githubusercontent.com/anyscale/e2e-llm-workflows/refs/heads/main/containerfile) you have a base image [`anyscale/ray-llm:2.44.1-py311-cu124`](https://hub.docker.com/layers/anyscale/ray-llm/2.44.1-py311-cu124/images/sha256-8099edda787fc96847af7e1c51f30ad09792aa250efa27f9aa825b15016e6b3f) followed by a list of pip packages. If you're not on [Anyscale](https://console.anyscale.com/), you can pull this docker image yourself and install the dependencies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e011ad13",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "%%bash\n",
    "# Install dependencies\n",
    "pip install -q \\\n",
    "    \"xgrammar==0.1.11\" \\\n",
    "    \"pynvml==12.0.0\" \\\n",
    "    \"hf_transfer==0.1.9\" \\\n",
    "    \"tensorboard==2.19.0\" \\\n",
    "    \"llamafactory@git+https://github.com/hiyouga/LLaMA-Factory.git@ac8c6fdd3ab7fb6372f231f238e6b8ba6a17eb16#egg=llamafactory\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ab9ce0",
   "metadata": {},
   "source": [
    "# Data ingestion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c01ce40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import textwrap\n",
    "from IPython.display import Code, Image, display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891098c0",
   "metadata": {},
   "source": [
    "Start by downloading the data from cloud storage to local shared storage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3334e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "rm -rf /mnt/cluster_storage/viggo  # clean up\n",
    "mkdir /mnt/cluster_storage/viggo\n",
    "wget https://viggo-ds.s3.amazonaws.com/train.jsonl -O /mnt/cluster_storage/viggo/train.jsonl\n",
    "wget https://viggo-ds.s3.amazonaws.com/val.jsonl -O /mnt/cluster_storage/viggo/val.jsonl\n",
    "wget https://viggo-ds.s3.amazonaws.com/test.jsonl -O /mnt/cluster_storage/viggo/test.jsonl\n",
    "wget https://viggo-ds.s3.amazonaws.com/dataset_info.json -O /mnt/cluster_storage/viggo/dataset_info.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a554eb30",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "head -n 1 /mnt/cluster_storage/viggo/train.jsonl | python3 -m json.tool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73597dd9",
   "metadata": {},
   "source": [
    "```json\n",
    "{\n",
    "    \"instruction\": \"Given a target sentence construct the underlying meaning representation of the input sentence as a single function with attributes and attribute values. This function should describe the target string accurately and the function must be one of the following ['inform', 'request', 'give_opinion', 'confirm', 'verify_attribute', 'suggest', 'request_explanation', 'recommend', 'request_attribute']. The attributes must be one of the following: ['name', 'exp_release_date', 'release_year', 'developer', 'esrb', 'rating', 'genres', 'player_perspective', 'has_multiplayer', 'platforms', 'available_on_steam', 'has_linux_release', 'has_mac_release', 'specifier']\",\n",
    "    \"input\": \"Blizzard North is mostly an okay developer, but they released Diablo II for the Mac and so that pushes the game from okay to good in my view.\",\n",
    "    \"output\": \"give_opinion(name[Diablo II], developer[Blizzard North], rating[good], has_mac_release[yes])\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8caca138",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/mnt/cluster_storage/viggo/train.jsonl\", \"r\") as fp:\n",
    "    first_line = fp.readline()\n",
    "    item = json.loads(first_line)\n",
    "system_content = item[\"instruction\"]\n",
    "print(textwrap.fill(system_content, width=80))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b97dc59",
   "metadata": {},
   "source": [
    "```text\n",
    "Given a target sentence construct the underlying meaning representation of the\n",
    "input sentence as a single function with attributes and attribute values. This\n",
    "function should describe the target string accurately and the function must be\n",
    "one of the following ['inform', 'request', 'give_opinion', 'confirm',\n",
    "'verify_attribute', 'suggest', 'request_explanation', 'recommend',\n",
    "'request_attribute']. The attributes must be one of the following: ['name',\n",
    "'exp_release_date', 'release_year', 'developer', 'esrb', 'rating', 'genres',\n",
    "'player_perspective', 'has_multiplayer', 'platforms', 'available_on_steam',\n",
    "'has_linux_release', 'has_mac_release', 'specifier']\n",
    "```\n",
    "\n",
    "You also have an info file that identifies the datasets and format --- Alpaca and ShareGPT formats, which are suitable for multimodal tasks and are supported by Anyscale --- to use for post training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d9622e",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Code(filename=\"/mnt/cluster_storage/viggo/dataset_info.json\", language=\"json\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0fae69",
   "metadata": {},
   "source": [
    "```json\n",
    "{\n",
    "    \"viggo-train\": {\n",
    "        \"file_name\": \"/mnt/cluster_storage/viggo/train.jsonl\",\n",
    "        \"formatting\": \"alpaca\",\n",
    "        \"columns\": {\n",
    "            \"prompt\": \"instruction\",\n",
    "            \"query\": \"input\",\n",
    "            \"response\": \"output\"\n",
    "        }\n",
    "    },\n",
    "    \"viggo-val\": {\n",
    "        \"file_name\": \"/mnt/cluster_storage/viggo/val.jsonl\",\n",
    "        \"formatting\": \"alpaca\",\n",
    "        \"columns\": {\n",
    "            \"prompt\": \"instruction\",\n",
    "            \"query\": \"input\",\n",
    "            \"response\": \"output\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d67de5a",
   "metadata": {},
   "source": [
    "# Distributed fine-tuning\n",
    "\n",
    "Use [Ray Train](https://docs.ray.io/en/latest/train/train.html) + [LLaMA-Factory](https://github.com/hiyouga/LLaMA-Factory) to perform multi-node training. Find the parameters for the training workload -- post-training method, dataset location, train/val details, etc. --- in the `llama3_lora_sft_ray.yaml` config file. See the recipes for more post-training methods, like SFT, pretraining, PPO, DPO, KTO, etc. [on GitHub](https://github.com/hiyouga/LLaMA-Factory/tree/main/examples).\n",
    "\n",
    "**Note**: Anyscale also supports using other tools like [axolotl](https://axolotl-ai-cloud.github.io/axolotl/docs/ray-integration.html) or even [Ray Train + HF Accelerate + FSDP/DeepSpeed](https://docs.ray.io/en/latest/train/huggingface-accelerate.html) directly for complete control of your post-training workloads.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/anyscale/foundational-ray-app/refs/heads/main/images/distributed_training.png\" width=800>\n",
    "\n",
    "## `config`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823b920e",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8107311",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Code(filename=\"lora_sft_ray.yaml\", language=\"yaml\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47c2d69",
   "metadata": {},
   "source": [
    "```yaml\n",
    "### model\n",
    "model_name_or_path: Qwen/Qwen2.5-7B-Instruct\n",
    "trust_remote_code: true\n",
    "\n",
    "\n",
    "### method\n",
    "stage: sft\n",
    "do_train: true\n",
    "finetuning_type: lora\n",
    "lora_rank: 8\n",
    "lora_target: all\n",
    "\n",
    "\n",
    "### dataset\n",
    "dataset: viggo-train\n",
    "dataset_dir: /mnt/cluster_storage/viggo  # Shared storage workers have access to.\n",
    "template: qwen\n",
    "cutoff_len: 2048\n",
    "max_samples: 1000\n",
    "overwrite_cache: true\n",
    "preprocessing_num_workers: 16\n",
    "dataloader_num_workers: 4\n",
    "\n",
    "\n",
    "### output\n",
    "output_dir: /mnt/cluster_storage/viggo/outputs  # Should be somewhere workers have access to, for example, S3, NFS.\n",
    "logging_steps: 10\n",
    "save_steps: 500\n",
    "plot_loss: true\n",
    "overwrite_output_dir: true\n",
    "save_only_model: false\n",
    "\n",
    "\n",
    "### Ray\n",
    "ray_run_name: lora_sft_ray\n",
    "ray_storage_path: /mnt/cluster_storage/viggo/saves  # Should be somewhere workers have access to, for example, S3, NFS\n",
    "ray_num_workers: 4\n",
    "resources_per_worker:\n",
    "  GPU: 1\n",
    "  anyscale/accelerator_shape:4xL4: 0.001  # Use this to specify a specific node shape,\n",
    "  # accelerator_type:A10G: 1           # Or use this to simply specify a GPU type.\n",
    "  # see https://docs.ray.io/en/master/ray-core/accelerator-types.html#accelerator-types for a full list of accelerator types\n",
    "placement_strategy: PACK\n",
    "\n",
    "\n",
    "### train\n",
    "per_device_train_batch_size: 1\n",
    "gradient_accumulation_steps: 8\n",
    "learning_rate: 1.0e-4\n",
    "num_train_epochs: 5.0\n",
    "lr_scheduler_type: cosine\n",
    "warmup_ratio: 0.1\n",
    "bf16: true\n",
    "ddp_timeout: 180000000\n",
    "resume_from_checkpoint: null\n",
    "\n",
    "\n",
    "### eval\n",
    "eval_dataset: viggo-val  # Uses same dataset_dir as training data.\n",
    "# val_size: 0.1  # Only if using part of training data for validation.\n",
    "per_device_eval_batch_size: 1\n",
    "eval_strategy: steps\n",
    "eval_steps: 500\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11719d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"ft-model\"  # Call it whatever you want.\n",
    "model_source = yaml.safe_load(open(\"lora_sft_ray.yaml\"))[\"model_name_or_path\"]  # HF model ID, S3 mirror config, or GCS mirror config.\n",
    "print (model_source)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdec2656",
   "metadata": {},
   "source": [
    "```text\n",
    "Qwen/Qwen2.5-7B-Instruct\n",
    "```\n",
    "\n",
    "## Multi-node training\n",
    "\n",
    "Use Ray Train + LlamaFactory to perform the mult-node train loop.\n",
    "\n",
    "<div class=\"alert alert-block alert\"> <b>Ray Train</b>\n",
    "\n",
    "Using [Ray Train](https://docs.ray.io/en/latest/train/train.html) has several advantages:\n",
    "\n",
    "- automatically handles **multi-node, multi-GPU** setup with no manual SSH setup or `hostfile` configs.\n",
    "- define **per-worker fractional resource requirements**, for example, 2 CPUs and 0.5 GPU per worker\n",
    "- run on **heterogeneous machines** and scale flexibly, for example, CPU for preprocessing and GPU for training\n",
    "- built-in **fault tolerance** through retry of failed workers, and continue from last checkpoint.\n",
    "- supports Data Parallel, Model Parallel, Parameter Server, and even custom strategies.\n",
    "- [Ray compiled graphs](https://docs.ray.io/en/latest/ray-core/compiled-graph/ray-compiled-graph.html) allow you to even define different parallelism for jointly optimizing multiple models. Megatron, DeepSpeed, etc. only allow for one global setting.\n",
    "\n",
    "[RayTurbo Train](https://docs.anyscale.com/rayturbo/rayturbo-train) offers even more improvement to the price-performance ratio, performance monitoring and more:\n",
    "\n",
    "- **elastic training** to scale to a dynamic number of workers, and continue training on fewer resources, even on spot instances.\n",
    "- **purpose-built dashboard** designed to streamline the debugging of Ray Train workloads:\n",
    "\n",
    "  - Monitoring: View the status of training runs and train workers.\n",
    "  - Metrics: See insights on training throughput and training system operation time.\n",
    "  - Profiling: Investigate bottlenecks, hangs, or errors from individual training worker processes.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/anyscale/foundational-ray-app/refs/heads/main/images/train_dashboard.png\" width=700>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf7ca48",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "%%bash\n",
    "# Run multinode distributed fine-tuning workload\n",
    "USE_RAY=1 llamafactory-cli train lora_sft_ray.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21eb164",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "    100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 155/155 [07:12<00:00,  2.85s/it][INFO|trainer.py:3942] 2025-04-11 14:57:59,207 >> Saving model checkpoint to /mnt/cluster_storage/viggo/outputs/checkpoint-155\n",
    "\n",
    "    Training finished iteration 1 at 2025-04-11 14:58:02. Total running time: 10min 24s\n",
    "    ‚ï≠‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïÆ\n",
    "    ‚îÇ Training result                         ‚îÇ\n",
    "    ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "    ‚îÇ checkpoint_dir_name   checkpoint_000000 ‚îÇ\n",
    "    ‚îÇ time_this_iter_s              521.83827 ‚îÇ\n",
    "    ‚îÇ time_total_s                  521.83827 ‚îÇ\n",
    "    ‚îÇ training_iteration                    1 ‚îÇ\n",
    "    ‚îÇ epoch                             4.704 ‚îÇ\n",
    "    ‚îÇ grad_norm                       0.14288 ‚îÇ\n",
    "    ‚îÇ learning_rate                        0. ‚îÇ\n",
    "    ‚îÇ loss                             0.0065 ‚îÇ\n",
    "    ‚îÇ step                                150 ‚îÇ\n",
    "    ‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ\n",
    "    Training saved a checkpoint for iteration 1 at: (local)/mnt/cluster_storage/viggo/saves/lora_sft_ray/TorchTrainer_95d16_00000_0_2025-04-11_14-47-37/checkpoint_000000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aecbd1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Code(filename=\"/mnt/cluster_storage/viggo/outputs/all_results.json\", language=\"json\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c54bf1e",
   "metadata": {},
   "source": [
    "```json\n",
    "{\n",
    "    \"epoch\": 4.864,\n",
    "    \"eval_viggo-val_loss\": 0.13618840277194977,\n",
    "    \"eval_viggo-val_runtime\": 20.2797,\n",
    "    \"eval_viggo-val_samples_per_second\": 35.208,\n",
    "    \"eval_viggo-val_steps_per_second\": 8.827,\n",
    "    \"total_flos\": 4.843098686147789e+16,\n",
    "    \"train_loss\": 0.2079355036479331,\n",
    "    \"train_runtime\": 437.2951,\n",
    "    \"train_samples_per_second\": 11.434,\n",
    "    \"train_steps_per_second\": 0.354\n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b90c81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(filename=\"/mnt/cluster_storage/viggo/outputs/training_loss.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61bec628",
   "metadata": {},
   "source": [
    "![train loss](https://raw.githubusercontent.com/anyscale/e2e-llm-workflows/refs/heads/main/images/loss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2eef50",
   "metadata": {},
   "source": [
    "## Observability\n",
    "\n",
    "<div class=\"alert alert-block alert\"> <b> üîé Monitoring and Debugging with Ray</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83e1cc0",
   "metadata": {},
   "source": [
    "OSS Ray offers an extensive [observability suite](https://docs.ray.io/en/latest/ray-observability/index.html) that offers logs and an observability dashboard that you can use to monitor and debug. The dashboard includes a lot of different components such as:\n",
    "\n",
    "- memory, utilization, etc. of the tasks running in the [cluster](https://docs.ray.io/en/latest/ray-observability/getting-started.html#dash-node-view)\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/anyscale/foundational-ray-app/refs/heads/main/images/cluster_util.png\" width=700>\n",
    "\n",
    "- views to see all running tasks, utilization across instance types, autoscaling, etc.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/anyscale/foundational-ray-app/refs/heads/main/images/observability_views.png\" width=1000>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468f225c",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert\"> <b> üîé‚ûï‚ûï Monitoring and debugging on Anyscale</b>\n",
    "\n",
    "While OSS Ray comes with an extensive observability suite, Anyscale takes it many steps further to make it even easier and faster to monitor and debug your workloads.\n",
    "\n",
    "- [unified log viewer](https://docs.anyscale.com/monitoring/accessing-logs/) to see logs from *all* your driver and worker processes\n",
    "- Ray workload specific dashboards, like Data, Train, etc., that can breakdown the tasks. For example, you can observe the preceding training workload live through the Train specific Ray Workloads dashboard:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/anyscale/e2e-llm-workflows/refs/heads/main/images/train_dashboard.png\" width=700>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace92935",
   "metadata": {},
   "source": [
    "## Save to cloud storage\n",
    "\n",
    "<div class=\"alert alert-block alert\"> <b> üóÇÔ∏è Storage on Anyscale</b>\n",
    "\n",
    "You can always store data inside [any storage buckets](https://docs.anyscale.com/configuration/storage/#private-storage-buckets) but Anyscale offers a [default storage bucket](https://docs.anyscale.com/configuration/storage/#anyscale-default-storage-bucket) to make things even easier. You also have plenty of other [storage options](https://docs.anyscale.com/configuration/storage/) as well, for example, shared at the cluster, user, and cloud levels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4db412",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Anyscale default storage bucket.\n",
    "echo $ANYSCALE_ARTIFACT_STORAGE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f797b3",
   "metadata": {},
   "source": [
    "```text\n",
    "s3://anyscale-test-data-cld-i2w99rzq8b6lbjkke9y94vi5/org_7c1Kalm9WcX2bNIjW53GUT/cld_kvedZWag2qA8i5BjxUevf5i7/artifact_storage\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c7faff",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Save fine-tuning artifacts to cloud storage\n",
    "STORAGE_PATH=\"$ANYSCALE_ARTIFACT_STORAGE/viggo\"\n",
    "LOCAL_OUTPUTS_PATH=\"/mnt/cluster_storage/viggo/outputs\"\n",
    "LOCAL_SAVES_PATH=\"/mnt/cluster_storage/viggo/saves\"\n",
    "\n",
    "# AWS S3 operations\n",
    "if [[ \"$STORAGE_PATH\" == s3://* ]]; then\n",
    "    if aws s3 ls \"$STORAGE_PATH\" > /dev/null 2>&1; then\n",
    "        aws s3 rm \"$STORAGE_PATH\" --recursive --quiet\n",
    "    fi\n",
    "    aws s3 cp \"$LOCAL_OUTPUTS_PATH\" \"$STORAGE_PATH/outputs\" --recursive --quiet\n",
    "    aws s3 cp \"$LOCAL_SAVES_PATH\" \"$STORAGE_PATH/saves\" --recursive --quiet\n",
    "\n",
    "# Google Cloud Storage operations\n",
    "elif [[ \"$STORAGE_PATH\" == gs://* ]]; then\n",
    "    if gsutil ls \"$STORAGE_PATH\" > /dev/null 2>&1; then\n",
    "        gsutil -m rm -q -r \"$STORAGE_PATH\"\n",
    "    fi\n",
    "    gsutil -m cp -q -r \"$LOCAL_OUTPUTS_PATH\" \"$STORAGE_PATH/outputs\"\n",
    "    gsutil -m cp -q -r \"$LOCAL_SAVES_PATH\" \"$STORAGE_PATH/saves\"\n",
    "\n",
    "else\n",
    "    echo \"Unsupported storage protocol: $STORAGE_PATH\"\n",
    "    exit 1\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247bfd59",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "ls /mnt/cluster_storage/viggo/saves/lora_sft_ray"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664376de",
   "metadata": {},
   "source": [
    "```text\n",
    "    TorchTrainer_95d16_00000_0_2025-04-11_14-47-37\n",
    "    TorchTrainer_f9e4e_00000_0_2025-04-11_12-41-34\n",
    "    basic-variant-state-2025-04-11_12-41-34.json\n",
    "    basic-variant-state-2025-04-11_14-47-37.json\n",
    "    experiment_state-2025-04-11_12-41-34.json\n",
    "    experiment_state-2025-04-11_14-47-37.json\n",
    "    trainer.pkl\n",
    "    tuner.pkl\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8d24dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA paths\n",
    "save_dir = Path(\"/mnt/cluster_storage/viggo/saves/lora_sft_ray\")\n",
    "trainer_dirs = [d for d in save_dir.iterdir() if d.name.startswith(\"TorchTrainer_\") and d.is_dir()]\n",
    "latest_trainer = max(trainer_dirs, key=lambda d: d.stat().st_mtime, default=None)\n",
    "lora_path = f\"{latest_trainer}/checkpoint_000000/checkpoint\"\n",
    "cloud_lora_path = os.path.join(os.getenv(\"ANYSCALE_ARTIFACT_STORAGE\"), lora_path.split(\"/mnt/cluster_storage/\")[-1])\n",
    "dynamic_lora_path, lora_id = cloud_lora_path.rsplit(\"/\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a783be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$lora_path\"\n",
    "ls $1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282b89cc",
   "metadata": {},
   "source": [
    "```text\n",
    "README.md\n",
    "adapter_config.json\n",
    "adapter_model.safetensors\n",
    "added_tokens.json\n",
    "merges.txt\n",
    "optimizer.pt\n",
    "rng_state_0.pth\n",
    "rng_state_1.pth\n",
    "rng_state_2.pth\n",
    "rng_state_3.pth\n",
    "scheduler.pt\n",
    "special_tokens_map.json\n",
    "tokenizer.json\n",
    "tokenizer_config.json\n",
    "trainer_state.json\n",
    "training_args.bin\n",
    "vocab.json\n",
    "```\n",
    "\n",
    "# Batch inference\n",
    "[`Overview`](https://docs.ray.io/en/latest/data/working-with-llms.html) |  [`API reference`](https://docs.ray.io/en/latest/data/api/llm.html)\n",
    "\n",
    "The `ray.data.llm` module integrates with key large language model (LLM) inference engines and deployed models to enable LLM batch inference. These LLM modules use [Ray Data](https://docs.ray.io/en/latest/data/data.html) under the hood, which makes it extremely easy to distribute workloads but also ensures that they happen:\n",
    "\n",
    "- **efficiently**: minimize CPU or GPU idle time with heterogeneous resource scheduling.\n",
    "- **at scale**: streaming execution to petabyte-scale datasets, especially when [working with LLMs](https://docs.ray.io/en/latest/data/working-with-llms.html).\n",
    "- **reliably** by checkpointing processes, especially when running workloads on spot instances with on-demand fallback.\n",
    "- **flexibility**: connect to data from any source, apply your transformations, and save to any format and location for your next workload.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/anyscale/foundational-ray-app/refs/heads/main/images/ray_data_solution.png\" width=800>\n",
    "\n",
    "[RayTurbo Data](https://docs.anyscale.com/rayturbo/rayturbo-data) has even more functionality on top of Ray Data:\n",
    "- **accelerated metadata fetching** to improve reading first time from large datasets\n",
    "- **optimized autoscaling** where Jobs can kick off before waiting for the entire cluster to start\n",
    "- **high reliability** where entire fails jobs like head node, cluster, uncaptured exceptions, etc., can resume from checkpoints. OSS Ray can only recover from worker node failures.\n",
    "\n",
    "Start by defining the [vLLM engine processor config](https://docs.ray.io/en/latest/data/api/doc/ray.data.llm.vLLMEngineProcessorConfig.html#ray.data.llm.vLLMEngineProcessorConfig) where you can select the model to use and the [engine behavior](https://docs.vllm.ai/en/stable/serving/engine_args.html). The model can come from [Hugging Face (HF) Hub](https://huggingface.co/models) or a local model path `/path/to/your/model`. Anyscale supports GPTQ, GGUF, or LoRA model formats supported.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/anyscale/e2e-llm-workflows/refs/heads/main/images/data_llm.png\" width=800>\n",
    "\n",
    "## vLLM engine processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73664746",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ray\n",
    "from ray.data.llm import vLLMEngineProcessorConfig\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a3b2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = vLLMEngineProcessorConfig(\n",
    "    model_source=model_source,\n",
    "    # runtime_env={\"env_vars\": {\"HF_TOKEN\": os.environ.get(\"HF_TOKEN\")}},\n",
    "    engine_kwargs={\n",
    "        \"enable_lora\": True,\n",
    "        \"max_lora_rank\": 8,\n",
    "        \"max_loras\": 1,\n",
    "        \"pipeline_parallel_size\": 1,\n",
    "        \"tensor_parallel_size\": 1,\n",
    "        \"enable_prefix_caching\": True,\n",
    "        \"enable_chunked_prefill\": True,\n",
    "        \"max_num_batched_tokens\": 4096,\n",
    "        \"max_model_len\": 4096,  # or increase KV cache size\n",
    "        # complete list: https://docs.vllm.ai/en/stable/serving/engine_args.html\n",
    "    },\n",
    "    concurrency=1,\n",
    "    batch_size=16,\n",
    "    accelerator_type=\"A10G\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121c3f81",
   "metadata": {},
   "source": [
    "## LLM processor\n",
    "\n",
    "Next, pass the config to an [LLM processor](https://docs.ray.io/en/master/data/api/doc/ray.data.llm.build_llm_processor.html#ray.data.llm.build_llm_processor) where you can define the preprocessing and postprocessing steps around inference. With your base model defined in the processor config, you can define the LoRA adapter layers as part of the preprocessing step of the LLM processor itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5eb5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.data.llm import build_llm_processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4494f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = build_llm_processor(\n",
    "    config,\n",
    "    preprocess=lambda row: dict(\n",
    "        model=lora_path,  # REMOVE this line if doing inference with just the base model\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_content},\n",
    "            {\"role\": \"user\", \"content\": row[\"input\"]}\n",
    "        ],\n",
    "        sampling_params={\n",
    "            \"temperature\": 0.3,\n",
    "            \"max_tokens\": 250,\n",
    "            # complete list: https://docs.vllm.ai/en/stable/api/inference_params.html\n",
    "        },\n",
    "    ),\n",
    "    postprocess=lambda row: {\n",
    "        **row,  # all contents\n",
    "        \"generated_output\": row[\"generated_text\"],\n",
    "        # add additional outputs\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e441a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation on test dataset\n",
    "ds = ray.data.read_json(\"/mnt/cluster_storage/viggo/test.jsonl\")  # complete list: https://docs.ray.io/en/latest/data/api/input_output.html\n",
    "ds = processor(ds)\n",
    "results = ds.take_all()\n",
    "results[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5712e52e",
   "metadata": {},
   "source": [
    "```json\n",
    "{\n",
    "  \"batch_uuid\": \"d7a6b5341cbf4986bb7506ff277cc9cf\",\n",
    "  \"embeddings\": null,\n",
    "  \"generated_text\": \"request(esrb)\",\n",
    "  \"generated_tokens\": [2035, 50236, 10681, 8, 151645],\n",
    "  \"input\": \"Do you have a favorite ESRB content rating?\",\n",
    "  \"instruction\": \"Given a target sentence construct the underlying meaning representation of the input sentence as a single function with attributes and attribute values. This function should describe the target string accurately and the function must be one of the following ['inform', 'request', 'give_opinion', 'confirm', 'verify_attribute', 'suggest', 'request_explanation', 'recommend', 'request_attribute']. The attributes must be one of the following: ['name', 'exp_release_date', 'release_year', 'developer', 'esrb', 'rating', 'genres', 'player_perspective', 'has_multiplayer', 'platforms', 'available_on_steam', 'has_linux_release', 'has_mac_release', 'specifier']\",\n",
    "  \"messages\": [\n",
    "    {\n",
    "      \"content\": \"Given a target sentence construct the underlying meaning representation of the input sentence as a single function with attributes and attribute values. This function should describe the target string accurately and the function must be one of the following ['inform', 'request', 'give_opinion', 'confirm', 'verify_attribute', 'suggest', 'request_explanation', 'recommend', 'request_attribute']. The attributes must be one of the following: ['name', 'exp_release_date', 'release_year', 'developer', 'esrb', 'rating', 'genres', 'player_perspective', 'has_multiplayer', 'platforms', 'available_on_steam', 'has_linux_release', 'has_mac_release', 'specifier']\",\n",
    "      \"role\": \"system\"\n",
    "    },\n",
    "    {\n",
    "      \"content\": \"Do you have a favorite ESRB content rating?\",\n",
    "      \"role\": \"user\"\n",
    "    }\n",
    "  ],\n",
    "  \"metrics\": {\n",
    "    \"arrival_time\": 1744408857.148983,\n",
    "    \"finished_time\": 1744408863.09091,\n",
    "    \"first_scheduled_time\": 1744408859.130259,\n",
    "    \"first_token_time\": 1744408862.7087252,\n",
    "    \"last_token_time\": 1744408863.089174,\n",
    "    \"model_execute_time\": null,\n",
    "    \"model_forward_time\": null,\n",
    "    \"scheduler_time\": 0.04162892400017881,\n",
    "    \"time_in_queue\": 1.981276035308838\n",
    "  },\n",
    "  \"model\": \"/mnt/cluster_storage/viggo/saves/lora_sft_ray/TorchTrainer_95d16_00000_0_2025-04-11_14-47-37/checkpoint_000000/checkpoint\",\n",
    "  \"num_generated_tokens\": 5,\n",
    "  \"num_input_tokens\": 164,\n",
    "  \"output\": \"request_attribute(esrb[])\",\n",
    "  \"params\": \"SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.3, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=250, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None)\",\n",
    "  \"prompt\": \"<|im_start|>system\\nGiven a target sentence construct the underlying meaning representation of the input sentence as a single function with attributes and attribute values. This function should describe the target string accurately and the function must be one of the following ['inform', 'request', 'give_opinion', 'confirm', 'verify_attribute', 'suggest', 'request_explanation', 'recommend', 'request_attribute']. The attributes must be one of the following: ['name', 'exp_release_date', 'release_year', 'developer', 'esrb', 'rating', 'genres', 'player_perspective', 'has_multiplayer', 'platforms', 'available_on_steam', 'has_linux_release', 'has_mac_release', 'specifier']<|im_end|>\\n<|im_start|>user\\nDo you have a favorite ESRB content rating?<|im_end|>\\n<|im_start|>assistant\\n\",\n",
    "  \"prompt_token_ids\": [151644, \"...\", 198],\n",
    "  \"request_id\": 94,\n",
    "  \"time_taken_llm\": 6.028705836999961,\n",
    "  \"generated_output\": \"request(esrb)\"\n",
    "}\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c111ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exact match (strict!)\n",
    "matches = 0\n",
    "for item in results:\n",
    "    if item[\"output\"] == item[\"generated_output\"]:\n",
    "        matches += 1\n",
    "matches / float(len(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91952d84",
   "metadata": {},
   "source": [
    "```text\n",
    "0.6879039704524469\n",
    "```\n",
    "\n",
    "**Note**: The objective of fine-tuning here isn't to create the most performant model but to show that you can leverage it for downstream workloads, like batch inference and online serving, at scale. Increase `num_train_epochs` if you want to though.\n",
    "\n",
    "Observe the individual steps in the batch inference workload through the Anyscale Ray Data dashboard:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/anyscale/e2e-llm-workflows/refs/heads/main/images/data_dashboard.png\" width=1000>\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "üí° For more advanced guides on topics like optimized model loading, multi-LoRA, openAI-compatible endpoints, etc., see [more examples](https://docs.ray.io/en/latest/data/working-with-llms.html) and the [API reference](https://docs.ray.io/en/latest/data/api/llm.html).\n",
    "\n",
    "</div>\n",
    "\n",
    "# Online serving\n",
    "\n",
    "[`Overview`](https://docs.ray.io/en/latest/serve/llm/serving-llms.html) | [`API reference`](https://docs.ray.io/en/latest/serve/api/index.html#llm-api)\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/anyscale/foundational-ray-app/refs/heads/main/images/ray_serve.png\" width=600>\n",
    "\n",
    "`ray.serve.llm` APIs allow users to deploy multiple LLM models together with a familiar Ray Serve API, while providing compatibility with the OpenAI API.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/anyscale/e2e-llm-workflows/refs/heads/main/images/serve_llm.png\" width=500>\n",
    "\n",
    "Ray Serve LLM has the following features:\n",
    "\n",
    "- Automatic scaling and load balancing\n",
    "- Unified multi-node multi-model deployment\n",
    "- OpenAI compatibility\n",
    "- Multi-LoRA support with shared base models\n",
    "- Deep integration with inference engines, vLLM to start\n",
    "- Composable multi-model LLM pipelines\n",
    "\n",
    "[RayTurbo Serve](https://docs.anyscale.com/rayturbo/rayturbo-serve) on Anyscale has even more functionality on top of Ray Serve:\n",
    "\n",
    "- **fast autoscaling and model loading** to get services up and running even faster: [5x improvements](https://www.anyscale.com/blog/autoscale-large-ai-models-faster) even for LLMs\n",
    "- 54% **higher QPS** and up-to 3x **streaming tokens per second** for high traffic serving use-cases\n",
    "- **replica compaction** into fewer nodes where possible to reduce resource fragmentation and improve hardware utilization\n",
    "- **zero-downtime** [incremental rollouts](https://docs.anyscale.com/platform/services/update-a-service/#resource-constrained-updates) so your service is never interrupted\n",
    "- [**different environments**](https://docs.anyscale.com/platform/services/multi-app/#multiple-applications-in-different-containers) for each service in a multi-serve application\n",
    "- **multi availability-zone** aware scheduling of Ray Serve replicas to provide higher redundancy to availability zone failures\n",
    "\n",
    "## LLM serve config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec22fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI  # to use openai api format\n",
    "from ray import serve\n",
    "from ray.serve.llm import LLMConfig, build_openai_app"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae4289d",
   "metadata": {},
   "source": [
    "Define an [LLM config](https://docs.ray.io/en/latest/serve/api/doc/ray.serve.llm.LLMConfig.html#ray.serve.llm.LLMConfig) where you can define where the model comes from, it's [autoscaling behavior](https://docs.ray.io/en/latest/serve/autoscaling-guide.html#serve-autoscaling), what hardware to use, and [engine arguments](https://docs.vllm.ai/en/stable/serving/engine_args.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4359fcb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define config\n",
    "llm_config = LLMConfig(\n",
    "    model_loading_config={\n",
    "        \"model_id\": model_id,\n",
    "        \"model_source\": model_source\n",
    "    },\n",
    "    lora_config={  # REMOVE this section if you're only using a base model.\n",
    "        \"dynamic_lora_loading_path\": dynamic_lora_path,\n",
    "        \"max_num_adapters_per_replica\": 16,  # You only have 1.\n",
    "    },\n",
    "    # runtime_env={\"env_vars\": {\"HF_TOKEN\": os.environ.get(\"HF_TOKEN\")}},\n",
    "    deployment_config={\n",
    "        \"autoscaling_config\": {\n",
    "            \"min_replicas\": 1,\n",
    "            \"max_replicas\": 2,\n",
    "            # Complete list: https://docs.ray.io/en/latest/serve/autoscaling-guide.html#serve-autoscaling\n",
    "        }\n",
    "    },\n",
    "    accelerator_type=\"A10G\",\n",
    "    engine_kwargs={\n",
    "        \"max_model_len\": 4096,  # Or increase KV cache size.\n",
    "        \"tensor_parallel_size\": 1,\n",
    "        \"enable_lora\": True,\n",
    "        # Complete list: https://docs.vllm.ai/en/stable/serving/engine_args.html\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9204734c",
   "metadata": {},
   "source": [
    "Now deploy the LLM config as an application. And because this is all built on top of [Ray Serve](https://docs.ray.io/en/latest/serve/index.html), you can have advanced service logic around composing models together, deploying multiple applications, model multiplexing, observability, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798bfdf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy\n",
    "app = build_openai_app({\"llm_configs\": [llm_config]})\n",
    "serve.run(app)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea333c2b",
   "metadata": {},
   "source": [
    "```text\n",
    "DeploymentHandle(deployment='LLMRouter')\n",
    "```\n",
    "\n",
    "## Service request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0086fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize client\n",
    "client = OpenAI(base_url=\"http://localhost:8000/v1\", api_key=\"fake-key\")\n",
    "response = client.chat.completions.create(\n",
    "    model=f\"{model_id}:{lora_id}\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"Given a target sentence construct the underlying meaning representation of the input sentence as a single function with attributes and attribute values. This function should describe the target string accurately and the function must be one of the following ['inform', 'request', 'give_opinion', 'confirm', 'verify_attribute', 'suggest', 'request_explanation', 'recommend', 'request_attribute']. The attributes must be one of the following: ['name', 'exp_release_date', 'release_year', 'developer', 'esrb', 'rating', 'genres', 'player_perspective', 'has_multiplayer', 'platforms', 'available_on_steam', 'has_linux_release', 'has_mac_release', 'specifier']\"},\n",
    "        {\"role\": \"user\", \"content\": \"Blizzard North is mostly an okay developer, but they released Diablo II for the Mac and so that pushes the game from okay to good in my view.\"},\n",
    "    ],\n",
    "    stream=True\n",
    ")\n",
    "for chunk in response:\n",
    "    if chunk.choices[0].delta.content is not None:\n",
    "        print(chunk.choices[0].delta.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768fdd6f",
   "metadata": {},
   "source": [
    "```text\n",
    "Avg prompt throughput: 20.3 tokens/s, Avg generation throughput: 0.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.3%, CPU KV cache usage: 0.0%.\n",
    "\n",
    "_opinion(name[Diablo II], developer[Blizzard North], rating[good], has_mac_release[yes])\n",
    "```\n",
    "\n",
    "And of course, you can observe the running service, like deployments and metrics like QPS, latency, etc., through the [Ray Dashboard](https://docs.ray.io/en/latest/ray-observability/getting-started.html)'s [Serve view](https://docs.ray.io/en/latest/ray-observability/getting-started.html#dash-serve-view):\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/anyscale/e2e-llm-workflows/refs/heads/main/images/serve_dashboard.png\" width=1000>\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "üí° See [more examples](https://docs.ray.io/en/latest/serve/llm/overview.html) and the [API reference](https://docs.ray.io/en/latest/serve/llm/api.html) for advanced guides on topics like structured outputs (like JSON), vision LMs, multi-LoRA on shared base models, using other inference engines (like `sglang`), fast model loading, etc.\n",
    "\n",
    "</div>\n",
    "\n",
    "## Production\n",
    "\n",
    "Seamlessly integrate with your existing CI/CD pipelines by leveraging the Anyscale [CLI](https://docs.anyscale.com/reference/quickstart-cli) or [SDK](https://docs.anyscale.com/reference/quickstart-sdk) to run [reliable batch jobs](https://docs.anyscale.com/platform/jobs) and deploy [highly available services](https://docs.anyscale.com/platform/services). Given you've been developing in an environment that's almost identical to production, meaning a multi-node cluster, this integration should drastically speed up your dev to prod velocity.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/anyscale/foundational-ray-app/refs/heads/main/images/cicd.png\" width=600>\n",
    "\n",
    "### Jobs\n",
    "\n",
    "[Anyscale Jobs](https://docs.anyscale.com/platform/jobs/) ([API ref](https://docs.anyscale.com/reference/job-api/)) allows you to execute discrete workloads in production such as batch inference, embeddings generation, or model fine-tuning.\n",
    "\n",
    "- [define and manage](https://docs.anyscale.com/platform/jobs/manage-jobs) your Jobs in many different ways (CLI, Python SDK)\n",
    "- set up [queues](https://docs.anyscale.com/platform/jobs/job-queues) and [schedules](https://docs.anyscale.com/platform/jobs/schedules)\n",
    "- set up all the [observability, alerting, etc.](https://docs.anyscale.com/platform/jobs/monitoring-and-debugging) around your Jobs\n",
    "\n",
    "### Services\n",
    "\n",
    "[Anyscale Services](https://docs.anyscale.com/platform/services/) ([API ref](https://docs.anyscale.com/reference/service-api/)) offers an extremely fault tolerant, scalable and optimized way to serve your Ray Serve applications.\n",
    "\n",
    "- you can [rollout and update](https://docs.anyscale.com/platform/services/update-a-service) services with canary deployment, meaning zero-downtime upgrades\n",
    "- [monitor](https://docs.anyscale.com/platform/services/monitoring) your Services through a dedicated Service page, unified log viewer, tracing, set up alerts, etc.\n",
    "- scale a service with `num_replicas=auto` and utilize replica compaction to consolidate nodes that are fractionally utilized\n",
    "- [head node fault tolerance](https://docs.anyscale.com/platform/services/production-best-practices#head-node-ft) (OSS Ray recovers from failed workers and replicas but not head node crashes)\n",
    "- serving [multiple applications](https://docs.anyscale.com/platform/services/multi-app) in a single Service\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/anyscale/foundational-ray-app/refs/heads/main/images/canary.png\" width=700>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdcc05cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# clean up\n",
    "rm -rf /mnt/cluster_storage/viggo\n",
    "STORAGE_PATH=\"$ANYSCALE_ARTIFACT_STORAGE/viggo\"\n",
    "if [[ \"$STORAGE_PATH\" == s3://* ]]; then\n",
    "    aws s3 rm \"$STORAGE_PATH\" --recursive --quiet\n",
    "elif [[ \"$STORAGE_PATH\" == gs://* ]]; then\n",
    "    gsutil -m rm -q -r \"$STORAGE_PATH\"\n",
    "fi"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
