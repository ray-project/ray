{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "myst": {
     "front_matter": {
      "orphan": true
     }
    }
   },
   "source": [
    "# Direct Preference Optimization (DPO) at Scale with QLoRA\n",
    "\n",
    "This guide provides a step-by-step workflow for preference fine-tuning the `Qwen/Qwen2.5-7B-Instruct` model on a multi-GPU Anyscale cluster. We will use LLaMA-Factory as the training framework and `QLoRA` to reduce memory requirements and enable efficient multi-GPU training.\n",
    "\n",
    "**What is Direct Preference Optimization (DPO)?** DPO aligns a model with human preferences using pairs of “chosen” and “rejected” responses. Rather than training a separate reward model, DPO directly optimizes the policy to increase the likelihood of preferred outputs and decrease the likelihood of rejected ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Set Up Your Environment\n",
    "### Dependencies\n",
    "First, we need to ensure our environment has the right libraries. We'll start with a pre-built container image and install LLaMA-Factory and DeepSpeed on top of it.\n",
    "\n",
    "Recommended Container Image:\n",
    "```bash\n",
    "anyscale/ray-llm:2.48.0-py311-cu128\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the following commands to install the required packages and optional tools for experiment tracking and faster downloads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "jupyter-server 1.24.0 requires anyio<4,>=3.1.0, but you have anyio 4.10.0 which is incompatible.\u001b[0m\u001b[31m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mSuccessfully registered `llamafactory` package to be installed on all cluster nodes.\u001b[0m\n",
      "\u001b[92mView and update dependencies here: https://console.anyscale.com/cld_kvedZWag2qA8i5BjxUevf5i7/prj_cz951f43jjdybtzkx1s5sjgz99/workspaces/expwrk_j3li62ul9bwvaathjuulbzf7wc?workspace-tab=dependencies\u001b[0m\n",
      "\u001b[92mSuccessfully registered `tensorboard` package to be installed on all cluster nodes.\u001b[0m\n",
      "\u001b[92mView and update dependencies here: https://console.anyscale.com/cld_kvedZWag2qA8i5BjxUevf5i7/prj_cz951f43jjdybtzkx1s5sjgz99/workspaces/expwrk_j3li62ul9bwvaathjuulbzf7wc?workspace-tab=dependencies\u001b[0m\n",
      "\u001b[92mSuccessfully registered `bitsandbytes` package to be installed on all cluster nodes.\u001b[0m\n",
      "\u001b[92mView and update dependencies here: https://console.anyscale.com/cld_kvedZWag2qA8i5BjxUevf5i7/prj_cz951f43jjdybtzkx1s5sjgz99/workspaces/expwrk_j3li62ul9bwvaathjuulbzf7wc?workspace-tab=dependencies\u001b[0m\n",
      "\u001b[92mSuccessfully registered `autoawq` package to be installed on all cluster nodes.\u001b[0m\n",
      "\u001b[92mView and update dependencies here: https://console.anyscale.com/cld_kvedZWag2qA8i5BjxUevf5i7/prj_cz951f43jjdybtzkx1s5sjgz99/workspaces/expwrk_j3li62ul9bwvaathjuulbzf7wc?workspace-tab=dependencies\u001b[0m\n",
      "\u001b[92mSuccessfully registered `hf_transfer` package to be installed on all cluster nodes.\u001b[0m\n",
      "\u001b[92mView and update dependencies here: https://console.anyscale.com/cld_kvedZWag2qA8i5BjxUevf5i7/prj_cz951f43jjdybtzkx1s5sjgz99/workspaces/expwrk_j3li62ul9bwvaathjuulbzf7wc?workspace-tab=dependencies\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Install the specific version of LLaMA-Factory\n",
    "pip install -q llamafactory@git+https://github.com/hiyouga/LLaMA-Factory.git@v0.9.3\n",
    "\n",
    "# (Optional) For visualizing training metrics and logs\n",
    "pip install -q tensorboard==2.20.0\n",
    "\n",
    "# (Optional) For lightweight 8-bit and 4-bit optimizers and inference\n",
    "pip install -q bitsandbytes==0.47.0\n",
    "\n",
    "# (Optional) For AWQ quantization support\n",
    "pip install -q autoawq==0.2.9\n",
    "\n",
    "# (Optional) For accelerated model downloads from Hugging Face\n",
    "pip install -q hf_transfer==0.1.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model and Resources\n",
    "\n",
    "| Item | Value |\n",
    "|------|-------|\n",
    "| **Base model** | [`Qwen/Qwen2.5-7B-Instruct`](https://huggingface.co/Qwen/Qwen2.5-7B-Instruct) |\n",
    "| **Workers** | 4 × L4 / A10G |\n",
    "\n",
    "> Compared to SFT, DPO holds two copies of the model (policy + reference), and alignment datasets often use long contexts, so it's the ideal workflow for memory optimization techniques like **QLoRA**. On 24 GB NVIDIA L4 GPUs, running DPO at FP16 for 7B models generally OOMs without QLoRA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understand the Dataset\n",
    "For this tutorial, we will use [`ultrafeedback.jsonl`](https://huggingface.co/datasets/kaitchup/UltraFeedback-prompt-chosen-rejected), a JSONL preference dataset tailored for Direct Preference Optimization (DPO). Each sample contains one instruction **prompt** and two candidate completions: a **preferred** (`chosen`) response and a **less preferred** (`rejected`) response.\n",
    "\n",
    "This dataset includes:\n",
    "- `prompt`: An instruction/question to answer (often multi-sentence, with constraints).\n",
    "- `chosen`: The response that best follows the instruction.\n",
    "- `rejected`: A weaker alternative for the same prompt (may span multiple lines).\n",
    "\n",
    "**Note:** Files are in JSON Lines format—one JSON object per line. Each record is independent (no multi-turn conversation thread).\n",
    "\n",
    "<details>\n",
    "  <summary>Dataset Example</summary>\n",
    "\n",
    "  ```json\n",
    "  {\n",
    "    \"prompt\": \"Paraphrase the given questions to have different wording. Your paraphrased questions should have the same answer as the original question. Try to change the sentence as much as possible using synonyms and/or rearranging the structure of the sentence. The questions are in three domains: presidents, national parks, and dogs. Each question has a keyword indicating its domain. Keywords are \\\"this national park\\\", \\\"this dog breed\\\", and \\\"this president\\\", which will be replaced with the name of an actual president, a national park, or a breed of dog. Hence, in paraphrasing, this keyword should also be used the same way. Do not write questions that compare or involve multiple domains. Do not write open-ended or subjective questions (e.g., questions that can be answered differently by different people.) Make your questions specific and concrete. Your question should have the same type of answer as the original question(e.g., if the question is extractive, the paraphrased question should be extractive as well.)\\n\\nWhat lakes are in this national park?\",\n",
    "\n",
    "    \"rejected\": \"What bodies of water are located in this national park? \\n\\nWhich president is commonly known for his efforts to protect natural resources?\\n\\nWhich president is recognized for their dedication to preserving the environment? \\n\\nWhat type of dog breed is known for its loyalty and affectionate nature?\\n\\nWhat breed of dog is renowned for its faithfulness and loving personality?\",\n",
    "    \n",
    "    \"chosen\": \"Which bodies of water can be found within the borders of this particular national park?\"\n",
    "  }\n",
    "  ```\n",
    "</details>\n",
    "\n",
    "### Register the local dataset\n",
    "\n",
    "To specify new datasets that are accessible across Ray worker nodes, you must first add all dataset files and a `dataset_info.json` to **[storage shared across nodes](https://docs.anyscale.com/configuration/storage#shared)** such as `/mnt/cluster_storage`. \n",
    "\n",
    "For example, if you wanted to run DPO post-training on the `ultrafeedback` dataset locally, first go through the following setup steps:\n",
    "\n",
    "`dataset_info.json`\n",
    "```json\n",
    "{\n",
    "  \"my_ultrafeedback\": {\n",
    "    \"file_name\": \"ultrafeedback.jsonl\",\n",
    "    \"ranking\": true,\n",
    "    \"columns\": {\n",
    "      \"prompt\": \"prompt\",\n",
    "      \"chosen\": \"chosen\",\n",
    "      \"rejected\": \"rejected\"\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "For a more detailed dataset preparation and formatting guide, follow [_](https://docs.anyscale.com/llm/fine-tuning/data-preparation#data-format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--2025-09-19 15:54:37--  https://anyscale-public-materials.s3.us-west-2.amazonaws.com/llm-finetuning/llama-factory/datasets/alpaca/ultrafeedback.jsonl\n",
      "Resolving anyscale-public-materials.s3.us-west-2.amazonaws.com (anyscale-public-materials.s3.us-west-2.amazonaws.com)... 3.5.77.216, 52.218.230.41, 52.92.229.66, ...\n",
      "Connecting to anyscale-public-materials.s3.us-west-2.amazonaws.com (anyscale-public-materials.s3.us-west-2.amazonaws.com)|3.5.77.216|:443... connected.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 291881 (285K) [application/x-www-form-urlencoded]\n",
      "Saving to: ‘/mnt/cluster_storage/ultrafeedback.jsonl’\n",
      "\n",
      "     0K .......... .......... .......... .......... .......... 17%  233M 0s\n",
      "    50K .......... .......... .......... .......... .......... 35% 35.1M 0s\n",
      "   100K .......... .......... .......... .......... .......... 52%  240M 0s\n",
      "   150K .......... .......... .......... .......... .......... 70% 80.9M 0s\n",
      "   200K .......... .......... .......... .......... .......... 87%  248M 0s\n",
      "   250K .......... .......... .......... .....                100%  264M=0.003s\n",
      "\n",
      "2025-09-19 15:54:37 (102 MB/s) - ‘/mnt/cluster_storage/ultrafeedback.jsonl’ saved [291881/291881]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Make sure all files are accessible to worker nodes\n",
    "# Create a copy of the data in /mnt/cluster_storage\n",
    "wget https://anyscale-public-materials.s3.us-west-2.amazonaws.com/llm-finetuning/llama-factory/datasets/alpaca/ultrafeedback.jsonl -O /mnt/cluster_storage/ultrafeedback.jsonl\n",
    "# Create a copy of the dataset registry in /mnt/cluster_storage\n",
    "cp ../dataset-configs/dataset_info.json /mnt/cluster_storage/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Create the Preference-Tuning Config (DPO + QLoRA)\n",
    "\n",
    "Next, create the YAML configuration file that defines your DPO (Direct Preference Optimization) run. It specifies the base model, quantization (QLoRA), dataset, DPO hyperparameters, logging, and Ray cluster resources.\n",
    "\n",
    "Here is the `qwen2.5_7b_qlora_dpo_ray.yaml` included in the workspace:\n",
    "\n",
    "**Important notes:**\n",
    "- **QLoRA quantization:** `quantization_bit: 4` with `quantization_method: bnb` applies quantization using bitsandbytes, reducing memory while preserving quality. If you use a model *pre-quantized* with AWQ, **omit** these keys.\n",
    "- **LoRA setup**: If you prefer standard LoRA, **disable quantization** by removing both `quantization_bit` and `quantization_method` from the config.\n",
    "- **Access & paths:** The YAML only needs to be on the **head node**, but any referenced paths (`dataset_dir`, `output_dir`) must live on storage **reachable by all workers** (e.g., `/mnt/cluster_storage/`).\n",
    "- **Gated models:** Qwen is generally ungated. For gated bases (e.g., Llama), add your `HF_TOKEN`.\n",
    "\n",
    "### LLaMA-Factory + Ray Configuration\n",
    "\n",
    "```yaml\n",
    "# train-configs/dpo_qlora.yaml\n",
    "\n",
    "### model\n",
    "trust_remote_code: true\n",
    "model_name_or_path: Qwen/Qwen2.5-7B-Instruct\n",
    "\n",
    "### method\n",
    "# If you instead want to use just LoRA, or use a pre-quantized model like Qwen/Qwen2.5-7B-Instruct-AWQ, then omit the quantization_bit/method keys below\n",
    "quantization_bit: 4 # 4-bit base weights (QLoRA). Use 8 for 8-bit; omit for FP16/BF16\n",
    "quantization_method: bnb  # QLoRA via BitsAndBytes or hqq / eetq\n",
    "\n",
    "stage: dpo\n",
    "do_train: true\n",
    "finetuning_type: lora\n",
    "lora_rank: 8\n",
    "lora_target: all\n",
    "pref_beta: 0.1\n",
    "pref_loss: sigmoid  # choices: [sigmoid (dpo), orpo, simpo]\n",
    "\n",
    "# local dataset\n",
    "dataset: my_ultrafeedback\n",
    "dataset_dir: /mnt/cluster_storage\n",
    "\n",
    "template: qwen\n",
    "cutoff_len: 1024\n",
    "max_samples: 1000\n",
    "overwrite_cache: true\n",
    "preprocessing_num_workers: 16\n",
    "\n",
    "### output\n",
    "output_dir: qwen2.5_7b_qlora_dpo\n",
    "logging_steps: 5\n",
    "save_steps: 5              # for tensorboard logging purpose too, can increase if not using tensorboard\n",
    "plot_loss: true\n",
    "report_to: tensorboard  # or none\n",
    "\n",
    "### train\n",
    "per_device_train_batch_size: 1\n",
    "gradient_accumulation_steps: 2\n",
    "num_train_epochs: 3.0  # low for demo purpose; adjust as needed\n",
    "learning_rate: 5.0e-6\n",
    "bf16: true\n",
    "lr_scheduler_type: cosine\n",
    "warmup_ratio: 0.1\n",
    "ddp_timeout: 180000000\n",
    "\n",
    "### ray\n",
    "ray_run_name: qwen2.5_7b_qlora_dpo\n",
    "ray_storage_path: /mnt/cluster_storage/\n",
    "ray_num_workers: 4  # Number of GPUs to use.\n",
    "resources_per_worker:\n",
    "  GPU: 1\n",
    "  anyscale/accelerator_shape:4xL4: 0.001  # Use this to specify a specific node shape.\n",
    "  # accelerator_type:L4: 0.001            # Or use this to simply specify a GPU type.\n",
    "  # See https://docs.ray.io/en/master/ray-core/accelerator-types.html#accelerator-types for a full list of accelerator types.\n",
    "\n",
    "ray_init_kwargs:\n",
    "  runtime_env:\n",
    "    env_vars:\n",
    "      # if using gated models like meta-llama/Llama-3.1-8B-Instruct\n",
    "      # HF_TOKEN: <your_huggingface_token>\n",
    "      # Enable faster downloads if hf_transfer is installed:\n",
    "      HF_HUB_ENABLE_HF_TRANSFER: '1'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Train and Monitor\n",
    "\n",
    "With all configuration in place, you can launch fine-tuning/post-training in one of two ways."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option A — Run from a Workspace (quick start)\n",
    "\n",
    "The `USE_RAY=1` prefix tells LLaMA-Factory to run in distributed mode on the Ray cluster attached to your workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-19 16:12:06 [__init__.py:248] No platform detected, vLLM is running on UnspecifiedPlatform\n",
      "WARNING 09-19 16:12:06 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('libcuda.so.1: cannot open shared object file: No such file or directory')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-19 16:12:10,738\tINFO worker.py:1747 -- Connecting to existing Ray cluster at address: 10.0.168.141:6379...\n",
      "2025-09-19 16:12:10,748\tINFO worker.py:1918 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32mhttps://session-c3rc1dvuypysehcmb91gu17t54.i.anyscaleuserdata.com \u001b[39m\u001b[22m\n",
      "2025-09-19 16:12:10,750\tINFO packaging.py:380 -- Pushing file package 'gcs://_ray_pkg_4743ba917c5b0b6789a61fbd792b5972f2c8ed63.zip' (0.10MiB) to Ray cluster...\n",
      "2025-09-19 16:12:10,751\tINFO packaging.py:393 -- Successfully pushed file package 'gcs://_ray_pkg_4743ba917c5b0b6789a61fbd792b5972f2c8ed63.zip'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "View detailed results here: /mnt/cluster_storage/qwen2.5_7b_qlora_dpo\n",
      "To visualize your results with TensorBoard, run: `tensorboard --logdir /tmp/ray/session_2025-09-19_15-49-42_124608_2430/artifacts/2025-09-19_16-12-10/qwen2.5_7b_qlora_dpo/driver_artifacts`\n",
      "\n",
      "Training started with configuration:\n",
      "╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
      "│ Training config                                                                                              │\n",
      "├──────────────────────────────────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ train_loop_config/args/bf16                                                                             True │\n",
      "│ train_loop_config/args/cutoff_len                                                                       1024 │\n",
      "│ train_loop_config/args/dataset                                                              my_ultrafeedback │\n",
      "│ train_loop_config/args/dataset_dir                                                      /mnt/cluster_storage │\n",
      "│ train_loop_config/args/ddp_timeout                                                                 180000000 │\n",
      "│ train_loop_config/args/do_train                                                                         True │\n",
      "│ train_loop_config/args/finetuning_type                                                                  lora │\n",
      "│ train_loop_config/args/gradient_accumulation_steps                                                         2 │\n",
      "│ train_loop_config/args/learning_rate                                                                   5e-06 │\n",
      "│ train_loop_config/args/logging_steps                                                                       5 │\n",
      "│ train_loop_config/args/lora_rank                                                                           8 │\n",
      "│ train_loop_config/args/lora_target                                                                       all │\n",
      "│ train_loop_config/args/lr_scheduler_type                                                              cosine │\n",
      "│ train_loop_config/args/max_samples                                                                      1000 │\n",
      "│ train_loop_config/args/model_name_or_path                                               ...en2.5-7B-Instruct │\n",
      "│ train_loop_config/args/num_train_epochs                                                                  3.0 │\n",
      "│ train_loop_config/args/output_dir                                                       qwen2.5_7b_qlora_dpo │\n",
      "│ train_loop_config/args/overwrite_cache                                                                  True │\n",
      "│ train_loop_config/args/per_device_train_batch_size                                                         1 │\n",
      "│ train_loop_config/args/plot_loss                                                                        True │\n",
      "│ train_loop_config/args/pref_beta                                                                         0.1 │\n",
      "│ train_loop_config/args/pref_loss                                                                     sigmoid │\n",
      "│ train_loop_config/args/preprocessing_num_workers                                                          16 │\n",
      "│ train_loop_config/args/quantization_bit                                                                    4 │\n",
      "│ train_loop_config/args/quantization_method                                                               bnb │\n",
      "│ train_loop_config/args/ray_init_kwargs/runtime_env/env_vars/HF_HUB_ENABLE_HF_TRANSFER                      1 │\n",
      "│ train_loop_config/args/ray_num_workers                                                                     4 │\n",
      "│ train_loop_config/args/ray_run_name                                                     qwen2.5_7b_qlora_dpo │\n",
      "│ train_loop_config/args/ray_storage_path                                                 .../cluster_storage/ │\n",
      "│ train_loop_config/args/report_to                                                                 tensorboard │\n",
      "│ train_loop_config/args/resources_per_worker/GPU                                                            1 │\n",
      "│ train_loop_config/args/resources_per_worker/anyscale/accelerator_shape:4xL4                            0.001 │\n",
      "│ train_loop_config/args/save_steps                                                                          5 │\n",
      "│ train_loop_config/args/stage                                                                             dpo │\n",
      "│ train_loop_config/args/template                                                                         qwen │\n",
      "│ train_loop_config/args/trust_remote_code                                                                True │\n",
      "│ train_loop_config/args/warmup_ratio                                                                      0.1 │\n",
      "│ train_loop_config/callbacks                                                             ... 0x73773c2f6850>] │\n",
      "╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m Setting up process group for: env:// [rank=0, world_size=4]\n",
      "\u001b[36m(TorchTrainer pid=6752, ip=10.0.183.178)\u001b[0m Started distributed worker processes: \n",
      "\u001b[36m(TorchTrainer pid=6752, ip=10.0.183.178)\u001b[0m - (node_id=c87e4ad0b35e4f468ebff6d2970822063a67672f55ac6702a530230f, ip=10.0.183.178, pid=6857) world_rank=0, local_rank=0, node_rank=0\n",
      "\u001b[36m(TorchTrainer pid=6752, ip=10.0.183.178)\u001b[0m - (node_id=c87e4ad0b35e4f468ebff6d2970822063a67672f55ac6702a530230f, ip=10.0.183.178, pid=6858) world_rank=1, local_rank=1, node_rank=0\n",
      "\u001b[36m(TorchTrainer pid=6752, ip=10.0.183.178)\u001b[0m - (node_id=c87e4ad0b35e4f468ebff6d2970822063a67672f55ac6702a530230f, ip=10.0.183.178, pid=6859) world_rank=2, local_rank=2, node_rank=0\n",
      "\u001b[36m(TorchTrainer pid=6752, ip=10.0.183.178)\u001b[0m - (node_id=c87e4ad0b35e4f468ebff6d2970822063a67672f55ac6702a530230f, ip=10.0.183.178, pid=6856) world_rank=3, local_rank=3, node_rank=0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m [WARNING|2025-09-19 16:12:24] llamafactory.hparams.parser:148 >> We recommend enable `upcast_layernorm` in quantized training.\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m [INFO|2025-09-19 16:12:24] llamafactory.hparams.parser:143 >> Set `ddp_find_unused_parameters` to False in DDP training since LoRA is enabled.\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m [INFO|2025-09-19 16:12:24] llamafactory.hparams.parser:406 >> Process rank: 0, world size: 4, device: cuda:0, distributed training: True, compute dtype: torch.bfloat16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m [INFO|tokenization_utils_base.py:2023] 2025-09-19 16:12:24,292 >> loading file vocab.json from cache at /home/ray/.cache/huggingface/hub/models--Qwen--Qwen2.5-7B-Instruct/snapshots/a09a35458c702b33eeacc393d103063234e8bc28/vocab.json\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m [INFO|tokenization_utils_base.py:2023] 2025-09-19 16:12:24,292 >> loading file merges.txt from cache at /home/ray/.cache/huggingface/hub/models--Qwen--Qwen2.5-7B-Instruct/snapshots/a09a35458c702b33eeacc393d103063234e8bc28/merges.txt\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m [INFO|tokenization_utils_base.py:2023] 2025-09-19 16:12:24,292 >> loading file tokenizer.json from cache at /home/ray/.cache/huggingface/hub/models--Qwen--Qwen2.5-7B-Instruct/snapshots/a09a35458c702b33eeacc393d103063234e8bc28/tokenizer.json\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m [INFO|tokenization_utils_base.py:2023] 2025-09-19 16:12:24,292 >> loading file added_tokens.json from cache at None\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m [INFO|tokenization_utils_base.py:2023] 2025-09-19 16:12:24,292 >> loading file special_tokens_map.json from cache at None\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m [INFO|tokenization_utils_base.py:2023] 2025-09-19 16:12:24,292 >> loading file tokenizer_config.json from cache at /home/ray/.cache/huggingface/hub/models--Qwen--Qwen2.5-7B-Instruct/snapshots/a09a35458c702b33eeacc393d103063234e8bc28/tokenizer_config.json\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m [INFO|tokenization_utils_base.py:2023] 2025-09-19 16:12:24,292 >> loading file chat_template.jinja from cache at None\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m [INFO|tokenization_utils_base.py:2299] 2025-09-19 16:12:24,543 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m [INFO|configuration_utils.py:698] 2025-09-19 16:12:25,178 >> loading configuration file config.json from cache at /home/ray/.cache/huggingface/hub/models--Qwen--Qwen2.5-7B-Instruct/snapshots/a09a35458c702b33eeacc393d103063234e8bc28/config.json\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m [INFO|configuration_utils.py:770] 2025-09-19 16:12:25,179 >> Model config Qwen2Config {\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"architectures\": [\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m     \"Qwen2ForCausalLM\"\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   ],\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"attention_dropout\": 0.0,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"bos_token_id\": 151643,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"eos_token_id\": 151645,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"hidden_act\": \"silu\",\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"hidden_size\": 3584,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"initializer_range\": 0.02,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"intermediate_size\": 18944,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"max_position_embeddings\": 32768,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"max_window_layers\": 28,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"model_type\": \"qwen2\",\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"num_attention_heads\": 28,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"num_hidden_layers\": 28,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"num_key_value_heads\": 4,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"rms_norm_eps\": 1e-06,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"rope_scaling\": null,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"rope_theta\": 1000000.0,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"sliding_window\": 131072,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"tie_word_embeddings\": false,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"torch_dtype\": \"bfloat16\",\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"transformers_version\": \"4.52.4\",\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"use_cache\": true,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"use_sliding_window\": false,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"vocab_size\": 152064\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m }\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m [INFO|tokenization_utils_base.py:2023] 2025-09-19 16:12:25,380 >> loading file vocab.json from cache at /home/ray/.cache/huggingface/hub/models--Qwen--Qwen2.5-7B-Instruct/snapshots/a09a35458c702b33eeacc393d103063234e8bc28/vocab.json\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m [INFO|tokenization_utils_base.py:2023] 2025-09-19 16:12:25,380 >> loading file merges.txt from cache at /home/ray/.cache/huggingface/hub/models--Qwen--Qwen2.5-7B-Instruct/snapshots/a09a35458c702b33eeacc393d103063234e8bc28/merges.txt\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m [INFO|tokenization_utils_base.py:2023] 2025-09-19 16:12:25,380 >> loading file tokenizer.json from cache at /home/ray/.cache/huggingface/hub/models--Qwen--Qwen2.5-7B-Instruct/snapshots/a09a35458c702b33eeacc393d103063234e8bc28/tokenizer.json\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m [INFO|tokenization_utils_base.py:2023] 2025-09-19 16:12:25,380 >> loading file added_tokens.json from cache at None\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m [INFO|tokenization_utils_base.py:2023] 2025-09-19 16:12:25,380 >> loading file special_tokens_map.json from cache at None\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m [INFO|tokenization_utils_base.py:2023] 2025-09-19 16:12:25,380 >> loading file tokenizer_config.json from cache at /home/ray/.cache/huggingface/hub/models--Qwen--Qwen2.5-7B-Instruct/snapshots/a09a35458c702b33eeacc393d103063234e8bc28/tokenizer_config.json\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m [INFO|tokenization_utils_base.py:2023] 2025-09-19 16:12:25,380 >> loading file chat_template.jinja from cache at None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m [INFO|2025-09-19 16:12:25] llamafactory.data.loader:143 >> Loading dataset ultrafeedback.jsonl...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m [INFO|tokenization_utils_base.py:2299] 2025-09-19 16:12:25,610 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "\u001b[36m(RayTrainWorker pid=6856, ip=10.0.183.178)\u001b[0m [rank3]:[W919 16:12:25.082181490 ProcessGroupNCCL.cpp:4715] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can pecify device_id in init_process_group() to force use of a particular device.\n",
      "Converting format of dataset (num_proc=16):   0%|          | 0/100 [00:00<?, ? examples/s]\n",
      "Converting format of dataset (num_proc=16): 100%|██████████| 100/100 [00:00<00:00, 575.25 examples/s]\n",
      "Running tokenizer on dataset (num_proc=16):   0%|          | 0/100 [00:00<?, ? examples/s]\n",
      "Running tokenizer on dataset (num_proc=16):   7%|▋         | 7/100 [00:00<00:06, 14.29 examples/s]\n",
      "Running tokenizer on dataset (num_proc=16):  14%|█▍        | 14/100 [00:00<00:03, 23.64 examples/s]\n",
      "Running tokenizer on dataset (num_proc=16):  28%|██▊       | 28/100 [00:00<00:01, 43.34 examples/s]\n",
      "Running tokenizer on dataset (num_proc=16):  46%|████▌     | 46/100 [00:00<00:00, 66.11 examples/s]\n",
      "Running tokenizer on dataset (num_proc=16):  58%|█████▊    | 58/100 [00:01<00:00, 60.65 examples/s]\n",
      "Running tokenizer on dataset (num_proc=16):  70%|███████   | 70/100 [00:01<00:00, 71.21 examples/s]\n",
      "Running tokenizer on dataset (num_proc=16):  82%|████████▏ | 82/100 [00:01<00:00, 77.63 examples/s]\n",
      "Running tokenizer on dataset (num_proc=16):  94%|█████████▍| 94/100 [00:01<00:00, 80.81 examples/s]\n",
      "Running tokenizer on dataset (num_proc=16): 100%|██████████| 100/100 [00:01<00:00, 58.62 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m training example:\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m chosen_input_ids:\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m [151644, 8948, 198, 2610, 525, 1207, 16948, 11, 3465, 553, 54364, 14817, 13, 1446, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 10398, 25, 16246, 264, 11652, 304, 8585, 11, 3410, 458, 13578, 62230, 81, 1475, 2319, 504, 279, 4024, 429, 51844, 279, 1852, 7290, 624, 2505, 25, 794, 3757, 264, 49410, 782, 963, 20731, 82008, 320, 69, 4517, 294, 51274, 3096, 24847, 82008, 8, 409, 85838, 512, 220, 21, 47349, 220, 16, 22, 17, 20, 13, 1967, 1723, 59304, 96858, 510, 5097, 25, 151645, 198, 151644, 77091, 198, 16, 13, 4270, 1342, 10632, 279, 2661, 11652, 304, 8585, 624, 623, 3757, 12224, 20731, 82008, 320, 59778, 315, 19833, 24847, 82008, 8, 504, 85838, 389, 5470, 220, 21, 11, 220, 16, 22, 17, 20, 13, 10964, 2841, 1033, 510, 41462, 20108, 312, 759, 12784, 424, 25, 512, 220, 21, 47349, 220, 16, 22, 17, 20, 11, 794, 3757, 264, 49410, 782, 963, 20731, 82008, 320, 4260, 36154, 294, 51274, 3096, 24847, 82008, 409, 85838, 701, 1842, 42053, 59304, 96858, 510, 17, 13, 3321, 4079, 519, 279, 2661, 11652, 304, 264, 2155, 1616, 11, 50010, 279, 1852, 7290, 624, 2304, 220, 21, 47349, 220, 16, 22, 17, 20, 11, 88260, 9281, 794, 3757, 1842, 20731, 82008, 320, 69, 4517, 294, 51274, 3096, 24847, 82008, 409, 85838, 568, 1967, 324, 6560, 29719, 1788, 510, 18, 13, 13293, 62230, 81, 1475, 11652, 429, 51844, 279, 1852, 7290, 624, 2304, 220, 21, 47349, 220, 16, 22, 17, 20, 11, 11300, 409, 794, 3757, 1842, 20731, 82008, 320, 10302, 517, 1187, 36154, 294, 51274, 3096, 24847, 82008, 409, 85838, 701, 45052, 14508, 15555, 939, 59304, 13, 151645, 198]\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m chosen_inputs:\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m <|im_start|>system\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m You are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m <|im_start|>user\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m Definition: Given a sentence in French, provide an equivalent paraphrased version from the original that retains the same meaning.\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m Input: St John a épousé Elizabeth Crowley (fille d'Ambrose Crowley) de Greenwich le 6 mars 1725. Leurs enfants étaient:\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m Output:<|im_end|>\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m <|im_start|>assistant\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m 1. Paraphrase the given sentence in French.\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m St John married Elizabeth Crowley (daughter of Ambrose Crowley) from Greenwich on March 6, 1725. Their children were:\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m Première rephrasage: le 6 mars 1725, St John a épousé Elizabeth Crowley (la fille d'Ambrose Crowley de Greenwich), et leurs enfants étaient:\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m 2. Rephrase the given sentence in a different way, retaining the same meaning.\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m Le 6 mars 1725, mariage entre St John et Elizabeth Crowley (fille d'Ambrose Crowley de Greenwich). Leur descendance est:\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m 3. Another paraphrased sentence that retains the same meaning.\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m Le 6 mars 1725, union de St John et Elizabeth Crowley (étant la fille d'Ambrose Crowley de Greenwich), ils ont eu des enfants.<|im_end|>\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m chosen_label_ids:\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 16, 13, 4270, 1342, 10632, 279, 2661, 11652, 304, 8585, 624, 623, 3757, 12224, 20731, 82008, 320, 59778, 315, 19833, 24847, 82008, 8, 504, 85838, 389, 5470, 220, 21, 11, 220, 16, 22, 17, 20, 13, 10964, 2841, 1033, 510, 41462, 20108, 312, 759, 12784, 424, 25, 512, 220, 21, 47349, 220, 16, 22, 17, 20, 11, 794, 3757, 264, 49410, 782, 963, 20731, 82008, 320, 4260, 36154, 294, 51274, 3096, 24847, 82008, 409, 85838, 701, 1842, 42053, 59304, 96858, 510, 17, 13, 3321, 4079, 519, 279, 2661, 11652, 304, 264, 2155, 1616, 11, 50010, 279, 1852, 7290, 624, 2304, 220, 21, 47349, 220, 16, 22, 17, 20, 11, 88260, 9281, 794, 3757, 1842, 20731, 82008, 320, 69, 4517, 294, 51274, 3096, 24847, 82008, 409, 85838, 568, 1967, 324, 6560, 29719, 1788, 510, 18, 13, 13293, 62230, 81, 1475, 11652, 429, 51844, 279, 1852, 7290, 624, 2304, 220, 21, 47349, 220, 16, 22, 17, 20, 11, 11300, 409, 794, 3757, 1842, 20731, 82008, 320, 10302, 517, 1187, 36154, 294, 51274, 3096, 24847, 82008, 409, 85838, 701, 45052, 14508, 15555, 939, 59304, 13, 151645, 198]\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m chosen_labels:\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m 1. Paraphrase the given sentence in French.\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m St John married Elizabeth Crowley (daughter of Ambrose Crowley) from Greenwich on March 6, 1725. Their children were:\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m Première rephrasage: le 6 mars 1725, St John a épousé Elizabeth Crowley (la fille d'Ambrose Crowley de Greenwich), et leurs enfants étaient:\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m 2. Rephrase the given sentence in a different way, retaining the same meaning.\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m Le 6 mars 1725, mariage entre St John et Elizabeth Crowley (fille d'Ambrose Crowley de Greenwich). Leur descendance est:\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m 3. Another paraphrased sentence that retains the same meaning.\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m Le 6 mars 1725, union de St John et Elizabeth Crowley (étant la fille d'Ambrose Crowley de Greenwich), ils ont eu des enfants.<|im_end|>\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m rejected_input_ids:\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m [151644, 8948, 198, 2610, 525, 1207, 16948, 11, 3465, 553, 54364, 14817, 13, 1446, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 10398, 25, 16246, 264, 11652, 304, 8585, 11, 3410, 458, 13578, 62230, 81, 1475, 2319, 504, 279, 4024, 429, 51844, 279, 1852, 7290, 624, 2505, 25, 794, 3757, 264, 49410, 782, 963, 20731, 82008, 320, 69, 4517, 294, 51274, 3096, 24847, 82008, 8, 409, 85838, 512, 220, 21, 47349, 220, 16, 22, 17, 20, 13, 1967, 1723, 59304, 96858, 510, 5097, 25, 151645, 198, 151644, 77091, 198, 1925, 220, 21, 339, 5470, 220, 16, 22, 17, 20, 11, 20731, 82008, 320, 54032, 24847, 82008, 594, 9803, 504, 85838, 8, 2684, 12224, 311, 794, 13, 3757, 11, 323, 3786, 807, 1030, 1493, 2841, 25, 151645, 198]\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m rejected_inputs:\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m <|im_start|>system\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m You are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m <|im_start|>user\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m Definition: Given a sentence in French, provide an equivalent paraphrased version from the original that retains the same meaning.\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m Input: St John a épousé Elizabeth Crowley (fille d'Ambrose Crowley) de Greenwich le 6 mars 1725. Leurs enfants étaient:\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m Output:<|im_end|>\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m <|im_start|>assistant\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m On 6th March 1725, Elizabeth Crowley (Ambrose Crowley's daughter from Greenwich) got married to St. John, and together they had these children:<|im_end|>\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m rejected_label_ids:\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 1925, 220, 21, 339, 5470, 220, 16, 22, 17, 20, 11, 20731, 82008, 320, 54032, 24847, 82008, 594, 9803, 504, 85838, 8, 2684, 12224, 311, 794, 13, 3757, 11, 323, 3786, 807, 1030, 1493, 2841, 25, 151645, 198]\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m rejected_labels:\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m On 6th March 1725, Elizabeth Crowley (Ambrose Crowley's daughter from Greenwich) got married to St. John, and together they had these children:<|im_end|>\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m [INFO|2025-09-19 16:12:29] llamafactory.model.model_utils.quantization:143 >> Quantizing model to 4 bit with bitsandbytes.\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m [INFO|2025-09-19 16:12:29] llamafactory.model.model_utils.kv_cache:143 >> KV cache is disabled during training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m [INFO|configuration_utils.py:698] 2025-09-19 16:12:28,998 >> loading configuration file config.json from cache at /home/ray/.cache/huggingface/hub/models--Qwen--Qwen2.5-7B-Instruct/snapshots/a09a35458c702b33eeacc393d103063234e8bc28/config.json\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m [INFO|configuration_utils.py:770] 2025-09-19 16:12:28,999 >> Model config Qwen2Config {\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"architectures\": [\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m     \"Qwen2ForCausalLM\"\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   ],\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"attention_dropout\": 0.0,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"bos_token_id\": 151643,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"eos_token_id\": 151645,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"hidden_act\": \"silu\",\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"hidden_size\": 3584,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"initializer_range\": 0.02,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"intermediate_size\": 18944,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"max_position_embeddings\": 32768,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"max_window_layers\": 28,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"model_type\": \"qwen2\",\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"num_attention_heads\": 28,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"num_hidden_layers\": 28,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"num_key_value_heads\": 4,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"rms_norm_eps\": 1e-06,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"rope_scaling\": null,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"rope_theta\": 1000000.0,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"sliding_window\": 131072,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"tie_word_embeddings\": false,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"torch_dtype\": \"bfloat16\",\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"transformers_version\": \"4.52.4\",\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"use_cache\": true,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"use_sliding_window\": false,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"vocab_size\": 152064\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m }\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m [INFO|modeling_utils.py:1151] 2025-09-19 16:12:29,836 >> loading weights file model.safetensors from cache at /home/ray/.cache/huggingface/hub/models--Qwen--Qwen2.5-7B-Instruct/snapshots/a09a35458c702b33eeacc393d103063234e8bc28/model.safetensors.index.json\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m [INFO|modeling_utils.py:2241] 2025-09-19 16:12:29,837 >> Instantiating Qwen2ForCausalLM model under default dtype torch.bfloat16.\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m [INFO|configuration_utils.py:1135] 2025-09-19 16:12:29,838 >> Generate config GenerationConfig {\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"bos_token_id\": 151643,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"eos_token_id\": 151645,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"use_cache\": false\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m }\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m \n",
      "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]\n",
      "Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:07,  2.57s/it]\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m [rank0]:[W919 16:12:26.723504597 ProcessGroupNCCL.cpp:4715] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can pecify device_id in init_process_group() to force use of a particular device.\u001b[32m [repeated 3x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "Loading checkpoint shards:  75%|███████▌  | 3/4 [00:09<00:03,  3.18s/it]\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:11<00:00,  2.90s/it]\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m [INFO|modeling_utils.py:5131] 2025-09-19 16:12:41,752 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m [INFO|modeling_utils.py:5139] 2025-09-19 16:12:41,752 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at Qwen/Qwen2.5-7B-Instruct.\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m [INFO|configuration_utils.py:1090] 2025-09-19 16:12:41,852 >> loading configuration file generation_config.json from cache at /home/ray/.cache/huggingface/hub/models--Qwen--Qwen2.5-7B-Instruct/snapshots/a09a35458c702b33eeacc393d103063234e8bc28/generation_config.json\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m [INFO|configuration_utils.py:1135] 2025-09-19 16:12:41,852 >> Generate config GenerationConfig {\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"bos_token_id\": 151643,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"do_sample\": true,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"eos_token_id\": [\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m     151645,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m     151643\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   ],\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"pad_token_id\": 151643,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"repetition_penalty\": 1.05,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"temperature\": 0.7,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"top_k\": 20,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"top_p\": 0.8\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m }\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m [INFO|2025-09-19 16:12:41] llamafactory.model.model_utils.checkpointing:143 >> Gradient checkpointing enabled.\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m [INFO|2025-09-19 16:12:41] llamafactory.model.model_utils.attention:143 >> Using torch SDPA for faster training and inference.\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m [INFO|2025-09-19 16:12:41] llamafactory.model.adapter:143 >> Upcasting trainable params to float32.\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m [INFO|2025-09-19 16:12:41] llamafactory.model.adapter:143 >> Fine-tuning method: LoRA\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m [INFO|2025-09-19 16:12:41] llamafactory.model.model_utils.misc:143 >> Found linear modules: q_proj,o_proj,up_proj,down_proj,v_proj,k_proj,gate_proj\n",
      "\u001b[36m(RayTrainWorker pid=6859, ip=10.0.183.178)\u001b[0m [INFO|2025-09-19 16:12:24] llamafactory.hparams.parser:406 >> Process rank: 2, world size: 4, device: cuda:2, distributed training: True, compute dtype: torch.bfloat16\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m [INFO|2025-09-19 16:12:42] llamafactory.model.loader:143 >> trainable params: 20,185,088 || all params: 7,635,801,600 || trainable%: 0.2643\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m [INFO|trainer.py:756] 2025-09-19 16:12:42,300 >> Using auto half precision backend\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m [INFO|trainer.py:2409] 2025-09-19 16:12:43,045 >> ***** Running training *****\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m [INFO|trainer.py:2410] 2025-09-19 16:12:43,045 >>   Num examples = 100\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m [INFO|trainer.py:2411] 2025-09-19 16:12:43,045 >>   Num Epochs = 3\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m [INFO|trainer.py:2412] 2025-09-19 16:12:43,045 >>   Instantaneous batch size per device = 1\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m [INFO|trainer.py:2415] 2025-09-19 16:12:43,045 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m [INFO|trainer.py:2416] 2025-09-19 16:12:43,045 >>   Gradient Accumulation steps = 2\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m [INFO|trainer.py:2417] 2025-09-19 16:12:43,045 >>   Total optimization steps = 39\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m [INFO|trainer.py:2418] 2025-09-19 16:12:43,048 >>   Number of trainable parameters = 20,185,088\n",
      "  0%|          | 0/39 [00:00<?, ?it/s].183.178)\u001b[0m \n",
      "  3%|▎         | 1/39 [00:04<02:52,  4.53s/it])\u001b[0m \n",
      "Loading checkpoint shards:  75%|███████▌  | 3/4 [00:09<00:03,  3.23s/it]\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:11<00:00,  2.95s/it]\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "  5%|▌         | 2/39 [00:10<03:15,  5.29s/it])\u001b[0m \n",
      "  8%|▊         | 3/39 [00:16<03:23,  5.64s/it])\u001b[0m \n",
      " 10%|█         | 4/39 [00:22<03:21,  5.75s/it])\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=6856, ip=10.0.183.178)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/mnt/cluster_storage/qwen2.5_7b_qlora_dpo/TorchTrainer_12134_00000_0_2025-09-19_16-12-10/checkpoint_000000)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m {'loss': 0.6977, 'grad_norm': 7.635711669921875, 'learning_rate': 5e-06, 'rewards/chosen': 0.009051240980625153, 'rewards/rejected': 0.014680067077279091, 'rewards/accuracies': 0.25, 'rewards/margins': -0.005628826562315226, 'logps/chosen': -280.187255859375, 'logps/rejected': -292.1568908691406, 'logits/chosen': -0.8685919046401978, 'logits/rejected': -0.8516700267791748, 'epoch': 0.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 5/39 [00:29<03:30,  6.20s/it][INFO|trainer.py:3993] 2025-09-19 16:13:13,581 >> Saving model checkpoint to qwen2.5_7b_qlora_dpo/checkpoint-5\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m [INFO|configuration_utils.py:698] 2025-09-19 16:13:13,823 >> loading configuration file config.json from cache at /home/ray/.cache/huggingface/hub/models--Qwen--Qwen2.5-7B-Instruct/snapshots/a09a35458c702b33eeacc393d103063234e8bc28/config.json\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m [INFO|configuration_utils.py:770] 2025-09-19 16:13:13,824 >> Model config Qwen2Config {\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"architectures\": [\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m     \"Qwen2ForCausalLM\"\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   ],\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"attention_dropout\": 0.0,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"bos_token_id\": 151643,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"eos_token_id\": 151645,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"hidden_act\": \"silu\",\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"hidden_size\": 3584,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"initializer_range\": 0.02,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"intermediate_size\": 18944,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"max_position_embeddings\": 32768,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"max_window_layers\": 28,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"model_type\": \"qwen2\",\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"num_attention_heads\": 28,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"num_hidden_layers\": 28,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"num_key_value_heads\": 4,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"rms_norm_eps\": 1e-06,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"rope_scaling\": null,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"rope_theta\": 1000000.0,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"sliding_window\": 131072,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"tie_word_embeddings\": false,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"torch_dtype\": \"bfloat16\",\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"transformers_version\": \"4.52.4\",\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"use_cache\": true,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"use_sliding_window\": false,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"vocab_size\": 152064\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m }\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m [INFO|tokenization_utils_base.py:2356] 2025-09-19 16:13:13,939 >> chat template saved in qwen2.5_7b_qlora_dpo/checkpoint-5/chat_template.jinja\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m [INFO|tokenization_utils_base.py:2525] 2025-09-19 16:13:13,940 >> tokenizer config file saved in qwen2.5_7b_qlora_dpo/checkpoint-5/tokenizer_config.json\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m [INFO|tokenization_utils_base.py:2534] 2025-09-19 16:13:13,940 >> Special tokens file saved in qwen2.5_7b_qlora_dpo/checkpoint-5/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training finished iteration 1 at 2025-09-19 16:13:16. Total running time: 1min 5s\n",
      "╭─────────────────────────────────────────╮\n",
      "│ Training result                         │\n",
      "├─────────────────────────────────────────┤\n",
      "│ checkpoint_dir_name   checkpoint_000000 │\n",
      "│ time_this_iter_s               58.76934 │\n",
      "│ time_total_s                   58.76934 │\n",
      "│ training_iteration                    1 │\n",
      "│ epoch                               0.4 │\n",
      "│ grad_norm                       7.63571 │\n",
      "│ learning_rate                   0.00001 │\n",
      "│ logits/chosen                  -0.86859 │\n",
      "│ logits/rejected                -0.85167 │\n",
      "│ logps/chosen                 -280.18726 │\n",
      "│ logps/rejected               -292.15689 │\n",
      "│ loss                             0.6977 │\n",
      "│ rewards/accuracies                 0.25 │\n",
      "│ rewards/chosen                  0.00905 │\n",
      "│ rewards/margins                -0.00563 │\n",
      "│ rewards/rejected                0.01468 │\n",
      "│ step                                  5 │\n",
      "╰─────────────────────────────────────────╯\n",
      "Training saved a checkpoint for iteration 1 at: (local)/mnt/cluster_storage/qwen2.5_7b_qlora_dpo/TorchTrainer_12134_00000_0_2025-09-19_16-12-10/checkpoint_000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 6/39 [00:36<03:36,  6.55s/it])\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/mnt/cluster_storage/qwen2.5_7b_qlora_dpo/TorchTrainer_12134_00000_0_2025-09-19_16-12-10/checkpoint_000000)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      " 18%|█▊        | 7/39 [00:42<03:19,  6.24s/it])\u001b[0m \n",
      " 21%|██        | 8/39 [00:47<03:04,  5.94s/it])\u001b[0m \n",
      " 23%|██▎       | 9/39 [00:54<03:12,  6.40s/it])\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m {'loss': 0.7005, 'grad_norm': 10.753315925598145, 'learning_rate': 4.752422169756048e-06, 'rewards/chosen': -0.011030399240553379, 'rewards/rejected': -0.0002653626725077629, 'rewards/accuracies': 0.4749999940395355, 'rewards/margins': -0.010765035636723042, 'logps/chosen': -278.4250183105469, 'logps/rejected': -295.75921630859375, 'logits/chosen': -0.8270877599716187, 'logits/rejected': -0.9758028984069824, 'epoch': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 10/39 [01:01<03:06,  6.44s/it][INFO|trainer.py:3993] 2025-09-19 16:13:45,664 >> Saving model checkpoint to qwen2.5_7b_qlora_dpo/checkpoint-10\n",
      "\u001b[36m(RayTrainWorker pid=6858, ip=10.0.183.178)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/mnt/cluster_storage/qwen2.5_7b_qlora_dpo/TorchTrainer_12134_00000_0_2025-09-19_16-12-10/checkpoint_000001)\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m [INFO|configuration_utils.py:698] 2025-09-19 16:13:45,894 >> loading configuration file config.json from cache at /home/ray/.cache/huggingface/hub/models--Qwen--Qwen2.5-7B-Instruct/snapshots/a09a35458c702b33eeacc393d103063234e8bc28/config.json\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m [INFO|configuration_utils.py:770] 2025-09-19 16:13:45,895 >> Model config Qwen2Config {\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"architectures\": [\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m     \"Qwen2ForCausalLM\"\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   ],\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"attention_dropout\": 0.0,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"bos_token_id\": 151643,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"eos_token_id\": 151645,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"hidden_act\": \"silu\",\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"hidden_size\": 3584,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"initializer_range\": 0.02,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"intermediate_size\": 18944,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"max_position_embeddings\": 32768,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"max_window_layers\": 28,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"model_type\": \"qwen2\",\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"num_attention_heads\": 28,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"num_hidden_layers\": 28,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"num_key_value_heads\": 4,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"rms_norm_eps\": 1e-06,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"rope_scaling\": null,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"rope_theta\": 1000000.0,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"sliding_window\": 131072,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"tie_word_embeddings\": false,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"torch_dtype\": \"bfloat16\",\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"transformers_version\": \"4.52.4\",\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"use_cache\": true,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"use_sliding_window\": false,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"vocab_size\": 152064\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m }\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m [INFO|tokenization_utils_base.py:2356] 2025-09-19 16:13:46,011 >> chat template saved in qwen2.5_7b_qlora_dpo/checkpoint-10/chat_template.jinja\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m [INFO|tokenization_utils_base.py:2525] 2025-09-19 16:13:46,011 >> tokenizer config file saved in qwen2.5_7b_qlora_dpo/checkpoint-10/tokenizer_config.json\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m [INFO|tokenization_utils_base.py:2534] 2025-09-19 16:13:46,011 >> Special tokens file saved in qwen2.5_7b_qlora_dpo/checkpoint-10/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training finished iteration 2 at 2025-09-19 16:13:48. Total running time: 1min 37s\n",
      "╭─────────────────────────────────────────╮\n",
      "│ Training result                         │\n",
      "├─────────────────────────────────────────┤\n",
      "│ checkpoint_dir_name   checkpoint_000001 │\n",
      "│ time_this_iter_s                32.0465 │\n",
      "│ time_total_s                   90.81584 │\n",
      "│ training_iteration                    2 │\n",
      "│ epoch                               0.8 │\n",
      "│ grad_norm                      10.75332 │\n",
      "│ learning_rate                        0. │\n",
      "│ logits/chosen                  -0.82709 │\n",
      "│ logits/rejected                 -0.9758 │\n",
      "│ logps/chosen                 -278.42502 │\n",
      "│ logps/rejected               -295.75922 │\n",
      "│ loss                             0.7005 │\n",
      "│ rewards/accuracies                0.475 │\n",
      "│ rewards/chosen                 -0.01103 │\n",
      "│ rewards/margins                -0.01077 │\n",
      "│ rewards/rejected               -0.00027 │\n",
      "│ step                                 10 │\n",
      "╰─────────────────────────────────────────╯\n",
      "Training saved a checkpoint for iteration 2 at: (local)/mnt/cluster_storage/qwen2.5_7b_qlora_dpo/TorchTrainer_12134_00000_0_2025-09-19_16-12-10/checkpoint_000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 11/39 [01:10<03:19,  7.12s/it]\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/mnt/cluster_storage/qwen2.5_7b_qlora_dpo/TorchTrainer_12134_00000_0_2025-09-19_16-12-10/checkpoint_000001)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      " 31%|███       | 12/39 [01:15<02:56,  6.53s/it]\u001b[0m \n",
      " 33%|███▎      | 13/39 [01:16<02:09,  4.97s/it]\u001b[0m \n",
      " 36%|███▌      | 14/39 [01:24<02:23,  5.74s/it]\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=6858, ip=10.0.183.178)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/mnt/cluster_storage/qwen2.5_7b_qlora_dpo/TorchTrainer_12134_00000_0_2025-09-19_16-12-10/checkpoint_000002)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m {'loss': 0.6083, 'grad_norm': 6.456727027893066, 'learning_rate': 4.058724504646834e-06, 'rewards/chosen': -0.014000813476741314, 'rewards/rejected': -0.05412696301937103, 'rewards/accuracies': 0.5555555820465088, 'rewards/margins': 0.04012615233659744, 'logps/chosen': -221.62258911132812, 'logps/rejected': -242.07455444335938, 'logits/chosen': -0.801766037940979, 'logits/rejected': -0.932135820388794, 'epoch': 1.16}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 15/39 [01:30<02:23,  5.98s/it][INFO|trainer.py:3993] 2025-09-19 16:14:14,942 >> Saving model checkpoint to qwen2.5_7b_qlora_dpo/checkpoint-15\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m [INFO|configuration_utils.py:698] 2025-09-19 16:14:15,184 >> loading configuration file config.json from cache at /home/ray/.cache/huggingface/hub/models--Qwen--Qwen2.5-7B-Instruct/snapshots/a09a35458c702b33eeacc393d103063234e8bc28/config.json\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m [INFO|configuration_utils.py:770] 2025-09-19 16:14:15,184 >> Model config Qwen2Config {\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"architectures\": [\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m     \"Qwen2ForCausalLM\"\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   ],\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"attention_dropout\": 0.0,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"bos_token_id\": 151643,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"eos_token_id\": 151645,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"hidden_act\": \"silu\",\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"hidden_size\": 3584,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"initializer_range\": 0.02,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"intermediate_size\": 18944,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"max_position_embeddings\": 32768,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"max_window_layers\": 28,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"model_type\": \"qwen2\",\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"num_attention_heads\": 28,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"num_hidden_layers\": 28,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"num_key_value_heads\": 4,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"rms_norm_eps\": 1e-06,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"rope_scaling\": null,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"rope_theta\": 1000000.0,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"sliding_window\": 131072,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"tie_word_embeddings\": false,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"torch_dtype\": \"bfloat16\",\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"transformers_version\": \"4.52.4\",\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"use_cache\": true,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"use_sliding_window\": false,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"vocab_size\": 152064\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m }\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m [INFO|tokenization_utils_base.py:2356] 2025-09-19 16:14:15,301 >> chat template saved in qwen2.5_7b_qlora_dpo/checkpoint-15/chat_template.jinja\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m [INFO|tokenization_utils_base.py:2525] 2025-09-19 16:14:15,302 >> tokenizer config file saved in qwen2.5_7b_qlora_dpo/checkpoint-15/tokenizer_config.json\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m [INFO|tokenization_utils_base.py:2534] 2025-09-19 16:14:15,302 >> Special tokens file saved in qwen2.5_7b_qlora_dpo/checkpoint-15/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training finished iteration 3 at 2025-09-19 16:14:17. Total running time: 2min 6s\n",
      "╭─────────────────────────────────────────╮\n",
      "│ Training result                         │\n",
      "├─────────────────────────────────────────┤\n",
      "│ checkpoint_dir_name   checkpoint_000002 │\n",
      "│ time_this_iter_s               29.18197 │\n",
      "│ time_total_s                   119.9978 │\n",
      "│ training_iteration                    3 │\n",
      "│ epoch                              1.16 │\n",
      "│ grad_norm                       6.45673 │\n",
      "│ learning_rate                        0. │\n",
      "│ logits/chosen                  -0.80177 │\n",
      "│ logits/rejected                -0.93214 │\n",
      "│ logps/chosen                 -221.62259 │\n",
      "│ logps/rejected               -242.07455 │\n",
      "│ loss                             0.6083 │\n",
      "│ rewards/accuracies              0.55556 │\n",
      "│ rewards/chosen                   -0.014 │\n",
      "│ rewards/margins                 0.04013 │\n",
      "│ rewards/rejected               -0.05413 │\n",
      "│ step                                 15 │\n",
      "╰─────────────────────────────────────────╯\n",
      "Training saved a checkpoint for iteration 3 at: (local)/mnt/cluster_storage/qwen2.5_7b_qlora_dpo/TorchTrainer_12134_00000_0_2025-09-19_16-12-10/checkpoint_000002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 16/39 [01:37<02:24,  6.27s/it]\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/mnt/cluster_storage/qwen2.5_7b_qlora_dpo/TorchTrainer_12134_00000_0_2025-09-19_16-12-10/checkpoint_000002)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      " 44%|████▎     | 17/39 [01:44<02:24,  6.59s/it]\u001b[0m \n",
      " 46%|████▌     | 18/39 [01:52<02:23,  6.85s/it]\u001b[0m \n",
      " 49%|████▊     | 19/39 [01:57<02:08,  6.44s/it]\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=6858, ip=10.0.183.178)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/mnt/cluster_storage/qwen2.5_7b_qlora_dpo/TorchTrainer_12134_00000_0_2025-09-19_16-12-10/checkpoint_000003)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m {'loss': 0.6945, 'grad_norm': 6.419449806213379, 'learning_rate': 3.056302334890786e-06, 'rewards/chosen': 0.0024192254059016705, 'rewards/rejected': 0.0013211145997047424, 'rewards/accuracies': 0.5750000476837158, 'rewards/margins': 0.0010981112718582153, 'logps/chosen': -276.6562194824219, 'logps/rejected': -287.2279357910156, 'logits/chosen': -0.8158325552940369, 'logits/rejected': -0.8227788209915161, 'epoch': 1.56}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████▏    | 20/39 [02:02<01:52,  5.93s/it][INFO|trainer.py:3993] 2025-09-19 16:14:46,893 >> Saving model checkpoint to qwen2.5_7b_qlora_dpo/checkpoint-20\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m [INFO|configuration_utils.py:698] 2025-09-19 16:14:47,131 >> loading configuration file config.json from cache at /home/ray/.cache/huggingface/hub/models--Qwen--Qwen2.5-7B-Instruct/snapshots/a09a35458c702b33eeacc393d103063234e8bc28/config.json\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m [INFO|configuration_utils.py:770] 2025-09-19 16:14:47,131 >> Model config Qwen2Config {\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"architectures\": [\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m     \"Qwen2ForCausalLM\"\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   ],\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"attention_dropout\": 0.0,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"bos_token_id\": 151643,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"eos_token_id\": 151645,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"hidden_act\": \"silu\",\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"hidden_size\": 3584,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"initializer_range\": 0.02,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"intermediate_size\": 18944,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"max_position_embeddings\": 32768,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"max_window_layers\": 28,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"model_type\": \"qwen2\",\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"num_attention_heads\": 28,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"num_hidden_layers\": 28,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"num_key_value_heads\": 4,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"rms_norm_eps\": 1e-06,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"rope_scaling\": null,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"rope_theta\": 1000000.0,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"sliding_window\": 131072,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"tie_word_embeddings\": false,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"torch_dtype\": \"bfloat16\",\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"transformers_version\": \"4.52.4\",\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"use_cache\": true,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"use_sliding_window\": false,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"vocab_size\": 152064\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m }\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m [INFO|tokenization_utils_base.py:2356] 2025-09-19 16:14:47,247 >> chat template saved in qwen2.5_7b_qlora_dpo/checkpoint-20/chat_template.jinja\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m [INFO|tokenization_utils_base.py:2525] 2025-09-19 16:14:47,247 >> tokenizer config file saved in qwen2.5_7b_qlora_dpo/checkpoint-20/tokenizer_config.json\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m [INFO|tokenization_utils_base.py:2534] 2025-09-19 16:14:47,247 >> Special tokens file saved in qwen2.5_7b_qlora_dpo/checkpoint-20/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training finished iteration 4 at 2025-09-19 16:14:49. Total running time: 2min 38s\n",
      "╭─────────────────────────────────────────╮\n",
      "│ Training result                         │\n",
      "├─────────────────────────────────────────┤\n",
      "│ checkpoint_dir_name   checkpoint_000003 │\n",
      "│ time_this_iter_s                31.9378 │\n",
      "│ time_total_s                   151.9356 │\n",
      "│ training_iteration                    4 │\n",
      "│ epoch                              1.56 │\n",
      "│ grad_norm                       6.41945 │\n",
      "│ learning_rate                        0. │\n",
      "│ logits/chosen                  -0.81583 │\n",
      "│ logits/rejected                -0.82278 │\n",
      "│ logps/chosen                 -276.65622 │\n",
      "│ logps/rejected               -287.22794 │\n",
      "│ loss                             0.6945 │\n",
      "│ rewards/accuracies                0.575 │\n",
      "│ rewards/chosen                  0.00242 │\n",
      "│ rewards/margins                  0.0011 │\n",
      "│ rewards/rejected                0.00132 │\n",
      "│ step                                 20 │\n",
      "╰─────────────────────────────────────────╯\n",
      "Training saved a checkpoint for iteration 4 at: (local)/mnt/cluster_storage/qwen2.5_7b_qlora_dpo/TorchTrainer_12134_00000_0_2025-09-19_16-12-10/checkpoint_000003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 21/39 [02:08<01:48,  6.05s/it]\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/mnt/cluster_storage/qwen2.5_7b_qlora_dpo/TorchTrainer_12134_00000_0_2025-09-19_16-12-10/checkpoint_000003)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      " 56%|█████▋    | 22/39 [02:12<01:32,  5.44s/it]\u001b[0m \n",
      " 59%|█████▉    | 23/39 [02:17<01:23,  5.23s/it]\u001b[0m \n",
      " 62%|██████▏   | 24/39 [02:23<01:19,  5.29s/it]\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=6856, ip=10.0.183.178)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/mnt/cluster_storage/qwen2.5_7b_qlora_dpo/TorchTrainer_12134_00000_0_2025-09-19_16-12-10/checkpoint_000004)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m {'loss': 0.6834, 'grad_norm': 8.747222900390625, 'learning_rate': 1.9436976651092143e-06, 'rewards/chosen': -0.008300685323774815, 'rewards/rejected': -0.032277125865221024, 'rewards/accuracies': 0.44999998807907104, 'rewards/margins': 0.023976439610123634, 'logps/chosen': -220.33480834960938, 'logps/rejected': -298.58892822265625, 'logits/chosen': -0.8311011791229248, 'logits/rejected': -0.881970226764679, 'epoch': 1.96}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 25/39 [02:27<01:11,  5.09s/it][INFO|trainer.py:3993] 2025-09-19 16:15:12,018 >> Saving model checkpoint to qwen2.5_7b_qlora_dpo/checkpoint-25\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m [INFO|configuration_utils.py:698] 2025-09-19 16:15:12,253 >> loading configuration file config.json from cache at /home/ray/.cache/huggingface/hub/models--Qwen--Qwen2.5-7B-Instruct/snapshots/a09a35458c702b33eeacc393d103063234e8bc28/config.json\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m [INFO|configuration_utils.py:770] 2025-09-19 16:15:12,254 >> Model config Qwen2Config {\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"architectures\": [\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m     \"Qwen2ForCausalLM\"\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   ],\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"attention_dropout\": 0.0,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"bos_token_id\": 151643,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"eos_token_id\": 151645,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"hidden_act\": \"silu\",\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"hidden_size\": 3584,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"initializer_range\": 0.02,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"intermediate_size\": 18944,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"max_position_embeddings\": 32768,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"max_window_layers\": 28,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"model_type\": \"qwen2\",\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"num_attention_heads\": 28,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"num_hidden_layers\": 28,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"num_key_value_heads\": 4,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"rms_norm_eps\": 1e-06,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"rope_scaling\": null,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"rope_theta\": 1000000.0,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"sliding_window\": 131072,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"tie_word_embeddings\": false,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"torch_dtype\": \"bfloat16\",\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"transformers_version\": \"4.52.4\",\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"use_cache\": true,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"use_sliding_window\": false,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"vocab_size\": 152064\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m }\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m [INFO|tokenization_utils_base.py:2356] 2025-09-19 16:15:12,370 >> chat template saved in qwen2.5_7b_qlora_dpo/checkpoint-25/chat_template.jinja\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m [INFO|tokenization_utils_base.py:2525] 2025-09-19 16:15:12,370 >> tokenizer config file saved in qwen2.5_7b_qlora_dpo/checkpoint-25/tokenizer_config.json\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m [INFO|tokenization_utils_base.py:2534] 2025-09-19 16:15:12,370 >> Special tokens file saved in qwen2.5_7b_qlora_dpo/checkpoint-25/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training finished iteration 5 at 2025-09-19 16:15:14. Total running time: 3min 3s\n",
      "╭─────────────────────────────────────────╮\n",
      "│ Training result                         │\n",
      "├─────────────────────────────────────────┤\n",
      "│ checkpoint_dir_name   checkpoint_000004 │\n",
      "│ time_this_iter_s               24.95763 │\n",
      "│ time_total_s                  176.89323 │\n",
      "│ training_iteration                    5 │\n",
      "│ epoch                              1.96 │\n",
      "│ grad_norm                       8.74722 │\n",
      "│ learning_rate                        0. │\n",
      "│ logits/chosen                   -0.8311 │\n",
      "│ logits/rejected                -0.88197 │\n",
      "│ logps/chosen                 -220.33481 │\n",
      "│ logps/rejected               -298.58893 │\n",
      "│ loss                             0.6834 │\n",
      "│ rewards/accuracies                 0.45 │\n",
      "│ rewards/chosen                  -0.0083 │\n",
      "│ rewards/margins                 0.02398 │\n",
      "│ rewards/rejected               -0.03228 │\n",
      "│ step                                 25 │\n",
      "╰─────────────────────────────────────────╯\n",
      "Training saved a checkpoint for iteration 5 at: (local)/mnt/cluster_storage/qwen2.5_7b_qlora_dpo/TorchTrainer_12134_00000_0_2025-09-19_16-12-10/checkpoint_000004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 26/39 [02:33<01:09,  5.37s/it]\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/mnt/cluster_storage/qwen2.5_7b_qlora_dpo/TorchTrainer_12134_00000_0_2025-09-19_16-12-10/checkpoint_000004)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      " 69%|██████▉   | 27/39 [02:40<01:10,  5.86s/it]\u001b[0m \n",
      " 72%|███████▏  | 28/39 [02:46<01:04,  5.88s/it]\u001b[0m \n",
      " 74%|███████▍  | 29/39 [02:51<00:54,  5.45s/it]\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m {'loss': 0.6069, 'grad_norm': 9.205820083618164, 'learning_rate': 9.412754953531664e-07, 'rewards/chosen': 0.010408895090222359, 'rewards/rejected': -0.0308152474462986, 'rewards/accuracies': 0.6111111640930176, 'rewards/margins': 0.04122414067387581, 'logps/chosen': -319.7648010253906, 'logps/rejected': -264.7530822753906, 'logits/chosen': -0.8459330797195435, 'logits/rejected': -0.958310604095459, 'epoch': 2.32}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 30/39 [02:58<00:54,  6.05s/it][INFO|trainer.py:3993] 2025-09-19 16:15:42,895 >> Saving model checkpoint to qwen2.5_7b_qlora_dpo/checkpoint-30\n",
      "\u001b[36m(RayTrainWorker pid=6858, ip=10.0.183.178)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/mnt/cluster_storage/qwen2.5_7b_qlora_dpo/TorchTrainer_12134_00000_0_2025-09-19_16-12-10/checkpoint_000005)\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m [INFO|configuration_utils.py:698] 2025-09-19 16:15:43,141 >> loading configuration file config.json from cache at /home/ray/.cache/huggingface/hub/models--Qwen--Qwen2.5-7B-Instruct/snapshots/a09a35458c702b33eeacc393d103063234e8bc28/config.json\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m [INFO|configuration_utils.py:770] 2025-09-19 16:15:43,141 >> Model config Qwen2Config {\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"architectures\": [\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m     \"Qwen2ForCausalLM\"\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   ],\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"attention_dropout\": 0.0,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"bos_token_id\": 151643,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"eos_token_id\": 151645,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"hidden_act\": \"silu\",\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"hidden_size\": 3584,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"initializer_range\": 0.02,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"intermediate_size\": 18944,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"max_position_embeddings\": 32768,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"max_window_layers\": 28,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"model_type\": \"qwen2\",\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"num_attention_heads\": 28,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"num_hidden_layers\": 28,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"num_key_value_heads\": 4,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"rms_norm_eps\": 1e-06,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"rope_scaling\": null,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"rope_theta\": 1000000.0,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"sliding_window\": 131072,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"tie_word_embeddings\": false,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"torch_dtype\": \"bfloat16\",\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"transformers_version\": \"4.52.4\",\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"use_cache\": true,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"use_sliding_window\": false,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"vocab_size\": 152064\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m }\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m [INFO|tokenization_utils_base.py:2356] 2025-09-19 16:15:43,257 >> chat template saved in qwen2.5_7b_qlora_dpo/checkpoint-30/chat_template.jinja\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m [INFO|tokenization_utils_base.py:2525] 2025-09-19 16:15:43,257 >> tokenizer config file saved in qwen2.5_7b_qlora_dpo/checkpoint-30/tokenizer_config.json\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m [INFO|tokenization_utils_base.py:2534] 2025-09-19 16:15:43,257 >> Special tokens file saved in qwen2.5_7b_qlora_dpo/checkpoint-30/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training finished iteration 6 at 2025-09-19 16:15:45. Total running time: 3min 34s\n",
      "╭─────────────────────────────────────────╮\n",
      "│ Training result                         │\n",
      "├─────────────────────────────────────────┤\n",
      "│ checkpoint_dir_name   checkpoint_000005 │\n",
      "│ time_this_iter_s               30.96431 │\n",
      "│ time_total_s                  207.85754 │\n",
      "│ training_iteration                    6 │\n",
      "│ epoch                              2.32 │\n",
      "│ grad_norm                       9.20582 │\n",
      "│ learning_rate                        0. │\n",
      "│ logits/chosen                  -0.84593 │\n",
      "│ logits/rejected                -0.95831 │\n",
      "│ logps/chosen                  -319.7648 │\n",
      "│ logps/rejected               -264.75308 │\n",
      "│ loss                             0.6069 │\n",
      "│ rewards/accuracies              0.61111 │\n",
      "│ rewards/chosen                  0.01041 │\n",
      "│ rewards/margins                 0.04122 │\n",
      "│ rewards/rejected               -0.03082 │\n",
      "│ step                                 30 │\n",
      "╰─────────────────────────────────────────╯\n",
      "Training saved a checkpoint for iteration 6 at: (local)/mnt/cluster_storage/qwen2.5_7b_qlora_dpo/TorchTrainer_12134_00000_0_2025-09-19_16-12-10/checkpoint_000005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 31/39 [03:05<00:49,  6.24s/it]\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/mnt/cluster_storage/qwen2.5_7b_qlora_dpo/TorchTrainer_12134_00000_0_2025-09-19_16-12-10/checkpoint_000005)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      " 82%|████████▏ | 32/39 [03:10<00:41,  5.97s/it]\u001b[0m \n",
      " 85%|████████▍ | 33/39 [03:14<00:32,  5.42s/it]\u001b[0m \n",
      " 87%|████████▋ | 34/39 [03:20<00:27,  5.53s/it]\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m {'loss': 0.683, 'grad_norm': 9.392861366271973, 'learning_rate': 2.4757783024395244e-07, 'rewards/chosen': -0.004318982362747192, 'rewards/rejected': -0.02901686355471611, 'rewards/accuracies': 0.5499999523162842, 'rewards/margins': 0.024697883054614067, 'logps/chosen': -224.56350708007812, 'logps/rejected': -296.472412109375, 'logits/chosen': -0.7405776977539062, 'logits/rejected': -0.8611633777618408, 'epoch': 2.72}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|████████▉ | 35/39 [03:27<00:23,  5.99s/it][INFO|trainer.py:3993] 2025-09-19 16:16:11,899 >> Saving model checkpoint to qwen2.5_7b_qlora_dpo/checkpoint-35\n",
      "\u001b[36m(RayTrainWorker pid=6858, ip=10.0.183.178)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/mnt/cluster_storage/qwen2.5_7b_qlora_dpo/TorchTrainer_12134_00000_0_2025-09-19_16-12-10/checkpoint_000006)\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m [INFO|configuration_utils.py:698] 2025-09-19 16:16:12,136 >> loading configuration file config.json from cache at /home/ray/.cache/huggingface/hub/models--Qwen--Qwen2.5-7B-Instruct/snapshots/a09a35458c702b33eeacc393d103063234e8bc28/config.json\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m [INFO|configuration_utils.py:770] 2025-09-19 16:16:12,137 >> Model config Qwen2Config {\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"architectures\": [\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m     \"Qwen2ForCausalLM\"\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   ],\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"attention_dropout\": 0.0,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"bos_token_id\": 151643,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"eos_token_id\": 151645,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"hidden_act\": \"silu\",\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"hidden_size\": 3584,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"initializer_range\": 0.02,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"intermediate_size\": 18944,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"max_position_embeddings\": 32768,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"max_window_layers\": 28,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"model_type\": \"qwen2\",\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"num_attention_heads\": 28,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"num_hidden_layers\": 28,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"num_key_value_heads\": 4,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"rms_norm_eps\": 1e-06,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"rope_scaling\": null,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"rope_theta\": 1000000.0,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"sliding_window\": 131072,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"tie_word_embeddings\": false,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"torch_dtype\": \"bfloat16\",\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"transformers_version\": \"4.52.4\",\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"use_cache\": true,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"use_sliding_window\": false,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"vocab_size\": 152064\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m }\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m [INFO|tokenization_utils_base.py:2356] 2025-09-19 16:16:12,253 >> chat template saved in qwen2.5_7b_qlora_dpo/checkpoint-35/chat_template.jinja\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m [INFO|tokenization_utils_base.py:2525] 2025-09-19 16:16:12,253 >> tokenizer config file saved in qwen2.5_7b_qlora_dpo/checkpoint-35/tokenizer_config.json\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m [INFO|tokenization_utils_base.py:2534] 2025-09-19 16:16:12,253 >> Special tokens file saved in qwen2.5_7b_qlora_dpo/checkpoint-35/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training finished iteration 7 at 2025-09-19 16:16:14. Total running time: 4min 3s\n",
      "╭─────────────────────────────────────────╮\n",
      "│ Training result                         │\n",
      "├─────────────────────────────────────────┤\n",
      "│ checkpoint_dir_name   checkpoint_000006 │\n",
      "│ time_this_iter_s               29.29587 │\n",
      "│ time_total_s                  237.15341 │\n",
      "│ training_iteration                    7 │\n",
      "│ epoch                              2.72 │\n",
      "│ grad_norm                       9.39286 │\n",
      "│ learning_rate                        0. │\n",
      "│ logits/chosen                  -0.74058 │\n",
      "│ logits/rejected                -0.86116 │\n",
      "│ logps/chosen                 -224.56351 │\n",
      "│ logps/rejected               -296.47241 │\n",
      "│ loss                              0.683 │\n",
      "│ rewards/accuracies                 0.55 │\n",
      "│ rewards/chosen                 -0.00432 │\n",
      "│ rewards/margins                  0.0247 │\n",
      "│ rewards/rejected               -0.02902 │\n",
      "│ step                                 35 │\n",
      "╰─────────────────────────────────────────╯\n",
      "Training saved a checkpoint for iteration 7 at: (local)/mnt/cluster_storage/qwen2.5_7b_qlora_dpo/TorchTrainer_12134_00000_0_2025-09-19_16-12-10/checkpoint_000006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 36/39 [03:35<00:19,  6.52s/it]\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/mnt/cluster_storage/qwen2.5_7b_qlora_dpo/TorchTrainer_12134_00000_0_2025-09-19_16-12-10/checkpoint_000006)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      " 95%|█████████▍| 37/39 [03:40<00:12,  6.02s/it]\u001b[0m \n",
      " 97%|█████████▋| 38/39 [03:46<00:06,  6.02s/it]\u001b[0m \n",
      "100%|██████████| 39/39 [03:47<00:00,  4.66s/it][INFO|trainer.py:3993] 2025-09-19 16:16:32,021 >> Saving model checkpoint to qwen2.5_7b_qlora_dpo/checkpoint-39\n",
      "\u001b[36m(RayTrainWorker pid=6858, ip=10.0.183.178)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/mnt/cluster_storage/qwen2.5_7b_qlora_dpo/TorchTrainer_12134_00000_0_2025-09-19_16-12-10/checkpoint_000007)\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m [INFO|configuration_utils.py:698] 2025-09-19 16:16:32,260 >> loading configuration file config.json from cache at /home/ray/.cache/huggingface/hub/models--Qwen--Qwen2.5-7B-Instruct/snapshots/a09a35458c702b33eeacc393d103063234e8bc28/config.json\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m [INFO|configuration_utils.py:770] 2025-09-19 16:16:32,260 >> Model config Qwen2Config {\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"architectures\": [\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m     \"Qwen2ForCausalLM\"\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   ],\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"attention_dropout\": 0.0,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"bos_token_id\": 151643,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"eos_token_id\": 151645,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"hidden_act\": \"silu\",\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"hidden_size\": 3584,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"initializer_range\": 0.02,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"intermediate_size\": 18944,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"max_position_embeddings\": 32768,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"max_window_layers\": 28,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"model_type\": \"qwen2\",\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"num_attention_heads\": 28,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"num_hidden_layers\": 28,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"num_key_value_heads\": 4,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"rms_norm_eps\": 1e-06,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"rope_scaling\": null,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"rope_theta\": 1000000.0,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"sliding_window\": 131072,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"tie_word_embeddings\": false,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"torch_dtype\": \"bfloat16\",\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"transformers_version\": \"4.52.4\",\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"use_cache\": true,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"use_sliding_window\": false,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"vocab_size\": 152064\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m }\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m [INFO|tokenization_utils_base.py:2356] 2025-09-19 16:16:32,375 >> chat template saved in qwen2.5_7b_qlora_dpo/checkpoint-39/chat_template.jinja\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m [INFO|tokenization_utils_base.py:2525] 2025-09-19 16:16:32,375 >> tokenizer config file saved in qwen2.5_7b_qlora_dpo/checkpoint-39/tokenizer_config.json\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m [INFO|tokenization_utils_base.py:2534] 2025-09-19 16:16:32,375 >> Special tokens file saved in qwen2.5_7b_qlora_dpo/checkpoint-39/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training finished iteration 8 at 2025-09-19 16:16:34. Total running time: 4min 23s\n",
      "╭─────────────────────────────────────────╮\n",
      "│ Training result                         │\n",
      "├─────────────────────────────────────────┤\n",
      "│ checkpoint_dir_name   checkpoint_000007 │\n",
      "│ time_this_iter_s               19.72356 │\n",
      "│ time_total_s                  256.87697 │\n",
      "│ training_iteration                    8 │\n",
      "│ epoch                              2.72 │\n",
      "│ grad_norm                       9.39286 │\n",
      "│ learning_rate                        0. │\n",
      "│ logits/chosen                  -0.74058 │\n",
      "│ logits/rejected                -0.86116 │\n",
      "│ logps/chosen                 -224.56351 │\n",
      "│ logps/rejected               -296.47241 │\n",
      "│ loss                              0.683 │\n",
      "│ rewards/accuracies                 0.55 │\n",
      "│ rewards/chosen                 -0.00432 │\n",
      "│ rewards/margins                  0.0247 │\n",
      "│ rewards/rejected               -0.02902 │\n",
      "│ step                                 35 │\n",
      "╰─────────────────────────────────────────╯\n",
      "Training saved a checkpoint for iteration 8 at: (local)/mnt/cluster_storage/qwen2.5_7b_qlora_dpo/TorchTrainer_12134_00000_0_2025-09-19_16-12-10/checkpoint_000007\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m {'train_runtime': 231.3853, 'train_samples_per_second': 1.297, 'train_steps_per_second': 0.169, 'train_loss': 0.6610336059179062, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m [INFO|trainer.py:2676] 2025-09-19 16:16:34,433 >> \n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m \n",
      "100%|██████████| 39/39 [03:50<00:00,  5.90s/it]\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m [INFO|trainer.py:3993] 2025-09-19 16:16:34,436 >> Saving model checkpoint to qwen2.5_7b_qlora_dpo\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m [INFO|configuration_utils.py:698] 2025-09-19 16:16:34,684 >> loading configuration file config.json from cache at /home/ray/.cache/huggingface/hub/models--Qwen--Qwen2.5-7B-Instruct/snapshots/a09a35458c702b33eeacc393d103063234e8bc28/config.json\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m [INFO|configuration_utils.py:770] 2025-09-19 16:16:34,685 >> Model config Qwen2Config {\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"architectures\": [\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m     \"Qwen2ForCausalLM\"\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   ],\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"attention_dropout\": 0.0,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"bos_token_id\": 151643,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"eos_token_id\": 151645,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"hidden_act\": \"silu\",\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"hidden_size\": 3584,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"initializer_range\": 0.02,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"intermediate_size\": 18944,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"max_position_embeddings\": 32768,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"max_window_layers\": 28,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"model_type\": \"qwen2\",\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"num_attention_heads\": 28,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"num_hidden_layers\": 28,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"num_key_value_heads\": 4,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"rms_norm_eps\": 1e-06,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"rope_scaling\": null,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"rope_theta\": 1000000.0,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"sliding_window\": 131072,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"tie_word_embeddings\": false,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"torch_dtype\": \"bfloat16\",\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"transformers_version\": \"4.52.4\",\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"use_cache\": true,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"use_sliding_window\": false,\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   \"vocab_size\": 152064\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m }\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m [INFO|tokenization_utils_base.py:2356] 2025-09-19 16:16:34,802 >> chat template saved in qwen2.5_7b_qlora_dpo/chat_template.jinja\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m [INFO|tokenization_utils_base.py:2525] 2025-09-19 16:16:34,803 >> tokenizer config file saved in qwen2.5_7b_qlora_dpo/tokenizer_config.json\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m [INFO|tokenization_utils_base.py:2534] 2025-09-19 16:16:34,803 >> Special tokens file saved in qwen2.5_7b_qlora_dpo/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m ***** train metrics *****\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   epoch                    =        3.0\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   total_flos               = 12512612GF\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   train_loss               =      0.661\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   train_runtime            = 0:03:51.38\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   train_samples_per_second =      1.297\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m   train_steps_per_second   =      0.169\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m Figure saved at: qwen2.5_7b_qlora_dpo/training_loss.png\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m Figure saved at: qwen2.5_7b_qlora_dpo/training_rewards_accuracies.png\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m [WARNING|2025-09-19 16:16:35] llamafactory.extras.ploting:148 >> No metric eval_loss to plot.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m [INFO|modelcard.py:450] 2025-09-19 16:16:35,095 >> Dropping the following result as it does not have all the necessary fields:\n",
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m {'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training completed after 8 iterations at 2025-09-19 16:16:36. Total running time: 4min 25s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-19 16:16:36,549\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/mnt/cluster_storage/qwen2.5_7b_qlora_dpo' in 0.0220s.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=6857, ip=10.0.183.178)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/mnt/cluster_storage/qwen2.5_7b_qlora_dpo/TorchTrainer_12134_00000_0_2025-09-19_16-12-10/checkpoint_000007)\u001b[32m [repeated 3x across cluster]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "USE_RAY=1 llamafactory-cli train ../train-configs/dpo_qlora.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option B — Run as an Anyscale Job (production)\n",
    "\n",
    "For longer or production runs, submit the training as an **Anyscale Job**. Jobs run outside your interactive session for better stability, retries, and durable logs. You’ll package LLaMA-Factory and other libraries in a container image and launch with a short job config. See [_](https://docs.anyscale.com/llm/fine-tuning/llamafactory-jobs) for the step-by-step guide."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitoring with TensorBoard\n",
    "If you enabled TensorBoard logging (`report_to: tensorboard` in your YAML), you can watch metrics (e.g., training loss) update live and compare multiple runs with the same run name side-by-side.\n",
    "\n",
    "- **While the job is running:** LLaMA-Factory prints a ready-to-run command that starts with `tensorboard --logdir`. Open a new terminal and run it. Example:\n",
    "  ```bash\n",
    "  tensorboard --logdir /tmp/ray/session_*/artifacts/*/qwen2.5_7b_qlora_dpo/driver_artifacts\n",
    "  ```\n",
    "\n",
    "- **After the job (shared storage):** Point TensorBoard at `{ray_storage_path}/{ray_run_name}/`. Each `TorchTrainer_*` subfolder holds event files for a single run. Using the parent folder aggregates all runs for easy comparison.\n",
    "  ```bash\n",
    "  tensorboard --logdir /mnt/cluster_storage/qwen2.5_7b_qlora_dpo\n",
    "  ```\n",
    "\n",
    "In your Anyscale workspace, look for the open **port 6006** labeled **TensorBoard** to view the dashboards.\n",
    "![Anyscale workspace showing open ports with TensorBoard on port 6006](https://anyscale-public-materials.s3.us-west-2.amazonaws.com/llm-finetuning/llama-factory/open-ports.png)\n",
    "\n",
    "**TensorBoard**\n",
    "![TensorBoard](https://anyscale-public-materials.s3.us-west-2.amazonaws.com/llm-finetuning/llama-factory/3.2.2/3.2.2-tensorboard.png)\n",
    "\n",
    "For a more detailed guide on tracking experiments with other tools such as WandB or MLFlow, see [_](https://docs.anyscale.com/llm/fine-tuning/observability-and-tracking.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Locate Checkpoints\n",
    "\n",
    "Checkpoints are written under `ray_storage_path/ray_run_name`. In this example run, the path is: `/mnt/cluster_storage/qwen2.5_32b_lora_sft`. \n",
    "\n",
    "Inside, you’ll see a **trainer session** directory named like:\n",
    "`TorchTrainer_ff224_00000_0_2025-09-19_15-57-20/`.\n",
    "\n",
    "- `TorchTrainer_*` is created **when the trainer starts**; the suffix encodes a short run id and the **start timestamp**.\n",
    "- Within that directory, checkpoints are named `checkpoint_000xxx/`, where the number is the saved ordered checkpoints. \n",
    "\n",
    "The save cadence is controlled by `save_strategy` and `save_steps`. For instructions on how to resume interrupted training via `resume_from_checkpoint` and more, see [_](https://docs.anyscale.com/llm/fine-tuning/checkpointing#artifacts-directory)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Export the Model\n",
    "\n",
    "If you use LoRA, you can keep the base model and adapter separate ([_](https://docs.anyscale.com/llm/serving/multi-lora)) or merge the adapter into the base model for low-latency inference. \n",
    "\n",
    "For full fine-tuning or freeze-tuning, export the fine-tuned model directly.\n",
    "\n",
    "You may optionally apply post-training quantization on merged or full models before serving. See [_](https://docs.anyscale.com/llm/fine-tuning/observability-and-tracking.md#ptq) for the exact export commands and options."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
