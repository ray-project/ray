{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Fine-Tuning (SFT) at Scale with DeepSpeed\n",
    "\n",
    "This guide provides a step-by-step workflow for supervised fine-tuning the `Qwen/Qwen2.5-32B-Instruct` model on a multi-GPU Anyscale cluster. We will use LLaMA-Factory for the training framework and `DeepSpeed` to efficiently manage memory and scale the training process.\n",
    "\n",
    "**What is Supervised Fine-Tuning (SFT)?** SFT is a technique to adapt a pre-trained model to specific tasks. By showing the model high-quality examples of instructions and their desired outputs, we teach it to follow new instructions more accurately.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Set Up Your Environment\n",
    "### Dependencies\n",
    "First, we need to ensure our environment has the right libraries. We'll start with a pre-built container image and install LLaMA-Factory and DeepSpeed on top of it.\n",
    "\n",
    "Recommended Container Image:\n",
    "```bash\n",
    "anyscale/ray-llm:2.48.0-py311-cu128\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the following commands to install the required packages and optional tools for experiment tracking and faster downloads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mSuccessfully registered `llamafactory` package to be installed on all cluster nodes.\u001b[0m\n",
      "\u001b[92mView and update dependencies here: https://console.anyscale.com/cld_kvedZWag2qA8i5BjxUevf5i7/prj_cz951f43jjdybtzkx1s5sjgz99/workspaces/expwrk_v9rjackrrlnihvmvv7fqkpc3mz?workspace-tab=dependencies\u001b[0m\n",
      "\u001b[92mSuccessfully registered `deepspeed` package to be installed on all cluster nodes.\u001b[0m\n",
      "\u001b[92mView and update dependencies here: https://console.anyscale.com/cld_kvedZWag2qA8i5BjxUevf5i7/prj_cz951f43jjdybtzkx1s5sjgz99/workspaces/expwrk_v9rjackrrlnihvmvv7fqkpc3mz?workspace-tab=dependencies\u001b[0m\n",
      "\u001b[92mSuccessfully registered `wandb` package to be installed on all cluster nodes.\u001b[0m\n",
      "\u001b[92mView and update dependencies here: https://console.anyscale.com/cld_kvedZWag2qA8i5BjxUevf5i7/prj_cz951f43jjdybtzkx1s5sjgz99/workspaces/expwrk_v9rjackrrlnihvmvv7fqkpc3mz?workspace-tab=dependencies\u001b[0m\n",
      "\u001b[92mSuccessfully registered `hf_transfer` package to be installed on all cluster nodes.\u001b[0m\n",
      "\u001b[92mView and update dependencies here: https://console.anyscale.com/cld_kvedZWag2qA8i5BjxUevf5i7/prj_cz951f43jjdybtzkx1s5sjgz99/workspaces/expwrk_v9rjackrrlnihvmvv7fqkpc3mz?workspace-tab=dependencies\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Install the specific version of LLaMA-Factory\n",
    "pip install -q llamafactory@git+https://github.com/hiyouga/LLaMA-Factory.git@v0.9.3\n",
    "\n",
    "# Install DeepSpeed for large-scale training\n",
    "pip install -q deepspeed==0.16.9\n",
    "\n",
    "# (Optional) For experiment tracking with Weights & Biases\n",
    "pip install -q wandb==0.21.3\n",
    "\n",
    "# (Optional) For accelerated model downloads from Hugging Face\n",
    "pip install -q hf_transfer==0.1.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model and Compute Resources\n",
    "\n",
    "DeepSpeed ZeRO-3 partitions parameters, gradients, and optimizer states across multiple GPUs, enabling supervised fine-tuning (SFT) of 30B+ LLMs on just 4 GPUs. This setup works with GPUs such as L40S (48 GB) or A100-40G. Before proceeding, check the availability of GPUs on your cloud.\n",
    "\n",
    "| Item | Value |\n",
    "|------|-------|\n",
    "| **Base model** | [`Qwen/Qwen2.5-32B-Instruct`](https://huggingface.co/Qwen/Qwen2.5-32B-Instruct) |\n",
    "| **Worker Nodes** | 4 × L40S / 4 x A100-40G |\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Prepare the Dataset\n",
    "\n",
    "### Understand the Dataset\n",
    "For this tutorial, we will use [`glaive_toolcall_en_demo`](https://huggingface.co/datasets/zuol/glaive_toolcall_en_demo/tree/main), a dataset designed to teach models how to use tools (also known as function calling).\n",
    "\n",
    "This dataset contains conversational examples where the model needs to interact with external tools. Each entry includes:\n",
    "* `conversations`: A turn-by-turn log between a human and the gpt assistant.\n",
    "* `tools`: A JSON schema describing the functions the model can call.\n",
    "\n",
    "Note: The `conversations` may include special turns like function_call (the model deciding to call a tool) and observation (the result returned from the tool). This structure is ideal for teaching the model sophisticated tool-use behavior. To maintain role alignment in ShareGPT format, a strict turn order must be followed: `human` and `observation` (tool output) must appear in odd-numbered positions (1, 3, 5, ...), while `gpt` and `function_call` must appear in even-numbered positions (2, 4, 6, ...). The model learns to generate the content in the `gpt` and `function_call` turns.\n",
    "\n",
    "<details>\n",
    "  <summary>Dataset Example</summary>\n",
    "\n",
    "  ```json\n",
    "  {\n",
    "    \"conversations\": [\n",
    "      {\n",
    "        \"from\": \"human\",\n",
    "        \"value\": \"Hi, I have some ingredients and I want to cook something. Can you help me find a recipe?\"\n",
    "      },\n",
    "      {\n",
    "        \"from\": \"gpt\",\n",
    "        \"value\": \"Of course! I can help you with that. Please tell me what ingredients you have.\"\n",
    "      },\n",
    "      {\n",
    "        \"from\": \"human\",\n",
    "        \"value\": \"I have chicken, bell peppers, and rice.\"\n",
    "      },\n",
    "      {\n",
    "        \"from\": \"function_call\",\n",
    "        \"value\": \"{\\\"name\\\": \\\"search_recipes\\\", \\\"arguments\\\": {\\\"ingredients\\\": [\\\"chicken\\\", \\\"bell peppers\\\", \\\"rice\\\"]}}\"\n",
    "      },\n",
    "      {\n",
    "        \"from\": \"observation\",\n",
    "        \"value\": \"{\\\"recipes\\\": [{\\\"name\\\": \\\"Chicken and Bell Pepper Stir Fry\\\", \\\"ingredients\\\": [\\\"chicken\\\", \\\"bell peppers\\\", \\\"rice\\\"], \\\"instructions\\\": \\\"Cut the chicken into small pieces. Slice the bell peppers. Cook the rice. Stir fry the chicken and bell peppers. Serve over rice.\\\"}, {\\\"name\\\": \\\"Chicken and Rice Casserole\\\", \\\"ingredients\\\": [\\\"chicken\\\", \\\"bell peppers\\\", \\\"rice\\\"], \\\"instructions\\\": \\\"Cook the chicken and rice separately. Mix them together with the bell peppers in a casserole dish. Bake until golden brown.\\\"}]}\"\n",
    "      },\n",
    "      {\n",
    "        \"from\": \"gpt\",\n",
    "        \"value\": \"I found two recipes for you. The first one is \\\"Chicken and Bell Pepper Stir Fry\\\". The instructions are: Cut the chicken into small pieces. Slice the bell peppers. Cook the rice. Stir fry the chicken and bell peppers. Serve over rice. The second one is \\\"Chicken and Rice Casserole\\\". The instructions are: Cook the chicken and rice separately. Mix them together with the bell peppers in a casserole dish. Bake until golden brown. Which one would you like to try?\"\n",
    "      },\n",
    "      {\n",
    "        \"from\": \"human\",\n",
    "        \"value\": \"They both sound delicious, but I think I'll try the stir fry. Can you order the ingredients for me?\"\n",
    "      },\n",
    "      {\n",
    "        \"from\": \"gpt\",\n",
    "        \"value\": \"I'm sorry, but as an AI, I don't have the capability to perform external tasks such as ordering ingredients. However, I can help you find more recipes or provide cooking tips if you need.\"\n",
    "      }\n",
    "    ],\n",
    "    \"tools\": \"[{\\\"name\\\": \\\"search_recipes\\\", \\\"description\\\": \\\"Search for recipes based on ingredients\\\", \\\"parameters\\\": {\\\"type\\\": \\\"object\\\", \\\"properties\\\": {\\\"ingredients\\\": {\\\"type\\\": \\\"array\\\", \\\"items\\\": {\\\"type\\\": \\\"string\\\"}, \\\"description\\\": \\\"The ingredients to search for\\\"}}, \\\"required\\\": [\\\"ingredients\\\"]}}]\"\n",
    "  }\n",
    "  ```\n",
    "\n",
    "</details>\n",
    "\n",
    "### Register the local dataset\n",
    "\n",
    "To specify new datasets that are accessible across Ray worker nodes, you must first add all dataset files and a `dataset_info.json` to **[storage shared across nodes](https://docs.anyscale.com/configuration/storage#shared)** such as `/mnt/cluster_storage`. \n",
    "\n",
    "For example, if you wanted to run SFT fine-tuning on the `glaive_toolcall_en_demo` dataset locally, first go through the following setup steps:\n",
    "\n",
    "`dataset_info.json`\n",
    "```json\n",
    "{\n",
    "  \"my_glaive_toolcall_en_demo\": {\n",
    "      \"file_name\": \"glaive_toolcall_en_demo.json\",\n",
    "      \"formatting\": \"sharegpt\",\n",
    "      \"columns\": {\n",
    "          \"messages\": \"conversations\",\n",
    "          \"tools\": \"tools\"\n",
    "      }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "For a more detailed dataset preparation and formatting guide, follow **TODO: link**:[_](3.1.3-data-prep-fine-tune.md)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--2025-09-15 08:52:01--  https://anyscale-public-materials.s3.us-west-2.amazonaws.com/llm-finetuning/llama-factory/datasets/sharegpt/glaive_toolcall_en_demo.json\n",
      "Resolving anyscale-public-materials.s3.us-west-2.amazonaws.com (anyscale-public-materials.s3.us-west-2.amazonaws.com)... 3.5.77.39, 3.5.82.180, 3.5.78.195, ...\n",
      "Connecting to anyscale-public-materials.s3.us-west-2.amazonaws.com (anyscale-public-materials.s3.us-west-2.amazonaws.com)|3.5.77.39|:443... connected.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 738925 (722K) [application/json]\n",
      "Saving to: ‘/mnt/cluster_storage/glaive_toolcall_en_demo.json’\n",
      "\n",
      "     0K .......... .......... .......... .......... ..........  6%  266M 0s\n",
      "    50K .......... .......... .......... .......... .......... 13%  107M 0s\n",
      "   100K .......... .......... .......... .......... .......... 20%  319M 0s\n",
      "   150K .......... .......... .......... .......... .......... 27%  364M 0s\n",
      "   200K .......... .......... .......... .......... .......... 34%  318M 0s\n",
      "   250K .......... .......... .......... .......... .......... 41%  314M 0s\n",
      "   300K .......... .......... .......... .......... .......... 48%  291M 0s\n",
      "   350K .......... .......... .......... .......... .......... 55%  317M 0s\n",
      "   400K .......... .......... .......... .......... .......... 62%  306M 0s\n",
      "   450K .......... .......... .......... .......... .......... 69%  362M 0s\n",
      "   500K .......... .......... .......... .......... .......... 76%  376M 0s\n",
      "   550K .......... .......... .......... .......... .......... 83%  305M 0s\n",
      "   600K .......... .......... .......... .......... .......... 90%  358M 0s\n",
      "   650K .......... .......... .......... .......... .......... 97%  376M 0s\n",
      "   700K .......... .......... .                               100% 36.1M=0.003s\n",
      "\n",
      "2025-09-15 08:52:01 (235 MB/s) - ‘/mnt/cluster_storage/glaive_toolcall_en_demo.json’ saved [738925/738925]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Make sure all files are accessible to worker nodes\n",
    "# Create a copy of the data in /mnt/cluster_storage\n",
    "wget https://anyscale-public-materials.s3.us-west-2.amazonaws.com/llm-finetuning/llama-factory/datasets/sharegpt/glaive_toolcall_en_demo.json -O /mnt/cluster_storage/glaive_toolcall_en_demo.json\n",
    "# Create a copy of the dataset registry in /mnt/cluster_storage\n",
    "cp ../dataset-configs/dataset_info.json /mnt/cluster_storage/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Create the Fine-Tuning Config (SFT + DeepSpeed)\n",
    "\n",
    "Next, create the main YAML configuration file—the master recipe for our fine-tuning job. It specifies the base model, the fine-tuning method (LoRA), the dataset, training hyperparameters, cluster resources, and more.\n",
    "\n",
    "Here is the `qwen2.5_deepspeed_lora_sft.yaml` included in the workspace:\n",
    "\n",
    "**Important notes:**\n",
    "- Include the `WANDB_API_KEY` in the config to enable Weights & Biases (W&B) tracking; otherwise, you’ll encounter an error. If you don’t want to use W&B, disable it by setting `report_to: none` in the config.\n",
    "- This YAML file only needs to be accessible to the head node; there’s no need to place it on storage accessible to worker nodes (e.g., `/mnt/cluster_storage/`).\n",
    "- However, any paths referenced by the config (e.g., `dataset_dir`, DeepSpeed JSON, `output_dir`) must be on storage accessible to all workers (e.g., `/mnt/cluster_storage/`).\n",
    "- Qwen models are generally not gated, but if you need to fine-tune a gated model such as Llama, include your `HF_TOKEN` in the config.\n",
    "- We set `accelerator_type` to `L40S` in the config; you can switch to other GPUs—such as `A100-40GB`—or any other GPU with comparable or more VRAM, depending on your cloud availability.\n",
    "\n",
    "### LLaMA-Factory + Ray Configuration\n",
    "\n",
    "```yaml\n",
    "# qwen2.5_deepspeed_lora_sft.yaml\n",
    "\n",
    "### model\n",
    "model_name_or_path: Qwen/Qwen2.5-32B-Instruct\n",
    "trust_remote_code: true\n",
    "\n",
    "### method\n",
    "stage: sft\n",
    "do_train: true\n",
    "finetuning_type: lora\n",
    "lora_rank: 8\n",
    "lora_target: all\n",
    "\n",
    "### deepspeed\n",
    "deepspeed: /mnt/cluster_storage/ds_z3_config.json # path to the DeepSpeed config\n",
    "\n",
    "### dataset\n",
    "# local dataset\n",
    "dataset: my_glaive_toolcall_en_demo\n",
    "dataset_dir: /mnt/cluster_storage\n",
    "\n",
    "template: qwen\n",
    "cutoff_len: 1024\n",
    "max_samples: 1000\n",
    "overwrite_cache: true\n",
    "preprocessing_num_workers: 16\n",
    "\n",
    "### output\n",
    "output_dir: qwen2.5_32b_lora_sft\n",
    "logging_steps: 5\n",
    "save_steps: 50\n",
    "plot_loss: true\n",
    "report_to: wandb # or none\n",
    "\n",
    "### train\n",
    "per_device_train_batch_size: 1 # adjust this depending on your GPU memory and sequence length\n",
    "gradient_accumulation_steps: 4\n",
    "num_train_epochs: 3.0\n",
    "learning_rate: 1.0e-4\n",
    "bf16: true\n",
    "lr_scheduler_type: cosine\n",
    "warmup_ratio: 0.1\n",
    "ddp_timeout: 180000000\n",
    "\n",
    "### ray\n",
    "ray_run_name: qwen2.5_32b_lora_sft\n",
    "ray_storage_path: /mnt/cluster_storage/\n",
    "ray_num_workers: 4  # Number of GPUs to use.\n",
    "resources_per_worker:\n",
    "  GPU: 1\n",
    "  accelerator_type:L40S: 0.001            # Use this to simply specify a GPU type (not guaranteed on the same node). You can use A100-40G if L40S is not available. \n",
    "  # anyscale/accelerator_shape:4xL40S: 0.001  # Use this to specify a specific node shape.\n",
    "  # See https://docs.ray.io/en/master/ray-core/accelerator-types.html#accelerator-types for a full list of accelerator types.\n",
    "ray_init_kwargs:\n",
    "  runtime_env:\n",
    "    env_vars:\n",
    "      # if using wandb for experiments tracking\n",
    "      WANDB_API_KEY: <your_wandb_token>\n",
    "      # if using gated models like meta-llama/Llama-3.1-8B-Instruct\n",
    "      # HF_TOKEN: <your_huggingface_token>\n",
    "      # if hf_transfer is installed\n",
    "      HF_HUB_ENABLE_HF_TRANSFER: '1'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:**\n",
    "This configuration assumes `4xL40S` GPUs are available in your cloud environment. If not, you can substitute with `4xA100-40G` (or another supported accelerator with similar VRAM)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DeepSpeed Configuration\n",
    "DeepSpeed is an open-source deep-learning optimization library developed by Microsoft, aimed at enabling large-model training. Higher ZeRO stages (1→3) and enabling CPU offload reduce GPU VRAM usage, but might cause slower training. \n",
    "\n",
    "To enable DeepSpeed, create a separate JSON config and reference it from your main training yaml config with: `deepspeed: /mnt/cluster_storage/ds_z3_config.json`.\n",
    "\n",
    "**Note**: you need to put the Deepspeed config in the **[storage shared across nodes](https://docs.anyscale.com/configuration/storage#shared)**。\n",
    "\n",
    "Below is a sample ZeRO-3 config:\n",
    "\n",
    "<details>\n",
    "  <summary>ds_z3_config.json</summary>\n",
    "\n",
    "  ```json\n",
    "  {\n",
    "    \"train_batch_size\": \"auto\",\n",
    "    \"train_micro_batch_size_per_gpu\": \"auto\",\n",
    "    \"gradient_accumulation_steps\": \"auto\",\n",
    "    \"gradient_clipping\": \"auto\",\n",
    "    \"zero_allow_untested_optimizer\": true,\n",
    "    \"fp16\": {\n",
    "      \"enabled\": \"auto\",\n",
    "      \"loss_scale\": 0,\n",
    "      \"loss_scale_window\": 1000,\n",
    "      \"initial_scale_power\": 16,\n",
    "      \"hysteresis\": 2,\n",
    "      \"min_loss_scale\": 1\n",
    "    },\n",
    "    \"bf16\": {\n",
    "      \"enabled\": \"auto\"\n",
    "    },\n",
    "    \"zero_optimization\": {\n",
    "      \"stage\": 3,\n",
    "      \"overlap_comm\": false,\n",
    "      \"contiguous_gradients\": true,\n",
    "      \"sub_group_size\": 1e9,\n",
    "      \"reduce_bucket_size\": \"auto\",\n",
    "      \"stage3_prefetch_bucket_size\": \"auto\",\n",
    "      \"stage3_param_persistence_threshold\": \"auto\",\n",
    "      \"stage3_max_live_parameters\": 1e9,\n",
    "      \"stage3_max_reuse_distance\": 1e9,\n",
    "      \"stage3_gather_16bit_weights_on_model_save\": true\n",
    "    }\n",
    "  }\n",
    "  ```\n",
    "\n",
    "</details>\n",
    "\n",
    "\n",
    "For a more detailed guide on acceleration and optimization methods including DeepSpeed on Ray, check [todo: add doc link](3.9-speed-and-memory-optimizations.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Create a copy of the DeepSpeed configuration file in /mnt/cluster_storage\n",
    "cp ../deepspeed-configs/ds_z3_config.json /mnt/cluster_storage/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Train and Monitor\n",
    "\n",
    "With all configuration in place, you can launch fine-tuning/post-training in one of two ways."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option A — Run from a Workspace (quick start)\n",
    "\n",
    "The `USE_RAY=1` prefix tells LLaMA-Factory to run in distributed mode on the Ray cluster attached to your workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-09 15:20:26,961] [WARNING] [real_accelerator.py:209:get_accelerator] Setting accelerator to CPU. If you have GPU or other accelerator, we were unable to detect it.\n",
      "[2025-09-09 15:20:26,962] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cpu (auto detect)\n",
      "INFO 09-09 15:20:29 [__init__.py:248] No platform detected, vLLM is running on UnspecifiedPlatform\n",
      "WARNING 09-09 15:20:29 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('libcuda.so.1: cannot open shared object file: No such file or directory')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-09 15:20:32,996\tINFO worker.py:1747 -- Connecting to existing Ray cluster at address: 10.0.51.64:6379...\n",
      "2025-09-09 15:20:33,007\tINFO worker.py:1918 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32mhttps://session-llcdv67sqfh5atv2kbdl8prt5n.i.anyscaleuserdata.com \u001b[39m\u001b[22m\n",
      "2025-09-09 15:20:33,009\tINFO packaging.py:380 -- Pushing file package 'gcs://_ray_pkg_1e8e0d3419b5e62f9bbdd40542f176752fa75f01.zip' (0.41MiB) to Ray cluster...\n",
      "2025-09-09 15:20:33,010\tINFO packaging.py:393 -- Successfully pushed file package 'gcs://_ray_pkg_1e8e0d3419b5e62f9bbdd40542f176752fa75f01.zip'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "View detailed results here: /mnt/cluster_storage/qwen2.5_32b_lora_sft\n",
      "To visualize your results with TensorBoard, run: `tensorboard --logdir /tmp/ray/session_2025-09-09_10-09-51_355612_2399/artifacts/2025-09-09_15-20-33/qwen2.5_32b_lora_sft/driver_artifacts`\n",
      "\u001b[36m(autoscaler +9s)\u001b[0m Tip: use `ray status` to view detailed cluster status. To disable these messages, set RAY_SCHEDULER_EVENTS=0.\n",
      "\u001b[36m(autoscaler +11s)\u001b[0m [autoscaler] [1xL4:4CPU-16GB] Attempting to add 1 node to the cluster (increasing from 0 to 1).\n",
      "\u001b[36m(autoscaler +11s)\u001b[0m [autoscaler] [1xL4:4CPU-16GB|g6.xlarge] [us-west-2a] [on-demand] Launched 1 instance.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-09 15:21:33,573\tWARNING insufficient_resources_manager.py:163 -- Ignore this message if the cluster is autoscaling. Training has not started in the last 60 seconds. This could be due to the cluster not having enough resources available. You asked for 1.0 CPUs and 1.0 GPUs, but the cluster only has 0 CPUs and 0 GPUs available. Stop the training and adjust the required resources (e.g. via the `ScalingConfig` or `resources_per_trial`, or `num_workers` for rllib), or add more resources to your cluster.\n",
      "2025-09-09 15:22:33,658\tWARNING insufficient_resources_manager.py:163 -- Ignore this message if the cluster is autoscaling. Training has not started in the last 60 seconds. This could be due to the cluster not having enough resources available. You asked for 1.0 CPUs and 1.0 GPUs, but the cluster only has 0 CPUs and 0 GPUs available. Stop the training and adjust the required resources (e.g. via the `ScalingConfig` or `resources_per_trial`, or `num_workers` for rllib), or add more resources to your cluster.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(TrainTrainable pid=3073, ip=10.0.33.92)\u001b[0m [2025-09-09 15:23:37,642] [WARNING] [real_accelerator.py:209:get_accelerator] Setting accelerator to CPU. If you have GPU or other accelerator, we were unable to detect it.\n",
      "\u001b[36m(TrainTrainable pid=3073, ip=10.0.33.92)\u001b[0m [2025-09-09 15:23:37,642] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cpu (auto detect)\n",
      "\n",
      "Training started with configuration:\n",
      "╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
      "│ Training config                                                                                              │\n",
      "├──────────────────────────────────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ train_loop_config/args/bf16                                                                             True │\n",
      "│ train_loop_config/args/cutoff_len                                                                       1024 │\n",
      "│ train_loop_config/args/dataset                                                          ..._toolcall_en_demo │\n",
      "│ train_loop_config/args/dataset_dir                                                      /mnt/cluster_storage │\n",
      "│ train_loop_config/args/ddp_timeout                                                                 180000000 │\n",
      "│ train_loop_config/args/do_train                                                                         True │\n",
      "│ train_loop_config/args/finetuning_type                                                                  lora │\n",
      "│ train_loop_config/args/gradient_accumulation_steps                                                         4 │\n",
      "│ train_loop_config/args/learning_rate                                                                  0.0001 │\n",
      "│ train_loop_config/args/logging_steps                                                                       5 │\n",
      "│ train_loop_config/args/lora_rank                                                                           8 │\n",
      "│ train_loop_config/args/lora_target                                                                       all │\n",
      "│ train_loop_config/args/lr_scheduler_type                                                              cosine │\n",
      "│ train_loop_config/args/max_samples                                                                       100 │\n",
      "│ train_loop_config/args/model_name_or_path                                               ...2.5-0.5B-Instruct │\n",
      "│ train_loop_config/args/num_train_epochs                                                                  1.0 │\n",
      "│ train_loop_config/args/output_dir                                                       qwen2.5_32b_lora_sft │\n",
      "│ train_loop_config/args/overwrite_cache                                                                  True │\n",
      "│ train_loop_config/args/per_device_train_batch_size                                                         1 │\n",
      "│ train_loop_config/args/plot_loss                                                                        True │\n",
      "│ train_loop_config/args/preprocessing_num_workers                                                          16 │\n",
      "│ train_loop_config/args/ray_init_kwargs/runtime_env/env_vars/HF_HUB_ENABLE_HF_TRANSFER                      1 │\n",
      "│ train_loop_config/args/ray_num_workers                                                                     1 │\n",
      "│ train_loop_config/args/ray_run_name                                                     qwen2.5_32b_lora_sft │\n",
      "│ train_loop_config/args/ray_storage_path                                                 .../cluster_storage/ │\n",
      "│ train_loop_config/args/report_to                                                                 tensorboard │\n",
      "│ train_loop_config/args/resources_per_worker/GPU                                                            1 │\n",
      "│ train_loop_config/args/resources_per_worker/accelerator_type:L4                                        0.001 │\n",
      "│ train_loop_config/args/save_steps                                                                         50 │\n",
      "│ train_loop_config/args/stage                                                                             sft │\n",
      "│ train_loop_config/args/template                                                                         qwen │\n",
      "│ train_loop_config/args/trust_remote_code                                                                True │\n",
      "│ train_loop_config/args/warmup_ratio                                                                      0.1 │\n",
      "│ train_loop_config/callbacks                                                             ... 0x739bdc718b90>] │\n",
      "╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m Setting up process group for: env:// [rank=0, world_size=1]\n",
      "\u001b[36m(TorchTrainer pid=3073, ip=10.0.33.92)\u001b[0m Started distributed worker processes: \n",
      "\u001b[36m(TorchTrainer pid=3073, ip=10.0.33.92)\u001b[0m - (node_id=42cfab8d697667e67bd8e5415464841428c1d9eae0a9dcafa4f85fc9, ip=10.0.33.92, pid=3150) world_rank=0, local_rank=0, node_rank=0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m [2025-09-09 15:23:46,015] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m [INFO|2025-09-09 15:23:48] llamafactory.hparams.parser:143 >> Set `ddp_find_unused_parameters` to False in DDP training since LoRA is enabled.\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m [INFO|2025-09-09 15:23:48] llamafactory.hparams.parser:406 >> Process rank: 0, world size: 1, device: cuda:0, distributed training: True, compute dtype: torch.bfloat16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m [INFO|tokenization_utils_base.py:2023] 2025-09-09 15:23:49,827 >> loading file vocab.json from cache at /home/ray/.cache/huggingface/hub/models--Qwen--Qwen2.5-0.5B-Instruct/snapshots/7ae557604adf67be50417f59c2c2f167def9a775/vocab.json\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m [INFO|tokenization_utils_base.py:2023] 2025-09-09 15:23:49,827 >> loading file merges.txt from cache at /home/ray/.cache/huggingface/hub/models--Qwen--Qwen2.5-0.5B-Instruct/snapshots/7ae557604adf67be50417f59c2c2f167def9a775/merges.txt\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m [INFO|tokenization_utils_base.py:2023] 2025-09-09 15:23:49,827 >> loading file tokenizer.json from cache at /home/ray/.cache/huggingface/hub/models--Qwen--Qwen2.5-0.5B-Instruct/snapshots/7ae557604adf67be50417f59c2c2f167def9a775/tokenizer.json\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m [INFO|tokenization_utils_base.py:2023] 2025-09-09 15:23:49,827 >> loading file added_tokens.json from cache at None\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m [INFO|tokenization_utils_base.py:2023] 2025-09-09 15:23:49,827 >> loading file special_tokens_map.json from cache at None\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m [INFO|tokenization_utils_base.py:2023] 2025-09-09 15:23:49,827 >> loading file tokenizer_config.json from cache at /home/ray/.cache/huggingface/hub/models--Qwen--Qwen2.5-0.5B-Instruct/snapshots/7ae557604adf67be50417f59c2c2f167def9a775/tokenizer_config.json\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m [INFO|tokenization_utils_base.py:2023] 2025-09-09 15:23:49,827 >> loading file chat_template.jinja from cache at None\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m [INFO|tokenization_utils_base.py:2299] 2025-09-09 15:23:50,071 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m [INFO|configuration_utils.py:698] 2025-09-09 15:23:50,721 >> loading configuration file config.json from cache at /home/ray/.cache/huggingface/hub/models--Qwen--Qwen2.5-0.5B-Instruct/snapshots/7ae557604adf67be50417f59c2c2f167def9a775/config.json\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m [INFO|configuration_utils.py:770] 2025-09-09 15:23:50,725 >> Model config Qwen2Config {\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m   \"architectures\": [\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m     \"Qwen2ForCausalLM\"\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m   ],\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m   \"attention_dropout\": 0.0,\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m   \"bos_token_id\": 151643,\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m   \"eos_token_id\": 151645,\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m   \"hidden_act\": \"silu\",\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m   \"hidden_size\": 896,\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m   \"initializer_range\": 0.02,\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m   \"intermediate_size\": 4864,\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m   \"max_position_embeddings\": 32768,\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m   \"max_window_layers\": 21,\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m   \"model_type\": \"qwen2\",\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m   \"num_attention_heads\": 14,\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m   \"num_hidden_layers\": 24,\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m   \"num_key_value_heads\": 2,\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m   \"rms_norm_eps\": 1e-06,\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m   \"rope_scaling\": null,\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m   \"rope_theta\": 1000000.0,\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m   \"sliding_window\": 32768,\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m   \"tie_word_embeddings\": true,\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m   \"torch_dtype\": \"bfloat16\",\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m   \"transformers_version\": \"4.52.4\",\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m   \"use_cache\": true,\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m   \"use_sliding_window\": false,\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m   \"vocab_size\": 151936\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m }\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m [INFO|tokenization_utils_base.py:2023] 2025-09-09 15:23:50,930 >> loading file vocab.json from cache at /home/ray/.cache/huggingface/hub/models--Qwen--Qwen2.5-0.5B-Instruct/snapshots/7ae557604adf67be50417f59c2c2f167def9a775/vocab.json\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m [INFO|tokenization_utils_base.py:2023] 2025-09-09 15:23:50,930 >> loading file merges.txt from cache at /home/ray/.cache/huggingface/hub/models--Qwen--Qwen2.5-0.5B-Instruct/snapshots/7ae557604adf67be50417f59c2c2f167def9a775/merges.txt\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m [INFO|tokenization_utils_base.py:2023] 2025-09-09 15:23:50,930 >> loading file tokenizer.json from cache at /home/ray/.cache/huggingface/hub/models--Qwen--Qwen2.5-0.5B-Instruct/snapshots/7ae557604adf67be50417f59c2c2f167def9a775/tokenizer.json\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m [INFO|tokenization_utils_base.py:2023] 2025-09-09 15:23:50,930 >> loading file added_tokens.json from cache at None\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m [INFO|tokenization_utils_base.py:2023] 2025-09-09 15:23:50,930 >> loading file special_tokens_map.json from cache at None\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m [INFO|tokenization_utils_base.py:2023] 2025-09-09 15:23:50,930 >> loading file tokenizer_config.json from cache at /home/ray/.cache/huggingface/hub/models--Qwen--Qwen2.5-0.5B-Instruct/snapshots/7ae557604adf67be50417f59c2c2f167def9a775/tokenizer_config.json\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m [INFO|tokenization_utils_base.py:2023] 2025-09-09 15:23:50,930 >> loading file chat_template.jinja from cache at None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m [INFO|2025-09-09 15:23:51] llamafactory.data.loader:143 >> Loading dataset glaive_toolcall_en_demo.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m [INFO|tokenization_utils_base.py:2299] 2025-09-09 15:23:51,159 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m Setting num_proc from 16 back to 1 for the train split to disable multiprocessing as it only contains one shard.\n",
      "Generating train split: 300 examples [00:00, 17097.28 examples/s]\n",
      "Converting format of dataset (num_proc=16):   0%|          | 0/100 [00:00<?, ? examples/s]\n",
      "Converting format of dataset (num_proc=16):  21%|██        | 21/100 [00:00<00:00, 189.68 examples/s]\n",
      "Converting format of dataset (num_proc=16): 100%|██████████| 100/100 [00:00<00:00, 405.61 examples/s]\n",
      "Running tokenizer on dataset (num_proc=16):   0%|          | 0/100 [00:00<?, ? examples/s]\n",
      "Running tokenizer on dataset (num_proc=16):   7%|▋         | 7/100 [00:00<00:11,  8.18 examples/s]\n",
      "Running tokenizer on dataset (num_proc=16):  21%|██        | 21/100 [00:01<00:04, 18.58 examples/s]\n",
      "Running tokenizer on dataset (num_proc=16):  28%|██▊       | 28/100 [00:01<00:03, 21.72 examples/s]\n",
      "Running tokenizer on dataset (num_proc=16):  34%|███▍      | 34/100 [00:02<00:03, 17.06 examples/s]\n",
      "Running tokenizer on dataset (num_proc=16):  46%|████▌     | 46/100 [00:02<00:02, 26.33 examples/s]\n",
      "Running tokenizer on dataset (num_proc=16):  52%|█████▏    | 52/100 [00:02<00:01, 27.08 examples/s]\n",
      "Running tokenizer on dataset (num_proc=16):  64%|██████▍   | 64/100 [00:02<00:01, 25.90 examples/s]\n",
      "Running tokenizer on dataset (num_proc=16):  70%|███████   | 70/100 [00:03<00:01, 27.86 examples/s]\n",
      "Running tokenizer on dataset (num_proc=16):  76%|███████▌  | 76/100 [00:03<00:00, 25.76 examples/s]\n",
      "Running tokenizer on dataset (num_proc=16):  82%|████████▏ | 82/100 [00:03<00:00, 29.36 examples/s]\n",
      "Running tokenizer on dataset (num_proc=16):  94%|█████████▍| 94/100 [00:03<00:00, 36.49 examples/s]\n",
      "Running tokenizer on dataset (num_proc=16): 100%|██████████| 100/100 [00:03<00:00, 39.11 examples/s]\n",
      "Running tokenizer on dataset (num_proc=16): 100%|██████████| 100/100 [00:03<00:00, 25.86 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m training example:\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m input_ids:\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m [151644, 8948, 198, 2610, 525, 1207, 16948, 11, 3465, 553, 54364, 14817, 13, 1446, 525, 264, 10950, 17847, 382, 2, 13852, 271, 2610, 1231, 1618, 825, 476, 803, 5746, 311, 7789, 448, 279, 1196, 3239, 382, 2610, 525, 3897, 448, 729, 32628, 2878, 366, 15918, 1472, 15918, 29, 11874, 9492, 510, 27, 15918, 397, 4913, 1313, 788, 330, 1688, 497, 330, 1688, 788, 5212, 606, 788, 330, 1836, 7080, 8923, 497, 330, 4684, 788, 330, 5890, 369, 18627, 3118, 389, 13966, 497, 330, 13786, 788, 5212, 1313, 788, 330, 1700, 497, 330, 13193, 788, 5212, 38120, 788, 5212, 1313, 788, 330, 1653, 497, 330, 3615, 788, 5212, 1313, 788, 330, 917, 14345, 330, 4684, 788, 330, 785, 13966, 311, 2711, 369, 9207, 2137, 330, 6279, 788, 4383, 38120, 1341, 3417, 532, 522, 15918, 1339, 2461, 1817, 729, 1618, 11, 470, 264, 2951, 1633, 448, 729, 829, 323, 5977, 2878, 220, 151657, 151658, 11874, 9492, 510, 151657, 198, 4913, 606, 788, 366, 1688, 11494, 8066, 330, 16370, 788, 366, 2116, 56080, 40432, 31296, 151658, 151645, 198, 151644, 872, 198, 13048, 11, 358, 614, 1045, 13966, 323, 358, 1366, 311, 4296, 2494, 13, 2980, 498, 1492, 752, 1477, 264, 11116, 30, 151645, 198, 151644, 77091, 198, 2124, 3308, 0, 358, 646, 1492, 498, 448, 429, 13, 5209, 3291, 752, 1128, 13966, 498, 614, 13, 151645, 198, 151644, 872, 198, 40, 614, 16158, 11, 28419, 57473, 11, 323, 19653, 13, 151645, 198, 151644, 77091, 198, 151657, 198, 4913, 606, 788, 330, 1836, 7080, 8923, 497, 330, 16370, 788, 5212, 38120, 788, 4383, 331, 9692, 497, 330, 17250, 57473, 497, 330, 23120, 1341, 11248, 151658, 151645, 198, 151644, 872, 198, 27, 14172, 9655, 397, 4913, 49859, 788, 61753, 606, 788, 330, 83863, 323, 17884, 51782, 64192, 52417, 497, 330, 38120, 788, 4383, 331, 9692, 497, 330, 17250, 57473, 497, 330, 23120, 7914, 330, 62295, 788, 330, 37666, 279, 16158, 1119, 2613, 9666, 13, 56476, 279, 28419, 57473, 13, 12514, 279, 19653, 13, 64192, 52546, 279, 16158, 323, 28419, 57473, 13, 52932, 916, 19653, 1189, 2137, 5212, 606, 788, 330, 83863, 323, 29516, 25442, 261, 1263, 497, 330, 38120, 788, 4383, 331, 9692, 497, 330, 17250, 57473, 497, 330, 23120, 7914, 330, 62295, 788, 330, 58382, 279, 16158, 323, 19653, 25156, 13, 19219, 1105, 3786, 448, 279, 28419, 57473, 304, 264, 272, 33758, 1263, 12000, 13, 52074, 3080, 20748, 13876, 1189, 92, 23439, 522, 14172, 9655, 29, 151645, 198, 151644, 77091, 198, 40, 1730, 1378, 18627, 369, 498, 13, 576, 1156, 825, 374, 330, 83863, 323, 17884, 51782, 64192, 52417, 3263, 576, 11221, 525, 25, 19649, 279, 16158, 1119, 2613, 9666, 13, 56476, 279, 28419, 57473, 13, 12514, 279, 19653, 13, 64192, 52546, 279, 16158, 323, 28419, 57473, 13, 52932, 916, 19653, 13, 576, 2086, 825, 374, 330, 83863, 323, 29516, 25442, 261, 1263, 3263, 576, 11221, 525, 25, 12514, 279, 16158, 323, 19653, 25156, 13, 19219, 1105, 3786, 448, 279, 28419, 57473, 304, 264, 272, 33758, 1263, 12000, 13, 52074, 3080, 20748, 13876, 13, 15920, 825, 1035, 498, 1075, 311, 1430, 30, 151645, 198, 151644, 872, 198, 6865, 2176, 5112, 17923, 11, 714, 358, 1744, 358, 3278, 1430, 279, 23065, 52546, 13, 2980, 498, 1973, 279, 13966, 369, 752, 30, 151645, 198, 151644, 77091, 198, 40, 2776, 14589, 11, 714, 438, 458, 15235, 11, 358, 1513, 944, 614, 279, 22302, 311, 2736, 9250, 9079, 1741, 438, 21391, 13966, 13, 4354, 11, 358, 646, 1492, 498, 1477, 803, 18627, 476, 3410, 17233, 10414, 421, 498, 1184, 13, 151645, 198]\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m inputs:\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m <|im_start|>system\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m # Tools\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m You may call one or more functions to assist with the user query.\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m You are provided with function signatures within <tools></tools> XML tags:\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m <tools>\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m {\"type\": \"function\", \"function\": {\"name\": \"search_recipes\", \"description\": \"Search for recipes based on ingredients\", \"parameters\": {\"type\": \"object\", \"properties\": {\"ingredients\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}, \"description\": \"The ingredients to search for\"}}, \"required\": [\"ingredients\"]}}}\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m </tools>\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m For each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m <tool_call>\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m {\"name\": <function-name>, \"arguments\": <args-json-object>}\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m </tool_call><|im_end|>\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m <|im_start|>user\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m Hi, I have some ingredients and I want to cook something. Can you help me find a recipe?<|im_end|>\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m <|im_start|>assistant\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m Of course! I can help you with that. Please tell me what ingredients you have.<|im_end|>\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m <|im_start|>user\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m I have chicken, bell peppers, and rice.<|im_end|>\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m <|im_start|>assistant\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m <tool_call>\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m {\"name\": \"search_recipes\", \"arguments\": {\"ingredients\": [\"chicken\", \"bell peppers\", \"rice\"]}}\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m </tool_call><|im_end|>\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m <|im_start|>user\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m <tool_response>\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m {\"recipes\": [{\"name\": \"Chicken and Bell Pepper Stir Fry\", \"ingredients\": [\"chicken\", \"bell peppers\", \"rice\"], \"instructions\": \"Cut the chicken into small pieces. Slice the bell peppers. Cook the rice. Stir fry the chicken and bell peppers. Serve over rice.\"}, {\"name\": \"Chicken and Rice Casserole\", \"ingredients\": [\"chicken\", \"bell peppers\", \"rice\"], \"instructions\": \"Cook the chicken and rice separately. Mix them together with the bell peppers in a casserole dish. Bake until golden brown.\"}]}\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m </tool_response><|im_end|>\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m <|im_start|>assistant\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m I found two recipes for you. The first one is \"Chicken and Bell Pepper Stir Fry\". The instructions are: Cut the chicken into small pieces. Slice the bell peppers. Cook the rice. Stir fry the chicken and bell peppers. Serve over rice. The second one is \"Chicken and Rice Casserole\". The instructions are: Cook the chicken and rice separately. Mix them together with the bell peppers in a casserole dish. Bake until golden brown. Which one would you like to try?<|im_end|>\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m <|im_start|>user\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m They both sound delicious, but I think I'll try the stir fry. Can you order the ingredients for me?<|im_end|>\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m <|im_start|>assistant\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m I'm sorry, but as an AI, I don't have the capability to perform external tasks such as ordering ingredients. However, I can help you find more recipes or provide cooking tips if you need.<|im_end|>\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m label_ids:\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 2124, 3308, 0, 358, 646, 1492, 498, 448, 429, 13, 5209, 3291, 752, 1128, 13966, 498, 614, 13, 151645, 198, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 151657, 198, 4913, 606, 788, 330, 1836, 7080, 8923, 497, 330, 16370, 788, 5212, 38120, 788, 4383, 331, 9692, 497, 330, 17250, 57473, 497, 330, 23120, 1341, 11248, 151658, 151645, 198, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 40, 1730, 1378, 18627, 369, 498, 13, 576, 1156, 825, 374, 330, 83863, 323, 17884, 51782, 64192, 52417, 3263, 576, 11221, 525, 25, 19649, 279, 16158, 1119, 2613, 9666, 13, 56476, 279, 28419, 57473, 13, 12514, 279, 19653, 13, 64192, 52546, 279, 16158, 323, 28419, 57473, 13, 52932, 916, 19653, 13, 576, 2086, 825, 374, 330, 83863, 323, 29516, 25442, 261, 1263, 3263, 576, 11221, 525, 25, 12514, 279, 16158, 323, 19653, 25156, 13, 19219, 1105, 3786, 448, 279, 28419, 57473, 304, 264, 272, 33758, 1263, 12000, 13, 52074, 3080, 20748, 13876, 13, 15920, 825, 1035, 498, 1075, 311, 1430, 30, 151645, 198, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 40, 2776, 14589, 11, 714, 438, 458, 15235, 11, 358, 1513, 944, 614, 279, 22302, 311, 2736, 9250, 9079, 1741, 438, 21391, 13966, 13, 4354, 11, 358, 646, 1492, 498, 1477, 803, 18627, 476, 3410, 17233, 10414, 421, 498, 1184, 13, 151645, 198]\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m labels:\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m Of course! I can help you with that. Please tell me what ingredients you have.<|im_end|>\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m <tool_call>\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m {\"name\": \"search_recipes\", \"arguments\": {\"ingredients\": [\"chicken\", \"bell peppers\", \"rice\"]}}\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m </tool_call><|im_end|>\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m I found two recipes for you. The first one is \"Chicken and Bell Pepper Stir Fry\". The instructions are: Cut the chicken into small pieces. Slice the bell peppers. Cook the rice. Stir fry the chicken and bell peppers. Serve over rice. The second one is \"Chicken and Rice Casserole\". The instructions are: Cook the chicken and rice separately. Mix them together with the bell peppers in a casserole dish. Bake until golden brown. Which one would you like to try?<|im_end|>\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m I'm sorry, but as an AI, I don't have the capability to perform external tasks such as ordering ingredients. However, I can help you find more recipes or provide cooking tips if you need.<|im_end|>\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m [INFO|2025-09-09 15:23:56] llamafactory.model.model_utils.kv_cache:143 >> KV cache is disabled during training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m [INFO|configuration_utils.py:698] 2025-09-09 15:23:56,533 >> loading configuration file config.json from cache at /home/ray/.cache/huggingface/hub/models--Qwen--Qwen2.5-0.5B-Instruct/snapshots/7ae557604adf67be50417f59c2c2f167def9a775/config.json\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m [INFO|configuration_utils.py:770] 2025-09-09 15:23:56,534 >> Model config Qwen2Config {\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m   \"architectures\": [\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m     \"Qwen2ForCausalLM\"\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m   ],\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m   \"attention_dropout\": 0.0,\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m   \"bos_token_id\": 151643,\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m   \"eos_token_id\": 151645,\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m   \"hidden_act\": \"silu\",\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m   \"hidden_size\": 896,\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m   \"initializer_range\": 0.02,\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m   \"intermediate_size\": 4864,\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m   \"max_position_embeddings\": 32768,\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m   \"max_window_layers\": 21,\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m   \"model_type\": \"qwen2\",\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m   \"num_attention_heads\": 14,\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m   \"num_hidden_layers\": 24,\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m   \"num_key_value_heads\": 2,\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m   \"rms_norm_eps\": 1e-06,\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m   \"rope_scaling\": null,\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m   \"rope_theta\": 1000000.0,\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m   \"sliding_window\": 32768,\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m   \"tie_word_embeddings\": true,\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m   \"torch_dtype\": \"bfloat16\",\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m   \"transformers_version\": \"4.52.4\",\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m   \"use_cache\": true,\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m   \"use_sliding_window\": false,\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m   \"vocab_size\": 151936\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m }\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m [INFO|modeling_utils.py:1151] 2025-09-09 15:24:00,389 >> loading weights file model.safetensors from cache at /home/ray/.cache/huggingface/hub/models--Qwen--Qwen2.5-0.5B-Instruct/snapshots/7ae557604adf67be50417f59c2c2f167def9a775/model.safetensors\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m [INFO|modeling_utils.py:2241] 2025-09-09 15:24:00,390 >> Instantiating Qwen2ForCausalLM model under default dtype torch.bfloat16.\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m [INFO|configuration_utils.py:1135] 2025-09-09 15:24:00,391 >> Generate config GenerationConfig {\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m   \"bos_token_id\": 151643,\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m   \"eos_token_id\": 151645,\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m   \"use_cache\": false\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m }\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m [INFO|modeling_utils.py:5131] 2025-09-09 15:24:00,890 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m [INFO|modeling_utils.py:5139] 2025-09-09 15:24:00,890 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at Qwen/Qwen2.5-0.5B-Instruct.\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m [INFO|configuration_utils.py:1090] 2025-09-09 15:24:01,217 >> loading configuration file generation_config.json from cache at /home/ray/.cache/huggingface/hub/models--Qwen--Qwen2.5-0.5B-Instruct/snapshots/7ae557604adf67be50417f59c2c2f167def9a775/generation_config.json\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m [INFO|configuration_utils.py:1135] 2025-09-09 15:24:01,217 >> Generate config GenerationConfig {\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m   \"bos_token_id\": 151643,\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m   \"do_sample\": true,\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m   \"eos_token_id\": [\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m     151645,\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m     151643\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m   ],\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m   \"pad_token_id\": 151643,\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m   \"repetition_penalty\": 1.1,\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m   \"temperature\": 0.7,\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m   \"top_k\": 20,\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m   \"top_p\": 0.8\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m }\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m [INFO|2025-09-09 15:24:01] llamafactory.model.model_utils.checkpointing:143 >> Gradient checkpointing enabled.\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m [INFO|2025-09-09 15:24:01] llamafactory.model.model_utils.attention:143 >> Using torch SDPA for faster training and inference.\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m [INFO|2025-09-09 15:24:01] llamafactory.model.adapter:143 >> Upcasting trainable params to float32.\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m [INFO|2025-09-09 15:24:01] llamafactory.model.adapter:143 >> Fine-tuning method: LoRA\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m [INFO|2025-09-09 15:24:01] llamafactory.model.model_utils.misc:143 >> Found linear modules: up_proj,v_proj,gate_proj,o_proj,q_proj,down_proj,k_proj\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m [INFO|2025-09-09 15:24:01] llamafactory.model.loader:143 >> trainable params: 4,399,104 || all params: 498,431,872 || trainable%: 0.8826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m [INFO|trainer.py:756] 2025-09-09 15:24:01,550 >> Using auto half precision backend\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m [INFO|trainer.py:2409] 2025-09-09 15:24:02,150 >> ***** Running training *****\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m [INFO|trainer.py:2410] 2025-09-09 15:24:02,150 >>   Num examples = 100\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m [INFO|trainer.py:2411] 2025-09-09 15:24:02,150 >>   Num Epochs = 1\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m [INFO|trainer.py:2412] 2025-09-09 15:24:02,150 >>   Instantaneous batch size per device = 1\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m [INFO|trainer.py:2415] 2025-09-09 15:24:02,150 >>   Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m [INFO|trainer.py:2416] 2025-09-09 15:24:02,150 >>   Gradient Accumulation steps = 4\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m [INFO|trainer.py:2417] 2025-09-09 15:24:02,150 >>   Total optimization steps = 25\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m [INFO|trainer.py:2418] 2025-09-09 15:24:02,153 >>   Number of trainable parameters = 4,399,104\n",
      "  0%|          | 0/25 [00:00<?, ?it/s].33.92)\u001b[0m \n",
      "  4%|▍         | 1/25 [00:01<00:45,  1.90s/it][0m \n",
      "  8%|▊         | 2/25 [00:02<00:29,  1.27s/it][0m \n",
      " 12%|█▏        | 3/25 [00:03<00:23,  1.06s/it][0m \n",
      " 16%|█▌        | 4/25 [00:04<00:20,  1.02it/s][0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m {'loss': 0.7165, 'grad_norm': 0.753259003162384, 'learning_rate': 9.949107209404665e-05, 'epoch': 0.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 5/25 [00:05<00:18,  1.06it/s][0m \n",
      " 24%|██▍       | 6/25 [00:06<00:17,  1.11it/s][0m \n",
      " 28%|██▊       | 7/25 [00:06<00:15,  1.13it/s][0m \n",
      " 32%|███▏      | 8/25 [00:07<00:14,  1.16it/s][0m \n",
      " 36%|███▌      | 9/25 [00:08<00:13,  1.16it/s][0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m {'loss': 0.8346, 'grad_norm': 0.7239006161689758, 'learning_rate': 8.274303669726426e-05, 'epoch': 0.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 10/25 [00:09<00:12,  1.17it/s]0m \n",
      " 44%|████▍     | 11/25 [00:10<00:11,  1.20it/s]0m \n",
      " 48%|████▊     | 12/25 [00:11<00:10,  1.19it/s]0m \n",
      " 52%|█████▏    | 13/25 [00:11<00:10,  1.19it/s]0m \n",
      " 56%|█████▌    | 14/25 [00:12<00:09,  1.19it/s]0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m {'loss': 0.6278, 'grad_norm': 0.8122984766960144, 'learning_rate': 5e-05, 'epoch': 0.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 15/25 [00:13<00:08,  1.20it/s]0m \n",
      " 64%|██████▍   | 16/25 [00:14<00:07,  1.21it/s]0m \n",
      " 68%|██████▊   | 17/25 [00:15<00:06,  1.22it/s]0m \n",
      " 72%|███████▏  | 18/25 [00:16<00:05,  1.21it/s]0m \n",
      " 76%|███████▌  | 19/25 [00:16<00:04,  1.21it/s]0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m {'loss': 0.3742, 'grad_norm': 0.7035364508628845, 'learning_rate': 1.725696330273575e-05, 'epoch': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 20/25 [00:17<00:04,  1.21it/s]0m \n",
      " 84%|████████▍ | 21/25 [00:18<00:03,  1.21it/s]0m \n",
      " 88%|████████▊ | 22/25 [00:19<00:02,  1.20it/s]0m \n",
      " 92%|█████████▏| 23/25 [00:20<00:01,  1.20it/s]0m \n",
      " 96%|█████████▌| 24/25 [00:21<00:00,  1.22it/s]0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m {'loss': 0.5312, 'grad_norm': 0.5648745894432068, 'learning_rate': 5.089279059533658e-07, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:21<00:00,  1.21it/s][INFO|trainer.py:3993] 2025-09-09 15:24:24,016 >> Saving model checkpoint to qwen2.5_32b_lora_sft/checkpoint-25\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m [INFO|configuration_utils.py:698] 2025-09-09 15:24:24,262 >> loading configuration file config.json from cache at /home/ray/.cache/huggingface/hub/models--Qwen--Qwen2.5-0.5B-Instruct/snapshots/7ae557604adf67be50417f59c2c2f167def9a775/config.json\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m [INFO|configuration_utils.py:770] 2025-09-09 15:24:24,262 >> Model config Qwen2Config {\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m   \"architectures\": [\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m     \"Qwen2ForCausalLM\"\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m   ],\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m   \"attention_dropout\": 0.0,\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m   \"bos_token_id\": 151643,\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m   \"eos_token_id\": 151645,\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m   \"hidden_act\": \"silu\",\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m   \"hidden_size\": 896,\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m   \"initializer_range\": 0.02,\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m   \"intermediate_size\": 4864,\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m   \"max_position_embeddings\": 32768,\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m   \"max_window_layers\": 21,\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m   \"model_type\": \"qwen2\",\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m   \"num_attention_heads\": 14,\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m   \"num_hidden_layers\": 24,\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m   \"num_key_value_heads\": 2,\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m   \"rms_norm_eps\": 1e-06,\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m   \"rope_scaling\": null,\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m   \"rope_theta\": 1000000.0,\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m   \"sliding_window\": 32768,\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m   \"tie_word_embeddings\": true,\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m   \"torch_dtype\": \"bfloat16\",\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m   \"transformers_version\": \"4.52.4\",\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m   \"use_cache\": true,\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m   \"use_sliding_window\": false,\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m   \"vocab_size\": 151936\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m }\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m [INFO|tokenization_utils_base.py:2356] 2025-09-09 15:24:24,312 >> chat template saved in qwen2.5_32b_lora_sft/checkpoint-25/chat_template.jinja\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m [INFO|tokenization_utils_base.py:2525] 2025-09-09 15:24:24,312 >> tokenizer config file saved in qwen2.5_32b_lora_sft/checkpoint-25/tokenizer_config.json\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m [INFO|tokenization_utils_base.py:2534] 2025-09-09 15:24:24,312 >> Special tokens file saved in qwen2.5_32b_lora_sft/checkpoint-25/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training finished iteration 1 at 2025-09-09 15:24:24. Total running time: 3min 51s\n",
      "╭─────────────────────────────────────────╮\n",
      "│ Training result                         │\n",
      "├─────────────────────────────────────────┤\n",
      "│ checkpoint_dir_name   checkpoint_000000 │\n",
      "│ time_this_iter_s               44.46439 │\n",
      "│ time_total_s                   44.46439 │\n",
      "│ training_iteration                    1 │\n",
      "│ epoch                                1. │\n",
      "│ grad_norm                       0.56487 │\n",
      "│ learning_rate                        0. │\n",
      "│ loss                             0.5312 │\n",
      "│ step                                 25 │\n",
      "╰─────────────────────────────────────────╯\n",
      "Training saved a checkpoint for iteration 1 at: (local)/mnt/cluster_storage/qwen2.5_32b_lora_sft/TorchTrainer_338d5_00000_0_2025-09-09_15-20-33/checkpoint_000000\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m {'train_runtime': 22.7555, 'train_samples_per_second': 4.395, 'train_steps_per_second': 1.099, 'train_loss': 0.616857385635376, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/mnt/cluster_storage/qwen2.5_32b_lora_sft/TorchTrainer_338d5_00000_0_2025-09-09_15-20-33/checkpoint_000000)\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m [INFO|trainer.py:2676] 2025-09-09 15:24:24,908 >> \n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m \n",
      "100%|██████████| 25/25 [00:22<00:00,  1.10it/s]0m \n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m [INFO|trainer.py:3993] 2025-09-09 15:24:24,910 >> Saving model checkpoint to qwen2.5_32b_lora_sft\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m [INFO|configuration_utils.py:698] 2025-09-09 15:24:25,137 >> loading configuration file config.json from cache at /home/ray/.cache/huggingface/hub/models--Qwen--Qwen2.5-0.5B-Instruct/snapshots/7ae557604adf67be50417f59c2c2f167def9a775/config.json\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m [INFO|configuration_utils.py:770] 2025-09-09 15:24:25,138 >> Model config Qwen2Config {\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m   \"architectures\": [\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m     \"Qwen2ForCausalLM\"\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m   ],\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m   \"attention_dropout\": 0.0,\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m   \"bos_token_id\": 151643,\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m   \"eos_token_id\": 151645,\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m   \"hidden_act\": \"silu\",\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m   \"hidden_size\": 896,\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m   \"initializer_range\": 0.02,\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m   \"intermediate_size\": 4864,\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m   \"max_position_embeddings\": 32768,\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m   \"max_window_layers\": 21,\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m   \"model_type\": \"qwen2\",\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m   \"num_attention_heads\": 14,\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m   \"num_hidden_layers\": 24,\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m   \"num_key_value_heads\": 2,\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m   \"rms_norm_eps\": 1e-06,\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m   \"rope_scaling\": null,\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m   \"rope_theta\": 1000000.0,\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m   \"sliding_window\": 32768,\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m   \"tie_word_embeddings\": true,\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m   \"torch_dtype\": \"bfloat16\",\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m   \"transformers_version\": \"4.52.4\",\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m   \"use_cache\": true,\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m   \"use_sliding_window\": false,\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m   \"vocab_size\": 151936\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m }\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m [INFO|tokenization_utils_base.py:2356] 2025-09-09 15:24:25,178 >> chat template saved in qwen2.5_32b_lora_sft/chat_template.jinja\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m [INFO|tokenization_utils_base.py:2525] 2025-09-09 15:24:25,178 >> tokenizer config file saved in qwen2.5_32b_lora_sft/tokenizer_config.json\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m [INFO|tokenization_utils_base.py:2534] 2025-09-09 15:24:25,178 >> Special tokens file saved in qwen2.5_32b_lora_sft/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m ***** train metrics *****\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m   epoch                    =        1.0\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m   total_flos               =   113387GF\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m   train_loss               =     0.6169\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m   train_runtime            = 0:00:22.75\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m   train_samples_per_second =      4.395\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m   train_steps_per_second   =      1.099\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m Figure saved at: qwen2.5_32b_lora_sft/training_loss.png\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m [WARNING|2025-09-09 15:24:25] llamafactory.extras.ploting:148 >> No metric eval_loss to plot.\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m [WARNING|2025-09-09 15:24:25] llamafactory.extras.ploting:148 >> No metric eval_accuracy to plot.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m [INFO|modelcard.py:450] 2025-09-09 15:24:25,399 >> Dropping the following result as it does not have all the necessary fields:\n",
      "\u001b[36m(RayTrainWorker pid=3150, ip=10.0.33.92)\u001b[0m {'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training completed after 1 iterations at 2025-09-09 15:24:26. Total running time: 3min 53s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-09 15:24:26,598\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/mnt/cluster_storage/qwen2.5_32b_lora_sft' in 0.0326s.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "USE_RAY=1 llamafactory-cli train ../train-configs/qwen2.5_deepspeed_lora_sft.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option B — Run as an Anyscale Job (production)\n",
    "\n",
    "For longer or production runs, submit the training as an **Anyscale Job**. Jobs run outside your interactive session for better stability, retries, and durable logs. You’ll package LLaMA-Factory and other libraries in a container image and launch with a short job config. See **[WIP Launching Fine-Tuning with Anyscale Jobs](3.10-launch-fine-tuning-with-anyscale-jobs.md)** for the step-by-step guide."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitoring with Weights & Biases (WandB)\n",
    "If you enabled Weights & Biases ( with `report_to: wandb` in the training config YAML file), you can monitor your training job in real-time. Look for the training loss to decrease steadily, which indicates the model is learning. For a more detailed guide on tracking experiments with other tools such as TensorBoard and MLFlow, see [todo: add doc link](3.5-observability-and-tracking.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WandB**\n",
    "![WandB](https://anyscale-public-materials.s3.us-west-2.amazonaws.com/llm-finetuning/llama-factory/3.2.1/3.2.1-wandb.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Locate Checkpoints\n",
    "\n",
    "Checkpoints are written under `ray_storage_path/ray_run_name`. In this example run, the path is: `/mnt/cluster_storage/qwen2.5_32b_lora_sft`. \n",
    "\n",
    "Inside, you’ll see a **trainer session** directory named like:\n",
    "`TorchTrainer_8c6a5_00000_0_2025-09-09_09-53-45/`.\n",
    "\n",
    "- `TorchTrainer_*` is created **when the trainer starts**; the suffix encodes a short run id and the **start timestamp**.\n",
    "- Within that directory, checkpoints are named `checkpoint_000xxx/`, where the number is the saved ordered checkpoints. \n",
    "\n",
    "The save cadence is controlled by `save_strategy` and `save_steps`. For instructions on how to resume interrupted training via `resume_from_checkpoint` and more, see [todo: add link for checkpointing](3.4-checkpointing.md#understanding-your-training-output-directory)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Export the Model\n",
    "\n",
    "If you use LoRA, you can keep the base model and adapter separate ([for multi-LoRA adapter use](https://docs.anyscale.com/llm/serving/multi-lora)) or merge the adapter into the base model for low-latency inference. \n",
    "\n",
    "For full fine-tuning or freeze-tuning, export the fine-tuned model directly.\n",
    "\n",
    "You may optionally apply post-training quantization on merged or full models before serving. See [todo: add doc link]() for the exact export commands and options."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
