{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Build a tool-using agent with LangChain / LangGraph, and Ray Serve on Anyscale\n",
        "\n",
        "This tutorial guides you through building and deploying a sophisticated, tool-using agent using LangChain, LangGraph, and Ray Serve on Anyscale.\n",
        "\n",
        "You'll create a scalable microservices architecture where each component—the agent, the LLM, and the tools—runs as an independent, autoscaling service.\n",
        "\n",
        "* The Agent (built with LangGraph) orchestrates tasks and manages conversation state.\n",
        "\n",
        "* The LLM (Qwen 4B) runs in its own service for dedicated, high-speed inference.\n",
        "\n",
        "* The Tools (a weather API) are exposed via the Model Context Protocol (MCP), an open standard that allows the agent to discover and use them dynamically.\n",
        "\n",
        "This decoupled design provides automatic scaling, fault isolation, and the flexibility to update or swap components (like LLMs or tools) without changing your agent's code.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Architecture overview\n",
        "\n",
        "This project uses a microservices architecture where three independent Ray Serve applications work together.\n",
        "\n",
        "### Components\n",
        "* Agent Service (LangGraph): The \"brain\" of the operation. It orchestrates the multi-step reasoning and manages the conversation state. It's lightweight (CPU-only) and deployed with Ray Serve.\n",
        "\n",
        "* LLM Service (Ray Serve LLM): The \"language engine.\" It runs the Qwen/Qwen3-4B-Instruct-2507-FP8 model, optimized for tool use. It's deployed with vLLM on a GPU (L4) for high-speed inference and provides an OpenAI-compatible API.\n",
        "\n",
        "* Tool Service (MCP): The \"hands.\" It exposes a weather API as a set of tools. The agent discovers these tools at runtime using the Model Context Protocol (MCP). It's also a stateless, CPU-only service.\n",
        "\n",
        "Benefits of this Architecture\n",
        "This microservice architecture allows each component to scale independently. Your GPU-intensive LLM service can scale up and down based on inference demand, separate from the lightweight, CPU-based agent orchestration.\n",
        "\n",
        "### Key benefits using Ray and Anyscale include:\n",
        "\n",
        "* Independent Scaling: Scale GPUs for the LLM and CPUs for the agent/tools separately.\n",
        "\n",
        "* High Availability: Zero-downtime updates and automatic recovery from failures.\n",
        "\n",
        "* Flexibility: Swap LLMs or add new tools simply by deploying a new service. The agent discovers them at runtime—no code changes needed.\n",
        "\n",
        "* Enhanced Observability: Anyscale provides comprehensive logs, metrics, and tracing for each service.\n",
        "\n",
        "### Additional resources\n",
        "\n",
        "For more information on LLM serving and Ray Serve, see the following:\n",
        "- [Anyscale LLM Serving documentation](https://docs.anyscale.com/llm/serving)\n",
        "- [Ray Serve LLM documentation](https://docs.ray.io/en/master/serve/llm/index.html)\n",
        "- [Anyscale LLM Serving Template](https://console.anyscale.com/template-preview/deployment-serve-llm)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "## Dependencies and Compute Resources Requirement\n",
        "\n",
        "This project uses `pyproject.toml` with locked versions in `uv.lock` for reproducible installations. Run the following command to install dependencies:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "vscode": {
          "languageId": "shellscript"
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m\u001b[1mDownloading\u001b[0m\u001b[39m cpython-3.12.12-linux-x86_64-gnu (download) \u001b[2m(31.8MiB)\u001b[0m\n",
            " \u001b[32m\u001b[1mDownloading\u001b[0m\u001b[39m cpython-3.12.12-linux-x86_64-gnu (download)\n",
            "Using CPython \u001b[36m3.12.12\u001b[39m\n",
            "Creating virtual environment at: \u001b[36m.venv\u001b[39m\n",
            "\u001b[2mResolved \u001b[1m58 packages\u001b[0m \u001b[2min 0.70ms\u001b[0m\u001b[0m\n",
            "\u001b[36m\u001b[1mDownloading\u001b[0m\u001b[39m tiktoken \u001b[2m(1.1MiB)\u001b[0m\n",
            "\u001b[36m\u001b[1mDownloading\u001b[0m\u001b[39m pydantic-core \u001b[2m(1.9MiB)\u001b[0m\n",
            "\u001b[36m\u001b[1mDownloading\u001b[0m\u001b[39m zstandard \u001b[2m(5.3MiB)\u001b[0m\n",
            " \u001b[32m\u001b[1mDownloading\u001b[0m\u001b[39m tiktoken\n",
            " \u001b[32m\u001b[1mDownloading\u001b[0m\u001b[39m pydantic-core\n",
            " \u001b[32m\u001b[1mDownloading\u001b[0m\u001b[39m zstandard\n",
            "\u001b[2mPrepared \u001b[1m55 packages\u001b[0m \u001b[2min 483ms\u001b[0m\u001b[0m\n",
            "\u001b[2mInstalled \u001b[1m55 packages\u001b[0m \u001b[2min 33ms\u001b[0m\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mannotated-types\u001b[0m\u001b[2m==0.7.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1manyio\u001b[0m\u001b[2m==4.11.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mattrs\u001b[0m\u001b[2m==25.3.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mcertifi\u001b[0m\u001b[2m==2025.8.3\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mcharset-normalizer\u001b[0m\u001b[2m==3.4.3\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mclick\u001b[0m\u001b[2m==8.3.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mdistro\u001b[0m\u001b[2m==1.9.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mfastapi\u001b[0m\u001b[2m==0.115.12\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mh11\u001b[0m\u001b[2m==0.16.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mhttpcore\u001b[0m\u001b[2m==1.0.9\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mhttpx\u001b[0m\u001b[2m==0.28.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mhttpx-sse\u001b[0m\u001b[2m==0.4.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1midna\u001b[0m\u001b[2m==3.10\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mjiter\u001b[0m\u001b[2m==0.11.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mjsonpatch\u001b[0m\u001b[2m==1.33\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mjsonpointer\u001b[0m\u001b[2m==3.0.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mjsonschema\u001b[0m\u001b[2m==4.25.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mjsonschema-specifications\u001b[0m\u001b[2m==2025.9.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mlangchain\u001b[0m\u001b[2m==1.0.5\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mlangchain-core\u001b[0m\u001b[2m==1.0.4\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mlangchain-mcp-adapters\u001b[0m\u001b[2m==0.1.12\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mlangchain-openai\u001b[0m\u001b[2m==1.0.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mlanggraph\u001b[0m\u001b[2m==1.0.3\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mlanggraph-checkpoint\u001b[0m\u001b[2m==2.1.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mlanggraph-prebuilt\u001b[0m\u001b[2m==1.0.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mlanggraph-sdk\u001b[0m\u001b[2m==0.2.9\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mlangsmith\u001b[0m\u001b[2m==0.4.31\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mmcp\u001b[0m\u001b[2m==1.15.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mopenai\u001b[0m\u001b[2m==2.7.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1morjson\u001b[0m\u001b[2m==3.11.3\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mormsgpack\u001b[0m\u001b[2m==1.10.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpackaging\u001b[0m\u001b[2m==25.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpydantic\u001b[0m\u001b[2m==2.11.9\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpydantic-core\u001b[0m\u001b[2m==2.33.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpydantic-settings\u001b[0m\u001b[2m==2.11.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpython-dotenv\u001b[0m\u001b[2m==1.1.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpython-multipart\u001b[0m\u001b[2m==0.0.20\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpyyaml\u001b[0m\u001b[2m==6.0.3\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mreferencing\u001b[0m\u001b[2m==0.36.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mregex\u001b[0m\u001b[2m==2025.9.18\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mrequests\u001b[0m\u001b[2m==2.32.5\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mrequests-toolbelt\u001b[0m\u001b[2m==1.0.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mrpds-py\u001b[0m\u001b[2m==0.27.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1msniffio\u001b[0m\u001b[2m==1.3.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1msse-starlette\u001b[0m\u001b[2m==3.0.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mstarlette\u001b[0m\u001b[2m==0.46.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtenacity\u001b[0m\u001b[2m==9.1.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtiktoken\u001b[0m\u001b[2m==0.11.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtqdm\u001b[0m\u001b[2m==4.67.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtyping-extensions\u001b[0m\u001b[2m==4.15.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtyping-inspection\u001b[0m\u001b[2m==0.4.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1murllib3\u001b[0m\u001b[2m==2.5.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1muvicorn\u001b[0m\u001b[2m==0.38.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mxxhash\u001b[0m\u001b[2m==3.5.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mzstandard\u001b[0m\u001b[2m==0.25.0\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "uv sync"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The deployment requires two compute resources: one L4 GPU (g6.2xlarge instance, 24 GB GPU memory) for the LLM service, and one m5d.xlarge (4 vCPU) for the MCP and agent services."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Implementation: Building the Services\n",
        "\n",
        "This project consists of several Python scripts that work together to create and serve the agent.\n",
        "\n",
        "### Step 1: Create the LLM service\n",
        "\n",
        "Check out the code in `llm_deploy_qwen.py`. This script deploys the Qwen LLM (`Qwen/Qwen3-4B-Instruct-2507-FP8`) as an OpenAI-compatible API endpoint using Ray Serve's `build_openai_app` utility. This allows you to use the Qwen model with any OpenAI-compatible client, including LangChain.\n",
        "\n",
        "The following are key configurations in this script:\n",
        "\n",
        "- **`accelerator_type=\"L4\"`**: Specifies the GPU type. L4 GPUs (Ada Lovelace architecture) are optimized for FP8 precision, making them cost-effective for this quantized model. For higher throughput, use H100 GPUs. For GPU selection guidance, see the [GPU guidance documentation](https://docs.anyscale.com/llm/serving/gpu-guidance).\n",
        "\n",
        "\n",
        "- **`enable_auto_tool_choice=True`**: Enables the model to automatically decide when to use tools based on the input. This is essential for agent workflows where the LLM needs to determine whether to call a tool or respond directly. For more information on tool calling, see the [tool and function calling documentation](https://docs.anyscale.com/llm/serving/tool-function-calling).\n",
        "\n",
        "- **`tool_call_parser=\"hermes\"`**: Specifies the parsing strategy for tool calls. The \"hermes\" parser is designed for models that follow the Hermes function-calling format, which Qwen models support.\n",
        "\n",
        "- **`trust_remote_code=True`**: Required when loading Qwen models from Hugging Face, as they use custom chat templates and tokenization logic that aren't part of the standard transformers library.\n",
        "\n",
        "**Additional LLM development resources:**\n",
        "- [LLM serving basics](https://docs.anyscale.com/llm/serving/intro)\n",
        "- [LLM serving examples and template](https://console.anyscale.com/template-preview/deployment-serve-llm): Comprehensive examples for deploying LLMs with Ray Serve\n",
        "- [Performance optimization documentation](https://docs.anyscale.com/llm/serving/performance-optimization)\n",
        "- [Configure structured output](https://docs.anyscale.com/llm/serving/structured-output): Ensure LLM responses match specific schemas\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 2: Create the MCP weather tool service\n",
        "\n",
        "Check out the code in `weather_mcp_ray.py` to deploy weather tools as an MCP (Model Context Protocol) service.\n",
        "\n",
        "**How the weather tool service works:**\n",
        "\n",
        "The `weather_mcp_ray.py` script uses `FastMCP` from `langchain_mcp_adapters` to define and expose weather-related tools. This service is a FastAPI application deployed with Ray Serve, making the tools available over HTTP.\n",
        "\n",
        "- **FastMCP framework**: The `FastMCP` class provides a way to define tools using Python decorators. Setting `stateless_http=True` makes it suitable for deployment as an HTTP service.\n",
        "\n",
        "- **Tool registration**: Each function decorated with `@mcp.tool()` becomes an automatically discoverable tool:\n",
        "  - `get_alerts(state: str)`: Fetches active weather alerts for a given U.S. state code.\n",
        "  - `get_forecast(latitude: float, longitude: float)`: Retrieves a 5-period forecast for specific coordinates.\n",
        "\n",
        "- **External API integration**: The service makes asynchronous HTTP requests to the National Weather Service (NWS) API using `httpx`. The `USER_AGENT` header is required by the NWS API to identify the client application.\n",
        "\n",
        "- **Tool metadata**: The docstrings for each tool function serve as descriptions that the agent uses to understand when and how to call each tool. This is crucial for the LLM to decide which tool to use.\n",
        "\n",
        "- **Ray Serve deployment**: When deployed with Ray Serve, this becomes a scalable microservice that can handle multiple concurrent tool requests from agent instances.\n",
        "\n",
        "**Important:** Ray Serve currently only supports stateless HTTP mode in MCP. Set `stateless_http=True` to prevent \"session not found\" errors when multiple replicas are running:\n",
        "\n",
        "```python\n",
        "mcp = FastMCP(\"weather\", stateless_http=True)\n",
        "```\n",
        "\n",
        "**Additional resources:**\n",
        "- [MCP quickstart guide](https://docs.anyscale.com/mcp/mcp-quickstart-guide)\n",
        "- [Deploy scalable MCP servers](https://docs.anyscale.com/mcp/scalable-remote-mcp-deployment)\n",
        "- [Anyscale MCP Deployment Template](https://console.anyscale.com/template-preview/mcp-ray-serve)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 3: Create the agent logic\n",
        "\n",
        "Check out the code in `agent_with_mcp.py` to define the agent that orchestrates the LLM and tools.\n",
        "\n",
        "The core function is `build_agent`:\n",
        "\n",
        "```python\n",
        "async def build_agent():\n",
        "    mcp_tools = await get_mcp_tools()\n",
        "\n",
        "    tools = []\n",
        "    if mcp_tools:\n",
        "        tools.extend(mcp_tools)\n",
        "    else:\n",
        "        # Fallback so you can verify tool-calling quickly.\n",
        "        tools.append(echo)\n",
        "\n",
        "    print(f\"\\n[Agent] Using {len(tools)} tool(s).\")\n",
        "\n",
        "    memory = MemorySaver()\n",
        "    agent = create_agent(\n",
        "        llm,\n",
        "        tools,\n",
        "        system_prompt=PROMPT,\n",
        "        checkpointer=memory,\n",
        "    )\n",
        "    return agent\n",
        "```\n",
        "\n",
        "**How the agent works:**\n",
        "\n",
        "- **LLM configuration**: Connects to your deployed Qwen model using the OpenAI-compatible API.\n",
        "\n",
        "- **Tool discovery**: Uses `MultiServerMCPClient` to automatically discover available tools from the MCP service.\n",
        "\n",
        "- **Agent creation**: Creates an agent with the LLM, tools, and system prompt using LangChain's `create_agent` function.\n",
        "\n",
        "- **Memory management**: Uses `MemorySaver` to maintain conversation state across multiple turns.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 4: Create the agent deployment script\n",
        "\n",
        "The `ray_serve_agent_deployment.py` script deploys the agent as a Ray Serve application with a `/chat` endpoint.\n",
        "\n",
        "```python\n",
        "import json\n",
        "from contextlib import asynccontextmanager\n",
        "from typing import AsyncGenerator\n",
        "from uuid import uuid4\n",
        "\n",
        "from fastapi import FastAPI, Request\n",
        "from fastapi.encoders import jsonable_encoder\n",
        "from starlette.responses import StreamingResponse\n",
        "from ray import serve\n",
        "\n",
        "from agent_with_mcp import build_agent  # Your factory that returns a Langchain / LangGraph agent.\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# FastAPI app with an async lifespan hook.\n",
        "# ----------------------------------------------------------------------\n",
        "@asynccontextmanager\n",
        "async def lifespan(app: FastAPI):\n",
        "    agent = await build_agent()  # Likely compiled with a checkpointer.\n",
        "    app.state.agent = agent\n",
        "    try:\n",
        "        yield\n",
        "    finally:\n",
        "        if hasattr(agent, \"aclose\"):\n",
        "            await agent.aclose()\n",
        "\n",
        "fastapi_app = FastAPI(lifespan=lifespan)\n",
        "\n",
        "@fastapi_app.post(\"/chat\")\n",
        "async def chat(request: Request):\n",
        "    \"\"\"\n",
        "    POST /chat\n",
        "    Body: {\"user_request\": \"<text>\", \"thread_id\": \"<optional>\", \"checkpoint_ns\": \"<optional>\"}\n",
        "\n",
        "    Streams LangGraph 'update' dicts as SSE (one JSON object per event).\n",
        "    \"\"\"\n",
        "    body = await request.json()\n",
        "    user_request: str = body.get(\"user_request\", \"\")\n",
        "\n",
        "    # Threading and checkpoint identifiers.\n",
        "    thread_id = (\n",
        "        body.get(\"thread_id\")\n",
        "        or request.headers.get(\"X-Thread-Id\")\n",
        "        or str(uuid4())  # New thread per request if none provided.\n",
        "    )\n",
        "    checkpoint_ns = body.get(\"checkpoint_ns\")  # Optional namespacing.\n",
        "\n",
        "    # Build config for LangGraph.\n",
        "    config = {\"configurable\": {\"thread_id\": thread_id}}\n",
        "    if checkpoint_ns:\n",
        "        config[\"configurable\"][\"checkpoint_ns\"] = checkpoint_ns\n",
        "\n",
        "    async def event_stream() -> AsyncGenerator[str, None]:\n",
        "        agent = request.app.state.agent\n",
        "        inputs = {\"messages\": [{\"role\": \"user\", \"content\": user_request}]}\n",
        "\n",
        "        try:\n",
        "            # Stream updates from the agent.\n",
        "            async for update in agent.astream(inputs, config=config, stream_mode=\"updates\"):\n",
        "                safe_update = jsonable_encoder(update)\n",
        "                # Proper SSE framing: \"data: <json>\\n\\n\".\n",
        "                yield f\"data: {json.dumps(safe_update)}\\n\\n\"\n",
        "        except Exception as e:\n",
        "            # Don't crash the SSE; surface one terminal error event and end.\n",
        "            err = {\"error\": type(e).__name__, \"detail\": str(e)}\n",
        "            yield f\"data: {json.dumps(err)}\\n\\n\"\n",
        "\n",
        "    # Expose thread id so the client can reuse it on the next call.\n",
        "    headers = {\"X-Thread-Id\": thread_id}\n",
        "\n",
        "    return StreamingResponse(\n",
        "        event_stream(),\n",
        "        media_type=\"text/event-stream\",\n",
        "        headers=headers,\n",
        "    )\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# Ray Serve deployment wrapper.\n",
        "# ----------------------------------------------------------------------\n",
        "@serve.deployment(ray_actor_options={\"num_cpus\": 1})\n",
        "@serve.ingress(fastapi_app)\n",
        "class LangGraphServeDeployment:\n",
        "    pass\n",
        "\n",
        "app = LangGraphServeDeployment.bind()\n",
        "\n",
        "# Deploy the agent app locally:\n",
        "# serve run ray_serve_agent_deployment:app\n",
        "\n",
        "# Deploy the agent using Anyscale service:\n",
        "# anyscale service deploy ray_serve_agent_deployment:app\n",
        "```\n",
        "\n",
        "**How deployment works:**\n",
        "\n",
        "- **FastAPI lifespan management**: Uses `@asynccontextmanager` to initialize the agent on startup and clean up on shutdown.\n",
        "\n",
        "- **Streaming endpoint**: The `/chat` endpoint accepts POST requests and returns server-sent events (SSE):\n",
        "  ```python\n",
        "  {\n",
        "    \"user_request\": \"What's the weather?\",\n",
        "    \"thread_id\": \"optional-thread-id\",\n",
        "    \"checkpoint_ns\": \"optional-namespace\"\n",
        "  }\n",
        "  ```\n",
        "\n",
        "- **Thread management**: Each conversation can have a `thread_id` to maintain context across requests. If no `thread_id` is provided, a new UUID is generated.\n",
        "\n",
        "- **Event streaming**: Uses LangGraph's `astream` to emit real-time updates (tool calls, reasoning steps, final answers) as JSON objects.\n",
        "\n",
        "- **Resource allocation**: The agent deployment is lightweight (0.2 CPUs per replica, no GPU) since heavy computation happens in the LLM service.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Deploy the services\n",
        "\n",
        "Now that you've reviewed the code, deploy each service to Anyscale.\n",
        "\n",
        "### Step 5: Deploy the LLM service\n",
        "\n",
        "Deploy the Qwen LLM service on Anyscale. This command creates a scalable endpoint for LLM inference:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "vscode": {
          "languageId": "shellscript"
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/ray/anaconda3/lib/python3.11/site-packages/google/rpc/__init__.py:18: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
            "  import pkg_resources\n",
            "(anyscale +1.7s) Restarting existing service 'llm_deploy_qwen_service'.\n",
            "(anyscale +3.6s) Uploading local dir '.' to cloud storage.\n",
            "(anyscale +4.2s) Including workspace-managed pip dependencies.\n",
            "(anyscale +5.1s) Service 'llm_deploy_qwen_service' deployed (version ID: 6uyk5r1b).\n",
            "(anyscale +5.1s) View the service in the UI: 'https://console.anyscale.com/services/service2_4ebm1f7su1fjgr6bflxgh7hqf6'\n",
            "(anyscale +5.1s) Query the service once it's running using the following curl command (add the path you want to query):\n",
            "(anyscale +5.1s) curl -H \"Authorization: Bearer VrBDo0s-qNOaP9kugBQtJQhGAIA6EUszb6iJHbB1xDQ\" https://llm-deploy-qwen-service-jgz99.cld-kvedzwag2qa8i5bj.s.anyscaleuserdata.com/\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "anyscale service deploy llm_deploy_qwen:app --name llm_deploy_qwen_service\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After deployment completes, you'll receive:\n",
        "- Service URL (for example, `https://llm-deploy-qwen-service-jgz99.cld-kvedzwag2qa8i5bj.s.anyscaleuserdata.com/v1`)\n",
        "- API token for authentication\n",
        "\n",
        "**Save these values—you'll need them to configure the agent.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 6: Deploy the weather MCP service\n",
        "\n",
        "Deploy the weather tool service. This creates an endpoint for the agent to discover and call weather tools:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "vscode": {
          "languageId": "shellscript"
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/ray/anaconda3/lib/python3.11/site-packages/google/rpc/__init__.py:18: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
            "  import pkg_resources\n",
            "(anyscale +1.1s) Restarting existing service 'weather_mcp_service'.\n",
            "(anyscale +1.9s) Uploading local dir '.' to cloud storage.\n",
            "(anyscale +2.6s) Including workspace-managed pip dependencies.\n",
            "(anyscale +3.4s) Service 'weather_mcp_service' deployed (version ID: 6vta7xsr).\n",
            "(anyscale +3.4s) View the service in the UI: 'https://console.anyscale.com/services/service2_gewuw3u78jnjv5wxzx53tnvdb2'\n",
            "(anyscale +3.4s) Query the service once it's running using the following curl command (add the path you want to query):\n",
            "(anyscale +3.4s) curl -H \"Authorization: Bearer uyOArxwCNeTpxn0odOW7hGY57tXQNNrF16Yy8ziskrY\" https://weather-mcp-service-jgz99.cld-kvedzwag2qa8i5bj.s.anyscaleuserdata.com/\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "anyscale service deploy weather_mcp_ray:app --name weather_mcp_service\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After deployment completes, you'll receive:\n",
        "- Service URL (for example, `https://weather-mcp-service-jgz99.cld-kvedzwag2qa8i5bj.s.anyscaleuserdata.com/mcp`)\n",
        "- API token for authentication\n",
        "\n",
        "**Important:** Make sure to include `/mcp` in your URL when configuring the agent.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 7: Configure the agent\n",
        "\n",
        "Update `agent_with_mcp.py` with the service endpoints you received from the deployments. Modify the following lines:\n",
        "\n",
        "```python\n",
        "API_KEY = \"<your-llm-service-token>\"\n",
        "OPENAI_COMPAT_BASE_URL = \"<your-llm-service-url>/v1\"  # Include \"/v1\".\n",
        "MODEL = \"Qwen/Qwen3-4B-Instruct-2507-FP8\"\n",
        "TEMPERATURE = 0.01\n",
        "WEATHER_MCP_BASE_URL = \"<your-mcp-service-url>/mcp\"  # Include \"/mcp\".\n",
        "WEATHER_MCP_TOKEN = \"<your-mcp-service-token>\"\n",
        "```\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 8: Deploy the agent service\n",
        "\n",
        "Deploy the agent itself. For local testing, use `serve run`. For production deployment on Anyscale, see next step.\n",
        "\n",
        "**For local deployment:**\n",
        "\n",
        "```bash\n",
        "serve run ray_serve_agent_deployment:app \n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test the agent\n",
        "\n",
        "### Step 9: Send test requests\n",
        "\n",
        "With the agent service running, send requests to the `/chat` endpoint. The following script sends a request and streams the response:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[thread_id: ab9ecc2b-6ec5-48d3-bd80-7f6f09659734]\n",
            "{\"model\": {\"messages\": [{\"content\": \"\", \"additional_kwargs\": {\"refusal\": null}, \"response_metadata\": {\"token_usage\": {\"completion_tokens\": 40, \"prompt_tokens\": 314, \"total_tokens\": 354, \"completion_tokens_details\": null, \"prompt_tokens_details\": null}, \"model_provider\": \"openai\", \"model_name\": \"Qwen/Qwen3-4B-Instruct-2507-FP8\", \"system_fingerprint\": null, \"id\": \"chatcmpl-194a9eba-b073-4a35-9ee2-032c31de689f\", \"finish_reason\": \"tool_calls\", \"logprobs\": null}, \"type\": \"ai\", \"name\": null, \"id\": \"lc_run--0467b4b4-4865-4bd1-b3c2-058ac4a6cb94-0\", \"tool_calls\": [{\"name\": \"get_forecast\", \"args\": {\"latitude\": 37.4419, \"longitude\": -122.1416}, \"id\": \"chatcmpl-tool-71fdcbd9053941cd84f8b77dff82a719\", \"type\": \"tool_call\"}], \"invalid_tool_calls\": [], \"usage_metadata\": {\"input_tokens\": 314, \"output_tokens\": 40, \"total_tokens\": 354, \"input_token_details\": {}, \"output_token_details\": {}}}]}}\n",
            "{\"tools\": {\"messages\": [{\"content\": \"Today:\\nTemperature: 62°F\\nWind: 3 to 10 mph NNW\\nForecast: Sunny. High near 62, with temperatures falling to around 59 in the afternoon. North northwest wind 3 to 10 mph.\\n---\\nTonight:\\nTemperature: 48°F\\nWind: 2 to 6 mph S\\nForecast: Partly cloudy, with a low around 48. South wind 2 to 6 mph.\\n---\\nWednesday:\\nTemperature: 62°F\\nWind: 6 mph S\\nForecast: Partly sunny. High near 62, with temperatures falling to around 59 in the afternoon. South wind around 6 mph.\\n---\\nWednesday Night:\\nTemperature: 50°F\\nWind: 3 to 9 mph S\\nForecast: Rain after 10pm. Mostly cloudy, with a low around 50. South wind 3 to 9 mph. Chance of precipitation is 80%. New rainfall amounts between a tenth and quarter of an inch possible.\\n---\\nThursday:\\nTemperature: 60°F\\nWind: 6 to 9 mph W\\nForecast: Rain. Mostly cloudy, with a high near 60. West wind 6 to 9 mph. Chance of precipitation is 80%. New rainfall amounts between a tenth and quarter of an inch possible.\", \"additional_kwargs\": {}, \"response_metadata\": {}, \"type\": \"tool\", \"name\": \"get_forecast\", \"id\": \"6f0a44ab-9231-4136-877c-013947457b52\", \"tool_call_id\": \"chatcmpl-tool-71fdcbd9053941cd84f8b77dff82a719\", \"artifact\": null, \"status\": \"success\"}]}}\n",
            "{\"model\": {\"messages\": [{\"content\": \"The weather in Palo Alto is currently sunny with a high of around 62°F and a low of 48°F tonight. Winds are from the north-northwest at 3 to 10 mph. \\n\\nOn Wednesday, it will be partly sunny with a high near 62°F and a low of 50°F, with a chance of rain after 10 PM (80% chance, light rainfall possible). \\n\\nThursday will be rainy and mostly cloudy, with a high near 60°F and west winds of 6 to 9 mph, also with an 80% chance of precipitation and light rainfall.\", \"additional_kwargs\": {\"refusal\": null}, \"response_metadata\": {\"token_usage\": {\"completion_tokens\": 133, \"prompt_tokens\": 647, \"total_tokens\": 780, \"completion_tokens_details\": null, \"prompt_tokens_details\": null}, \"model_provider\": \"openai\", \"model_name\": \"Qwen/Qwen3-4B-Instruct-2507-FP8\", \"system_fingerprint\": null, \"id\": \"chatcmpl-4a931f3f-db70-490f-bc00-83576b358bdd\", \"finish_reason\": \"stop\", \"logprobs\": null}, \"type\": \"ai\", \"name\": null, \"id\": \"lc_run--aa5da1f9-cb56-4bb5-b943-895cc113531c-0\", \"tool_calls\": [], \"invalid_tool_calls\": [], \"usage_metadata\": {\"input_tokens\": 647, \"output_tokens\": 133, \"total_tokens\": 780, \"input_token_details\": {}, \"output_token_details\": {}}}]}}\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import requests\n",
        "\n",
        "SERVER_URL = \"http://127.0.0.1:8000/chat\"  # For local deployment.\n",
        "HEADERS = {\"Content-Type\": \"application/json\"}\n",
        "\n",
        "def chat(user_request: str, thread_id: str | None = None) -> None:\n",
        "    \"\"\"Send a chat request to the agent and stream the response.\"\"\"\n",
        "    payload = {\"user_request\": user_request}\n",
        "    if thread_id:\n",
        "        payload[\"thread_id\"] = thread_id\n",
        "\n",
        "    with requests.post(SERVER_URL, headers=HEADERS, json=payload, stream=True) as resp:\n",
        "        resp.raise_for_status()\n",
        "        # Capture thread_id for multi-turn conversations.\n",
        "        server_thread = resp.headers.get(\"X-Thread-Id\")\n",
        "        if not thread_id and server_thread:\n",
        "            print(f\"[thread_id: {server_thread}]\")\n",
        "        # Stream SSE events.\n",
        "        for line in resp.iter_lines():\n",
        "            if not line:\n",
        "                continue\n",
        "            txt = line.decode(\"utf-8\")\n",
        "            if txt.startswith(\"data: \"):\n",
        "                txt = txt[len(\"data: \"):]\n",
        "            print(txt, flush=True)\n",
        "\n",
        "# Test the agent.\n",
        "chat(\"What's the weather in Palo Alto?\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 10: Deploy the agent to production on Anyscale\n",
        "\n",
        "After testing the agent locally, deploy it to Anyscale for production use. This creates a scalable, managed endpoint with enterprise features.\n",
        "\n",
        "#### Why deploy to Anyscale\n",
        "\n",
        "**Production benefits:**\n",
        "- **Auto-scaling**: Automatically scales replicas based on request volume (0 to N replicas)\n",
        "- **High availability**: Zero-downtime deployments with automatic failover  \n",
        "- **Observability**: Built-in metrics, logs, and distributed tracing\n",
        "- **Cost optimization**: Scale to zero when idle (with appropriate configuration)\n",
        "- **Load balancing**: Distributes requests across multiple agent replicas\n",
        "- **Fault isolation**: Agent, LLM, and tools run as separate services\n",
        "\n",
        "#### Deploy the agent service\n",
        "\n",
        "Run the following command to deploy your agent to Anyscale. This command packages your code and creates a production-ready service:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "vscode": {
          "languageId": "shellscript"
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/ray/anaconda3/lib/python3.11/site-packages/google/rpc/__init__.py:18: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
            "  import pkg_resources\n",
            "(anyscale +0.8s) Starting new service 'agent_service_langchain'.\n",
            "(anyscale +1.5s) Uploading local dir '.' to cloud storage.\n",
            "(anyscale +2.1s) Including workspace-managed pip dependencies.\n",
            "(anyscale +3.0s) Service 'agent_service_langchain' deployed (version ID: bkr6yywq).\n",
            "(anyscale +3.0s) View the service in the UI: 'https://console.anyscale.com/services/service2_ikfi286bzvx7929zhgwvucw2qt'\n",
            "(anyscale +3.0s) Query the service once it's running using the following curl command (add the path you want to query):\n",
            "(anyscale +3.0s) curl -H \"Authorization: Bearer nZp2BEjdloNlwGyxoWSpdalYGtkhfiHtfXhmV4BQuyk\" https://agent-service-langchain-jgz99.cld-kvedzwag2qa8i5bj.s.anyscaleuserdata.com/\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "anyscale service deploy ray_serve_agent_deployment:app --name agent_service_langchain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Understanding the deployment output\n",
        "\n",
        "After running the deployment command, you'll receive:\n",
        "- **Service URL**: The HTTPS endpoint for your agent (e.g., `https://agent-service-langchain-jgz99.cld-kvedzwag2qa8i5bj.s.anyscaleuserdata.com`)\n",
        "- **Authorization token**: Bearer token for authenticating requests\n",
        "- **Service UI link**: Direct link to monitor your service in the Anyscale console\n",
        "\n",
        "#### Test the production agent\n",
        "\n",
        "Once deployed, test your production agent with authenticated requests. Update the following code with your deployment details:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[thread_id: 83c20767-1b5e-4616-af0f-81808be5ffbc]\n",
            "{\"model\": {\"messages\": [{\"content\": \"\", \"additional_kwargs\": {\"refusal\": null}, \"response_metadata\": {\"token_usage\": {\"completion_tokens\": 40, \"prompt_tokens\": 314, \"total_tokens\": 354, \"completion_tokens_details\": null, \"prompt_tokens_details\": null}, \"model_provider\": \"openai\", \"model_name\": \"Qwen/Qwen3-4B-Instruct-2507-FP8\", \"system_fingerprint\": null, \"id\": \"chatcmpl-977afc75-ae84-47a6-921b-a4e20f83707f\", \"finish_reason\": \"tool_calls\", \"logprobs\": null}, \"type\": \"ai\", \"name\": null, \"id\": \"lc_run--b6da0939-6b77-498f-bc37-e310f5306709-0\", \"tool_calls\": [{\"name\": \"get_forecast\", \"args\": {\"latitude\": 37.4419, \"longitude\": -122.1416}, \"id\": \"chatcmpl-tool-ade83d4018e34dfda64ebe49c13a5313\", \"type\": \"tool_call\"}], \"invalid_tool_calls\": [], \"usage_metadata\": {\"input_tokens\": 314, \"output_tokens\": 40, \"total_tokens\": 354, \"input_token_details\": {}, \"output_token_details\": {}}}]}}\n",
            "{\"tools\": {\"messages\": [{\"content\": \"Today:\\nTemperature: 62°F\\nWind: 3 to 10 mph NNW\\nForecast: Sunny. High near 62, with temperatures falling to around 59 in the afternoon. North northwest wind 3 to 10 mph.\\n---\\nTonight:\\nTemperature: 48°F\\nWind: 2 to 6 mph S\\nForecast: Partly cloudy, with a low around 48. South wind 2 to 6 mph.\\n---\\nWednesday:\\nTemperature: 62°F\\nWind: 6 mph S\\nForecast: Partly sunny. High near 62, with temperatures falling to around 59 in the afternoon. South wind around 6 mph.\\n---\\nWednesday Night:\\nTemperature: 50°F\\nWind: 3 to 9 mph S\\nForecast: Rain after 10pm. Mostly cloudy, with a low around 50. South wind 3 to 9 mph. Chance of precipitation is 80%. New rainfall amounts between a tenth and quarter of an inch possible.\\n---\\nThursday:\\nTemperature: 60°F\\nWind: 6 to 9 mph W\\nForecast: Rain. Mostly cloudy, with a high near 60. West wind 6 to 9 mph. Chance of precipitation is 80%. New rainfall amounts between a tenth and quarter of an inch possible.\", \"additional_kwargs\": {}, \"response_metadata\": {}, \"type\": \"tool\", \"name\": \"get_forecast\", \"id\": \"c5d63e49-af8d-4656-b4a4-eb653d801a65\", \"tool_call_id\": \"chatcmpl-tool-ade83d4018e34dfda64ebe49c13a5313\", \"artifact\": null, \"status\": \"success\"}]}}\n",
            "{\"model\": {\"messages\": [{\"content\": \"The weather in Palo Alto is currently sunny with a high of around 62°F and a low of 48°F tonight. Winds are from the north-northwest at 3 to 10 mph. \\n\\nOn Wednesday, it will be partly sunny with a high near 62°F and a low of 50°F, with a chance of rain after 10 PM (80% chance, light rainfall possible). \\n\\nThursday will be rainy and mostly cloudy with a high near 60°F and west winds of 6 to 9 mph, with the same 80% chance of precipitation and light rainfall.\", \"additional_kwargs\": {\"refusal\": null}, \"response_metadata\": {\"token_usage\": {\"completion_tokens\": 132, \"prompt_tokens\": 647, \"total_tokens\": 779, \"completion_tokens_details\": null, \"prompt_tokens_details\": null}, \"model_provider\": \"openai\", \"model_name\": \"Qwen/Qwen3-4B-Instruct-2507-FP8\", \"system_fingerprint\": null, \"id\": \"chatcmpl-2790229d-f708-4171-ab3d-071c70fcc078\", \"finish_reason\": \"stop\", \"logprobs\": null}, \"type\": \"ai\", \"name\": null, \"id\": \"lc_run--ec59fdb7-906c-46da-8001-c6785507f785-0\", \"tool_calls\": [], \"invalid_tool_calls\": [], \"usage_metadata\": {\"input_tokens\": 647, \"output_tokens\": 132, \"total_tokens\": 779, \"input_token_details\": {}, \"output_token_details\": {}}}]}}\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import requests\n",
        "\n",
        "base_url = \"https://agent-service-langchain-jgz99.cld-kvedzwag2qa8i5bj.s.anyscaleuserdata.com\" ## replace with your service url\n",
        "token = \"nZp2BEjdloNlwGyxoWSpdalYGtkhfiHtfXhmV4BQuyk\" ## replace with your service bearer token\n",
        "\n",
        "SERVER_URL = f\"{base_url}/chat\"  # For Anyscale deployment.\n",
        "HEADERS = {\"Content-Type\": \"application/json\",\n",
        "\"Authorization\": f\"Bearer {token}\"\n",
        "}\n",
        "\n",
        "def chat(user_request: str, thread_id: str | None = None) -> None:\n",
        "    \"\"\"Send a chat request to the agent and stream the response.\"\"\"\n",
        "    payload = {\"user_request\": user_request}\n",
        "    if thread_id:\n",
        "        payload[\"thread_id\"] = thread_id\n",
        "\n",
        "    with requests.post(SERVER_URL, headers=HEADERS, json=payload, stream=True) as resp:\n",
        "        resp.raise_for_status()\n",
        "        # Capture thread_id for multi-turn conversations.\n",
        "        server_thread = resp.headers.get(\"X-Thread-Id\")\n",
        "        if not thread_id and server_thread:\n",
        "            print(f\"[thread_id: {server_thread}]\")\n",
        "        # Stream SSE events.\n",
        "        for line in resp.iter_lines():\n",
        "            if not line:\n",
        "                continue\n",
        "            txt = line.decode(\"utf-8\")\n",
        "            if txt.startswith(\"data: \"):\n",
        "                txt = txt[len(\"data: \"):]\n",
        "            print(txt, flush=True)\n",
        "\n",
        "# Test the agent.\n",
        "chat(\"What's the weather in Palo Alto?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Next steps\n",
        "\n",
        "You've successfully built, deployed, and tested a multi-tool agent using Ray Serve on Anyscale. This architecture demonstrates how to build production-ready AI applications with independent scaling, fault isolation, and dynamic tool discovery.\n",
        "\n",
        "### Extend your agent\n",
        "\n",
        "**Add more tools**  \n",
        "Extend the MCP service with additional capabilities such as database queries, API integrations, or custom business logic. The MCP protocol allows your agent to discover new tools dynamically without code changes. For implementation examples, see the [Anyscale MCP Deployment Template](https://console.anyscale.com/template-preview/mcp-ray-serve).\n",
        "\n",
        "**Swap or upgrade LLMs**  \n",
        "Replace the Qwen model with other tool-calling models such as GPT-4, Claude, or Llama variants. Since the LLM runs as a separate service, you can A/B test different models or perform zero-downtime upgrades. For deployment patterns, see the [Anyscale LLM Serving Template](https://console.anyscale.com/template-preview/deployment-serve-llm).\n",
        "\n",
        "**Build complex workflows**  \n",
        "Implement sophisticated reasoning patterns with LangGraph, such as multi-agent collaboration, iterative refinement, or conditional branching based on tool outputs.\n",
        "\n",
        "### Optimize for production\n",
        "\n",
        "**Monitor performance**  \n",
        "Use Anyscale's built-in observability to track:\n",
        "- Request latency and token throughput\n",
        "- GPU utilization and memory usage\n",
        "- Tool call patterns and success rates\n",
        "- Cost per request across services\n",
        "\n",
        "For detailed metrics guidance, see [Monitor and debug Anyscale workloads](https://docs.anyscale.com/monitoring).\n",
        "\n",
        "**Scale efficiently**  \n",
        "Configure auto-scaling policies for each service independently:\n",
        "- Scale the LLM service based on GPU utilization\n",
        "- Scale the agent service based on request volume\n",
        "- Scale tool services based on specific workload patterns\n",
        "\n",
        "See [Ray Serve autoscaling configuration](https://docs.anyscale.com/llm/serving/parameter-tuning#ray-serve-autoscaling-configuration)\n",
        "\n",
        "### Production best practices\n",
        "\n",
        "Anyscale services provide enterprise-grade features for running agents in production. Key capabilities include:\n",
        "\n",
        "- **Zero-downtime deployments**: Update models or agent logic without interrupting service. See [Update an Anyscale service](https://docs.anyscale.com/services/update).\n",
        "\n",
        "- **Multi-version management**: Deploy up to 10 versions behind a single endpoint for A/B testing and canary deployments. See [Deploy multiple versions of an Anyscale service](https://docs.anyscale.com/services/versions).\n",
        "\n",
        "- **High availability**: Distribute replicas across availability zones with automatic failover. See [Configure head node fault tolerance](https://docs.anyscale.com/administration/resource-management/head-node-fault-tolerance).\n",
        "\n",
        "\n",
        "For comprehensive guidance on production deployments, see the [Anyscale Services documentation](https://docs.anyscale.com/services) and [Ray Serve on the Anyscale Runtime](https://docs.anyscale.com/runtime/serve).\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
