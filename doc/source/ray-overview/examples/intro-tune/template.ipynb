{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Ray Tune\n",
    "\n",
    "This template provides a hands-on introduction to **Ray Tune** — a scalable hyperparameter tuning library built on [Ray](https://docs.ray.io/en/latest/). You will learn what Ray Tune is, why it matters, and how to use its core APIs to efficiently search for the best hyperparameters for your models.\n",
    "\n",
    "\n",
    "**Here is the roadmap for this template:**\n",
    "\n",
    "- **Part 1:** Baseline PyTorch Training\n",
    "- **Part 2:** Why Ray Tune?\n",
    "- **Part 3:** Getting Started — Your First Tune Job\n",
    "- **Part 4:** Ray Tune Core Concepts\n",
    "- **Part 5:** Tuning the PyTorch Model with Ray Tune\n",
    "- **Part 6:** Search Algorithms and Stopping Criteria\n",
    "- **Part 7:** Checkpointing, Storage, and Fault Tolerance\n",
    "- **Part 8:** Monitoring with the Ray Dashboard\n",
    "- **Part 9:** Advanced Patterns\n",
    "- **Summary and Next Steps**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "from typing import Any\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize\n",
    "from torchvision.models import resnet18\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune import Checkpoint, Stopper\n",
    "from ray.tune.search.optuna import OptunaSearch\n",
    "from ray.tune.schedulers import ASHAScheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note on Storage\n",
    "\n",
    "Throughout this tutorial, we use `/mnt/cluster_storage` to represent a shared storage location. In a multi-node cluster, Ray workers on different nodes cannot access the head node's local file system. Use a [shared storage solution](https://docs.anyscale.com/configuration/storage#shared) accessible from every node.\n",
    "\n",
    "## Part 1: Baseline PyTorch Training\n",
    "\n",
    "We begin with a standard PyTorch training loop to establish a baseline. Our running example throughout this template is:\n",
    "\n",
    "- **Objective**: Classify handwritten digits (0-9)\n",
    "- **Model**: ResNet18 adapted for single-channel MNIST images\n",
    "- **Evaluation Metric**: CrossEntropy Loss\n",
    "- **Dataset**: MNIST (60,000 training images, 28×28 grayscale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_data_loader(batch_size: int) -> DataLoader:\n",
    "    transform = Compose([ToTensor(), Normalize((0.5,), (0.5,))])\n",
    "    train_data = MNIST(root=\"./data\", train=True, download=True, transform=transform)\n",
    "    return DataLoader(train_data, batch_size=batch_size, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is our baseline training function with hardcoded hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop_torch(num_epochs: int = 2, batch_size: int = 128, lr: float = 1e-3):\n",
    "    criterion = CrossEntropyLoss()\n",
    "\n",
    "    model = resnet18()\n",
    "    model.conv1 = torch.nn.Conv2d(\n",
    "        1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
    "    )\n",
    "    model.to(\"cuda\")\n",
    "\n",
    "    data_loader = build_data_loader(batch_size)\n",
    "    optimizer = Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for images, labels in data_loader:\n",
    "            images, labels = images.to(\"cuda\"), labels.to(\"cuda\")\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f\"Epoch {epoch}, Loss: {loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We schedule this on a GPU worker node using `@ray.remote` (GPU-intensive work should not run directly on the head node):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.init(ignore_reinit_error=True)\n",
    "\n",
    "@ray.remote(num_gpus=1, resources={\"accelerator_type:T4\": 0.0001})\n",
    "def run_baseline():\n",
    "    train_loop_torch(num_epochs=2)\n",
    "\n",
    "ray.get(run_baseline.remote())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Can we do better?** The model has several hyperparameters — learning rate, batch size, number of epochs — that we chose somewhat arbitrarily. Tuning them systematically could improve performance, but searching over combinations is expensive and slow when done sequentially.\n",
    "\n",
    "[Ray Tune](https://docs.ray.io/en/latest/tune/) is a distributed hyperparameter tuning library that can run many trials in parallel across your cluster, dramatically speeding up the search.\n",
    "\n",
    "## Part 2: Why Ray Tune?\n",
    "\n",
    "Here is a summary of the key challenges Ray Tune solves:\n",
    "\n",
    "| Challenge | Detail | **Ray Tune Solution** |\n",
    "| --- | --- | --- |\n",
    "| **Scale hyperparameter tuning** | Large search spaces and GPU-hungry models take days serially. | Distributes trials across all CPUs and GPUs in your cluster, so hundreds of experiments run in parallel. |\n",
    "| **Optimise hardware utilisation** | Naive scheduling may let trials hoard GPUs or leave nodes idle. | Uses gang/atomic scheduling via [placement groups](https://docs.ray.io/en/latest/ray-core/scheduling/placement-group.html) to grant each trial exactly the resources it needs. |\n",
    "| **Early-terminate bad trials** | Running every trial to completion wastes time and money. | Plug in schedulers like **ASHA** or **Population-Based Training (PBT)** to stop under-performers early. |\n",
    "| **Ecosystem integration** | Teams already use Optuna, HyperOpt, etc.; rewriting is non-trivial. | Integrates with existing search libraries (Optuna, HyperOpt, Ax) and experiment tracking tools. |\n",
    "| **Observability** | Hard to debug and profile many distributed trials. | The **Ray Dashboard** provides visibility into running trials, resource usage, and logs. |\n",
    "| **Fault tolerance** | Long experiments may be interrupted by pre-emptions or node failures. | Trials checkpoint automatically, and experiments can be resumed end-to-end. |\n",
    "\n",
    "Now let's see Ray Tune in action, starting with a simple example before applying it to our PyTorch model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Getting Started — Your First Tune Job\n",
    "\n",
    "We start with a minimal toy example to learn the core Ray Tune API.\n",
    "\n",
    "### Step 1: Define the training function\n",
    "\n",
    "A Ray Tune training function must accept a `config` dictionary and report metrics back using `tune.report()`.\n",
    "\n",
    "> **API Note:** Use `tune.report()` to report metrics back to Tune. You can also `return` or `yield` a metrics dictionary from your trainable function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_simple_model(distance: np.ndarray, a: float) -> np.ndarray:\n",
    "    return distance * a\n",
    "\n",
    "def train_my_simple_model(config: dict[str, Any]) -> None:\n",
    "    distances = np.array([0.1, 0.2, 0.3, 0.4, 0.5])\n",
    "    total_amts = distances * 10\n",
    "\n",
    "    a = config[\"a\"]\n",
    "    predictions = my_simple_model(distances, a)\n",
    "    rmse = np.sqrt(np.mean((total_amts - predictions) ** 2))\n",
    "\n",
    "    tune.report({\"rmse\": rmse})  # Report metrics to Ray Tune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Set up the Tuner\n",
    "\n",
    "A `tune.Tuner` takes three key arguments: the `trainable` function, a `param_space` defining the search space, and a `tune_config` specifying the metric and number of trials.\n",
    "\n",
    "> **API Note:** Always use `tune.Tuner` + `tuner.fit()`. The older `tune.run()` API is deprecated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = tune.Tuner(\n",
    "    trainable=train_my_simple_model,\n",
    "    param_space={\n",
    "        \"a\": tune.randint(0, 20),\n",
    "    },\n",
    "    tune_config=tune.TuneConfig(\n",
    "        metric=\"rmse\",\n",
    "        mode=\"min\",\n",
    "        num_samples=5,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Run the Tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = tuner.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Inspect the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_result = results.get_best_result()\n",
    "print(f\"Best hyperparameters: {best_result.config}\")\n",
    "print(f\"Best RMSE: {best_result.metrics['rmse']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also look at the full results table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.get_dataframe()[[\"rmse\", \"config/a\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recap\n",
    "\n",
    "A `tune.Tuner` accepts:\n",
    "- **`trainable`** — a training function (or class) to be tuned\n",
    "- **`param_space`** — a dictionary defining the hyperparameter search space\n",
    "- **`tune_config`** — configuration for the metric to optimize (`metric`, `mode`) and how many trials to run (`num_samples`)\n",
    "\n",
    "`tuner.fit()` runs multiple trials in parallel, each with a different set of hyperparameters, and returns a `ResultGrid` from which you can retrieve the best configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Ray Tune Core Concepts\n",
    "\n",
    "You might be wondering:\n",
    "- How does the tuner allocate resources to each trial?\n",
    "- How does it decide which hyperparameters to try next?\n",
    "- How does it decide when to stop a trial early?\n",
    "\n",
    "Ray Tune has three key configurable dimensions:\n",
    "\n",
    "**1. Resource allocation** — Each trial runs as a separate process. By default, each consumes 1 CPU. Specify resources explicitly with `tune.with_resources()`.\n",
    "\n",
    "**2. Search algorithm** — Determines how the next trial's hyperparameters are chosen. Default: `BasicVariantGenerator` (random/grid search). Alternatives: Optuna, Bayesian Optimization, HyperOpt.\n",
    "\n",
    "**3. Scheduler** — Controls whether to stop, pause, or prioritize trials based on intermediate results. Default: `FIFOScheduler` (no early stopping). Alternatives: ASHA, PBT.\n",
    "\n",
    "Here is the same toy example with all defaults explicitly specified:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = tune.Tuner(\n",
    "    trainable=tune.with_resources(train_my_simple_model, {\"cpu\": 1}),\n",
    "    param_space={\"a\": tune.randint(0, 20)},\n",
    "    tune_config=tune.TuneConfig(\n",
    "        mode=\"min\",\n",
    "        metric=\"rmse\",\n",
    "        num_samples=5,\n",
    "        search_alg=tune.search.BasicVariantGenerator(),\n",
    "        scheduler=tune.schedulers.FIFOScheduler(),\n",
    "    ),\n",
    ")\n",
    "results = tuner.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect the results to see how different trial configurations performed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.get_dataframe()[[\"rmse\", \"config/a\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a diagram showing the relationship between these Ray Tune components:\n",
    "\n",
    "<img src=\"https://docs.ray.io/en/latest/_images/tune_flow.png\" width=\"800\" />\n",
    "\n",
    "To learn more about these concepts, visit the [Ray Tune Key Concepts documentation](https://docs.ray.io/en/latest/tune/key-concepts.html).\n",
    "\n",
    "With the fundamentals in place, let's now apply Ray Tune to our actual PyTorch model.\n",
    "\n",
    "## Part 5: Tuning the PyTorch Model with Ray Tune\n",
    "\n",
    "We follow the same four steps, now applied to our ResNet18/MNIST model.\n",
    "\n",
    "### Step 1: Refactor the training function\n",
    "\n",
    "We move the PyTorch code into a Tune-compatible function that accepts `config` and reports metrics via `tune.report()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_pytorch(config):\n",
    "    criterion = CrossEntropyLoss()\n",
    "\n",
    "    model = resnet18()\n",
    "    model.conv1 = torch.nn.Conv2d(\n",
    "        1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
    "    )\n",
    "    model.to(\"cuda\")\n",
    "\n",
    "    optimizer = Adam(model.parameters(), lr=config[\"lr\"])\n",
    "    data_loader = build_data_loader(config[\"batch_size\"])\n",
    "\n",
    "    for epoch in range(config[\"num_epochs\"]):\n",
    "        for images, labels in data_loader:\n",
    "            images, labels = images.to(\"cuda\"), labels.to(\"cuda\")\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Report metrics at the end of each epoch\n",
    "        tune.report({\"loss\": loss.item()})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Set up the Tuner\n",
    "\n",
    "We allocate 1 GPU per trial and search over the learning rate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = tune.Tuner(\n",
    "    trainable=tune.with_resources(train_pytorch, {\"gpu\": 1}),\n",
    "    param_space={\n",
    "        \"num_epochs\": 3,\n",
    "        \"batch_size\": 128,\n",
    "        \"lr\": tune.loguniform(1e-4, 1e-1),\n",
    "    },\n",
    "    tune_config=tune.TuneConfig(\n",
    "        mode=\"min\",\n",
    "        metric=\"loss\",\n",
    "        num_samples=4,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Run the Tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = tuner.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Inspect the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_result = results.get_best_result()\n",
    "print(f\"Best config: {best_result.config}\")\n",
    "print(f\"Best loss: {best_result.metrics['loss']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.get_dataframe()[[\"loss\", \"config/lr\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Search Algorithms and Stopping Criteria\n",
    "\n",
    "Now that we've seen the basic Tune workflow, let's explore how to make it smarter — with better search algorithms and stopping strategies.\n",
    "\n",
    "### Search Space Primitives\n",
    "\n",
    "Ray Tune provides a rich set of search space primitives:\n",
    "\n",
    "| **Primitive** | **Example** | **Use Case** |\n",
    "|--------------|-------------|-------------|\n",
    "| `tune.randint(lower, upper)` | `tune.randint(1, 10)` | Integer hyperparameters (depth, layers) |\n",
    "| `tune.uniform(lower, upper)` | `tune.uniform(0.0, 1.0)` | Continuous range (dropout rate) |\n",
    "| `tune.loguniform(lower, upper)` | `tune.loguniform(1e-5, 1e-1)` | Log-scale search (learning rate) |\n",
    "| `tune.choice(list)` | `tune.choice([\"relu\", \"gelu\"])` | Categorical choices |\n",
    "| `tune.grid_search(list)` | `tune.grid_search([32, 64, 128])` | Exhaustive grid over values |\n",
    "\n",
    "### Advanced Search Algorithms\n",
    "\n",
    "Beyond random/grid search, you can plug in sophisticated search algorithms. Here's an example using Optuna for Bayesian optimization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = tune.Tuner(\n",
    "    trainable=tune.with_resources(train_pytorch, {\"gpu\": 1}),\n",
    "    param_space={\n",
    "        \"num_epochs\": 3,\n",
    "        \"batch_size\": 128,\n",
    "        \"lr\": tune.loguniform(1e-4, 1e-1),\n",
    "    },\n",
    "    tune_config=tune.TuneConfig(\n",
    "        num_samples=4,\n",
    "        metric=\"loss\",\n",
    "        mode=\"min\",\n",
    "        search_alg=OptunaSearch(),\n",
    "    ),\n",
    ")\n",
    "results = tuner.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Best config (Optuna): {results.get_best_result().config}\")\n",
    "results.get_dataframe()[[\"loss\", \"config/lr\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ray Tune integrates with many search libraries:\n",
    "\n",
    "| **Library** | **Search Algorithm** | **Best For** |\n",
    "|------------|---------------------|--------------|\n",
    "| Built-in | `BasicVariantGenerator` | Simple random/grid search |\n",
    "| [Optuna](https://optuna.org/) | `OptunaSearch` | Bayesian optimization with pruning |\n",
    "| [HyperOpt](http://hyperopt.github.io/hyperopt/) | `HyperOptSearch` | Tree-structured Parzen Estimators |\n",
    "| [Ax](https://ax.dev/) | `AxSearch` | Bayesian optimization |\n",
    "\n",
    "See the full list in the [Search Algorithm API docs](https://docs.ray.io/en/latest/tune/api/suggestion.html).\n",
    "\n",
    "### Stopping Criteria\n",
    "\n",
    "Ray Tune offers several ways to stop trials and experiments early:\n",
    "\n",
    "**1. Metric-based stopping** — Define a custom `Stopper` to stop individual trials or the entire experiment based on metric thresholds. The `__call__` method returns `True` to stop a specific trial, and `stop_all()` returns `True` to stop the entire experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomStopper(Stopper):\n",
    "    def __init__(self):\n",
    "        self.should_stop = False\n",
    "\n",
    "    def __call__(self, trial_id: str, result: dict) -> bool:\n",
    "        if result[\"loss\"] > 1.0 and result[\"training_iteration\"] > 5:\n",
    "            return True  # Stop this underperforming trial\n",
    "        if result[\"loss\"] <= 0.05:\n",
    "            self.should_stop = True  # Found a great result\n",
    "        return False\n",
    "\n",
    "    def stop_all(self) -> bool:\n",
    "        return self.should_stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Time-based stopping** — You can stop trials after a certain duration with `RunConfig(stop={\"time_total_s\": 120})`, or cap the full experiment time with `TuneConfig(time_budget_s=600.0)`.\n",
    "\n",
    "**3. Early stopping with schedulers** — The `ASHAScheduler` (Asynchronous Successive Halving) is the most commonly used early stopping scheduler. It terminates underperforming trials early, freeing resources for more promising ones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = tune.Tuner(\n",
    "    trainable=tune.with_resources(train_pytorch, {\"gpu\": 1}),\n",
    "    param_space={\n",
    "        \"num_epochs\": 10,\n",
    "        \"batch_size\": 128,\n",
    "        \"lr\": tune.loguniform(1e-4, 1e-1),\n",
    "    },\n",
    "    tune_config=tune.TuneConfig(\n",
    "        metric=\"loss\",\n",
    "        mode=\"min\",\n",
    "        num_samples=8,\n",
    "        scheduler=ASHAScheduler(\n",
    "            max_t=10,        # Max training iterations\n",
    "            grace_period=2,  # Min iterations before stopping is allowed\n",
    "        ),\n",
    "    ),\n",
    ")\n",
    "results = tuner.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Best config (ASHA): {results.get_best_result().config}\")\n",
    "results.get_dataframe()[[\"loss\", \"training_iteration\", \"config/lr\"]].sort_values(\"loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the full list of schedulers in the [Scheduler API docs](https://docs.ray.io/en/latest/tune/api/schedulers.html).\n",
    "\n",
    "## Part 7: Checkpointing, Storage, and Fault Tolerance\n",
    "\n",
    "For production-grade experiments, you need persistent storage, checkpointing, and fault tolerance. This section covers these operational concerns at a high level.\n",
    "\n",
    "### Persistent Storage\n",
    "\n",
    "On a distributed cluster, Ray Tune needs a persistent storage location accessible from all nodes to save checkpoints and experiment state. Configure it via `tune.RunConfig(storage_path=\"/mnt/cluster_storage\")`.\n",
    "\n",
    "<img src=\"https://docs.ray.io/en/latest/_images/checkpoint_lifecycle.png\" alt=\"Checkpoint Lifecycle\" width=\"700\"/>\n",
    "\n",
    "The checkpoint lifecycle: saved locally → uploaded to persistent storage via `tune.report()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpointing Trials\n",
    "\n",
    "To make trials resumable, save model state as a `Checkpoint` inside `tune.report()`. Here is the pattern for PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_pytorch_with_checkpoints(config):\n",
    "    model = resnet18()\n",
    "    model.conv1 = torch.nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "    model.to(\"cuda\")\n",
    "    optimizer = Adam(model.parameters(), lr=config[\"lr\"])\n",
    "    criterion = CrossEntropyLoss()\n",
    "    data_loader = build_data_loader(config[\"batch_size\"])\n",
    "    start_epoch = 0\n",
    "\n",
    "    # Resume from checkpoint if available\n",
    "    checkpoint = tune.get_checkpoint()\n",
    "    if checkpoint:\n",
    "        with checkpoint.as_directory() as ckpt_dir:\n",
    "            state = torch.load(os.path.join(ckpt_dir, \"model.pt\"), weights_only=False)\n",
    "            model.load_state_dict(state[\"model\"])\n",
    "            optimizer.load_state_dict(state[\"optimizer\"])\n",
    "            start_epoch = state[\"epoch\"] + 1\n",
    "\n",
    "    for epoch in range(start_epoch, config[\"num_epochs\"]):\n",
    "        for images, labels in data_loader:\n",
    "            images, labels = images.to(\"cuda\"), labels.to(\"cuda\")\n",
    "            loss = criterion(model(images), labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Save checkpoint with each metric report\n",
    "        with tempfile.TemporaryDirectory() as tmp_dir:\n",
    "            torch.save(\n",
    "                {\"model\": model.state_dict(), \"optimizer\": optimizer.state_dict(), \"epoch\": epoch},\n",
    "                os.path.join(tmp_dir, \"model.pt\"),\n",
    "            )\n",
    "            tune.report(\n",
    "                {\"loss\": loss.item()},\n",
    "                checkpoint=Checkpoint.from_directory(tmp_dir),\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fault Tolerance\n",
    "\n",
    "Ray Tune provides two mechanisms for handling failures:\n",
    "\n",
    "**1. Automatic trial retries** — Configure `FailureConfig` to retry failed trials automatically. For example, `tune.FailureConfig(max_failures=3)` will retry each trial up to 3 times.\n",
    "\n",
    "**2. Experiment recovery** — If the entire experiment fails (e.g., driver crash), you can resume it with `tune.Tuner.restore(path=..., trainable=..., restart_errored=True)`. This picks up where the experiment left off, skipping completed trials and restarting errored ones.\n",
    "\n",
    "Here is a complete example combining checkpointing and fault tolerance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = tune.Tuner(\n",
    "    trainable=tune.with_resources(train_pytorch_with_checkpoints, {\"gpu\": 1}),\n",
    "    param_space={\n",
    "        \"num_epochs\": 5,\n",
    "        \"batch_size\": 128,\n",
    "        \"lr\": tune.loguniform(1e-4, 1e-1),\n",
    "    },\n",
    "    tune_config=tune.TuneConfig(\n",
    "        metric=\"loss\",\n",
    "        mode=\"min\",\n",
    "        num_samples=4,\n",
    "    ),\n",
    "    run_config=tune.RunConfig(\n",
    "        storage_path=\"/mnt/cluster_storage\",\n",
    "        name=\"resnet18_fault_tolerant\",\n",
    "        failure_config=tune.FailureConfig(max_failures=2),\n",
    "    ),\n",
    ")\n",
    "results = tuner.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Best config: {results.get_best_result().config}\")\n",
    "print(f\"Best loss: {results.get_best_result().metrics['loss']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resource Management\n",
    "\n",
    "When running many concurrent trials, OOM errors can occur. Mitigate this by:\n",
    "- **Setting memory resources:** `tune.with_resources(trainable, {\"gpu\": 1, \"memory\": 6 * 1024**3})`\n",
    "- **Limiting concurrency:** `tune.TuneConfig(max_concurrent_trials=4)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Monitoring with the Ray Dashboard\n",
    "\n",
    "The Ray Dashboard provides observability into your Tune experiments.\n",
    "\n",
    "**Jobs tab** — View running Tune jobs and click into individual experiments.\n",
    "\n",
    "> **TODO (Human Review Needed):** Add screenshot of Ray Dashboard showing a Tune job.\n",
    "> - Action needed: Run a Tune experiment and capture a screenshot of the Jobs tab.\n",
    "\n",
    "**Task-level view** — Each trial runs as a `trainable` actor with its own task history. You can inspect:\n",
    "- Which trials are running vs. waiting for resources\n",
    "- CPU and memory usage per trial\n",
    "- `bundle_reservation_check_func` tasks that reserve resources via placement groups\n",
    "\n",
    "**Resource monitoring** — Track GPU utilization, memory, and CPU usage across the cluster. On Anyscale, use the **Metrics tab** for GPU utilization, memory, network I/O, and disk I/O.\n",
    "\n",
    "For more details, see the [Ray Dashboard documentation](https://docs.ray.io/en/latest/ray-observability/getting-started.html) and the [Anyscale monitoring and debugging guide](https://docs.anyscale.com/monitoring)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 9: Advanced Patterns\n",
    "\n",
    "### Passing Data to Trials\n",
    "\n",
    "By default, each trial loads its own copy of the data, which is wasteful. You can place data in the Ray object store once and share it across trials using `tune.with_parameters()`. Call a `@ray.remote` function to load data into the object store, then pass the reference to your trainable:\n",
    "\n",
    "```\n",
    "trainable_with_data = tune.with_parameters(train_func, data=data_ref)\n",
    "```\n",
    "\n",
    "See the [Tune FAQ on data passing](https://docs.ray.io/en/latest/tune/faq.html) for complete examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integrating with Ray Train\n",
    "\n",
    "For distributed multi-GPU training combined with hyperparameter tuning, wrap your Ray Train `Trainer` creation in a driver function that Tune calls with different hyperparameter configurations. Each Tune trial launches a full Ray Train distributed training run.\n",
    "\n",
    "> **API Note:** The older `Tuner(trainer)` API that directly accepts a Trainer instance is deprecated since Ray 2.43. Use the function-based driver pattern shown below instead.\n",
    "\n",
    "```\n",
    "from ray.train.torch import TorchTrainer\n",
    "from ray.tune.integration.ray_train import TuneReportCallback\n",
    "\n",
    "def train_driver_fn(config):\n",
    "    trainer = TorchTrainer(\n",
    "        train_loop_per_worker=...,\n",
    "        train_loop_config=config[\"train_loop_config\"],\n",
    "        scaling_config=ray.train.ScalingConfig(num_workers=2, use_gpu=True),\n",
    "        run_config=ray.train.RunConfig(\n",
    "            name=f\"train-trial_id={tune.get_context().get_trial_id()}\",\n",
    "            callbacks=[TuneReportCallback()],\n",
    "        ),\n",
    "    )\n",
    "    trainer.fit()\n",
    "\n",
    "tuner = tune.Tuner(\n",
    "    train_driver_fn,\n",
    "    param_space={\"train_loop_config\": {\"lr\": tune.loguniform(1e-4, 1e-1)}},\n",
    "    tune_config=tune.TuneConfig(num_samples=4, max_concurrent_trials=2),\n",
    ")\n",
    "results = tuner.fit()\n",
    "```\n",
    "\n",
    "Key details:\n",
    "- **`TuneReportCallback`** propagates metrics reported by Ray Train workers back to Tune, so the `Tuner` can track and compare trial results.\n",
    "- **`tune.get_context().get_trial_id()`** ensures each Train run gets a unique name tied to the Tune trial, which is required for proper fault tolerance.\n",
    "- **`max_concurrent_trials`** limits how many Train runs compete for cluster resources at once. Set this based on your GPU budget (e.g., `total_gpus // gpus_per_trial`).\n",
    "\n",
    "See the [Ray Train + Tune guide](https://docs.ray.io/en/latest/train/user-guides/hyperparameter-optimization.html) for full details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Next Steps\n",
    "\n",
    "In this template, you learned:\n",
    "\n",
    "- **What** Ray Tune is — a scalable, distributed hyperparameter tuning library\n",
    "- **Why** to use it — parallel trial execution, smart search algorithms, early stopping, fault tolerance, and ecosystem integration\n",
    "- **How** to use it — defining trainable functions with `tune.report()`, configuring `tune.Tuner` with search spaces and `TuneConfig`, running experiments with `tuner.fit()`, and retrieving best results\n",
    "- **Core concepts** — resources (`tune.with_resources`), search algorithms (random, grid, Optuna), schedulers (FIFO, ASHA), stopping criteria\n",
    "- **Operational features** — checkpointing, persistent storage, fault tolerance, experiment recovery, monitoring\n",
    "\n",
    "### Deprecated APIs to Avoid\n",
    "\n",
    "| **Deprecated** | **Use Instead** |\n",
    "|---------------|----------------|\n",
    "| `tune.run()` | `tune.Tuner` + `tuner.fit()` |\n",
    "| `tune.Tuner(trainer)` (passing a Trainer directly) | Function-based driver pattern (see Part 9) |\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **[Ray Tune User Guide](https://docs.ray.io/en/latest/tune/getting-started.html)** — Complete guide to Ray Tune\n",
    "2. **[Search Algorithm Reference](https://docs.ray.io/en/latest/tune/api/suggestion.html)** — All supported search algorithms\n",
    "3. **[Scheduler Reference](https://docs.ray.io/en/latest/tune/api/schedulers.html)** — All supported schedulers including ASHA and PBT\n",
    "4. **[Ray Train + Tune Integration](https://docs.ray.io/en/latest/train/user-guides/hyperparameter-optimization.html)** — Combining distributed training with HPO\n",
    "5. **[Tune Examples Gallery](https://docs.ray.io/en/latest/tune/examples/index.html)** — End-to-end examples with popular frameworks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}