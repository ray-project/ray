{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04-d1 Generative computer-vision pattern with Ray Train\n",
    "This notebook builds a **mini diffusion pipeline** on the **Food-101-Lite** dataset and runs it end-to-end on an Anyscale cluster with **Ray Train V2**.\n",
    "\n",
    "### What you learn and take away  \n",
    "* How to use **Ray Data** to decode and preprocess large image datasets in parallel  \n",
    "* How to split and shard datasets for **distributed training** across multiple Ray workers  \n",
    "* How to wrap a custom `LightningModule` with Ray Train to scale out **PyTorch code without boilerplate**  \n",
    "* How to **enable fault tolerance** by saving and restoring model checkpoints with `ray.train.report()`  \n",
    "* How to run training and evaluation with **no changes to your core model code** as Ray handles multi-node orchestration  \n",
    "* How to generate images post-training using the same Ray-hosted environment  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What problem are you solving? (Diffusion as image de-noising)\n",
    "\n",
    "You’re training a **generative model** that learns to produce realistic Red-Green-Blue (RGB) images from pure noise  \n",
    "by learning how to *reverse* a noising process.\n",
    "\n",
    "This approach builds on **de-noising diffusion models**: instead of modeling the full image distribution $p(x)$ directly,  \n",
    "teach the model to reverse a *known* corruption process that gradually adds noise to clean images.\n",
    "\n",
    "---\n",
    "\n",
    "## Input: Images as tensors\n",
    "\n",
    "Each training example is a 3-channel RGB image:\n",
    "\n",
    "$$\n",
    "x_0 \\in [-1, 1]^{3 \\times H \\times W}\n",
    "$$\n",
    "\n",
    "Normalize pixel values to \\[-1, 1\\] and train on **Food-101-Lite**, a small 10-class subset of Food-101.\n",
    "\n",
    "---\n",
    "\n",
    "## Forward process: adding noise\n",
    "\n",
    "During training, sample a timestep $t \\in \\{0, \\dots, T{-}1\\}$  \n",
    "and inject Gaussian noise into the image:\n",
    "\n",
    "$$\\varepsilon \\sim \\mathcal{N}(0, 1), \\quad x_{t} = x_0 + \\varepsilon$$\n",
    "\n",
    "The model sees $x_{t}$ and must learn to recover the corrupting noise $\\varepsilon$.\n",
    "\n",
    "---\n",
    "\n",
    "## Training objective\n",
    "\n",
    "Train a convolutional network $f_\\theta$ to predict the noise:\n",
    "\n",
    "$$\\mathcal{L} = \\mathbb{E}_{x_0, \\varepsilon, t}\\ \\big\\|f_\\theta(x_{t}, t) - \\varepsilon\\big\\|_2^2$$\n",
    "\n",
    "This is an **Mean Squared Error (MSE) loss**, and it encourages the model to de-noise corrupted images.\n",
    "\n",
    "---\n",
    "\n",
    "## Reverse diffusion: sampling new images\n",
    "\n",
    "At generation time, start from pure noise $x_T \\sim \\mathcal{N}(0, 1)$ and step backward:\n",
    "\n",
    "$$x_{t} \\leftarrow x_{t} - \\eta \\cdot f_\\theta(x_{t}, t), \\quad t = T{-}1, \\dots, 0$$\n",
    "\n",
    "After $T$ steps, $x_0$ is a fully generated image — a sample from the learned data distribution.\n",
    "\n",
    "---\n",
    "\n",
    "## Why this works\n",
    "\n",
    "- Diffusion models sidestep unstable Generative Adversarial Network (GAN) training and can model complex, multimodal image distributions  \n",
    "- The forward process stays fixed and simple (just add noise), which makes the learning problem tractable  \n",
    "- At inference time, sampling becomes iterative de-noising — easy to debug, modify, and extend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to migrate this diffusion-policy workload to a distributed setup using Ray on Anyscale\n",
    "\n",
    "This tutorial walks through the end-to-end process of **migrating a local image-based diffusion policy to a distributed Ray cluster running on Anyscale**.\n",
    "\n",
    "Here’s how you make that transition:\n",
    "\n",
    "1. **Local Joint Photographic Experts Groups (JPEG) → Distributed Ray Dataset**  \n",
    "   Preprocess and store Food-101 images as Parquet, then use **Ray Data** to load and decode the dataset in parallel across the cluster. Each worker gets its own shard, streamed efficiently for GPU training.\n",
    "\n",
    "2. **Single-GPU PyTorch → Multi-node Distributed Training**  \n",
    "   Wrap your Lightning model in a Ray Train `train_loop`, then launch distributed training using **TorchTrainer** with 8 GPU workers—each operating on its own data partition with no manual coordination.\n",
    "\n",
    "3. **Manual Checkpoints → Automatic Fault Tolerance**  \n",
    "  Save a checkpoint after every epoch using `ray.train.report(checkpoint=...)`, and configure Ray to **auto-resume from the most recent checkpoint** if a job fails or you relaunch it.\n",
    "\n",
    "4. **Manual Data Management → Declarative Scaling with Ray**  \n",
    "   Instead of slicing data or managing worker processes yourself, declare your intent with `ScalingConfig`, `CheckpointConfig`, and `FailureConfig`, and let **Ray + Anyscale handle the orchestration**.\n",
    "\n",
    "5. **Single-node Sampling → Remote Inference Tasks**  \n",
    "   After training, run **reverse diffusion sampling** as Ray tasks on GPU nodes, making it easy to scale post-training inference or build a lightweight visual demo.\n",
    "\n",
    "This pattern transforms a simple single-node PyTorch loop into a **scalable, fault-tolerant, multi-node training pipeline** with just a few lines of Ray-specific code, and it runs seamlessly on any cluster provisioned with Anyscale.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports and setup  \n",
    "Pull in standard Python utilities, Ray (core, Data, Train, Lightning), and PyTorch Lightning.  \n",
    "Make sure you set the Anyscale cluster to Ray ≥ 2.48, so you get Ray Train V2 semantics automatically enabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch==2.8.0 in /home/ray/anaconda3/lib/python3.12/site-packages (2.8.0)\n",
      "Requirement already satisfied: torchvision==0.23.0 in /home/ray/anaconda3/lib/python3.12/site-packages (0.23.0)\n",
      "Requirement already satisfied: matplotlib==3.10.6 in /home/ray/anaconda3/lib/python3.12/site-packages (3.10.6)\n",
      "Requirement already satisfied: pyarrow==14.0.2 in /home/ray/anaconda3/lib/python3.12/site-packages (14.0.2)\n",
      "Requirement already satisfied: datasets==2.19.2 in /home/ray/anaconda3/lib/python3.12/site-packages (2.19.2)\n",
      "Requirement already satisfied: lightning==2.5.5 in /home/ray/anaconda3/lib/python3.12/site-packages (2.5.5)\n",
      "Requirement already satisfied: filelock in /home/ray/anaconda3/lib/python3.12/site-packages (from torch==2.8.0) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/ray/anaconda3/lib/python3.12/site-packages (from torch==2.8.0) (4.12.2)\n",
      "Requirement already satisfied: setuptools in /home/ray/anaconda3/lib/python3.12/site-packages (from torch==2.8.0) (71.1.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/ray/anaconda3/lib/python3.12/site-packages (from torch==2.8.0) (1.14.0)\n",
      "Requirement already satisfied: networkx in /home/ray/anaconda3/lib/python3.12/site-packages (from torch==2.8.0) (3.5)\n",
      "Requirement already satisfied: jinja2 in /home/ray/anaconda3/lib/python3.12/site-packages (from torch==2.8.0) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /home/ray/anaconda3/lib/python3.12/site-packages (from torch==2.8.0) (2023.5.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /home/ray/anaconda3/lib/python3.12/site-packages (from torch==2.8.0) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /home/ray/anaconda3/lib/python3.12/site-packages (from torch==2.8.0) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /home/ray/anaconda3/lib/python3.12/site-packages (from torch==2.8.0) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /home/ray/anaconda3/lib/python3.12/site-packages (from torch==2.8.0) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /home/ray/anaconda3/lib/python3.12/site-packages (from torch==2.8.0) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /home/ray/anaconda3/lib/python3.12/site-packages (from torch==2.8.0) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /home/ray/anaconda3/lib/python3.12/site-packages (from torch==2.8.0) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /home/ray/anaconda3/lib/python3.12/site-packages (from torch==2.8.0) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /home/ray/anaconda3/lib/python3.12/site-packages (from torch==2.8.0) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /home/ray/anaconda3/lib/python3.12/site-packages (from torch==2.8.0) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /home/ray/anaconda3/lib/python3.12/site-packages (from torch==2.8.0) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /home/ray/anaconda3/lib/python3.12/site-packages (from torch==2.8.0) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /home/ray/anaconda3/lib/python3.12/site-packages (from torch==2.8.0) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /home/ray/anaconda3/lib/python3.12/site-packages (from torch==2.8.0) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.4.0 in /home/ray/anaconda3/lib/python3.12/site-packages (from torch==2.8.0) (3.4.0)\n",
      "Requirement already satisfied: numpy in /home/ray/anaconda3/lib/python3.12/site-packages (from torchvision==0.23.0) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/ray/anaconda3/lib/python3.12/site-packages (from torchvision==0.23.0) (11.3.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/ray/anaconda3/lib/python3.12/site-packages (from matplotlib==3.10.6) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/ray/anaconda3/lib/python3.12/site-packages (from matplotlib==3.10.6) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/ray/anaconda3/lib/python3.12/site-packages (from matplotlib==3.10.6) (4.60.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/ray/anaconda3/lib/python3.12/site-packages (from matplotlib==3.10.6) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ray/anaconda3/lib/python3.12/site-packages (from matplotlib==3.10.6) (23.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/ray/anaconda3/lib/python3.12/site-packages (from matplotlib==3.10.6) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/ray/anaconda3/lib/python3.12/site-packages (from matplotlib==3.10.6) (2.8.2)\n",
      "Requirement already satisfied: pyarrow-hotfix in /home/ray/anaconda3/lib/python3.12/site-packages (from datasets==2.19.2) (0.7)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/ray/anaconda3/lib/python3.12/site-packages (from datasets==2.19.2) (0.3.8)\n",
      "Requirement already satisfied: pandas in /home/ray/anaconda3/lib/python3.12/site-packages (from datasets==2.19.2) (2.3.1)\n",
      "Requirement already satisfied: requests>=2.32.1 in /home/ray/anaconda3/lib/python3.12/site-packages (from datasets==2.19.2) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /home/ray/anaconda3/lib/python3.12/site-packages (from datasets==2.19.2) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /home/ray/anaconda3/lib/python3.12/site-packages (from datasets==2.19.2) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /home/ray/anaconda3/lib/python3.12/site-packages (from datasets==2.19.2) (0.70.16)\n",
      "Requirement already satisfied: aiohttp in /home/ray/anaconda3/lib/python3.12/site-packages (from datasets==2.19.2) (3.11.16)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.2 in /home/ray/anaconda3/lib/python3.12/site-packages (from datasets==2.19.2) (0.35.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ray/anaconda3/lib/python3.12/site-packages (from datasets==2.19.2) (6.0.1)\n",
      "Requirement already satisfied: lightning-utilities<2.0,>=0.10.0 in /home/ray/anaconda3/lib/python3.12/site-packages (from lightning==2.5.5) (0.15.2)\n",
      "Requirement already satisfied: torchmetrics<3.0,>0.7.0 in /home/ray/anaconda3/lib/python3.12/site-packages (from lightning==2.5.5) (1.8.2)\n",
      "Requirement already satisfied: pytorch-lightning in /home/ray/anaconda3/lib/python3.12/site-packages (from lightning==2.5.5) (2.5.5)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/ray/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets==2.19.2) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/ray/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets==2.19.2) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/ray/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets==2.19.2) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/ray/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets==2.19.2) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/ray/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets==2.19.2) (6.0.5)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/ray/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets==2.19.2) (0.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/ray/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets==2.19.2) (1.18.3)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/ray/anaconda3/lib/python3.12/site-packages (from huggingface-hub>=0.21.2->datasets==2.19.2) (1.1.10)\n",
      "Requirement already satisfied: six>=1.5 in /home/ray/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib==3.10.6) (1.16.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/ray/anaconda3/lib/python3.12/site-packages (from requests>=2.32.1->datasets==2.19.2) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ray/anaconda3/lib/python3.12/site-packages (from requests>=2.32.1->datasets==2.19.2) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ray/anaconda3/lib/python3.12/site-packages (from requests>=2.32.1->datasets==2.19.2) (1.26.19)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ray/anaconda3/lib/python3.12/site-packages (from requests>=2.32.1->datasets==2.19.2) (2025.1.31)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/ray/anaconda3/lib/python3.12/site-packages (from sympy>=1.13.3->torch==2.8.0) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ray/anaconda3/lib/python3.12/site-packages (from jinja2->torch==2.8.0) (2.1.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ray/anaconda3/lib/python3.12/site-packages (from pandas->datasets==2.19.2) (2022.7.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/ray/anaconda3/lib/python3.12/site-packages (from pandas->datasets==2.19.2) (2025.2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 00. Runtime setup — install same deps and set env vars\n",
    "import os, sys, subprocess\n",
    "\n",
    "# Non-secret env var (safe to set here)\n",
    "os.environ[\"RAY_TRAIN_V2_ENABLED\"] = \"1\"\n",
    "\n",
    "# Install Python dependencies (same pinned versions as build.sh)\n",
    "subprocess.check_call([\n",
    "    sys.executable, \"-m\", \"pip\", \"install\", \"--no-cache-dir\",\n",
    "    \"torch==2.8.0\",\n",
    "    \"torchvision==0.23.0\",\n",
    "    \"matplotlib==3.10.6\",\n",
    "    \"pyarrow==14.0.2\",\n",
    "    \"datasets==2.19.2\",\n",
    "    \"lightning==2.5.5\",\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 01. Imports\n",
    "\n",
    "# Standard libraries\n",
    "import os, io, json, shutil\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "# Ray\n",
    "import ray, ray.data\n",
    "from ray.train import ScalingConfig, get_context, RunConfig, FailureConfig, CheckpointConfig, Checkpoint, get_checkpoint\n",
    "from ray.train.torch import TorchTrainer\n",
    "from ray.train.lightning import RayLightningEnvironment\n",
    "\n",
    "# PyTorch / Lightning\n",
    "import lightning.pytorch as pl\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# Dataset\n",
    "from datasets import load_dataset\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from tqdm import tqdm  \n",
    "from torchvision.transforms import Compose, Resize, CenterCrop\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load 10 % of Food-101  \n",
    "Next, grab roughly 7 500 images, exactly 10 % of Food-101—using a single call to `load_dataset`. This trimmed subset trains quickly while still being large enough to demonstrate Ray’s scaling behaviour.\n",
    "\n",
    "NOTE: skip cells 02-05 if the dataset is already downloaded, as this is the same dataset as in tutorial 04a."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 02. Load 10% of food101 (~7,500 images)\n",
    "ds = load_dataset(\"food101\", split=\"train[:10%]\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Resize and encode Iimages  \n",
    "Preprocess each image: resize to 256 pixel, center-crop to 224 pixel (the size expected by most ImageNet models), and then convert the result to raw JPEG bytes. By storing bytes instead of full Python Imaging Library (PIL) objects, you keep the dataset compact and Parquet-friendly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 03. Resize + encode as JPEG bytes\n",
    "transform = Compose([Resize(256), CenterCrop(224)])\n",
    "records = []\n",
    "\n",
    "for example in tqdm(ds, desc=\"Preprocessing images\", unit=\"img\"):\n",
    "    try:\n",
    "        img = transform(example[\"image\"])\n",
    "        buf = io.BytesIO()\n",
    "        img.save(buf, format=\"JPEG\")\n",
    "        records.append({\n",
    "            \"image_bytes\": buf.getvalue(),\n",
    "            \"label\": example[\"label\"]\n",
    "        })\n",
    "    except Exception as e:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visual sanity check  \n",
    "Before committing to hours of training, take nine random samples and plot them with their class names. This quick inspection lets you confirm that images are correctly resized and preprocessed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 04. Visualize the dataset\n",
    "\n",
    "label_names = ds.features[\"label\"].names  # maps int → string\n",
    "\n",
    "samples = random.sample(records, 9)\n",
    "\n",
    "fig, axs = plt.subplots(3, 3, figsize=(8, 8))\n",
    "fig.suptitle(\"Sample Resized Images from food101-lite\", fontsize=16)\n",
    "\n",
    "for ax, rec in zip(axs.flatten(), samples):\n",
    "    img = Image.open(io.BytesIO(rec[\"image_bytes\"]))\n",
    "    label_name = label_names[rec[\"label\"]]\n",
    "    ax.imshow(img)\n",
    "    ax.set_title(label_name)\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Persist to Parquet  \n",
    "Now, write the images and labels to a Parquet file. Because Parquet is columnar, you can read just the columns you need during training, which speeds up IO---especially when multiple workers are reading in parallel under Ray."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 05. Write Dataset to Parquet\n",
    "\n",
    "output_dir = \"/mnt/cluster_storage/food101_lite/parquet_256\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "table = pa.Table.from_pydict({\n",
    "    \"image_bytes\": [r[\"image_bytes\"] for r in records],\n",
    "    \"label\": [r[\"label\"] for r in records]\n",
    "})\n",
    "pq.write_table(table, os.path.join(output_dir, \"shard_0.parquet\"))\n",
    "\n",
    "print(f\"Wrote {len(records)} records to {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Load and decode with Ray Data  \n",
    "Read the Parquet shard into a **Ray Dataset**, decode the JPEG bytes to ** Channel-Height-Width (CHW) float32 tensors**, scale to \\[-1, 1\\], and drop the original byte column.  \n",
    "Because `decode_and_normalize` is stateless, the default **task-based** execution is perfect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 06. Load & Decode Food-101-Lite\n",
    "\n",
    "# Path to Parquet shards written earlier\n",
    "PARQUET_PATH = \"/mnt/cluster_storage/food101_lite/parquet_256\"\n",
    "\n",
    "# Read the Parquet files (≈7 500 rows with JPEG bytes + label)\n",
    "ds = ray.data.read_parquet(PARQUET_PATH)\n",
    "print(\"Raw rows:\", ds.count())\n",
    "\n",
    "# Decode JPEG → CHW float32 in [‑1, 1]\n",
    "\n",
    "def decode_and_normalize(batch_df):\n",
    "    \"\"\"Decode JPEG bytes and scale to [-1, 1].\"\"\"\n",
    "    images = []\n",
    "    for b in batch_df[\"image_bytes\"]:\n",
    "        img = Image.open(io.BytesIO(b)).convert(\"RGB\")\n",
    "        arr = np.asarray(img, dtype=np.float32) / 255.0       # H × W × 3, 0‑1\n",
    "        arr = (arr - 0.5) / 0.5                               # ‑1 … 1\n",
    "        arr = arr.transpose(2, 0, 1)                          # 3 × H × W (CHW)\n",
    "        images.append(arr)\n",
    "    return {\"image\": images}\n",
    "\n",
    "# Apply in parallel\n",
    "#   batch_format=\"pandas\" → batch_df is a DataFrame, return dict of lists.\n",
    "#   default task‑based compute is sufficient for a stateless function.\n",
    "\n",
    "ds = ds.map_batches(\n",
    "    decode_and_normalize,\n",
    "    batch_format=\"pandas\",\n",
    "    # Use the default (task‑based) compute strategy since `decode_and_normalize` is a plain function.\n",
    "    num_cpus=1,\n",
    ")\n",
    "\n",
    "# Drop the original JPEG column to save memory\n",
    "if \"image_bytes\" in ds.schema().names:\n",
    "    ds = ds.drop_columns([\"image_bytes\", \"label\"])\n",
    "\n",
    "print(\"Decoded rows:\", ds.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Shuffle and Train/Val split  \n",
    "Perform a reproducible shuffle, then split 80 % / 20 % into `train_ds` and `val_ds`.  \n",
    "Each split remains a first-class Ray Dataset, enabling distributed, sharded DataLoaders later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 07. Shuffle & Train/Val Split\n",
    "\n",
    "# Typical 80 / 20 split\n",
    "TOTAL = ds.count()\n",
    "train_count = int(TOTAL * 0.8)\n",
    "ds = ds.random_shuffle()\n",
    "train_ds, val_ds = ds.split_at_indices([train_count])\n",
    "print(\"Train rows:\", train_ds.count())\n",
    "print(\"Val rows:\",   val_ds.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Pixel diffusion LightningModule  \n",
    "A minimal **de-noising diffusion** policy:  \n",
    "* Input = noisy image + scalar timestep (packed as a 4-channel tensor)  \n",
    "* Output = predicted noise ϵ  \n",
    "Log per-epoch losses and save them to a shared JSON so every worker can later plot global curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 08. Pixel De-noising Diffusion Model\n",
    "\n",
    "class PixelDiffusion(pl.LightningModule):\n",
    "    \"\"\"Tiny CNN that predicts noise ϵ given noisy image + timestep.\"\"\"\n",
    "\n",
    "    def __init__(self, max_t=1000, log_path=None):\n",
    "        super().__init__()\n",
    "        self.max_t = max_t\n",
    "        self.log_path = log_path or \"/mnt/cluster_storage/generative_cv/epoch_metrics.json\"\n",
    "\n",
    "        # Network: (3 + 1)‑channel input → 3‑channel noise prediction\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(4, 32, 3, padding=1), nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, 3, padding=1), nn.ReLU(),\n",
    "            nn.Conv2d(32, 3, 3, padding=1),\n",
    "        )\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "        self._train_losses, self._val_losses = [], []\n",
    "\n",
    "    # ---------- forward ----------\n",
    "    def forward(self, noisy_img, t):\n",
    "        \"\"\"noisy_img: Bx3xHxW,  t: B (int) or Bx1 scalar\"\"\"\n",
    "        b, _, h, w = noisy_img.shape\n",
    "        t_scaled = (t / self.max_t).view(-1, 1, 1, 1).float().to(noisy_img.device)\n",
    "        t_img = t_scaled.expand(-1, 1, h, w)\n",
    "        x = torch.cat([noisy_img, t_img], dim=1)  # 4 channels\n",
    "        return self.net(x)\n",
    "    \n",
    "    # ---------- training / validation steps ----------\n",
    "    def _shared_step(self, batch):\n",
    "        clean = batch[\"image\"].to(self.device)             # Bx3xHxW, ‑1…1\n",
    "        noise = torch.randn_like(clean)                    # ϵ ~ N(0, 1)\n",
    "        t = torch.randint(0, self.max_t, (clean.size(0),), device=self.device)\n",
    "        noisy = clean + noise                              # x_t = x_0 + ϵ\n",
    "        pred_noise = self(noisy, t)\n",
    "        return self.loss_fn(pred_noise, noise)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss = self._shared_step(batch)\n",
    "        self._train_losses.append(loss.item())\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss = self._shared_step(batch)\n",
    "        self._val_losses.append(loss.item())\n",
    "        return loss\n",
    "\n",
    "    # ---------- epoch end logging ----------\n",
    "    def on_train_epoch_end(self):\n",
    "        rank = get_context().get_world_rank()\n",
    "        if rank == 0:\n",
    "            train_avg = np.mean(self._train_losses)\n",
    "            val_avg   = np.mean(self._val_losses) if self._val_losses else None\n",
    "            if val_avg is not None:\n",
    "                print(f\"[Epoch {self.current_epoch}] train={train_avg:.4f}  val={val_avg:.4f}\")\n",
    "            else:\n",
    "                print(f\"[Epoch {self.current_epoch}] train={train_avg:.4f}  val=N/A\")\n",
    "\n",
    "            # Append to shared JSON so you can plot later\n",
    "            if os.path.exists(self.log_path):\n",
    "                with open(self.log_path, \"r\") as f: logs = json.load(f)\n",
    "            else:\n",
    "                logs = []\n",
    "            logs.append({\"epoch\": self.current_epoch+1, \"train_loss\": train_avg, \"val_loss\": val_avg})\n",
    "            with open(self.log_path, \"w\") as f: json.dump(logs, f)\n",
    "\n",
    "        # Clear per‑epoch trackers\n",
    "        self._train_losses.clear(); self._val_losses.clear()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=2e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Ray Train `train_loop` (checkpoint and resume)  \n",
    "Core training logic run **once per Ray worker**:  \n",
    "1. Shard-aware DataLoaders with `get_dataset_shard`.  \n",
    "2. Auto-resume from the latest Ray Checkpoint (if present).  \n",
    "3. Manual per-epoch checkpointing: save `model.pt` and `meta.pt`, then call `report(metrics, checkpoint=…)`.  \n",
    "This makes the run fully **fault-tolerant**. If a worker crashes, Ray restarts the group and re-enters the loop with the latest checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 09. Train loop for Ray TorchTrainer\n",
    "\n",
    "def train_loop(config):\n",
    "    \"\"\"Ray Train per-worker function with checkpointing and resume support.\"\"\"\n",
    "    import os, torch, uuid, json\n",
    "    from ray.train import get_checkpoint, get_context, report, Checkpoint\n",
    "\n",
    "    # Paths\n",
    "    LOG_PATH = \"/mnt/cluster_storage/generative_cv/epoch_metrics.json\"\n",
    "    CKPT_ROOT = \"/mnt/cluster_storage/generative_cv/food101_diffusion_ckpts\"\n",
    "\n",
    "    rank = get_context().get_world_rank()\n",
    "    if rank == 0:\n",
    "        os.makedirs(CKPT_ROOT, exist_ok=True)\n",
    "        if not get_checkpoint() and os.path.exists(LOG_PATH):\n",
    "            os.remove(LOG_PATH)\n",
    "\n",
    "    # Data\n",
    "    train_ds = ray.train.get_dataset_shard(\"train\")\n",
    "    val_ds   = ray.train.get_dataset_shard(\"val\")\n",
    "    train_loader = train_ds.iter_torch_batches(batch_size=32)\n",
    "    val_loader   = val_ds.iter_torch_batches(batch_size=32)\n",
    "\n",
    "    # Model\n",
    "    model = PixelDiffusion()\n",
    "    start_epoch = 0\n",
    "\n",
    "    # Resume from checkpoint if present\n",
    "    ckpt = get_checkpoint()\n",
    "    if ckpt:\n",
    "        with ckpt.as_directory() as d:\n",
    "            model.load_state_dict(torch.load(os.path.join(d, \"model.pt\"), map_location=\"cpu\"))\n",
    "            start_epoch = torch.load(os.path.join(d, \"meta.pt\")).get(\"epoch\", 0) + 1\n",
    "        if rank == 0:\n",
    "            print(f\"[Rank {rank}] Resumed from checkpoint at epoch {start_epoch}\")\n",
    "\n",
    "    # Trainer\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=config.get(\"epochs\", 10),\n",
    "        accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n",
    "        devices=1,\n",
    "        plugins=[RayLightningEnvironment()],\n",
    "        enable_progress_bar=False,\n",
    "        check_val_every_n_epoch=1,\n",
    "    )\n",
    "\n",
    "    # Train loop: run each epoch, checkpoint manually\n",
    "    for epoch in range(start_epoch, config.get(\"epochs\", 10)):\n",
    "        trainer.fit_loop.max_epochs = epoch + 1\n",
    "        trainer.fit_loop.current_epoch = epoch\n",
    "        trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
    "\n",
    "        if rank == 0:\n",
    "            # Save model checkpoint\n",
    "            out_dir = os.path.join(CKPT_ROOT, f\"epoch_{epoch}_{uuid.uuid4().hex}\")\n",
    "            os.makedirs(out_dir, exist_ok=True)\n",
    "            torch.save(model.state_dict(), os.path.join(out_dir, \"model.pt\"))\n",
    "            torch.save({\"epoch\": epoch}, os.path.join(out_dir, \"meta.pt\"))\n",
    "            ckpt_out = Checkpoint.from_directory(out_dir)\n",
    "        else:\n",
    "            ckpt_out = None\n",
    "\n",
    "        # Report with checkpoint so Ray saves it\n",
    "        report({\"epoch\": epoch}, checkpoint=ckpt_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Launch distributed Training with TorchTrainer  \n",
    "Ask for **8 GPU workers**, keep the five most-recent checkpoints, and allow up to three automatic retries.  \n",
    "`result.checkpoint` captures the checkpoint from the highest epoch (because you used `epoch` as the score attribute---you can change this to other metrics such as validation loss or training loss)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. Launch distributed training\n",
    "\n",
    "trainer = TorchTrainer(\n",
    "    train_loop,\n",
    "    scaling_config=ScalingConfig(num_workers=8, use_gpu=True),\n",
    "    datasets={\"train\": train_ds, \"val\": val_ds},\n",
    "    run_config=RunConfig(\n",
    "        name=\"food101_diffusion_ft\",\n",
    "        storage_path=\"/mnt/cluster_storage/generative_cv/food101_diffusion_results\",\n",
    "        checkpoint_config=CheckpointConfig(\n",
    "            checkpoint_frequency=1,\n",
    "            num_to_keep=5,\n",
    "            checkpoint_score_attribute=\"epoch\",\n",
    "            checkpoint_score_order=\"max\",\n",
    "        ),\n",
    "        failure_config=FailureConfig(max_failures=3),\n",
    "    ),\n",
    ")\n",
    "\n",
    "result = trainer.fit()\n",
    "print(\"Training complete →\", result.metrics)\n",
    "best_ckpt = result.checkpoint  # checkpoint from highest reported epoch (you can change score attr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Plot loss curves  \n",
    "Parse the JSON written by `PixelDiffusion.on_train_epoch_end`, convert to a DataFrame, and render train versus val MSE loss.  \n",
    "This is a good practice for quick health checks without external tooling.\n",
    "\n",
    "**Why is validation loss lower than training loss?**  \n",
    "You measure training loss *before* weights update and include fresh noise every step, while validation runs in `eval()` mode with no gradient updates, often making it slightly lower, especially early in training.  \n",
    "This is normal behavior in this sort of scenario and usually means the model is generalizing well, and not over-fitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11. Plot train/val loss curves\n",
    "\n",
    "LOG_PATH = \"/mnt/cluster_storage/generative_cv/epoch_metrics.json\"\n",
    "with open(LOG_PATH, \"r\") as f:\n",
    "    logs = json.load(f)\n",
    "\n",
    "df = pd.DataFrame(logs)\n",
    "df[\"val_loss\"] = pd.to_numeric(df[\"val_loss\"], errors=\"coerce\")\n",
    "\n",
    "plt.figure(figsize=(7,4))\n",
    "plt.plot(df[\"epoch\"], df[\"train_loss\"], marker=\"o\", label=\"Train\")\n",
    "plt.plot(df[\"epoch\"], df[\"val_loss\"],   marker=\"o\", label=\"Val\")\n",
    "plt.xlabel(\"Epoch\"); plt.ylabel(\"MSE Loss\"); plt.title(\"Pixel Diffusion - Loss per Epoch\")\n",
    "plt.grid(True); plt.legend(); plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Resume from latest checkpoint  \n",
    "Calling `trainer.fit()` again detects the run snapshot, loads the latest checkpoint, and (because `epochs=10`) exits immediately, proving that the resume path works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12. Run the trainer again to demonstrate resuming from latest checkpoint  \n",
    "\n",
    "result = trainer.fit()\n",
    "print(\"Training complete →\", result.metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Reverse diffusion sampler  \n",
    "A simple Euler-style loop that starts from Gaussian noise and iteratively subtracts the model’s predicted noise.  \n",
    "This isn't production-grade sampling, but it's suitable for illustrating inference after training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13. Reverse diffusion sampling\n",
    "\n",
    "def sample_image(model, steps=50, device=\"cpu\"):\n",
    "    \"\"\"Generate an image by iteratively de-noising random noise.\"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        img = torch.randn(1, 3, 224, 224, device=device)\n",
    "        for step in reversed(range(steps)):\n",
    "            t = torch.tensor([step], device=device)\n",
    "            pred_noise = model(img, t)\n",
    "            img = img - pred_noise * 0.1                      # simple Euler update\n",
    "        # Rescale back to [0,1]\n",
    "        img = torch.clamp((img * 0.5 + 0.5), 0.0, 1.0)\n",
    "        return img.squeeze(0).cpu().permute(1,2,0).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Generate and display samples from the **best checkpoint**  \n",
    "Load the model weights from `best_ckpt`, move to GPU if available, generate three images, and show them side-by-side.  \n",
    "Remember: with a tiny CNN and only 10 epochs, these samples look noise-like. If you replace the backbone or train longer, you expect to see better quality.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 14. Generate and display samples\n",
    "\n",
    "# Load model from Ray Train checkpoint\n",
    "from ray.train import Checkpoint\n",
    "\n",
    "assert best_ckpt is not None, \"Checkpoint is missing. Did training run and complete?\"\n",
    "\n",
    "with best_ckpt.as_directory() as ckpt_dir:\n",
    "    model = PixelDiffusion()\n",
    "    model.load_state_dict(torch.load(os.path.join(ckpt_dir, \"model.pt\"), map_location=\"cpu\"))\n",
    "\n",
    "model = model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Generate three images\n",
    "samples = [sample_image(model, steps=50, device=model.device) for _ in range(3)]\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=(9, 3))\n",
    "for ax, img in zip(axs, samples):\n",
    "    ax.imshow(img)\n",
    "    ax.axis(\"off\")\n",
    "plt.suptitle(\"Food‑101 Diffusion Samples (unconditional)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Clean up shared storage  \n",
    "Reclaim cluster disk space by deleting the entire tutorial output directory.  \n",
    "Run this only when you’re **sure** you don’t need the checkpoints or metrics anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15. Cleanup -- delete checkpoints and metrics from model training\n",
    "\n",
    "TARGET_PATH = \"/mnt/cluster_storage/generative_cv\"\n",
    "\n",
    "if os.path.exists(TARGET_PATH):\n",
    "    shutil.rmtree(TARGET_PATH)\n",
    "    print(f\"✅ Deleted everything under {TARGET_PATH}\")\n",
    "else:\n",
    "    print(f\"⚠️ Path does not exist: {TARGET_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrap up and next steps\n",
    "\n",
    "In this tutorial, you used **Ray Train and Ray Data on Anyscale** to scale a compact diffusion-policy workload, from raw JPEG bytes to distributed training and sampling, without changing the core PyTorch logic. You should now feel confident:\n",
    "\n",
    "* Using **Ray Data** to decode, normalize, and shard large image datasets in parallel  \n",
    "* Scaling training across multiple GPUs using **TorchTrainer** and a Ray-native `train_loop`  \n",
    "* Managing distributed training state with **Ray Checkpoints** and automatic resume  \n",
    "* Running fault-tolerant multi-node jobs on Anyscale without orchestration scripts  \n",
    "* Performing post-training sampling or evaluation using **Ray tasks** on GPU workers\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Where can you take this next?\n",
    "\n",
    "Below are a few directions you might explore to adapt or extend the pattern:\n",
    "\n",
    "1. **Backbones and architecture upgrades**  \n",
    "   * Swap in a larger ResNet or another vision model for much better generative performance.  \n",
    "   * Try pre-trained encoders and fine-tune only the diffusion-specific layers.\n",
    "\n",
    "2. **Conditional diffusion**  \n",
    "   * Use the `label` column to condition the model (for example, class-conditioning).  \n",
    "   * Compare unconditional versus conditional generation side by side.\n",
    "\n",
    "3. **Sampling improvements**  \n",
    "   * Replace naive reverse diffusion with De-noising Diffusion Implicit Models (DDIM), Pseudo Numerical Methods for Diffusion Models (PNDM), or learned de-noisers.  \n",
    "   * Add timestep embeddings or noise schedules to increase model expressiveness.\n",
    "\n",
    "4. **Longer training and mixed precision**  \n",
    "   * Increase the `max_epochs` and enable Automatic Mixed Precision (AMP) for faster training with less memory.  \n",
    "   * Visualize convergence and training stability across longer runs.\n",
    "\n",
    "5. **Hyperparameter sweeps**  \n",
    "   * Use **Ray Tune** to search over learning rates, model size, or sampling steps.  \n",
    "   * Leverage Tune’s reporting to schedule early stopping or checkpoint pruning.\n",
    "\n",
    "6. **Data handling and scaling**  \n",
    "   * Shard the dataset into multiple Parquet files and distribute across more workers.  \n",
    "   * Store and load datasets from S3 or other cloud storage.\n",
    "\n",
    "7. **Image quality evaluation**  \n",
    "   * Log Fréchet Inception Distance (FID) scores, perceptual similarity, or diffusion-specific metrics.  \n",
    "   * Compare generated samples from different checkpoints or backbones.\n",
    "\n",
    "8. **Model serving**  \n",
    "   * Package the reverse sampler into a Ray task or **Ray Serve** endpoint.  \n",
    "   * Run a demo app that generates images on demand from a class name or random seed.\n",
    "\n",
    "9. **End-to-end MLOps**  \n",
    "   * Register the best checkpoint with MLflow or Weights & Biases.  \n",
    "   * Wrap the training loop in a Ray Job and run it on a schedule with Anyscale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
