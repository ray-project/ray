{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04e Recommendation system pattern with Ray Train \n",
    "This notebook builds a **scalable matrix factorization recommendation system** using the **MovieLens 100K** dataset, fully distributed on an Anyscale cluster with **Ray Train V2** and **Ray Data**.\n",
    "\n",
    "## What you learn and take away  \n",
    "* How to use **Ray Data** to load, encode, and shard tabular datasets across many workers  \n",
    "* How to **stream training data** directly into PyTorch using `iter_torch_batches()`  \n",
    "* How to build a **custom training loop with validation and checkpointing** using `ray.train.report()`  \n",
    "* How to use **Ray Train V2's fault-tolerant trainer** to resume training from the latest checkpoint with no extra logic  \n",
    "* How to separate **training, evaluation, and inference** while keeping all code modular and distributed-ready  \n",
    "* How to run real-world recommendation workloads with **no changes to your model code**, using Ray’s orchestration  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What problem are you solving? (matrix factorization for recommendations)\n",
    "\n",
    "Build a **collaborative filtering recommendation system** that predicts how much a user likes an item  \n",
    "based on **historical interaction data**—in this case, user ratings from the MovieLens 100K dataset.\n",
    "\n",
    "Use **matrix factorization**, a classic yet scalable approach where you embed each user and item in a latent space.  \n",
    "The model learns to represent users and items as vectors and predicts ratings by computing their dot product.\n",
    "\n",
    "---\n",
    "\n",
    "## Input: user–item–rating triples\n",
    "\n",
    "Each row in the dataset represents a user’s explicit rating of a movie:\n",
    "\n",
    "$$\n",
    "(u, {i}, r) \\in \\{\\text{users}\\} \\times \\{\\text{items}\\} \\times \\{1, 2, 3, 4, 5\\}\n",
    "$$\n",
    "\n",
    "Encode these using contiguous integer indices (`user_idx`, `item_idx`)  \n",
    "and normalize them for efficient embedding lookup and training.\n",
    "\n",
    "---\n",
    "\n",
    "## Model: embedding-based matrix factorization\n",
    "\n",
    "Learn an embedding vector for each user and each item:\n",
    "\n",
    "$$\n",
    "U_{u} \\in \\mathbb{R}^d, \\quad V_{i} \\in \\mathbb{R}^d\n",
    "$$\n",
    "\n",
    "The predicted rating is the dot product of these vectors:\n",
    "\n",
    "$$\n",
    "\\hat{r}_{u,{i}} = U_{u}^\\top V_{i}\n",
    "$$\n",
    "\n",
    "The embedding dimension $d$ controls model capacity.\n",
    "\n",
    "---\n",
    "\n",
    "## Training objective\n",
    "\n",
    "Minimize **Mean Squared Error (MSE)** between predicted and actual ratings:\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = \\mathbb{E}_{(u, {i}, r)}\\ \\big(\\hat{r}_{u,{i}} - r\\big)^2\n",
    "$$\n",
    "\n",
    "This encourages the model to assign higher scores to user–item pairs that historically received high ratings.\n",
    "\n",
    "---\n",
    "\n",
    "## Inference: ranking items per user\n",
    "\n",
    "Once the model is trained, you can recommend items by computing predicted scores for a target user  \n",
    "against **all items in the catalog**:\n",
    "\n",
    "$$\n",
    "\\hat{r}_{u, *} = U_{u}^\\top V^\\top\n",
    "$$\n",
    "\n",
    "Sort these scores and return the top-N items as personalized recommendations.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to migrate this recommendation system workload to a distributed setup using Ray on Anyscale\n",
    "\n",
    "This tutorial **migrates a local matrix factorization pipeline for recommendation into a distributed, fault-tolerant training loop using Ray Train and Ray Data on Anyscale**.\n",
    "\n",
    "Approach the transition with the following steps:\n",
    "\n",
    "1. **Convert Pandas DataFrame to sharded Ray Dataset**  \n",
    "   Load MovieLens 100K as a pandas DataFrame, encode the IDs, and use `ray.data.from_pandas_refs()` to create a **multi-block Ray Dataset**. Each block is a training shard that Ray can distribute across workers.\n",
    "\n",
    "2. **Stream Torch data loaders**  \n",
    "   Instead of manually writing PyTorch `Dataset` logic, use `iter_torch_batches()` from **Ray Data** to stream batches directly into each worker. Ray handles all the parallelism and sharding behind the scenes.\n",
    "\n",
    "3. **Convert a single-node PyTorch process to a multi-GPU distributed training**  \n",
    "   Write a minimal `train_loop_per_worker` that runs on each Ray worker. Using `TorchTrainer` and `prepare_model()`, scale this loop across 8 GPU workers automatically, where each works on its own data shard.\n",
    "\n",
    "4. **Configure structured epoch logging and checkpoints**  \n",
    "   Each epoch logs `train_loss` and `val_loss` to a shared JSON file, and report checkpoints with `ray.train.report(checkpoint=...)`. This enables **automatic recovery and metric tracking** without any additional code.\n",
    "\n",
    "5. **Declaratively configure tolerance, checkpointing and scaling**  \n",
    "   Configure fault tolerance, checkpointing, and scaling using `ScalingConfig`, `CheckpointConfig`, and `FailureConfig`. This lets Ray and Anyscale handle retries, recovery, and GPU orchestration.\n",
    "\n",
    "6. **Write lightweight Python functions for post- inferfence**  \n",
    "   After training, load the latest checkpoint and generate top-N recommendations for any user with a simple forward pass. No retraining, no re-initialization, just pure PyTorch inference.\n",
    "\n",
    "With just a few changes to your core code, scale a traditional recommendation pipeline across a Ray cluster with **distributed data loading, checkpointing, fault tolerance, and parallel training**, all fully managed by Anyscale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports  \n",
    "Start by importing all the libraries you need for the rest of the notebook. These include standard utilities like `os`, `json`, and `pandas`, as well as deep learning libraries like PyTorch and visualization tools like `matplotlib`.\n",
    "\n",
    "Also, import everything needed for **distributed training and data processing with Ray**:\n",
    "- `ray` and `ray.data` provide the high-level distributed data API.\n",
    "- `ray.train` gives you `TorchTrainer`, `ScalingConfig`, checkpointing, and metrics reporting.\n",
    "- `prepare_model` wraps your PyTorch model for multi-worker training with Distributed Data Parallel (DDP).\n",
    "\n",
    "A few extra helpers like `tqdm` and `train_test_split` round out the list for progress bars and quick offline preprocessing.\n",
    "\n",
    "This notebook assumes Ray is already running (for example, with Anyscale), so you don’t call `ray.init()` manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 00. Runtime setup — install same deps as build.sh and set env vars\n",
    "import os, sys, subprocess\n",
    "\n",
    "# Non-secret env var \n",
    "os.environ[\"RAY_TRAIN_V2_ENABLED\"] = \"1\"\n",
    "\n",
    "# Install Python dependencies \n",
    "subprocess.check_call([\n",
    "    sys.executable, \"-m\", \"pip\", \"install\", \"--no-cache-dir\",\n",
    "    \"torch==2.8.0\",\n",
    "    \"matplotlib==3.10.6\",\n",
    "    \"pyarrow==14.0.2\",\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 01. Imports\n",
    "\n",
    "# Standard libraries\n",
    "import os\n",
    "import uuid\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import zipfile\n",
    "import shutil\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Ray\n",
    "import ray\n",
    "import ray.data\n",
    "from ray.train import ScalingConfig, RunConfig, CheckpointConfig, FailureConfig, Checkpoint, get_checkpoint, get_context,  get_dataset_shard, report\n",
    "from ray.train.torch import TorchTrainer, prepare_model\n",
    "\n",
    "# Other\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load MovieLens 100K dataset  \n",
    "Download and extract the [MovieLens 100K](https://grouplens.org/datasets/movielens/100k/) dataset and persist a cleaned version to cluster storage under `/mnt/cluster_storage/rec_sys_tutorial/raw/ratings.csv`.\n",
    "\n",
    "The MovieLens 100K dataset contains 100,000 ratings across 943 users and 1,682 movies. It’s small enough to train quickly, but realistic enough to demonstrate scaling and checkpointing with Ray Train.\n",
    "\n",
    "If you already downloaded and extracted the dataset, skip both steps to save time. The output is a CSV with four columns: `user_id`, `item_id`, `rating`, and `timestamp`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 02. Load MovieLens 100K Dataset and store in /mnt/cluster_storage/\n",
    "\n",
    "# Define clean working paths\n",
    "DATA_URL = \"http://files.grouplens.org/datasets/movielens/ml-100k.zip\"\n",
    "LOCAL_ZIP = \"/mnt/cluster_storage/rec_sys_tutorial/ml-100k.zip\"\n",
    "EXTRACT_DIR = \"/mnt/cluster_storage/rec_sys_tutorial/ml-100k\"\n",
    "OUTPUT_CSV = \"/mnt/cluster_storage/rec_sys_tutorial/raw/ratings.csv\"\n",
    "\n",
    "# Ensure target directories exist\n",
    "os.makedirs(\"/mnt/cluster_storage/rec_sys_tutorial/raw\", exist_ok=True)\n",
    "\n",
    "# Download only if not already done\n",
    "if not os.path.exists(LOCAL_ZIP):\n",
    "    !wget -q $DATA_URL -O $LOCAL_ZIP\n",
    "\n",
    "# Extract cleanly\n",
    "if not os.path.exists(EXTRACT_DIR):\n",
    "    with zipfile.ZipFile(LOCAL_ZIP, 'r') as zip_ref:\n",
    "        zip_ref.extractall(\"/mnt/cluster_storage/rec_sys_tutorial\")\n",
    "\n",
    "# Load raw file\n",
    "raw_path = os.path.join(EXTRACT_DIR, \"u.data\")\n",
    "df = pd.read_csv(raw_path, sep=\"\\t\", names=[\"user_id\", \"item_id\", \"rating\", \"timestamp\"])\n",
    "\n",
    "# Save cleaned version\n",
    "df.to_csv(OUTPUT_CSV, index=False)\n",
    "\n",
    "print(f\"✅ Loaded {len(df):,} ratings → {OUTPUT_CSV}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preprocess IDs and create Ray Dataset  \n",
    "Begin preprocessing by encoding `user_id` and `item_id` into contiguous integer indices required for embedding layers. The model uses these encoded columns—`user_idx` and `item_idx`—during training.\n",
    "\n",
    "After encoding, drop the original IDs and split the dataset into 64 chunks. Serialize each chunk and push to Ray’s object store using `ray.put(...)`. This allows Ray Data to construct a distributed dataset in the next step without creating a bottleneck on a single worker or process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 03. Preprocess IDs and create Ray Dataset in parallel\n",
    "\n",
    "# Load CSV\n",
    "df = pd.read_csv(\"/mnt/cluster_storage/rec_sys_tutorial/raw/ratings.csv\")\n",
    "\n",
    "# Encode user_id and item_id\n",
    "user2idx = {uid: j for j, uid in enumerate(sorted(df[\"user_id\"].unique()))}\n",
    "item2idx = {iid: j for j, iid in enumerate(sorted(df[\"item_id\"].unique()))}\n",
    "\n",
    "df[\"user_idx\"] = df[\"user_id\"].map(user2idx)\n",
    "df[\"item_idx\"] = df[\"item_id\"].map(item2idx)\n",
    "df = df[[\"user_idx\", \"item_idx\", \"rating\", \"timestamp\"]]\n",
    "\n",
    "# Split into multiple chunks for parallel ingestion\n",
    "NUM_SPLITS = 64  # adjust based on cluster size\n",
    "dfs = np.array_split(df, NUM_SPLITS)\n",
    "object_refs = [ray.put(split) for split in dfs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualize dataset: ratings, users, and items  \n",
    "Before training, visualize the distribution of ratings, user activity, and item popularity. These quick checks help you verify that the dataset parses correctly and reveal useful patterns:\n",
    "\n",
    "- The first plot shows the overall rating distribution (1–5 stars). As expected, you see a skew toward 4 and 5.\n",
    "- The second plot shows how many ratings each user has submitted. There’s a long tail: a few power users, but many light users.\n",
    "- The third plot shows how often users rated each item. Again, you see a long-tail distribution common in recommendation settings.\n",
    "\n",
    "These histograms give you a sense of sparsity and coverage, both of which influence model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 04. Visualize dataset: ratings, user and item activity\n",
    "\n",
    "# Plot rating distribution\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "df[\"rating\"].hist(bins=[0.5,1.5,2.5,3.5,4.5,5.5], edgecolor='black')\n",
    "plt.title(\"Rating Distribution\")\n",
    "plt.xlabel(\"Rating\"); plt.ylabel(\"Frequency\")\n",
    "\n",
    "# Plot number of ratings per user\n",
    "plt.subplot(1, 3, 2)\n",
    "df[\"user_idx\"].value_counts().hist(bins=30, edgecolor='black')\n",
    "plt.title(\"Ratings per User\")\n",
    "plt.xlabel(\"# Ratings\"); plt.ylabel(\"Users\")\n",
    "\n",
    "# Plot number of ratings per item\n",
    "plt.subplot(1, 3, 3)\n",
    "df[\"item_idx\"].value_counts().hist(bins=30, edgecolor='black')\n",
    "plt.title(\"Ratings per Item\")\n",
    "plt.xlabel(\"# Ratings\"); plt.ylabel(\"Items\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create Ray Dataset from encoded chunks  \n",
    "Now, convert your list of encoded pandas chunks into a Ray Dataset using `from_pandas_refs(...)`. This method ensures that each chunk becomes its own block, enabling parallel data processing across the cluster.\n",
    "\n",
    "The result is a distributed Ray Dataset with one block per chunk, which is ideal for streaming batches during training. Confirm the number of blocks and show a few rows to verify the format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 05. Create Ray Dataset from refs (uses multiple blocks or workers)\n",
    "\n",
    "ratings_ds = ray.data.from_pandas_refs(object_refs)\n",
    "print(\"✅ Ray Dataset created with\", ratings_ds.num_blocks(), \"blocks\")\n",
    "ratings_ds.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train/validation split using Ray Data  \n",
    "Next, split the dataset into training and validation sets. First, shuffle the entire Ray Dataset to ensure randomization, then split by row index, using 80% for training and 20% for validation.\n",
    "\n",
    "This approach is simple and scalable: Ray handles the shuffling and slicing in parallel across blocks. Also, set a fixed seed to ensure the split is reproducible. After you split it, each dataset remains a fully distributed Ray Dataset, ready to stream into workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 06. Train/val split using Ray Data\n",
    "\n",
    "# Parameters\n",
    "TRAIN_FRAC = 0.8\n",
    "SEED = 42  # for reproducibility\n",
    "\n",
    "total_rows = ratings_ds.count()\n",
    "train_size = int(total_rows * TRAIN_FRAC)\n",
    "\n",
    "ratings_ds = ratings_ds.random_shuffle(seed=SEED)\n",
    "train_ds, val_ds = ratings_ds.split_at_indices([train_size])\n",
    "\n",
    "print(f\"✅ Train/Val Split:\")\n",
    "print(f\"  Train → {train_ds.count():,} rows\")\n",
    "print(f\"  Val   → {val_ds.count():,} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Define matrix factorization model  \n",
    "Define a simple but effective matrix factorization model using PyTorch. A learned embedding vector represents each user and item. The model predicts a rating by taking the dot product of the corresponding user and item embeddings.\n",
    "\n",
    "This architecture is commonly used in collaborative filtering and serves as a strong baseline for recommendation tasks. It's also well-suited for scaling with Ray Train and DistributedDataParallel (DDP)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 07. Define matrix factorization model\n",
    "\n",
    "class MatrixFactorizationModel(nn.Module):\n",
    "    def __init__(self, num_users: int, num_items: int, embedding_dim: int = 64):\n",
    "        super().__init__()\n",
    "        self.user_embedding = nn.Embedding(num_users, embedding_dim)\n",
    "        self.item_embedding = nn.Embedding(num_items, embedding_dim)\n",
    "\n",
    "    def forward(self, user_idx, item_idx):\n",
    "        user_vecs = self.user_embedding(user_idx)\n",
    "        item_vecs = self.item_embedding(item_idx)\n",
    "        dot_product = (user_vecs * item_vecs).sum(dim=1)\n",
    "        return dot_product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Define Ray Train loop (with validation, logging, and checkpointing)  \n",
    "Define the `train_loop_per_worker`, the core function Ray executes on each worker. This loop handles everything from dataset loading and model training to validation, logging, and checkpointing.\n",
    "\n",
    "Each worker receives its own shard of the training and validation sets with `get_dataset_shard`. Use `iter_torch_batches` to stream batches directly into PyTorch.\n",
    "\n",
    "For each epoch:\n",
    "- Compute the average training loss across all batches.\n",
    "- Evaluate the model on the validation set and compute the average validation loss.\n",
    "- On rank 0 only, save a checkpoint (model weights and epoch metadata) and append the losses to a shared JSON log.\n",
    "\n",
    "Finally, call `ray.train.report` to expose metrics and checkpoints to the Ray controller. This enables fault tolerance, auto-resume, and metrics tracking with zero additional setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 08. Define Ray Train loop (with val Loss and checkpointing and logging)\n",
    "\n",
    "def train_loop_per_worker(config):\n",
    "\n",
    "    # Get dataset shards for this worker\n",
    "    train_ds = get_dataset_shard(\"train\")\n",
    "    val_ds   = get_dataset_shard(\"val\")\n",
    "    train_loader = train_ds.iter_torch_batches(batch_size=512, dtypes=torch.float32)\n",
    "    val_loader   = val_ds.iter_torch_batches(batch_size=512, dtypes=torch.float32)\n",
    "\n",
    "    # Create model and optimizer\n",
    "    model = MatrixFactorizationModel(\n",
    "        num_users=config[\"num_users\"],\n",
    "        num_items=config[\"num_items\"],\n",
    "        embedding_dim=config.get(\"embedding_dim\", 64)\n",
    "    )\n",
    "    model = prepare_model(model)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config.get(\"lr\", 1e-3))\n",
    "\n",
    "    # Paths for checkpointing and logging\n",
    "    CKPT_DIR = \"/mnt/cluster_storage/rec_sys_tutorial/checkpoints\"\n",
    "    LOG_PATH = \"/mnt/cluster_storage/rec_sys_tutorial/epoch_metrics.json\"\n",
    "    os.makedirs(CKPT_DIR, exist_ok=True)\n",
    "    rank = int(os.environ.get(\"RANK\", \"0\"))  # Worker rank\n",
    "    start_epoch = 0\n",
    "\n",
    "    # Resume from checkpoint if available\n",
    "    ckpt = get_checkpoint()\n",
    "    if ckpt:\n",
    "        with ckpt.as_directory() as ckpt_dir:\n",
    "            model.load_state_dict(torch.load(os.path.join(ckpt_dir, \"model.pt\"), map_location=\"cpu\"))\n",
    "            start_epoch = torch.load(os.path.join(ckpt_dir, \"meta.pt\")).get(\"epoch\", 0) + 1\n",
    "        if rank == 0:\n",
    "            print(f\"[Rank {rank}] ✅ Resumed from checkpoint at epoch {start_epoch}\")\n",
    "\n",
    "    # Clean up log file on first run (only if not resuming)\n",
    "    if rank == 0 and start_epoch == 0 and os.path.exists(LOG_PATH):\n",
    "        os.remove(LOG_PATH)\n",
    "\n",
    "    # ----------------- Training Loop ----------------- #\n",
    "    for epoch in range(start_epoch, config.get(\"epochs\", 5)):\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "\n",
    "        # Train over each batch\n",
    "        for batch in train_loader:\n",
    "            user = batch[\"user_idx\"].long()\n",
    "            item = batch[\"item_idx\"].long()\n",
    "            rating = batch[\"rating\"].float()\n",
    "\n",
    "            pred = model(user, item)\n",
    "            loss = F.mse_loss(pred, rating)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_losses.append(loss.item())\n",
    "\n",
    "        avg_train_loss = sum(train_losses) / len(train_losses)\n",
    "\n",
    "        # ---------- Validation Pass ----------\n",
    "        model.eval()\n",
    "        val_losses = []\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                user = batch[\"user_idx\"].long()\n",
    "                item = batch[\"item_idx\"].long()\n",
    "                rating = batch[\"rating\"].float()\n",
    "\n",
    "                pred = model(user, item)\n",
    "                loss = F.mse_loss(pred, rating)\n",
    "                val_losses.append(loss.item())\n",
    "\n",
    "        avg_val_loss = sum(val_losses) / len(val_losses)\n",
    "\n",
    "        # Log to stdout\n",
    "        print(f\"[Epoch {epoch}] Train MSE: {avg_train_loss:.4f} | Val MSE: {avg_val_loss:.4f}\")\n",
    "\n",
    "        # ---------- Save Checkpoint (Rank 0 Only) ----------\n",
    "        if rank == 0:\n",
    "            out_dir = os.path.join(CKPT_DIR, f\"epoch_{epoch}_{uuid.uuid4().hex}\")\n",
    "            os.makedirs(out_dir, exist_ok=True)\n",
    "            torch.save(model.state_dict(), os.path.join(out_dir, \"model.pt\"))\n",
    "            torch.save({\"epoch\": epoch}, os.path.join(out_dir, \"meta.pt\"))\n",
    "            ckpt_out = Checkpoint.from_directory(out_dir)\n",
    "        else:\n",
    "            ckpt_out = None\n",
    "\n",
    "        # ---------- Append Metrics to JSON Log (Rank 0) ----------\n",
    "        if rank == 0:\n",
    "            logs = []\n",
    "            if os.path.exists(LOG_PATH):\n",
    "                try:\n",
    "                    with open(LOG_PATH, \"r\") as f:\n",
    "                        logs = json.load(f)\n",
    "                except json.JSONDecodeError:\n",
    "                    print(\"⚠️  JSON log unreadable. Starting fresh.\")\n",
    "                    logs = []\n",
    "\n",
    "            logs.append({\n",
    "                \"epoch\": epoch,\n",
    "                \"train_loss\": avg_train_loss,\n",
    "                \"val_loss\": avg_val_loss\n",
    "            })\n",
    "            with open(LOG_PATH, \"w\") as f:\n",
    "                json.dump(logs, f)\n",
    "\n",
    "        # ---------- Report to Ray Train ----------\n",
    "        report({\n",
    "            \"epoch\": epoch,\n",
    "            \"train_loss\": avg_train_loss,\n",
    "            \"val_loss\": avg_val_loss\n",
    "        }, checkpoint=ckpt_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Launch distributed training with Ray Train  \n",
    "Now, launch distributed training using `TorchTrainer`, Ray Train’s high-level orchestration interface. Provide it with:\n",
    "\n",
    "- Your custom `train_loop_per_worker` function\n",
    "- A `train_config` dictionary that specifies model dimensions, learning rate, and number of epochs\n",
    "- The sharded `train` and `val` Ray Datasets\n",
    "- A `ScalingConfig` that sets the number of workers and GPU usage\n",
    "\n",
    "Also, configure checkpointing and fault tolerance:\n",
    "- Ray keeps the 3 most recent checkpoints\n",
    "- Failed workers retry up to two times\n",
    "\n",
    "Calling `trainer.fit()` kicks off training across the cluster. If any workers fail or disconnect, Ray restarts them and resume from the latest checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 09. Launch distributed training with Ray TorchTrainer\n",
    "\n",
    "# Define config params\n",
    "train_config = {\n",
    "    \"num_users\": df[\"user_idx\"].nunique(),\n",
    "    \"num_items\": df[\"item_idx\"].nunique(),\n",
    "    \"embedding_dim\": 64,\n",
    "    \"lr\": 1e-3,\n",
    "    \"epochs\": 20,\n",
    "}\n",
    "\n",
    "trainer = TorchTrainer(\n",
    "    train_loop_per_worker=train_loop_per_worker,\n",
    "    train_loop_config=train_config,\n",
    "    scaling_config=ScalingConfig(\n",
    "        num_workers=8,       # Increase as needed\n",
    "        use_gpu=True         # Set to True if training on GPUs\n",
    "    ),\n",
    "    datasets={\"train\": train_ds, \"val\": val_ds},\n",
    "    run_config=RunConfig(\n",
    "        name=\"mf_ray_train\",\n",
    "        storage_path=\"/mnt/cluster_storage/rec_sys_tutorial/results\",\n",
    "        checkpoint_config=CheckpointConfig(num_to_keep=3),\n",
    "        failure_config=FailureConfig(max_failures=2)\n",
    "    )\n",
    ")\n",
    "\n",
    "# Run distributed training\n",
    "result = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Plot train and validation loss curves  \n",
    "After training, load the logged epoch metrics from the shared JSON file and plot the train and validation loss curves.\n",
    "\n",
    "This visualization helps you evaluate model behavior across epochs, whether it’s under-fitting, over-fitting, or converging steadily. You compute both curves using MSE (Mean Squared Error), which is the same loss function used during training.\n",
    "\n",
    "Plotting these curves also serves as a sanity check to ensure that checkpointing, logging, and training progressed as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. Plot train/val loss curves\n",
    "\n",
    "# Path to training metrics log\n",
    "LOG_PATH = \"/mnt/cluster_storage/rec_sys_tutorial/epoch_metrics.json\"\n",
    "\n",
    "# Load and convert to DataFrame\n",
    "with open(LOG_PATH, \"r\") as f:\n",
    "    logs = json.load(f)\n",
    "\n",
    "df = pd.DataFrame(logs)\n",
    "df[\"train_loss\"] = pd.to_numeric(df[\"train_loss\"], errors=\"coerce\")\n",
    "df[\"val_loss\"] = pd.to_numeric(df[\"val_loss\"], errors=\"coerce\")\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(7, 4))\n",
    "plt.plot(df[\"epoch\"], df[\"train_loss\"], marker=\"o\", label=\"Train\")\n",
    "plt.plot(df[\"epoch\"], df[\"val_loss\"], marker=\"o\", label=\"Val\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"MSE Loss\")\n",
    "plt.title(\"Matrix Factorization - Loss per Epoch\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Resume training from checkpoint  \n",
    "Run `trainer.fit()` again to resume training from the most recent checkpoint. Since `TorchTrainer` was originally configured with `resume_from_checkpoint=True` and a persistent `storage_path`, Ray automatically restores the latest saved model state and continues training from the correct epoch.\n",
    "\n",
    "This demonstrates Ray Train’s built-in support for fault tolerance and iterative experimentation, allowing training to pick up exactly where it left off without manual intervention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11. Run trainer.fit() again to resume from last checkpoint\n",
    "\n",
    "result = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Inference: recommend top-N items for a user  \n",
    "To demonstrate inference, generate top-10 item recommendations for a randomly selected user.\n",
    "\n",
    "First, reload the original `ratings.csv` and rebuild the user and item ID mappings used during training. Then, load the latest model checkpoint and restore the trained embedding weights. If you trained the model with DDP, strip the `'module.'` prefix from checkpoint keys.\n",
    "\n",
    "Next, select a user, compute their embedding, and take the dot product against all item embeddings to produce predicted scores. Finally, extract the top-N items with the highest scores and print their IDs and associated scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12. Inference: recommend top-N items for a user\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Step 1: Reload original ratings CSV and mappings\n",
    "# ---------------------------------------------\n",
    "df = pd.read_csv(\"/mnt/cluster_storage/rec_sys_tutorial/raw/ratings.csv\")\n",
    "\n",
    "# Recompute ID mappings (same as during preprocessing)\n",
    "unique_users = sorted(df[\"user_id\"].unique())\n",
    "unique_items = sorted(df[\"item_id\"].unique())\n",
    "\n",
    "user2idx = {uid: j for j, uid in enumerate(unique_users)}\n",
    "item2idx = {iid: j for j, iid in enumerate(unique_items)}\n",
    "idx2item = {v: k for k, v in item2idx.items()}\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Step 2: Load model from checkpoint\n",
    "# ---------------------------------------------\n",
    "model = MatrixFactorizationModel(\n",
    "    num_users=len(user2idx),\n",
    "    num_items=len(item2idx),\n",
    "    embedding_dim=train_config[\"embedding_dim\"]\n",
    ")\n",
    "\n",
    "with result.checkpoint.as_directory() as ckpt_dir:\n",
    "    state_dict = torch.load(os.path.join(ckpt_dir, \"model.pt\"), map_location=\"cpu\")\n",
    "\n",
    "    # Remove 'module.' prefix if using DDP-trained model\n",
    "    if any(k.startswith(\"module.\") for k in state_dict):\n",
    "        state_dict = {k.replace(\"module.\", \"\"): v for k, v in state_dict.items()}\n",
    "\n",
    "    model.load_state_dict(state_dict)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Step 3: Select a user and generate recommendations\n",
    "# ---------------------------------------------\n",
    "# Choose a random user from the original dataset\n",
    "original_user_id = df[\"user_id\"].sample(1).iloc[0]\n",
    "user_idx = user2idx[original_user_id]\n",
    "\n",
    "print(f\"Generating recommendations for user_id={original_user_id} (internal idx={user_idx})\")\n",
    "\n",
    "# Compute scores for all items for this user\n",
    "with torch.no_grad():\n",
    "    user_vector = model.user_embedding(torch.tensor([user_idx]))           # [1, D]\n",
    "    item_vectors = model.item_embedding.weight                             # [num_items, D]\n",
    "    scores = torch.matmul(user_vector, item_vectors.T).squeeze(0)          # [num_items]\n",
    "\n",
    "    topk = torch.topk(scores, k=10)\n",
    "    top_item_ids = [idx2item[j.item()] for j in topk.indices]\n",
    "    top_scores = topk.values.tolist()\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Step 4: Print top-N recommendations\n",
    "# ---------------------------------------------\n",
    "print(\"\\nTop 10 Recommended Item IDs:\")\n",
    "for i, (item_id, score) in enumerate(zip(top_item_ids, top_scores), 1):\n",
    "    print(f\"{i:2d}. Item ID: {item_id} | Score: {score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Join top-N item IDs with movie titles  \n",
    "To make your recommendations more interpretable, join the top-10 recommended `item_id`s with movie titles from the original `u.item` metadata file.\n",
    "\n",
    "Load only the relevant columns—`item_id` and `title`—from `u.item`, then merge them with the top-N predictions you computed in the previous step. The result is a user-friendly list of movie titles with associated predicted scores, rather than raw item IDs.\n",
    "\n",
    "This small addition makes the model outputs easier to understand and more useful for downstream applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13. Join top-N item IDs with movie titles from u.item\n",
    "\n",
    "item_metadata = pd.read_csv(\n",
    "    \"/mnt/cluster_storage/rec_sys_tutorial/ml-100k/u.item\",\n",
    "    sep=\"|\",\n",
    "    encoding=\"latin-1\",\n",
    "    header=None,\n",
    "    usecols=[0, 1],  # Only item_id and title\n",
    "    names=[\"item_id\", \"title\"]\n",
    ")\n",
    "\n",
    "# Join with top-N items\n",
    "top_items_df = pd.DataFrame({\n",
    "    \"item_id\": top_item_ids,\n",
    "    \"score\": top_scores\n",
    "})\n",
    "\n",
    "merged = top_items_df.merge(item_metadata, on=\"item_id\", how=\"left\")\n",
    "\n",
    "print(\"\\nTop 10 Recommended Movies:\")\n",
    "for j, row in merged.iterrows():\n",
    "    print(f\"{j+1:2d}. {row['title']} | Score: {row['score']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Clean up shared storage  \n",
    "Reclaim cluster disk space by deleting the entire tutorial output directory.  \n",
    "Run this only when you’re **sure** you don’t need the checkpoints or metrics anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 14. Cleanup -- delete checkpoints and metrics from model training\n",
    "\n",
    "TARGET_PATH = \"/mnt/cluster_storage/rec_sys_tutorial\"\n",
    "\n",
    "if os.path.exists(TARGET_PATH):\n",
    "    shutil.rmtree(TARGET_PATH)\n",
    "    print(f\"✅ Deleted everything under {TARGET_PATH}\")\n",
    "else:\n",
    "    print(f\"⚠️ Path does not exist: {TARGET_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrap up and next steps\n",
    "\n",
    "In this tutorial, you used **Ray Train and Ray Data on Anyscale** to scale a full matrix factorization recommendation system, end-to-end, from a raw CSV to multi-GPU distributed training and personalized top-N item recommendations.\n",
    "\n",
    "You should now feel confident:\n",
    "\n",
    "* Using **Ray Data** to preprocess, encode, and shard large tabular datasets  \n",
    "* Streaming data into PyTorch with `iter_torch_batches()` for efficient training  \n",
    "* Scaling matrix factorization across multiple GPUs with **Ray Train’s `TorchTrainer`**  \n",
    "* Saving and resuming training with distributed **Ray Checkpoints**  \n",
    "* Running multi-node, fault-tolerant jobs without touching orchestration code  \n",
    "* Performing post-training inference using Ray-restored model checkpoints and learned user and item embeddings\n",
    "\n",
    "---\n",
    "\n",
    "### Where can you take this next?\n",
    "\n",
    "The following are a few directions you can explore to extend or adapt this workload:\n",
    "\n",
    "1. **Ranking metrics and evaluation**  \n",
    "   * Add metrics like **Root Mean Squared Error (RMSE)**, **Normalized Discounted Cumulative Gain (NDCG)**, or **Hit@K** to evaluate recommendation quality.  \n",
    "   * Filter out already-rated items during inference to measure novelty.\n",
    "\n",
    "2. **Two-tower and deep models**  \n",
    "   * Replace dot product with a **two-tower neural model** or a **deep MLP**.  \n",
    "   * Add side features (for example, timestamp, genre) into each tower for better personalization.\n",
    "\n",
    "3. **Recommendation personalization**  \n",
    "   * Store and cache user embeddings after training.  \n",
    "   * Run lightweight inference tasks to generate recommendations in real-time.\n",
    "\n",
    "4. **Content-based or hybrid models**  \n",
    "   * Join movie metadata (genres, tags) and build a hybrid collaborative–content model.  \n",
    "   * Embed titles or genres using pre-trained language models.\n",
    "\n",
    "5. **Hyperparameter optimization**  \n",
    "   * Use **Ray Tune** to sweep embedding sizes, learning rates, or regularization.  \n",
    "   * Track performance over epochs and checkpoint the best models automatically.\n",
    "\n",
    "6. **Data scaling**  \n",
    "   * Switch from MovieLens 100K to 1M or 10M as Ray Data handles it seamlessly.  \n",
    "   * Save and load from cloud object storage (S3, GCS) for real-world deployments.\n",
    "\n",
    "7. **Production inference**  \n",
    "   * Wrap the recommendation system into a **Ray Serve** endpoint for serving top-N results.  \n",
    "   * Build a simple demo that recommends movies to live users.\n",
    "\n",
    "8. **End-to-end MLOps**  \n",
    "   * Register the best model with MLflow or Weights & Biases.  \n",
    "   * Package the training job as a Ray job and schedule it with Anyscale.\n",
    "\n",
    "9. **Multi-tenant recommendation systems**  \n",
    "   * Extend this to support **multiple audiences** or contexts (for example, multi-country, A/B groups).  \n",
    "   * Train and serve context-aware models in parallel using Ray.\n",
    "\n",
    "This pattern gives you a solid foundation for scaling recommendation workloads across real datasets and real infrastructure—without rewriting your model or managing your cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
