{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05a Hyperparameter tuning with Ray Tune and PyTorch  \n",
    "This notebook runs **hyperparameter tuning experiments** on a PyTorch CNN using **Ray Tune**, a scalable library for experiment management and search. The workload runs on the driver, but it sets the stage for the next tutorial where you’ll combine **Ray Tune with Ray Train** for distributed training.  \n",
    "\n",
    "## What you learn and take away  \n",
    "* How to define a **search space** for model hyperparameters using Ray Tune’s sampling APIs  \n",
    "* How to run **multiple training trials in parallel** with Ray’s orchestration  \n",
    "* How to add **checkpointing and resume** logic to your training function with `tune.get_checkpoint()`  \n",
    "* How to use **schedulers like Asynchronous Successive Halving (ASHA)** to early stop under-performing trials and speed up search  \n",
    "* How to retrieve and test the **best model checkpoint** at the end of tuning  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What problem are you solving? (hyperparameter tuning for image classification)  \n",
    "\n",
    "You’re training a **convolutional neural network (CNN)** to classify images from the **Canadian Institute For Advanced Research (CIFAR)-10 dataset**, which contains 60,000 color images across 10 categories (airplane, car, bird, cat, etc.).  \n",
    "\n",
    "Instead of fixing your model’s architecture and training parameters manually, you use **hyperparameter tuning** to automatically search over different configurations—like hidden layer sizes, learning rate, and batch size—and find the best-performing model.  \n",
    "\n",
    "This approach helps you answer key questions:  \n",
    "- How large should the fully connected layers be?  \n",
    "- What learning rate leads to the fastest convergence?  \n",
    "- Which batch size balances training speed and generalization?  \n",
    "\n",
    "By running many experiments in parallel and comparing validation accuracy and loss, you discover a set of hyperparameters that gives you the **highest test accuracy** for CIFAR-10 classification.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A deeper mathematical view  \n",
    "\n",
    "You train the CNN to minimize the **cross-entropy loss** between predicted probabilities $\\hat{y}$ and ground-truth labels $y$:  \n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\theta) = - \\frac{1}{N} \\sum_{i=1}^N \\sum_{c=1}^{10} \\mathbf{1}[y_i = c] \\cdot \\log \\hat{y}_{i,c}(\\theta)\n",
    "$$  \n",
    "\n",
    "where $\\theta$ are the model parameters, $N$ is the batch size, and $\\hat{y}_{i,c}$ is the predicted probability of class $c$ for image $i$.  \n",
    "\n",
    "Hyperparameter tuning searches over a configuration space:  \n",
    "\n",
    "$$\n",
    "\\mathcal{H} = \\{ l_1, l_2, \\eta, B \\}\n",
    "$$  \n",
    "\n",
    "- $l_1, l_2$: hidden layer widths in the fully connected layers  \n",
    "- $\\eta$: learning rate  \n",
    "- $B$: batch size  \n",
    "\n",
    "The goal is to find:  \n",
    "\n",
    "$$\n",
    "h^* = \\arg\\min_{h \\in \\mathcal{H}} \\ \\mathbb{E}_{(x,y) \\sim \\mathcal{D}_{\\text{val}}}\\big[\\mathcal{L}(f(x; \\theta_h), y)\\big]\n",
    "$$  \n",
    "\n",
    "That is, choose the hyperparameter setting $h^*$ that minimizes validation loss on unseen data.  \n",
    "\n",
    "By leveraging **Ray Tune’s search algorithms and schedulers**, you efficiently explore $\\mathcal{H}$ in parallel, prune poor configurations early, and converge to the hyperparameters that yield the **best generalization on CIFAR-10**.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A closer look at ASHA  \n",
    "\n",
    "When running many hyperparameter trials, it’s wasteful to let every configuration train for the full number of epochs. The **Asynchronous Successive Halving Algorithm (ASHA)** speeds things up by **early stopping** under-performing trials while letting promising ones continue.  \n",
    "\n",
    "Conceptually, ASHA divides training into **rungs**: after a certain number of epochs $t_r$, the top fraction $1 / \\eta$ of trials (where $\\eta$ is the *reduction factor*) advance to the next rung, while the algorithm prunes poor performers immediately.   \n",
    "\n",
    "Formally, if you start with $n$ trials, then at rung $r$:  \n",
    "\n",
    "$$\n",
    "n_{r} = \\left\\lceil \\frac{n}{\\eta^r} \\right\\rceil\n",
    "$$  \n",
    "\n",
    "trials survive to the next stage.  \n",
    "\n",
    "Unlike classic **Successive Halving**, ASHA runs in an **asynchronous** mode: trials don’t wait for each other to finish before making pruning decisions. This makes it ideal for distributed environments like Ray, where trial run times can vary and resources become free at different times.  \n",
    "\n",
    "The key parameters you configure in Ray Tune are:  \n",
    "- **`max_t` ($T$)**: maximum number of epochs a trial can run  \n",
    "- **`reduction_factor` ($\\eta$)**: how aggressively the algorithm prunes trials    \n",
    "- **`grace_period`**: the minimum number of epochs the algorithm guarantees for every trial   \n",
    "\n",
    "This balance between **exploration** (trying many hyperparameter combinations) and **exploitation** (training the best ones longer) is what makes ASHA so effective for scalable tuning.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports  \n",
    "Start by importing all the libraries you need for the rest of the notebook. These include standard utilities like `os`, `tempfile`, and `shutil` for filesystem management and cleanup, as well as scientific libraries like **NumPy** for random sampling in the hyperparameter search space.  \n",
    "\n",
    "Bring in **PyTorch** modules for building and training the CNN model:  \n",
    "- `torch`, `nn`, and `optim` for model definition and optimization  \n",
    "- `F` for activation functions and loss operations  \n",
    "- `torchvision` and `transforms` for dataset loading and preprocessing  \n",
    "- `random_split` for creating train/validation splits  \n",
    "\n",
    "Add **FileLock** to prevent race conditions when downloading CIFAR-10 data in parallel across trials.  \n",
    "\n",
    "Finally, import everything needed for **hyperparameter tuning with Ray**:  \n",
    "- `tune` for defining and executing trials, checkpointing, and metric reporting  \n",
    "- `ASHAScheduler` for early stopping under-performing trials  \n",
    "- `RunConfig` for experiment-level configuration like storage paths and naming  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 00. Runtime setup — install same deps as build.sh and set env vars\n",
    "import os, sys, subprocess\n",
    "\n",
    "# Non-secret env var \n",
    "os.environ[\"RAY_TRAIN_V2_ENABLED\"] = \"1\"\n",
    "\n",
    "# Install Python dependencies \n",
    "subprocess.check_call([\n",
    "    sys.executable, \"-m\", \"pip\", \"install\", \"--no-cache-dir\",\n",
    "    \"torch==2.8.0\",\n",
    "    \"torchvision==0.23.0\",\n",
    "    \"matplotlib==3.10.6\",\n",
    "    \"pyarrow==14.0.2\",\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 01. Imports\n",
    "\n",
    "# --- Standard libraries ---\n",
    "import os              # Filesystem utilities (paths, directories)\n",
    "import tempfile        # Temporary directories for checkpoints\n",
    "import shutil          # Cleanup of files and directories\n",
    "\n",
    "# --- Analytics / plotting ---\n",
    "import pandas as pd                    # Converting output to dataframe for plotting\n",
    "import matplotlib.pyplot as plt        # For generating plots \n",
    "\n",
    "# --- Scientific computing ---\n",
    "import numpy as np     # Numerical operations, used for random sampling in search space\n",
    "\n",
    "# --- PyTorch (deep learning) ---\n",
    "import torch\n",
    "import torch.nn as nn                  # Neural network modules (layers, models)\n",
    "import torch.nn.functional as F        # Functional API for activations/losses\n",
    "import torch.optim as optim            # Optimizers (e.g., SGD, Adam)\n",
    "import torchvision                     # Popular vision datasets and pretrained models\n",
    "import torchvision.transforms as transforms  # Image preprocessing pipelines\n",
    "from torch.utils.data import random_split    # Train/validation dataset splitting\n",
    "\n",
    "# --- Utilities ---\n",
    "from filelock import FileLock          # Prevents race conditions when multiple workers download CIFAR-10\n",
    "\n",
    "# --- Ray (tuning and orchestration) ---\n",
    "from ray import train, tune            # Core APIs for metric reporting and trial execution\n",
    "from ray.tune.schedulers import ASHAScheduler  # Asynchronous HyperBand for early stopping\n",
    "from ray.air.config import RunConfig   # Configure experiment metadata (name, storage, logging)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and prepare CIFAR-10 data  \n",
    "\n",
    "Start by creating helper functions to load and preprocess the **CIFAR-10 dataset**.  \n",
    "\n",
    "- **`load_data`**:  \n",
    "  - Applies a standard normalization transform with per-channel mean and standard deviation.  \n",
    "  - Uses a `FileLock` so multiple Ray Tune workers can safely download the dataset without corrupting files.  \n",
    "  - Returns both the training and test datasets.  \n",
    "\n",
    "- **`create_dataloaders`**:  \n",
    "  - Splits the training set into an 80/20 train/validation split using `random_split`.  \n",
    "  - Wraps each subset into a `DataLoader`, controlling `batch_size`, shuffling, and parallelism with `num_workers`.  \n",
    "  - Returns a **train loader** (shuffled) and a **validation loader** (deterministic).  \n",
    "\n",
    "This setup ensures consistent preprocessing and safe parallel dataset access across trials.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 02. Load and prepare CIFAR-10 data  \n",
    "\n",
    "def load_data(data_dir=\"/mnt/cluster_storage/cifar10\"):\n",
    "    \"\"\"\n",
    "    Download and load the CIFAR-10 dataset with standard preprocessing.\n",
    "    Returns the full training set and the test set.\n",
    "    \"\"\"\n",
    "    # Define preprocessing: convert to tensor and normalize channels\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),  # Convert images to PyTorch tensors [0,1]\n",
    "        transforms.Normalize(    # Normalize with dataset mean & std (per channel)\n",
    "            (0.4914, 0.4822, 0.4465),   # mean (R, G, B)\n",
    "            (0.2023, 0.1994, 0.2010)    # std (R, G, B)\n",
    "        ),\n",
    "    ])\n",
    "\n",
    "    # FileLock ensures that multiple parallel workers downloading CIFAR-10\n",
    "    # don't interfere with each other (prevents race conditions).\n",
    "    with FileLock(os.path.expanduser(\"~/.data.lock\")):\n",
    "        trainset = torchvision.datasets.CIFAR10(\n",
    "            root=data_dir, train=True, download=True, transform=transform)\n",
    "\n",
    "        testset = torchvision.datasets.CIFAR10(\n",
    "            root=data_dir, train=False, download=True, transform=transform)\n",
    "\n",
    "    return trainset, testset\n",
    "\n",
    "def create_dataloaders(trainset, batch_size, num_workers=8):\n",
    "    \"\"\"\n",
    "    Split the CIFAR-10 training set into train/validation subsets,\n",
    "    and wrap them in DataLoader objects.\n",
    "    \"\"\"\n",
    "    # Compute split sizes: 80% train, 20% validation\n",
    "    train_size = int(len(trainset) * 0.8)\n",
    "\n",
    "    # Randomly partition the dataset into train/val subsets\n",
    "    train_subset, val_subset = random_split(trainset, [train_size, len(trainset) - train_size])\n",
    "\n",
    "    # Training loader: shuffle for stochastic gradient descent\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_subset,\n",
    "        batch_size=batch_size, \n",
    "        shuffle=True,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    # Validation loader: no shuffle (deterministic evaluation)\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        val_subset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False, \n",
    "        num_workers=num_workers\n",
    "    )\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load synthetic test data  \n",
    "\n",
    "Define a helper function to quickly generate **synthetic datasets** for debugging and smoke tests.  \n",
    "\n",
    "- **`torchvision.datasets.FakeData`** creates random tensors shaped like CIFAR-10 images `(3, 32, 32)` with 10 classes.  \n",
    "- The function generates a small fake training set (128 samples) and a fake test set (16 samples).    \n",
    "- This runs much faster than downloading CIFAR-10 and is useful for verifying that your code, model, and Ray Tune setup all work end-to-end before running full experiments.  \n",
    "\n",
    "The function returns two datasets: a synthetic **train set** and a synthetic **test set**.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 03. Load synthetic test data \n",
    "\n",
    "def load_test_data():\n",
    "    \"\"\"\n",
    "    Create small synthetic datasets for quick smoke testing.\n",
    "    Useful to validate the training loop and Ray Tune integration\n",
    "    without downloading or processing the full CIFAR-10 dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    # Generate a fake training set of 128 samples\n",
    "    # Each sample is shaped like a CIFAR-10 image: (3 channels, 32x32 pixels)\n",
    "    # Labels are drawn from 10 possible classes.\n",
    "    trainset = torchvision.datasets.FakeData(\n",
    "        size=128,                     # number of samples\n",
    "        image_size=(3, 32, 32),       # match CIFAR-10 format\n",
    "        num_classes=10,               # same number of categories as CIFAR-10\n",
    "        transform=transforms.ToTensor()  # convert to PyTorch tensors\n",
    "    )\n",
    "\n",
    "    # Generate a smaller fake test set of 16 samples\n",
    "    testset = torchvision.datasets.FakeData(\n",
    "        size=16,\n",
    "        image_size=(3, 32, 32),\n",
    "        num_classes=10,\n",
    "        transform=transforms.ToTensor()\n",
    "    )\n",
    "    \n",
    "    # Return both sets so they can be wrapped into DataLoaders\n",
    "    return trainset, testset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define a CNN model  \n",
    "\n",
    "Define a simple **convolutional neural network (CNN)** for classifying CIFAR-10 images. This architecture is intentionally lightweight, making it easy to train and fast to evaluate during hyperparameter search.  \n",
    "\n",
    "- **Convolutional layers**:  \n",
    "  - `conv1`: 3 input channels (Red/Green/Blue) for 6 feature maps  \n",
    "  - `conv2`: 6 feature maps for 16 feature maps  \n",
    "  - Each followed by ReLU activation and 2×2 max pooling  \n",
    "\n",
    "- **Fully connected layers**:  \n",
    "  - `fc1`: flattens the feature maps into a vector and maps to a hidden layer of size `l1`  \n",
    "  - `fc2`: maps to another hidden layer of size `l2`  \n",
    "  - `fc3`: final classification layer mapping to 10 output classes (CIFAR-10 categories)  \n",
    "\n",
    "- **Forward pass**:  \n",
    "  - Applies convolution, ReLU, then pooling twice  \n",
    "  - Flattens feature maps to a vector  \n",
    "  - Passes through two ReLU-activated fully connected layers  \n",
    "  - Outputs logits for 10 classes  \n",
    "\n",
    "This network provides a baseline model for tuning, and you treat `l1` and `l2` as hyperparameters to explore different hidden layer sizes.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 04. Define CNN model \n",
    "\n",
    "class Net(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple convolutional neural network (CNN) for CIFAR-10 classification.\n",
    "    Consists of two convolutional layers with pooling, followed by three\n",
    "    fully connected (dense) layers. Hidden layer sizes l1 and l2 are tunable.\n",
    "    \"\"\"\n",
    "    def __init__(self, l1=120, l2=84):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        # --- Convolutional feature extractor ---\n",
    "        # First conv: input = 3 channels (RGB), output = 6 feature maps, kernel size = 5\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=6, kernel_size=5)\n",
    "\n",
    "        # Max pooling: downsample feature maps by factor of 2 (2x2 pooling window)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Second conv: input = 6 feature maps, output = 16 feature maps, kernel size = 5\n",
    "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5)\n",
    "\n",
    "        # --- Fully connected classifier ---\n",
    "        # Flattened input size = 16 feature maps * 5 * 5 spatial size = 400\n",
    "        # Map this to first hidden layer of size l1 (tunable hyperparameter)\n",
    "        self.fc1 = nn.Linear(in_features=16 * 5 * 5, out_features=l1)\n",
    "\n",
    "        # Second hidden layer of size l2 (also tunable)\n",
    "        self.fc2 = nn.Linear(in_features=l1, out_features=l2)\n",
    "\n",
    "        # Final classification layer: map to 10 classes (CIFAR-10)\n",
    "        self.fc3 = nn.Linear(in_features=l2, out_features=10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Define forward pass through the network.\n",
    "        \"\"\"\n",
    "        # Apply conv1 → ReLU → pooling\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "\n",
    "        # Apply conv2 → ReLU → pooling\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "\n",
    "        # Flatten from (batch_size, 16, 5, 5) → (batch_size, 400)\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "\n",
    "        # Fully connected layers with ReLU activations\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "\n",
    "        # Final linear layer (logits for 10 classes)\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Define a training function  \n",
    "\n",
    "Define the **training loop** that Ray Tune executes for each trial. This function encapsulates model setup, checkpoint handling, training, validation, and metric reporting.  \n",
    "\n",
    "- **Model setup**:  \n",
    "  - Builds a new `Net` model with tunable hidden sizes `l1` and `l2`.  \n",
    "  - Moves the model to the appropriate device (CPU or GPU). If multiple GPUs are available, wraps the model in `nn.DataParallel`.  \n",
    "\n",
    "- **Loss and optimizer**:  \n",
    "  - Uses **cross-entropy loss** for multi-class classification.  \n",
    "  - Optimizes with **SGD** including momentum and weight decay.  \n",
    "\n",
    "- **Checkpoint loading**:  \n",
    "  - If a checkpoint exists from a previous run, restores both the model and optimizer state using `tune.get_checkpoint()`.  \n",
    "  - This enables **fault tolerance** and resuming from prior progress.  \n",
    "\n",
    "- **Data setup**:  \n",
    "  - Loads either the **real CIFAR-10 dataset** or a **synthetic smoke-test dataset**, based on the `smoke_test` flag.  \n",
    "  - Wraps data into train/validation `DataLoader`s with configurable batch size and number of workers.  \n",
    "\n",
    "- **Training loop**:  \n",
    "  - Runs for `max_num_epochs`, performing forward, backward, and optimization passes.  \n",
    "  - Computes average training loss across batches.  \n",
    "  - After each epoch, evaluates on the validation set and calculates both **validation loss** and **accuracy**.  \n",
    "\n",
    "- **Metric reporting and checkpointing**:  \n",
    "  - Packages metrics into a dictionary (`loss`, `accuracy`) and reports them to Ray Tune with `tune.report()`.  \n",
    "  - Saves model and optimizer state into a temporary directory checkpoint, which Ray manages automatically.  \n",
    "\n",
    "At the end, the function prints `\"Finished Training!\"` to signal completion of the trial.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 05. Define a training function \n",
    "\n",
    "def train_cifar(config):\n",
    "    \"\"\"\n",
    "    Train a CIFAR-10 CNN model with hyperparameters provided in `config`.\n",
    "    Supports checkpointing and metric reporting for Ray Tune.\n",
    "    \"\"\"\n",
    "\n",
    "    # --- Model setup ---\n",
    "    # Initialize network with tunable hidden sizes (l1, l2).\n",
    "    net = Net(config[\"l1\"], config[\"l2\"])\n",
    "    device = config[\"device\"]\n",
    "\n",
    "    # If using CUDA with multiple GPUs, wrap in DataParallel for multi-GPU training.\n",
    "    if device == \"cuda\":\n",
    "        net = nn.DataParallel(net)\n",
    "    net.to(device)\n",
    "\n",
    "    # --- Loss and optimizer ---\n",
    "    criterion = nn.CrossEntropyLoss() # standard classification loss\n",
    "    optimizer = optim.SGD(\n",
    "        net.parameters(),\n",
    "        lr=config[\"lr\"],          # learning rate (tunable)\n",
    "        momentum=0.9,             # helps accelerate gradients\n",
    "        weight_decay=5e-5         # L2 regularization\n",
    "    )\n",
    "\n",
    "    # --- Resume from checkpoint (if available) ---\n",
    "    # This allows interrupted or failed trials to pick up from the last saved state.\n",
    "    if tune.get_checkpoint():\n",
    "        loaded_checkpoint = tune.get_checkpoint()\n",
    "        with loaded_checkpoint.as_directory() as loaded_checkpoint_dir:\n",
    "            model_state, optimizer_state = torch.load(\n",
    "                os.path.join(loaded_checkpoint_dir, \"checkpoint.pt\")\n",
    "            )\n",
    "            net.load_state_dict(model_state)  # restore model weights \n",
    "            optimizer.load_state_dict(optimizer_state)  # restore optimizer state\n",
    "\n",
    "    # --- Data setup ---\n",
    "    # Use synthetic data for quick smoke tests, otherwise load full CIFAR-10.\n",
    "    if config[\"smoke_test\"]:\n",
    "        trainset, _ = load_test_data()\n",
    "    else:\n",
    "        trainset, _ = load_data()\n",
    "\n",
    "    # Create train/validation DataLoaders\n",
    "    train_loader, val_loader = create_dataloaders(\n",
    "        trainset, \n",
    "        config[\"batch_size\"],                         # tunable batch size\n",
    "        num_workers=0 if config[\"smoke_test\"] else 8  # no workers for synthetic data\n",
    "    )\n",
    "\n",
    "    # --- Training loop ---\n",
    "    for epoch in range(config[\"max_num_epochs\"]):  # loop over epochs\n",
    "        net.train()  # set model to training mode\n",
    "        running_loss = 0.0\n",
    "\n",
    "        # Iterate over training batches\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # forward, backward, and optimize\n",
    "            optimizer.zero_grad()       # reset gradients\n",
    "            outputs = net(inputs)       # forward pass\n",
    "            loss = criterion(outputs, labels)  # compute loss\n",
    "            loss.backward()             # backpropagation\n",
    "            optimizer.step()            # update weights\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "\n",
    "        # --- Validation loop ---\n",
    "        net.eval()  # set model to eval mode\n",
    "        val_loss = 0.0\n",
    "        correct = total = 0\n",
    "        with torch.no_grad():  # no gradients during evaluation\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = net(inputs)\n",
    "                val_loss += criterion(outputs, labels).item()\n",
    "\n",
    "                # Compute classification accuracy\n",
    "                _, predicted = outputs.max(1)  # predicted class = argmax\n",
    "                total += labels.size(0)\n",
    "                correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "        # --- Report metrics to Ray Tune ---\n",
    "        metrics = {\n",
    "            \"loss\": val_loss / len(val_loader),     # average validation loss\n",
    "            \"accuracy\": correct / total,            # validation accuracy\n",
    "        }\n",
    "\n",
    "        # --- Save checkpoint ---\n",
    "        # Store model and optimizer state so trial can resume later if needed.\n",
    "        with tempfile.TemporaryDirectory() as temp_checkpoint_dir:\n",
    "            path = os.path.join(temp_checkpoint_dir, \"checkpoint.pt\")\n",
    "            torch.save(\n",
    "                (net.state_dict(), optimizer.state_dict()), path\n",
    "            )\n",
    "            checkpoint = tune.Checkpoint.from_directory(temp_checkpoint_dir)\n",
    "\n",
    "             # Report both metrics and checkpoint to Ray Tune\n",
    "            tune.report(metrics, checkpoint=checkpoint)\n",
    "            \n",
    "    print(\"Finished Training!\")  # Final message at end of training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluate the best model  \n",
    "\n",
    "After hyperparameter tuning finishes, you want to verify the performance of the **best trial** on the held-out test set.  \n",
    "\n",
    "- **Model reconstruction**:  \n",
    "  - Rebuilds the CNN (`Net`) using the hyperparameters (`l1`, `l2`) from the best configuration.  \n",
    "  - Moves the model to the same device (CPU or GPU) used during training.  \n",
    "  - Restores trained weights from the best trial’s final checkpoint.  \n",
    "\n",
    "- **Dataset selection**:  \n",
    "  - If running a quick smoke test, loads the synthetic `FakeData` test set.  \n",
    "  - Otherwise, loads the real CIFAR-10 test set.  \n",
    "\n",
    "- **Evaluation loop**:  \n",
    "  - Wraps the test set in a DataLoader with small batches (`batch_size=4`).  \n",
    "  - Iterates over the test data without gradients (`torch.no_grad()`).  \n",
    "  - Computes predicted labels with `argmax` on the model outputs.  \n",
    "  - Accumulates the total number of correct predictions.  \n",
    "\n",
    "- **Final result**:  \n",
    "  - Prints the **test set accuracy** of the best trial, providing an unbiased estimate of model generalization after tuning.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 06. Evaluate the best model\n",
    "\n",
    "def test_best_model(best_result, smoke_test=False):\n",
    "    \"\"\"\n",
    "    Evaluate the best model found during Ray Tune search on the test set.\n",
    "    Restores the trained weights from the best trial's checkpoint and\n",
    "    computes classification accuracy.\n",
    "    \"\"\"\n",
    "\n",
    "    # --- Rebuild the best model architecture ---\n",
    "    # Use the trial’s hyperparameters (hidden layer sizes l1 and l2)\n",
    "    best_trained_model = Net(best_result.config[\"l1\"], best_result.config[\"l2\"])\n",
    "    device = best_result.config[\"device\"]\n",
    "\n",
    "    # If running on GPU(s), wrap the model in DataParallel for multi-GPU support\n",
    "    if device == \"cuda\":\n",
    "        best_trained_model = nn.DataParallel(best_trained_model)\n",
    "    best_trained_model.to(device)\n",
    "\n",
    "    # --- Load weights from checkpoint ---\n",
    "    # Convert checkpoint object to a directory, then restore model state_dict\n",
    "    checkpoint_path = os.path.join(best_result.checkpoint.to_directory(), \"checkpoint.pt\")\n",
    "    model_state, _optimizer_state = torch.load(checkpoint_path)  # optimizer state not needed here\n",
    "    best_trained_model.load_state_dict(model_state)\n",
    "\n",
    "    # --- Select test dataset ---\n",
    "    # Use synthetic FakeData if in smoke test mode, otherwise use real CIFAR-10 test set\n",
    "    if smoke_test:\n",
    "        _trainset, testset = load_test_data()\n",
    "    else:\n",
    "        _trainset, testset = load_data()\n",
    "\n",
    "    # --- Prepare DataLoader for evaluation ---\n",
    "    # Small testing batch size (4) is fine since evaluation isn’t performance-critical\n",
    "    testloader = torch.utils.data.DataLoader(\n",
    "        testset, batch_size=4, shuffle=False, num_workers=2\n",
    "    )\n",
    "\n",
    "    # --- Run evaluation ---\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():  # disable gradients for faster inference\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            # Forward pass through trained model\n",
    "            outputs = best_trained_model(images)\n",
    "\n",
    "            # Get predicted class = index of max logit\n",
    "            _, predicted = outputs.max(1)\n",
    "\n",
    "            # Update totals for accuracy calculation\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            \n",
    "    # --- Print final accuracy ---\n",
    "    print(f\"Best trial test set accuracy: {correct / total}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Smoke test flag  \n",
    "\n",
    "Define a flag to control whether the tutorial runs on the **real CIFAR-10 dataset** or a **small synthetic dataset** for quick debugging.  \n",
    "\n",
    "- `SMOKE_TEST = True`:  \n",
    "  - Uses `FakeData` with a tiny dataset size.  \n",
    "  - Runs only a couple of epochs and trials.  \n",
    "  - Useful to verify the code and Ray Tune setup without waiting for full training.  \n",
    "\n",
    "- `SMOKE_TEST = False`:  \n",
    "  - Uses the full CIFAR-10 dataset.  \n",
    "  - Runs the complete training and hyperparameter search.  \n",
    "  - Provides realistic results and accuracy metrics.  \n",
    "\n",
    "This toggle lets you switch between **fast iteration** (for development) and **full-scale experiments** (for real results).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 07. Smoke test flag\n",
    "\n",
    "# Set this to True for a smoke test that runs with a small synthetic dataset.\n",
    "SMOKE_TEST = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Define the hyperparameter search space and configuration  \n",
    "\n",
    "Define the **search space** and runtime parameters that Ray Tune explores during hyperparameter tuning.  \n",
    "\n",
    "- **Model architecture**:  \n",
    "  - `l1`: size of the first fully connected layer, sampled as a power of 2 between 4 and 256.  \n",
    "  - `l2`: size of the second fully connected layer, sampled similarly.  \n",
    "\n",
    "- **Optimization**:  \n",
    "  - `lr`: learning rate, sampled from a log-uniform distribution between $10^{-4}$ and $10^{-1}$.  \n",
    "  - `batch_size`: chosen from {2, 4, 8, 16}.  \n",
    "\n",
    "- **Runtime configuration**:  \n",
    "  - `smoke_test`: whether to run with synthetic `FakeData` or full CIFAR-10.  \n",
    "  - `num_trials`: number of hyperparameter configurations to test (10 by default, 2 in smoke test).  \n",
    "  - `max_num_epochs`: number of epochs per trial (10 by default, 2 in smoke test).  \n",
    "  - `device`: automatically set to `\"cuda\"` if a GPU is available, otherwise `\"cpu\"`.  \n",
    "\n",
    "This configuration dictionary balances **search diversity** (random sampling of layer sizes and learning rates) with **practical runtime controls** for development vs. full experiments.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 08. Define the hyperparameter search space and configuration\n",
    "\n",
    "config = {\n",
    "    \"l1\": tune.sample_from(lambda _: 2**np.random.randint(2, 9)),   # size of 1st FC layer\n",
    "    \"l2\": tune.sample_from(lambda _: 2**np.random.randint(2, 9)),   # size of 2nd FC layer\n",
    "    \"lr\": tune.loguniform(1e-4, 1e-1),                              # learning rate\n",
    "    \"batch_size\": tune.choice([2, 4, 8, 16]),                       # training batch size\n",
    "    \"smoke_test\": SMOKE_TEST,                                       # toggle for FakeData vs CIFAR-10\n",
    "    \"num_trials\": 10 if not SMOKE_TEST else 2,                      # number of hyperparam trials\n",
    "    \"max_num_epochs\": 10 if not SMOKE_TEST else 2,                  # training epochs per trial\n",
    "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",       # use GPU if available\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Run hyperparameter tuning with Ray Tune  \n",
    "\n",
    "Define the **main entry point** for running hyperparameter search with Ray Tune.  \n",
    "\n",
    "- **Scheduler**:  \n",
    "  - Uses `ASHAScheduler`, an asynchronous variant of Successive Halving, which prunes poorly performing trials early.  \n",
    "  - Configured with:  \n",
    "    - `time_attr=\"training_iteration\"`: each epoch counts as one iteration.  \n",
    "    - `max_t`: maximum number of epochs per trial.  \n",
    "    - `grace_period=1`: ensures every trial runs at least one epoch before pruning.  \n",
    "    - `reduction_factor=2`: halves the number of trials promoted at each rung.  \n",
    "\n",
    "- **Tuner setup**:  \n",
    "  - Wraps the `train_cifar` function with `tune.with_resources` to allocate **2 CPUs** and the specified number of GPUs per trial.  \n",
    "  - `TuneConfig` specifies:  \n",
    "    - Optimization target (`loss`, minimized).  \n",
    "    - Scheduler (`ASHA`) for pruning.  \n",
    "    - Number of samples (`num_trials`).  \n",
    "  - `RunConfig` gives the experiment a name (`cifar10_tune_demo`) and a persistent storage path for results and checkpoints.  \n",
    "  - `param_space=config` passes in the hyperparameter search space.  \n",
    "\n",
    "- **Execution**:  \n",
    "  - Runs all trials in parallel with `tuner.fit()`.  \n",
    "  - Retrieves the **best result** by lowest validation loss.  \n",
    "  - Prints the best configuration, final validation loss, and validation accuracy.  \n",
    "\n",
    "- **Final test**:  \n",
    "  - Calls `test_best_model` to evaluate the best checkpoint on the held-out test set.  \n",
    "\n",
    "The final line calls `main(...)`, automatically choosing `gpus_per_trial=1` if a GPU is available, otherwise defaulting to CPU.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 09. Run hyperparameter tuning with Ray Tune \n",
    "\n",
    "def main(config, gpus_per_trial=1):\n",
    "    \"\"\"\n",
    "    Run Ray Tune hyperparameter search on CIFAR-10 with the given config.\n",
    "    Uses ASHAScheduler for early stopping and reports the best trial result.\n",
    "    \"\"\"\n",
    "\n",
    "    # --- Define scheduler ---\n",
    "    # ASHAScheduler prunes bad trials early and promotes promising ones.\n",
    "    scheduler = ASHAScheduler(\n",
    "        time_attr=\"training_iteration\",      # metric for progress = epoch count\n",
    "        max_t=config[\"max_num_epochs\"],      # maximum epochs per trial\n",
    "        grace_period=1,                      # min epochs before pruning is allowed\n",
    "        reduction_factor=2                   # at each rung, keep ~1/2 of trials\n",
    "    )\n",
    "    \n",
    "    # --- Define Ray Tune tuner ---\n",
    "    tuner = tune.Tuner(\n",
    "        # Wrap training function and specify trial resources\n",
    "        tune.with_resources(\n",
    "            tune.with_parameters(train_cifar),   # training loop\n",
    "            resources={\"cpu\": 2, \"gpu\": gpus_per_trial}  # per-trial resources\n",
    "        ),\n",
    "        tune_config=tune.TuneConfig(\n",
    "            metric=\"loss\",                # optimize validation loss\n",
    "            mode=\"min\",                   # minimize the metric\n",
    "            scheduler=scheduler,          # use ASHA for early stopping\n",
    "            num_samples=config[\"num_trials\"],  # number of hyperparam trials\n",
    "        ),\n",
    "        run_config=RunConfig(\n",
    "            name=\"cifar10_tune_demo\",     # experiment name\n",
    "            storage_path=\"/mnt/cluster_storage/ray-results\"  # save results here\n",
    "        ),\n",
    "        param_space=config,               # hyperparameter search space\n",
    "    )\n",
    "\n",
    "    # --- Execute trials ---\n",
    "    results = tuner.fit()                 # launch tuning job\n",
    "    \n",
    "    # --- Retrieve best result ---\n",
    "    best_result = results.get_best_result(\"loss\", \"min\")  # lowest validation loss\n",
    "\n",
    "    # --- Print summary of best trial ---\n",
    "    print(f\"Best trial config: {best_result.config}\")\n",
    "    print(f\"Best trial final validation loss: {best_result.metrics['loss']}\")\n",
    "    print(f\"Best trial final validation accuracy: {best_result.metrics['accuracy']}\")\n",
    "\n",
    "    # --- Evaluate best model on test set ---\n",
    "    test_best_model(best_result, smoke_test=config[\"smoke_test\"])\n",
    "\n",
    "    return results, best_result   \n",
    "\n",
    "# --- Run main entry point ---\n",
    "# Use 1 GPU per trial if available, otherwise run on CPU only\n",
    "results, best_result = main(config, gpus_per_trial=1 if torch.cuda.is_available() else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Analyze and visualize tuning results  \n",
    "\n",
    "After tuning, it’s useful to dig into the results across trials to understand how different hyperparameters influenced performance.  \n",
    "\n",
    "With Ray Tune, you can easily extract a summary of all trials and create simple plots:  \n",
    "\n",
    "- **Trial summary table**: list each trial with its config (`l1`, `l2`, `lr`, `batch_size`) and final metrics (`val_loss`, `val_accuracy`).  \n",
    "- **Hyperparameter vs. metric plots**: see trends, for example, how learning rate or batch size affects validation accuracy.  \n",
    "- **Best trials comparison**: sort trials by validation accuracy and inspect the top-k configs.  \n",
    "\n",
    "This analysis helps you go beyond “what’s the best config?” to **why** certain hyperparameters perform better. It’s also a sanity check to confirm ASHA pruned poor performers effectively.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. Analyze and visualize tuning results \n",
    "\n",
    "# Convert all trial results into a DataFrame\n",
    "df = results.get_dataframe()\n",
    "\n",
    "# Show the top 5 trials by validation accuracy\n",
    "top5 = df.sort_values(\"accuracy\", ascending=False).head(5)\n",
    "print(\"🔝 Top 5 Trials by Validation Accuracy:\")\n",
    "display(top5[[\"config/l1\", \"config/l2\", \"config/lr\", \"config/batch_size\", \"loss\", \"accuracy\"]])\n",
    "\n",
    "# Plot learning rate versus validation accuracy\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.scatter(df[\"config/lr\"], df[\"accuracy\"], alpha=0.7)\n",
    "plt.xscale(\"log\")\n",
    "plt.xlabel(\"Learning Rate\")\n",
    "plt.ylabel(\"Validation Accuracy\")\n",
    "plt.title(\"Learning Rate vs Accuracy\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot batch size versus validation accuracy\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.scatter(df[\"config/batch_size\"], df[\"accuracy\"], alpha=0.7)\n",
    "plt.xlabel(\"Batch Size\")\n",
    "plt.ylabel(\"Validation Accuracy\")\n",
    "plt.title(\"Batch Size vs Accuracy\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Plot learning curves across all trials  \n",
    "\n",
    "Visualize the **validation loss and accuracy** curves for all trials, with the best trial highlighted.  \n",
    "\n",
    "- Uses the `metrics_dataframe` from Ray Tune to extract per-epoch metrics.  \n",
    "- Treats `training_iteration` as the epoch index because the code calls `tune.report()` once per epoch.  \n",
    "- Plots all trials in the background with low opacity (**blue for loss, orange for accuracy**) so you can see overall trends.  \n",
    "- Overlays the **best trial** with bold, opaque lines and markers, making it easy to distinguish from the rest.  \n",
    "\n",
    "In **smoke test mode**, you only see two points because each trial runs for only 2 epochs. In a full CIFAR-10 run, this plot shows richer training curves across all epochs, letting you compare convergence patterns between hyperparameter configurations.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11. Plot learning curves across all trials\n",
    "\n",
    "# Expect: results, best_result already defined from:\n",
    "# results, best_result = main(config, gpus_per_trial=1 if torch.cuda.is_available() else 0)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(8, 8), sharex=True)\n",
    "\n",
    "# --- Plot validation loss ---\n",
    "for res in results:\n",
    "    hist = res.metrics_dataframe\n",
    "    if hist is None or hist.empty:\n",
    "        continue\n",
    "    epoch = hist[\"training_iteration\"] if \"training_iteration\" in hist else pd.Series(range(1, len(hist) + 1))\n",
    "    axes[0].plot(epoch, hist[\"loss\"], color=\"blue\", alpha=0.15)\n",
    "\n",
    "best_hist = best_result.metrics_dataframe\n",
    "epoch_best = best_hist[\"training_iteration\"] if \"training_iteration\" in best_hist else pd.Series(range(1, len(best_hist) + 1))\n",
    "axes[0].plot(epoch_best, best_hist[\"loss\"], marker=\"o\", linewidth=2.5, color=\"blue\", label=\"Best — Val Loss\")\n",
    "\n",
    "axes[0].set_ylabel(\"Validation Loss\")\n",
    "axes[0].set_title(\"All Trials (faded) + Best Trial (bold)\")\n",
    "axes[0].grid(True)\n",
    "axes[0].legend()\n",
    "\n",
    "# --- Plot validation accuracy ---\n",
    "for res in results:\n",
    "    hist = res.metrics_dataframe\n",
    "    if hist is None or hist.empty:\n",
    "        continue\n",
    "    epoch = hist[\"training_iteration\"] if \"training_iteration\" in hist else pd.Series(range(1, len(hist) + 1))\n",
    "    axes[1].plot(epoch, hist[\"accuracy\"], color=\"orange\", alpha=0.15)\n",
    "\n",
    "axes[1].plot(epoch_best, best_hist[\"accuracy\"], marker=\"s\", linewidth=2.5, color=\"orange\", label=\"Best — Val Accuracy\")\n",
    "\n",
    "axes[1].set_xlabel(\"Epoch\")\n",
    "axes[1].set_ylabel(\"Validation Accuracy\")\n",
    "axes[1].grid(True)\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Clean up cluster storage  \n",
    "\n",
    "After finishing the tutorial, it’s good practice to clean up the files you generated to free space on shared storage.  \n",
    "\n",
    "- Defines a list of paths to remove:  \n",
    "  - `/mnt/cluster_storage/cifar10`: cached CIFAR-10 dataset.  \n",
    "  - `/mnt/cluster_storage/ray-results`: Ray Tune results and checkpoints.  \n",
    "\n",
    "- Iterates over each path:  \n",
    "  - If the path exists, deletes it recursively with `shutil.rmtree()`.  \n",
    "  - If the path doesn’t exist, prints a message and skips it.  \n",
    "\n",
    "At the end, prints a confirmation that cleanup is complete.  \n",
    "\n",
    "This step ensures your cluster stays tidy, especially if you’ve run multiple tuning jobs or smoke tests.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12. Cleanup cluster storage  \n",
    "\n",
    "# Paths you used in the script\n",
    "paths_to_clean = [\n",
    "    \"/mnt/cluster_storage/cifar10\",       # dataset\n",
    "    \"/mnt/cluster_storage/ray-results\",   # Tune results & checkpoints\n",
    "]\n",
    "\n",
    "for p in paths_to_clean:\n",
    "    if os.path.exists(p):\n",
    "        print(f\"Removing {p} ...\")\n",
    "        shutil.rmtree(p, ignore_errors=True)\n",
    "    else:\n",
    "        print(f\"{p} does not exist, skipping.\")\n",
    "\n",
    "print(\"Cleanup complete ✅\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrap up and next steps  \n",
    "\n",
    "In this tutorial, you used **Ray Tune with PyTorch** to run hyperparameter search experiments on CIFAR-10, from defining a search space and training loop, to running trials with ASHA, checkpointing progress, and evaluating the best model on a test set. You even visualized learning curves across all trials to see how different configurations performed.  \n",
    "\n",
    "You should now feel confident:  \n",
    "\n",
    "* Defining **hyperparameter search spaces** with Ray Tune’s sampling APIs  \n",
    "* Writing a **training loop with checkpointing and resume** logic  \n",
    "* Running multiple **trials in parallel with `ASHAScheduler`** to prune under-performers  \n",
    "* Retrieving the **best model checkpoint** and evaluating on a held-out test set  \n",
    "* Visualizing and analyzing **trial results** across hyperparameter configurations  \n",
    "\n",
    "---\n",
    "\n",
    "### Where can you take this next?  \n",
    "\n",
    "The following are a few directions to extend or adapt this workload:  \n",
    "\n",
    "1. **Stronger models**  \n",
    "   * Replace the small CNN with **ResNet** or another modern architecture from `torchvision.models`.  \n",
    "   * Add data augmentation (random crops, flips) to improve CIFAR-10 accuracy.  \n",
    "\n",
    "2. **Distributed training**  \n",
    "   * Combine Ray Tune with **Ray Train** so each trial runs on multiple GPUs or nodes.  \n",
    "   * Scale up trials to larger datasets without changing your training code.  \n",
    "\n",
    "3. **Alternative search algorithms**  \n",
    "   * Swap ASHA for **Bayesian optimization** or **Population Based Training (PBT)**.  \n",
    "   * Compare efficiency and results.  \n",
    "\n",
    "4. **Experiment tracking**  \n",
    "   * Integrate with **TensorBoard, Weights & Biases, or MLflow** using Ray callbacks.  \n",
    "   * Track and visualize experiments at scale.  \n",
    "\n",
    "5. **Production readiness**  \n",
    "   * Export the best model and serve it with **Ray Serve** for real-time inference.  \n",
    "   * Wrap the tuning logic into a **Ray job** and schedule it on Anyscale.  \n",
    "\n",
    "This tutorial gives you a solid foundation for running hyperparameter tuning at scale. With Ray and Anyscale, you can seamlessly move from single-node tuning to multi-node distributed training and beyond—without rewriting your PyTorch model code.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
