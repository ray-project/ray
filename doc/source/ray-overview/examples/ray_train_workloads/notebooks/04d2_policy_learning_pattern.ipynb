{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04-d2 Diffusion-Policy Pattern with Ray Train  \n",
    "This notebook builds a **mini diffusion-policy pipeline** on a **real Pendulum-v1 offline dataset** and runs it end-to-end on an Anyscale cluster with **Ray Train V2**.\n",
    "\n",
    "## What you learn and take away  \n",
    "* How to use **Ray Data** to stream and preprocess Gymnasium rollouts in parallel across CPU workers  \n",
    "* How to scale training across **multiple A10G GPUs** using `TorchTrainer` with a minimal `LightningModule`  \n",
    "* How to **checkpoint every epoch** with `ray.train.report()` for robust fault tolerance and auto-resume  \n",
    "* How to log and visualize metrics using **Ray’s built-in results and observability tooling**  \n",
    "* How to generate actions from a trained policy directly in-notebook, with **no need to repackage or redeploy**  \n",
    "* How to run the full pipeline on **Anyscale Workspaces** with no infrastructure setup or cluster config required  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What problem are you solving? (Inverted Pendulum, Diffusion-Style)\n",
    "\n",
    "You’re training a policy to **swing up and balance an inverted pendulum** — a classic control problem.  \n",
    "In the Gym `Pendulum-v1` env|ironment, the agent sees the current state of the pendulum and must decide what **torque** to apply at the pivot.\n",
    "\n",
    "---\n",
    "\n",
    "## What's a policy?\n",
    "\n",
    "A **policy** is a function that maps the current state to an action:\n",
    "\n",
    "$$\n",
    "\\pi_\\theta(s_{k}) \\;\\longrightarrow\\; u_{k}\n",
    "$$\n",
    "\n",
    "Here:\n",
    "- The **state** $s_k$ describes where the pendulum is and how fast it’s moving  \n",
    "- The **action** $u_k$ is the torque you apply to influence future motion  \n",
    "- The **goal** is to learn a policy that keeps the pendulum upright by generating the right torque at every step\n",
    "\n",
    "---\n",
    "\n",
    "## Environment state and action\n",
    "\n",
    "At each timestep:\n",
    "\n",
    "| Symbol        | Dim    | Meaning                           |\n",
    "|---------------|--------|-----------------------------------|\n",
    "| $\\theta_{k}$    | scalar | Angle of the pendulum             |\n",
    "| $\\dot\\theta_{k}$| scalar | Angular velocity                  |\n",
    "| $u_{k}$         | scalar | Torque applied to the base        |\n",
    "\n",
    "The pendulum starts hanging down and must swing up and balanced.\n",
    "\n",
    "Encode the state as:\n",
    "\n",
    "$$\n",
    "s_{k} = [\\cos\\theta_{k},\\ \\sin\\theta_{k},\\ \\dot\\theta_{k}] \\in \\mathbb{R}^3\n",
    "$$\n",
    "\n",
    "This avoids angle discontinuities (no $\\pm\\pi$ jumps) and keeps values in $[-1, 1]$.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Dataset tuples\n",
    "\n",
    "Train on a **log of actions** from a random policy, then inject artificial noise to simulate the diffusion process:\n",
    "\n",
    "$$\n",
    "\\varepsilon_{k} \\sim \\mathcal{N}(0, 1), \\quad t_{k} \\sim \\text{Uniform}\\{0,\\dots,T{-}1\\}\n",
    "$$\n",
    "\n",
    "and construct a noisy action:\n",
    "\n",
    "$$\n",
    "\\tilde{u}_k = u_{k} + \\varepsilon_{k}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Training objective\n",
    "\n",
    "Train a model $f_\\theta$ to predict the injected noise, given the state, the noisy action, and the timestep:\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = \\mathbb{E}_{s_{k},\\varepsilon_k,t_{k}}\\ \\big\\|f_\\theta(s_k, \\tilde{u}_k, t_{k}) - \\varepsilon_k\\big\\|_2^2\n",
    "$$\n",
    "\n",
    "Minimizing this loss teaches the model to **de-noise** $\\tilde{u}_{k}$ back toward the expert action $u_k$.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Reverse diffusion (sampling)\n",
    "\n",
    "At inference time, start from noise $x_T \\sim \\mathcal{N}(0, 1)$ and de-noise step by step:\n",
    "\n",
    "$$\n",
    "x_{t} \\;\\leftarrow\\; x_{t} - \\eta \\cdot f_\\theta(s, x_{t}, t), \\quad t = T{-}1, \\dots, 0\n",
    "$$\n",
    "\n",
    "After $T$ steps:\n",
    "\n",
    "$$\n",
    "x_0 \\approx u^\\star\n",
    "$$\n",
    "\n",
    "is a valid torque for the current state — a sample from your learned diffusion policy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to scale this policy learning workload using Ray on Anyscale\n",
    "\n",
    "This tutorial shows how to take a **local PyTorch + Gymnasium workflow** and migrate it to a fully **distributed, fault-tolerant Ray pipeline running on Anyscale** with minimal code changes.\n",
    "\n",
    "Here’s how the transition works:\n",
    "\n",
    "1. **Gym rollouts → Ray Dataset**  \n",
    "   Generate simulation rollouts from `Pendulum-v1` and stream them directly into a **Ray Dataset**, enabling distributed preprocessing (For example, normalization) and automatic partitioning across workers.\n",
    "\n",
    "2. **Local Training → Cluster-scale Distributed Training**  \n",
    "   Wrap a minimal `LightningModule` in a Ray Train `train_loop`, then launch training with **TorchTrainer** across 8 A10G GPUs. Ray handles data sharding, worker setup, and device placement without boilerplate.\n",
    "\n",
    "3. **Manual State Saving → Structured Checkpointing & Resumption**  \n",
    "   At the end of each epoch, save model weights and metadata with `ray.train.report(checkpoint=...)`. Ray then **auto-resumes training** from the latest checkpoint after restarts. This requires no further logic.\n",
    "\n",
    "4. **Ad-hoc Coordination → Declarative Orchestration**  \n",
    "   Replace manual logging, retry logic, and resource management with **Ray-native configs** (`ScalingConfig`, `CheckpointConfig`, `FailureConfig`), letting Ray + Anyscale own the orchestration.\n",
    "\n",
    "5. **Notebook-only Inference → Cluster-aware Evaluation**  \n",
    "   After training, perform **reverse diffusion sampling** in-notebook using the latest checkpoint—but this can easily scale to distributed Ray tasks or serve as the basis for a production rollout.\n",
    "\n",
    "This flow upgrades a local notebook into a **multi-node, resilient training + inference pipeline**, using Ray’s native abstractions and running seamlessly inside an Anyscale Workspace, without sacrificing dev agility.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports and setup\n",
    "\n",
    "Standard scientific-Python stack, plus **Ray** for distributed data/training\n",
    "and **Lightning** for ergonomic model training.\n",
    "Make sure your Anyscale cluster has Ray ≥ 2.48, so you get Ray Train V2 semantics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 00. Runtime setup — install same deps as build.sh and set env vars\n",
    "import os, sys, subprocess\n",
    "\n",
    "# Non-secret env var \n",
    "os.environ[\"RAY_TRAIN_V2_ENABLED\"] = \"1\"\n",
    "\n",
    "# Install Python dependencies \n",
    "subprocess.check_call([\n",
    "    sys.executable, \"-m\", \"pip\", \"install\", \"--no-cache-dir\",\n",
    "    \"torch==2.8.0\",\n",
    "    \"matplotlib==3.10.6\",\n",
    "    \"lightning==2.5.5\",\n",
    "    \"pyarrow==14.0.2\",\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 01. Imports\n",
    "\n",
    "# Standard Python packages for math, plotting, and data handling\n",
    "import os, shutil\n",
    "import json\n",
    "import uuid\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import gymnasium as gym\n",
    "\n",
    "# Ray libraries for distributed data and training\n",
    "import ray\n",
    "import ray.data\n",
    "from ray.train.lightning import RayLightningEnvironment  # Make sure RAY_TRAIN_V2_ENABLED=1 in \"Environment variables\"\n",
    "from ray.train import ScalingConfig, RunConfig, FailureConfig, CheckpointConfig, get_context, get_checkpoint, report, Checkpoint\n",
    "from ray.train.torch import TorchTrainer\n",
    "\n",
    "# PyTorch Lightning and base PyTorch for model definition and training\n",
    "import lightning.pytorch as pl\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generate a real pendulum dataset\n",
    "\n",
    "Roll out a random policy for **10 000 steps**, logging:\n",
    "\n",
    "| field | shape | description |\n",
    "|-------|-------|-------------|\n",
    "| `obs`          | `(3,)`  | `[cos θ, sin θ, θ̇]` |\n",
    "| `noisy_action` | `(1,)`  | ground-truth action + Gaussian noise |\n",
    "| `noise`        | `(1,)`  | the injected noise (supervision target) |\n",
    "| `timestep`     | `()`    | random diffusion step ∈ [0, 999] |\n",
    "\n",
    "You wrap the list of dicts in a **Ray Dataset** for automatic sharding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 02. Generate Pendulum offline dataset \n",
    "\n",
    "def make_pendulum_dataset(n_steps: int = 10_000):\n",
    "    \"\"\"\n",
    "    Roll out a random policy in Pendulum-v1 and log (obs, noisy_action, noise, timestep).\n",
    "    Returns a Ray Dataset ready for sharding.\n",
    "    \"\"\"\n",
    "    env = gym.make(\"Pendulum-v1\")\n",
    "    obs, _ = env.reset(seed=0)\n",
    "    data = []\n",
    "\n",
    "    for _ in range(n_steps):\n",
    "        action = env.action_space.sample().astype(np.float32)      # shape (1,)\n",
    "        noise   = np.random.randn(*action.shape).astype(np.float32)\n",
    "        noisy_action = action + noise                              # add Gaussian noise\n",
    "        timestep = np.random.randint(0, 1000, dtype=np.int64)\n",
    "\n",
    "        data.append(\n",
    "            {\n",
    "                \"obs\":        obs.astype(np.float32),              # shape (3,)\n",
    "                \"noisy_action\": noisy_action,                      # shape (1,)\n",
    "                \"noise\":        noise,                             # shape (1,)\n",
    "                \"timestep\":     timestep,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Step environment\n",
    "        obs, _, terminated, truncated, _ = env.step(action)\n",
    "        if terminated or truncated:\n",
    "            obs, _ = env.reset()\n",
    "\n",
    "    return ray.data.from_items(data)\n",
    "\n",
    "ds = make_pendulum_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Normalize and split\n",
    "\n",
    "Pendulum states lie roughly in **[–π, π]**.  \n",
    "Scale to **[–1, 1]**, then **shuffle** and split 80 / 20 into train and val shards.\n",
    "All transformations execute in parallel across the Ray cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 03. Normalize and split (vector obs ∈ [-π, π])\n",
    "\n",
    "# Normalize pixel values from [0, 1] to [-1, 1] for training\n",
    "def normalize(batch):\n",
    "    # Pendulum observations are roughly in [-π, π] → scale to [-1, 1]\n",
    "    batch[\"obs\"] = batch[\"obs\"] / np.pi\n",
    "    return batch\n",
    "\n",
    "# Apply normalization in parallel using Ray Data\n",
    "ds = ds.map_batches(normalize, batch_format=\"numpy\")\n",
    "\n",
    "# Count total number of items (triggers actual execution)\n",
    "total = ds.count()\n",
    "print(\"Total dataset size:\", total)\n",
    "\n",
    "# Shuffle and split dataset into 80% training and 20% validation\n",
    "split_idx = int(total * 0.8)\n",
    "ds = ds.random_shuffle()\n",
    "train_ds, val_ds = ds.split_at_indices([split_idx])\n",
    "\n",
    "print(\"Train size:\", train_ds.count())\n",
    "print(\"Val size:\", val_ds.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. DiffusionPolicy LightningModule\n",
    "\n",
    "A tiny MLP that predicts the injected noise ϵ given:\n",
    "\n",
    "- 3D normalized state  \n",
    "- 1D *noisy* action  \n",
    "- scalar timestep (normalized by `max_t`)\n",
    "\n",
    "It logs per-epoch losses to a shared JSON so you can plot later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 04. DiffusionPolicy for low-dim observation (3D) and action (1D)\n",
    "\n",
    "class DiffusionPolicy(pl.LightningModule):\n",
    "    def __init__(self, obs_dim: int = 3, act_dim: int = 1, max_t: int = 1000,\n",
    "                 log_path: str = \"/mnt/cluster_storage/pendulum_diffusion/epoch_metrics.json\"):\n",
    "        super().__init__()\n",
    "        self.max_t   = max_t\n",
    "        self.log_path = log_path\n",
    "\n",
    "        # 3D obs  + 1-D action  + 1 timestep  → 1-D noise\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_dim + act_dim + 1, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, act_dim),\n",
    "        )\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "        self._train_losses, self._val_losses = [], []\n",
    "\n",
    "    # ---------- forward ----------\n",
    "    def forward(self, obs, noisy_action, timestep):\n",
    "        t = timestep.view(-1, 1).float() / self.max_t\n",
    "        x = torch.cat([obs, noisy_action, t], dim=1)\n",
    "        return self.net(x)\n",
    "\n",
    "    # ---------- shared step ----------\n",
    "    def _shared_step(self, batch):\n",
    "        pred = self.forward(batch[\"obs\"].float(),\n",
    "                            batch[\"noisy_action\"],\n",
    "                            batch[\"timestep\"])\n",
    "        return self.loss_fn(pred, batch[\"noise\"])\n",
    "\n",
    "    def training_step(self, batch, _):\n",
    "        loss = self._shared_step(batch)\n",
    "        self._train_losses.append(loss.item())\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, _):\n",
    "        loss = self._shared_step(batch)\n",
    "        self._val_losses.append(loss.item())\n",
    "        return loss\n",
    "\n",
    "    # ---------- epoch-end ----------\n",
    "    def on_train_epoch_end(self):\n",
    "        rank = get_context().get_world_rank()\n",
    "        if rank == 0:\n",
    "            tr_avg = float(np.mean(self._train_losses))\n",
    "            va_avg = float(np.mean(self._val_losses)) if self._val_losses else None\n",
    "            print(f\"[Epoch {self.current_epoch}] \"\n",
    "                  f\"train={tr_avg:.4f}  val={va_avg if va_avg is not None else 'N/A'}\")\n",
    "\n",
    "            os.makedirs(os.path.dirname(self.log_path), exist_ok=True)\n",
    "            logs = []\n",
    "            if os.path.exists(self.log_path):\n",
    "                with open(self.log_path, \"r\") as f:\n",
    "                    logs = json.load(f)\n",
    "            logs.append({\"epoch\": self.current_epoch+1,\n",
    "                         \"train_loss\": tr_avg,\n",
    "                         \"val_loss\": va_avg})\n",
    "            with open(self.log_path, \"w\") as f:\n",
    "                json.dump(logs, f)\n",
    "\n",
    "        self._train_losses.clear(); self._val_losses.clear()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Distributed Train loop with checkpointing \n",
    "\n",
    "This per-worker function demonstrates:\n",
    "\n",
    "* **Ray Data to PyTorch**: `iter_torch_batches()` each epoch  \n",
    "* **Lightning-on-Ray**: single-GPU trainer per worker  \n",
    "* **Fault tolerance**: resume from the latest Ray Train checkpoint  \n",
    "* **Manual checkpoint**: save `model.pt` + `meta.pt` every epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 05. Training loop\n",
    "\n",
    "# Training function that runs on each Ray worker\n",
    "def train_loop(config):\n",
    "\n",
    "    # ---------- Paths for logs & checkpoints ----------\n",
    "    LOG_PATH  = \"/mnt/cluster_storage/pendulum_diffusion/epoch_metrics.json\"\n",
    "    CKPT_ROOT = \"/mnt/cluster_storage/pendulum_diffusion/pendulum_diffusion_ckpts\"\n",
    "\n",
    "    rank = get_context().get_world_rank()\n",
    "\n",
    "    # Create log/checkpoint dirs on rank 0\n",
    "    if rank == 0:\n",
    "        os.makedirs(CKPT_ROOT, exist_ok=True)\n",
    "        if not get_checkpoint() and os.path.exists(LOG_PATH):\n",
    "            os.remove(LOG_PATH)\n",
    "\n",
    "    # ---------- Load Ray Dataset shards ----------\n",
    "    train_ds = ray.train.get_dataset_shard(\"train\")\n",
    "    val_ds   = ray.train.get_dataset_shard(\"val\")\n",
    "\n",
    "    # ---------- Instantiate model ----------\n",
    "    model = DiffusionPolicy()\n",
    "    start_epoch = 0\n",
    "\n",
    "    # ---------- Resume from checkpoint ----------\n",
    "    ckpt = get_checkpoint()\n",
    "    if ckpt:\n",
    "        with ckpt.as_directory() as d:\n",
    "            model.load_state_dict(torch.load(os.path.join(d, \"model.pt\"), map_location=\"cpu\"))\n",
    "            start_epoch = torch.load(os.path.join(d, \"meta.pt\")).get(\"epoch\", 0) + 1\n",
    "        if rank == 0:\n",
    "            print(f\"[Rank {rank}] Resumed from checkpoint at epoch {start_epoch}\")\n",
    "\n",
    "    # ---------- Lightning Trainer ----------\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=config.get(\"epochs\", 10),\n",
    "        accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n",
    "        devices=1,\n",
    "        plugins=[RayLightningEnvironment()],\n",
    "        enable_progress_bar=False,\n",
    "        check_val_every_n_epoch=1,\n",
    "    )\n",
    "\n",
    "    # ---------- Training Loop ----------\n",
    "    for epoch in range(start_epoch, config.get(\"epochs\", 10)):\n",
    "\n",
    "        # Re-materialize fresh batches each epoch (avoid stale iterator errors)\n",
    "        train_data = list(train_ds.iter_torch_batches(\n",
    "            batch_size=32,\n",
    "            local_shuffle_buffer_size=1024,\n",
    "            prefetch_batches=1,\n",
    "            drop_last=True\n",
    "        ))\n",
    "        val_data = list(val_ds.iter_torch_batches(\n",
    "            batch_size=32,\n",
    "            local_shuffle_buffer_size=1024,\n",
    "            prefetch_batches=1,\n",
    "            drop_last=True\n",
    "        ))\n",
    "\n",
    "        # Wrap lists in PyTorch DataLoaders\n",
    "        train_loader = DataLoader(train_data, batch_size=None)\n",
    "        val_loader   = DataLoader(val_data, batch_size=None)\n",
    "\n",
    "        # Run one epoch (advance trainer manually)\n",
    "        trainer.fit_loop.max_epochs = epoch + 1\n",
    "        trainer.fit_loop.current_epoch = epoch\n",
    "        trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
    "\n",
    "        # ---------- Save checkpoint ----------\n",
    "        if rank == 0:\n",
    "            out_dir = os.path.join(CKPT_ROOT, f\"epoch_{epoch}_{uuid.uuid4().hex}\")\n",
    "            os.makedirs(out_dir, exist_ok=True)\n",
    "            torch.save(model.state_dict(), os.path.join(out_dir, \"model.pt\"))\n",
    "            torch.save({\"epoch\": epoch}, os.path.join(out_dir, \"meta.pt\"))\n",
    "            ckpt_out = Checkpoint.from_directory(out_dir)\n",
    "        else:\n",
    "            ckpt_out = None\n",
    "\n",
    "        # Report metrics + checkpoint back to Ray Train\n",
    "        report({\"epoch\": epoch}, checkpoint=ckpt_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Launch Ray TorchTrainer\n",
    "\n",
    "Eight A10G workers train in parallel.  \n",
    "`RunConfig` keeps the **five most recent checkpoints** and automatically restarts\n",
    "up to **3** times on failure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 06. Launch Ray Trainer\n",
    "\n",
    "# Configure Ray TorchTrainer to run the distributed training job\n",
    "trainer = TorchTrainer(\n",
    "    train_loop,\n",
    "    scaling_config=ScalingConfig(num_workers=8, use_gpu=True),\n",
    "    datasets={\"train\": train_ds, \"val\": val_ds},\n",
    "    run_config=RunConfig(\n",
    "        name=\"pendulum_diffusion_ft\",\n",
    "        storage_path=\"/mnt/cluster_storage/pendulum_diffusion/pendulum_diffusion_results\",\n",
    "        checkpoint_config=CheckpointConfig(\n",
    "            checkpoint_frequency=1,       # save every epoch\n",
    "            num_to_keep=5,\n",
    "            checkpoint_score_attribute=\"epoch\",\n",
    "            checkpoint_score_order=\"max\",\n",
    "        ),\n",
    "        failure_config=FailureConfig(max_failures=3),\n",
    "    ),\n",
    ")\n",
    "\n",
    "result = trainer.fit()\n",
    "print(\"Training complete →\", result.metrics)\n",
    "best_ckpt = result.checkpoint        # Checkpoint from last reported epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Plot train / val Loss\n",
    "\n",
    "Read the shared `epoch_metrics.json` and visualize convergence curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 07. Plot training and validation loss\n",
    "\n",
    "# Load training logs from shared file and plot losses\n",
    "log_path = \"/mnt/cluster_storage/pendulum_diffusion/epoch_metrics.json\"\n",
    "\n",
    "with open(log_path, \"r\") as f:\n",
    "    logs = json.load(f)\n",
    "\n",
    "# Convert logs to pandas DataFrame\n",
    "df = pd.DataFrame(logs)\n",
    "df[\"val_loss\"] = pd.to_numeric(df[\"val_loss\"], errors=\"coerce\")  # handle None\n",
    "df_grouped = df.groupby(\"epoch\", as_index=False).mean(numeric_only=True)\n",
    "\n",
    "\n",
    "# Plot training and validation loss curves\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(df_grouped[\"epoch\"], df_grouped[\"train_loss\"], marker=\"o\", label=\"Train Loss\")\n",
    "plt.plot(df_grouped[\"epoch\"], df_grouped[\"val_loss\"], marker=\"o\", label=\"Val Loss\")\n",
    "plt.title(\"Training & Validation Loss per Epoch\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Reverse diffusion helper\n",
    "\n",
    "Iteratively de-noise a random action vector **50 steps** back to a feasible Pendulum command.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 08. Reverse diffusion sampling for 1-D action\n",
    "\n",
    "# Function to simulate reverse diffusion process\n",
    "def sample_action(model, obs, n_steps=50, device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    Runs reverse diffusion starting from noise to generate a Pendulum action.\n",
    "    obs: torch.Tensor of shape (3,)\n",
    "    returns: torch.Tensor of shape (1,)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        obs = obs.unsqueeze(0).to(device)      # [1, 3]\n",
    "        obs = obs / np.pi                      # Same normalization used in training\n",
    "\n",
    "        x = torch.randn(1, 1).to(device)       # Start from noise in action space\n",
    "\n",
    "        for step in reversed(range(n_steps)):\n",
    "            t = torch.tensor([step], device=device)\n",
    "            pred_noise = model(obs, x, t)\n",
    "            x = x - pred_noise * 0.1\n",
    "\n",
    "        return x.squeeze(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Sample an action from the trained policy\n",
    "\n",
    "Finally, load the **latest epoch checkpoint**, supply a sample state  \n",
    "`[cos θ = 1, sin θ = 0, θ̇ = 0]`, and generate a 1-D torque command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 09. In-notebook sampling from trained model\n",
    "\n",
    "# A plausible pendulum state: [cos(theta), sin(theta), theta_dot]\n",
    "obs_sample = torch.tensor([1.0, 0.0, 0.0], dtype=torch.float32)   # shape (3,)\n",
    "\n",
    "# Load the most recent model checkpoint from the checkpoint directory\n",
    "CKPT_DIR = \"/mnt/cluster_storage/pendulum_diffusion/pendulum_diffusion_ckpts\"\n",
    "\n",
    "# Pick latest by sorted creation time (or filename if using uuid naming)\n",
    "latest = sorted(os.listdir(CKPT_DIR))[-1]\n",
    "model_path = os.path.join(CKPT_DIR, latest, \"model.pt\")\n",
    "\n",
    "model = DiffusionPolicy(obs_dim=3, act_dim=1)\n",
    "model.load_state_dict(torch.load(model_path, map_location=\"cpu\"))\n",
    "model = model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Run reverse diffusion sampling\n",
    "action = sample_action(model, obs_sample, n_steps=50, device=\"cpu\")\n",
    "print(\"Sampled action:\", action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Clean up\n",
    "\n",
    "When you're finished, release Ray resources and clear any temporary files.  \n",
    "This ensures the cluster is ready for other jobs and avoids unnecessary storage costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. Cleanup -- delete checkpoints and metrics from model training\n",
    "\n",
    "TARGET_PATH = \"/mnt/cluster_storage/pendulum_diffusion\"\n",
    "\n",
    "if os.path.exists(TARGET_PATH):\n",
    "    shutil.rmtree(TARGET_PATH)\n",
    "    print(f\"✅ Deleted everything under {TARGET_PATH}\")\n",
    "else:\n",
    "    print(f\"⚠️ Path does not exist: {TARGET_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrap up and next steps  \n",
    "\n",
    "You transformed a synthetic control demo into a **Ray-native, real-data pipeline**, training a diffusion policy across multiple GPUs, surviving worker restarts, and sampling feasible actions, all within a distributed Ray environment.\n",
    "\n",
    "You should now feel confident:\n",
    "\n",
    "* Logging continuous-control trajectories directly into a **Ray Dataset** for scalable preprocessing  \n",
    "* Streaming data into a **Ray Train** workload using Ray Data and Lightning with minimal integration overhead  \n",
    "* Saving structured checkpoints with `ray.train.report()` and leveraging **Ray’s fault-tolerant recovery**  \n",
    "* Running reverse diffusion sampling directly in-notebook—or scaling it up as **Ray remote tasks**  \n",
    "\n",
    "---\n",
    "\n",
    "### Where can you take this next?\n",
    "\n",
    "The following are a few directions you can explore to extend or adapt this workload:\n",
    "\n",
    "1. **Evaluate in the environment**  \n",
    "   * Load the best checkpoint, deploy the policy in Gym’s `Pendulum-v1`, and log episode returns.  \n",
    "   * Compare against baseline behavior cloning or TD3/TD3+BC.\n",
    "\n",
    "2. **Larger and richer datasets**  \n",
    "   * Generate 100 k+ steps with a scripted controller or collect data from a learned agent.  \n",
    "   * Swap in other classic-control tasks like `CartPole` or `MountainCar`.\n",
    "\n",
    "3. **Model and loss upgrades**  \n",
    "   * Add timestep embeddings or a small transformer for better temporal reasoning.  \n",
    "   * Experiment with different noise schedules or auxiliary consistency losses.\n",
    "\n",
    "4. **Hyperparameter sweeps**  \n",
    "   * Wrap the training loop in **Ray Tune** and grid-search learning rate, hidden size, or diffusion steps.  \n",
    "   * Use Tune’s automatic checkpoint pruning to keep only the top-N runs.\n",
    "\n",
    "5. **Mixed precision and performance**  \n",
    "   * Enable `torch.set_float32_matmul_precision('high')` to leverage A10G Tensor Cores.  \n",
    "   * Profile GPU utilization across workers and tune batch size accordingly.\n",
    "\n",
    "6. **Real robotics logs**  \n",
    "   * Replace Pendulum with logs from a real robotic apparatus stored in Parquet; Ray Data shards them the same way.\n",
    "\n",
    "7. **Serving the policy**  \n",
    "   * Export the trained MLP to TorchScript and deploy with **Ray Serve** for low-latency inference.  \n",
    "   * Hook it to a real-time simulator or a web dashboard.\n",
    "\n",
    "8. **End-to-end MLOps**  \n",
    "   * Track checkpoints and metrics with MLflow or Weights & Biases.  \n",
    "   * Schedule nightly Ray jobs on Anyscale to retrain as new data arrives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
