{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04a Computer-vision pattern with Ray Train\n",
    "\n",
    "This notebook is an end-to-end, **real-world computer-vision workflow** that runs seamlessly on an Anyscale cluster using **Ray Train**. You start by pulling a slice of the Food-101 dataset, push it through a lightweight preprocessing pipeline, store it efficiently in Parquet, and then fine-tune a ResNet-18 in a fault-tolerant, distributed manner. Along the way, you lean on Ray’s helpers to prepare data loaders, coordinate workers, checkpoint automatically, resume after failure, and even launch GPU inference jobs—all without writing a single line of low-level distributed code.\n",
    "\n",
    "## What you learn and take away\n",
    "\n",
    "- Launch distributed training with **Ray Train’s `TorchTrainer`** and configure it for multi-GPU, multi-node execution.  \n",
    "- Use **Ray Train’s built-in utilities** (`prepare_model`, `prepare_data_loader`, `get_checkpoint`, `train.report`) to wrap your existing PyTorch code without modifying your modeling logic.  \n",
    "- Save and resume from **automatic, fault-tolerant checkpoints** across epochs.  \n",
    "- Offload batch **inference as a Ray remote task**, allowing you to treat inference as a scalable workload.  \n",
    "- Run end-to-end training and evaluation without needing to understand the low-level mechanics of distributed systems.\n",
    "\n",
    "By the end of the tutorial you have a working model, clear loss curves, and a hands-on experience of how Ray Train simplifies distributed computer-vision workloads."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What problem are you solving? (image classification with Food-101-Lite)\n",
    "\n",
    "This notebook trains a neural network to **classify food photos** into one of **10 categories**  \n",
    "using the **Food-101-Lite** dataset—a compact, 10-class subset of the original Food-101 benchmark.\n",
    "\n",
    "---\n",
    "\n",
    "## Inputs  \n",
    "\n",
    "Every sample is a 3-channel Red-Green-Blue (RGB) image, resized to $224 \\times 224$:\n",
    "\n",
    "$$\n",
    "x \\;\\in\\; [0,1]^{3 \\times 224 \\times 224}\\;.\n",
    "$$\n",
    "\n",
    "You apply standard vision transforms (normalization, random crop/flip) and batch the data with plain **PyTorch DataLoader** (wrapped by `ray.train.torch.prepare_data_loader` for distributed training).\n",
    "\n",
    "---\n",
    "\n",
    "## Labels  \n",
    "\n",
    "Each image belongs to one of ten classes:\n",
    "\n",
    "['pizza', 'hamburger', 'sushi', 'ramen', 'fried rice',\n",
    "'steak', 'hot dog', 'pancake', 'burrito', 'caesar salad']\n",
    "\n",
    "\n",
    "The label is an integer $y \\in \\{0, \\dots, 9\\}$ used for supervision.\n",
    "\n",
    "---\n",
    "\n",
    "## What does the model learn?\n",
    "\n",
    "You train a compact CNN (For example, **ResNet-18**) to map an image \\(x\\) to class probabilities:\n",
    "\n",
    "$$\n",
    "f_\\theta(x)\\;=\\;\\hat{y}\\;\\in\\;\\mathbb{R}^{10}.\n",
    "$$\n",
    "\n",
    "Training minimizes the **cross-entropy loss**,\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(x,y)\\;=\\;-\\log \\bigl(\\hat{y}_{\\,y}\\bigr),\n",
    "$$\n",
    "\n",
    "so the network assigns high likelihood to the correct class.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to migrate this computer vision workload to a distributed setup using Ray on Anyscale\n",
    "In this tutorial, you start with a small PyTorch-based image classification task—training a ResNet-18 on a 10% slice of the Food-101 dataset, and progressively migrate it into a fully distributed, fault-tolerant training job using **Ray Train on Anyscale**. Your goal is to show you exactly how to scale *your existing workflow* without rewriting it from scratch.\n",
    "\n",
    "Use the following steps to migrate:\n",
    "\n",
    "1. **Preprocess data and persist it in a distributed-friendly format**  \n",
    "   You take raw images from Hugging Face’s `food101` dataset, apply `torchvision` resizing and center-cropping, and serialize them to **Parquet** using `pyarrow`. The system writes these Parquet files to the **Anyscale cluster’s shared storage volume** (`/mnt/cluster_storage`), so any node can access them, on any worker, without duplication or sync issues.\n",
    "\n",
    "2. **Create a lightweight PyTorch `Dataset` for Parquet ingestion**  \n",
    "   Instead of using Ray Data or Hugging Face `Dataset`, you implement a custom `Food101Dataset` that reads directly from the Parquet files. This provides control over the way the system reads rows and row groups. While this isn’t yet fully distributed, it allows you to simulate a real-world scenario where a developer starts with something simple before optimizing. **Note:** you use PyTorch style data loading in this tutorial to demonstrate (1) low level control in a PyTorch native environment and (2) how to move pre-existing PyTorch code into a distributed Anyscale environment. Other tutorials in this module incorporate Ray Data, so you can see how the two approaches differ.\n",
    "\n",
    "3. **Integrate Ray Train into the training loop**  \n",
    "   You encapsulate your existing PyTorch training logic in a `train_loop_per_worker()` function, which Ray Train executes on each worker, typically one per GPU. Inside this loop, you:\n",
    "\n",
    "   - Wrap the model with `prepare_model()` to make it compatible with distributed data parallelism.  \n",
    "   - Wrap the `DataLoader` with `prepare_data_loader()` to enable device placement and Ray worker context handling.  \n",
    "   - Add a `torch.utils.data.DistributedSampler` to each `DataLoader`, so that **data is correctly sharded across workers**—each worker only processes a unique subset of the training and validation datasets.  \n",
    "   - As required by the `DistributedSampler`, all `sampler.set_epoch(epoch)` at the start of each epoch to reshuffle the data correctly.\n",
    "   - Use Ray’s `Checkpoint` API to save and resume from checkpoints as needed.  \n",
    "   - Report training and validation metrics with `train.report()` after each epoch.\n",
    "\n",
    "4. **Launch training with `TorchTrainer` on an Anyscale cluster**  \n",
    "   You instantiate a `TorchTrainer` that runs:\n",
    "   - With `num_workers=8` and `use_gpu=True`. For example, across 8 A10 or A100 GPUs on Anyscale.  \n",
    "   - With `RunConfig` that sets checkpoint retention and auto-resume (with `max_failures=3`).  \n",
    "   - On infrastructure that's provisioned and scheduled by Anyscale with no manual Ray cluster setup required.  \n",
    "\n",
    "   Once launched, Ray automatically handles:\n",
    "   - Multi-node orchestration  \n",
    "   - Worker assignment and device pinning  \n",
    "   - Failure recovery and retry logic  \n",
    "   - Checkpointing and logging\n",
    "\n",
    "5. **Validate fault tolerance**  \n",
    "   You run `trainer.fit()` a second time. If manual intervention or failure interrupts the previous training, Ray picks up from the latest checkpoint. This shows **real-world robustness** without any manual checkpoint management or scripting.\n",
    "\n",
    "6. **Launch distributed GPU inference tasks**  \n",
    "   At the end, you define a Ray remote function (`@ray.remote(num_gpus=1)`) that loads the best checkpoint and runs inference on a single image from the validation set. You run this task on one GPU from the cluster.\n",
    "\n",
    "All of this runs inside a **managed Anyscale workspace**. You don’t need to start or SSH into clusters, worry about node IP, or configure NCCL. The entire setup is **declarative and self-contained in this notebook**, and you can re-run it or scale it up by changing a single parameter (`num_workers`).\n",
    "\n",
    "This tutorial mirrors how many ML teams operate in practice: starting with a working PyTorch training loop and migrating it to the cloud without rewriting core logic. With Ray Train on Anyscale, the migration is clean, incremental, and production-ready."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports  \n",
    "Before you start, gather every library you're going to rely on throughout this notebook. Pull in core Python utilities for file handling and plotting, PyTorch and TorchVision for deep-learning components, Ray Train for distributed orchestration, Hugging Face Datasets for quick data access, and PyArrow plus Pandas for fast Parquet IO. Importing everything up-front keeps the rest of the tutorial clean and predictable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 00. Runtime setup — install same deps and set env vars\n",
    "import os, sys, subprocess\n",
    "\n",
    "# Non-secret env var \n",
    "os.environ[\"RAY_TRAIN_V2_ENABLED\"] = \"1\"\n",
    "\n",
    "# Install Python dependencies \n",
    "subprocess.check_call([\n",
    "    sys.executable, \"-m\", \"pip\", \"install\", \"--no-cache-dir\",\n",
    "    \"torch==2.8.0\",\n",
    "    \"torchvision==0.23.0\",\n",
    "    \"matplotlib==3.10.6\",\n",
    "    \"pyarrow==14.0.2\",\n",
    "    \"datasets==2.19.2\",\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 01. Imports\n",
    "\n",
    "# ————————————————————————\n",
    "# Standard Library Utilities\n",
    "# ————————————————————————\n",
    "import os, io, tempfile, shutil  # file I/O and temp dirs\n",
    "import json                      # reading/writing configs\n",
    "import random, uuid              # randomness and unique IDs\n",
    "\n",
    "# ————————————————————————\n",
    "# Core Data & Storage Libraries\n",
    "# ————————————————————————\n",
    "import pandas as pd              # tabular data handling\n",
    "import numpy as np               # numerical ops\n",
    "import pyarrow as pa             # in-memory columnar format\n",
    "import pyarrow.parquet as pq     # reading/writing Parquet files\n",
    "from tqdm import tqdm            # progress bars\n",
    "\n",
    "# ————————————————————————\n",
    "# Image Handling & Visualization\n",
    "# ————————————————————————\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt  # plotting loss curves, images\n",
    "\n",
    "# ————————————————————————\n",
    "# PyTorch + TorchVision Core\n",
    "# ————————————————————————\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms as T\n",
    "from torchvision.models import resnet18\n",
    "from torchvision.transforms import Compose, Resize, CenterCrop\n",
    "\n",
    "# ————————————————————————\n",
    "# Ray Train: Distributed Training Primitives\n",
    "# ————————————————————————\n",
    "import ray\n",
    "import ray.train as train\n",
    "from ray.train.torch import (\n",
    "    prepare_model,\n",
    "    prepare_data_loader,\n",
    "    TorchTrainer,\n",
    ")\n",
    "from ray.train import (\n",
    "    ScalingConfig,\n",
    "    RunConfig,\n",
    "    FailureConfig,\n",
    "    CheckpointConfig,\n",
    "    Checkpoint,\n",
    "    get_checkpoint,\n",
    "    get_context,\n",
    ")\n",
    "\n",
    "# ————————————————————————\n",
    "# Dataset Access\n",
    "# ————————————————————————\n",
    "from datasets import load_dataset  # Hugging Face Datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load 10 % of Food-101  \n",
    "Next, get roughly 7,500 images, exactly 10% of Food-101—using a single call to `load_dataset`. This trimmed subset trains quickly while still being large enough to demonstrate Ray’s scaling behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 02. Load 10% of food101 (~7,500 images)\n",
    "ds = load_dataset(\"food101\", split=\"train[:10%]\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Resize and encode images  \n",
    "Preprocess each image: resize to 256 pixels, center-crop to 224 pixels (the size expected by most ImageNet models), and then convert the result to raw Joint Photographic Experts Group (JPEG) bytes. By storing bytes instead of full Python Imaging Library (PIL) objects, you keep the dataset compact and Parquet-friendly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 03. Resize and encode as JPEG bytes\n",
    "transform = Compose([Resize(256), CenterCrop(224)])\n",
    "records = []\n",
    "\n",
    "for example in tqdm(ds, desc=\"Preprocessing images\", unit=\"img\"):\n",
    "    try:\n",
    "        img = transform(example[\"image\"])\n",
    "        buf = io.BytesIO()\n",
    "        img.save(buf, format=\"JPEG\")\n",
    "        records.append({\n",
    "            \"image_bytes\": buf.getvalue(),\n",
    "            \"label\": example[\"label\"]\n",
    "        })\n",
    "    except Exception as e:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visual sanity check  \n",
    "Before committing to hours of training, take nine random samples and plot them with their class names. This quick inspection lets you properly align labels and confirm that images are correctly resized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 04. Visualize the dataset\n",
    "\n",
    "label_names = ds.features[\"label\"].names  # maps int → string\n",
    "\n",
    "samples = random.sample(records, 9)\n",
    "\n",
    "fig, axs = plt.subplots(3, 3, figsize=(8, 8))\n",
    "fig.suptitle(\"Sample Resized Images from food101-lite\", fontsize=16)\n",
    "\n",
    "for ax, rec in zip(axs.flatten(), samples):\n",
    "    img = Image.open(io.BytesIO(rec[\"image_bytes\"]))\n",
    "    label_name = label_names[rec[\"label\"]]\n",
    "    ax.imshow(img)\n",
    "    ax.set_title(label_name)\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Persist to Parquet  \n",
    "Write the images and labels to a Parquet file. Because Parquet is columnar, you can read just the columns you need during training, which speeds up IO—especially when multiple workers are reading in parallel under Ray."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 05. Write Dataset to Parquet\n",
    "\n",
    "output_dir = \"/mnt/cluster_storage/food101_lite/parquet_256\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "table = pa.Table.from_pydict({\n",
    "    \"image_bytes\": [r[\"image_bytes\"] for r in records],\n",
    "    \"label\": [r[\"label\"] for r in records]\n",
    "})\n",
    "pq.write_table(table, os.path.join(output_dir, \"shard_0.parquet\"))\n",
    "\n",
    "print(f\"Wrote {len(records)} records to {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Custom `Food101Dataset` for Parquet  \n",
    "To feed data into PyTorch, define a custom `Dataset`. You cache Parquet metadata, map global indices to specific row groups, and pull only the row you need. Each `__getitem__` returns an `(image, label)` pair that's immediately ready for further transforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 06. Define PyTorch Dataset that loads from Parquet\n",
    "\n",
    "class Food101Dataset(Dataset):\n",
    "    def __init__(self, parquet_path: str, transform=None):\n",
    "        self.parquet_file = pq.ParquetFile(parquet_path)\n",
    "        self.transform = transform\n",
    "\n",
    "        # Precompute a global row index to (row_group_idx, local_idx) map\n",
    "        self.row_group_map = []\n",
    "        for rg_idx in range(self.parquet_file.num_row_groups):\n",
    "            rg_meta = self.parquet_file.metadata.row_group(rg_idx)\n",
    "            num_rows = rg_meta.num_rows\n",
    "            self.row_group_map.extend([(rg_idx, i) for i in range(num_rows)])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.row_group_map)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row_group_idx, local_idx = self.row_group_map[idx]\n",
    "        # Read only the relevant row group (in memory-efficient batch---for scalability)\n",
    "        table = self.parquet_file.read_row_group(row_group_idx, columns=[\"image_bytes\", \"label\"])\n",
    "        row = table.to_pandas().iloc[local_idx]\n",
    "\n",
    "        img = Image.open(io.BytesIO(row[\"image_bytes\"])).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, row[\"label\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Image transform  \n",
    "Create a transform pipeline: `ToTensor()` followed by ImageNet mean and standard-deviation normalisation. By applying the transform inside the dataset, you make sure every worker, no matter where it runs, processes images in exactly the same way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 07. Define data preprocessing transform\n",
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD  = [0.229, 0.224, 0.225]\n",
    "\n",
    "transform = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Train/validation split  \n",
    "Shuffle the full Parquet table once (seeded for reproducibility) and then slice off the last 500 rows to construct the validation set. Write the train and validation partitions to their own Parquet files so you can load them independently later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 08. Create train/val Parquet splits \n",
    "full_path = \"/mnt/cluster_storage/food101_lite/parquet_256/shard_0.parquet\"\n",
    "\n",
    "df = (\n",
    "    pq.read_table(full_path)\n",
    "    .to_pandas()\n",
    "    .sample(frac=1.0, random_state=42)  # shuffle for reproducibility\n",
    ")\n",
    "\n",
    "df[:-500].to_parquet(\"/mnt/cluster_storage/food101_lite/train.parquet\")   # training\n",
    "df[-500:].to_parquet(\"/mnt/cluster_storage/food101_lite/val.parquet\")     # validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Inspect a DataLoader batch  \n",
    "Before you scale out, build a regular single-process `DataLoader`, pull one batch, and print its shape. This tiny test reassures you that batching, multiprocessing, and transforms work correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 09. Observe data shape\n",
    "\n",
    "loader = DataLoader(\n",
    "    Food101Dataset(\"/mnt/cluster_storage/food101_lite/train.parquet\", transform=transform),\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    ")\n",
    "\n",
    "for images, labels in loader:\n",
    "    print(images.shape, labels.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Helper: Ray-prepared DataLoaders  \n",
    "Wrap the DataLoader with `prepare_data_loader`. Ray injects a `DistributedSampler`, pins the loader to the correct GPU, and handles worker-rank bookkeeping. With this helper, your training loop can be isolated from distributed details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. Define helper to create prepared DataLoader\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "\n",
    "def build_dataloader(parquet_path: str, batch_size: int, shuffle=True):\n",
    "    dataset = Food101Dataset(parquet_path, transform=transform)\n",
    "\n",
    "    # Add a DistributedSampler to shard data across workers\n",
    "    sampler = DistributedSampler(\n",
    "        dataset,\n",
    "        num_replicas=train.get_context().get_world_size(),\n",
    "        rank=train.get_context().get_world_rank(),\n",
    "        shuffle=shuffle,\n",
    "        drop_last=shuffle,\n",
    "    )\n",
    "\n",
    "    loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        sampler=sampler,\n",
    "        num_workers=2,\n",
    "    )\n",
    "    return prepare_data_loader(loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. `train_loop_per_worker`  \n",
    "This is the workhorse called by each Ray worker. Inside, build the model, optimizer, and loss, try to resume from any existing checkpoint, run the training and validation loops, log metrics, and (on rank 0) save new checkpoints and append results to a history file. At the end, compute a final validation accuracy for good measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11. Define Ray Train train_loop_per_worker\n",
    "def train_loop_per_worker(config):\n",
    "\n",
    "    # === Model ===\n",
    "    net = resnet18(num_classes=101)\n",
    "    model = prepare_model(net)\n",
    "\n",
    "    # === Optimizer / Loss ===\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config[\"lr\"])\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # === Resume from Checkpoint ===\n",
    "    checkpoint = get_checkpoint()\n",
    "    start_epoch = 0\n",
    "    if checkpoint:\n",
    "        with checkpoint.as_directory() as ckpt_dir:\n",
    "            model.load_state_dict(torch.load(os.path.join(ckpt_dir, \"model.pt\")))\n",
    "            optimizer.load_state_dict(torch.load(os.path.join(ckpt_dir, \"optimizer.pt\")))\n",
    "            start_epoch = torch.load(os.path.join(ckpt_dir, \"extra.pt\"))[\"epoch\"]\n",
    "        print(f\"[Rank {get_context().get_world_rank()}] Resumed from checkpoint at epoch {start_epoch}\")\n",
    "\n",
    "    # === DataLoaders ===\n",
    "    train_loader = build_dataloader(\n",
    "        \"/mnt/cluster_storage/food101_lite/train.parquet\", config[\"batch_size\"], shuffle=True\n",
    "    )\n",
    "    val_loader = build_dataloader(\n",
    "        \"/mnt/cluster_storage/food101_lite/val.parquet\", config[\"batch_size\"], shuffle=False\n",
    "    )\n",
    "\n",
    "    # === Training Loop ===\n",
    "    for epoch in range(start_epoch, config[\"epochs\"]):\n",
    "        train_loader.sampler.set_epoch(epoch)  # required when using DistributedSampler\n",
    "        model.train()\n",
    "        train_loss_total = 0.0\n",
    "        train_batches = 0\n",
    "\n",
    "        for xb, yb in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(model(xb), yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss_total += loss.item()\n",
    "            train_batches += 1\n",
    "\n",
    "        train_loss = train_loss_total / train_batches\n",
    "\n",
    "        # === Validation Loop ===\n",
    "        model.eval()\n",
    "        val_loss_total = 0.0\n",
    "        val_batches = 0\n",
    "        with torch.no_grad():\n",
    "            for val_xb, val_yb in val_loader:\n",
    "                val_loss_total += criterion(model(val_xb), val_yb).item()\n",
    "                val_batches += 1\n",
    "        val_loss = val_loss_total / val_batches\n",
    "\n",
    "        metrics = {\"train_loss\": train_loss, \"val_loss\": val_loss, \"epoch\": epoch}\n",
    "        if train.get_context().get_world_rank() == 0:\n",
    "            print(metrics)\n",
    "\n",
    "        # === Save checkpoint only on rank 0 ===\n",
    "        if get_context().get_world_rank() == 0:\n",
    "            ckpt_dir = f\"/mnt/cluster_storage/food101_lite/tmp_checkpoints/epoch_{epoch}_{uuid.uuid4().hex}\"\n",
    "            os.makedirs(ckpt_dir, exist_ok=True)\n",
    "            torch.save(model.state_dict(), os.path.join(ckpt_dir, \"model.pt\"))\n",
    "            torch.save(optimizer.state_dict(), os.path.join(ckpt_dir, \"optimizer.pt\"))\n",
    "            torch.save({\"epoch\": epoch}, os.path.join(ckpt_dir, \"extra.pt\"))\n",
    "            checkpoint = Checkpoint.from_directory(ckpt_dir)\n",
    "        else:\n",
    "            checkpoint = None\n",
    "\n",
    "        # Append metrics to a file (only on rank 0)\n",
    "        if train.get_context().get_world_rank() == 0:\n",
    "            with open(\"/mnt/cluster_storage/food101_lite/results/history.csv\", \"a\") as f:\n",
    "                f.write(f\"{epoch},{train_loss},{val_loss}\\n\")\n",
    "        train.report(metrics, checkpoint=checkpoint)\n",
    "\n",
    "    correct, total = 0, 0\n",
    "    model.eval()\n",
    "    for xb, yb in val_loader:\n",
    "        xb, yb = xb.cuda(), yb.cuda()\n",
    "        pred = model(xb).argmax(dim=1)\n",
    "        correct += (pred == yb).sum().item()\n",
    "        total += yb.size(0)\n",
    "    accuracy = correct / total\n",
    "    print(f\"Val Accuracy: {accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Launch distributed training with `TorchTrainer`  \n",
    "Instantiate a `TorchTrainer`. Ask for eight GPU workers, enable up to three automatic retries, and tell Ray to keep the five checkpoints with the lowest validation loss. One call to `trainer.fit()` kicks off a fault-tolerant job on your Anyscale cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12. Run Training with Ray Train \n",
    "\n",
    "trainer = TorchTrainer(\n",
    "    train_loop_per_worker=train_loop_per_worker,\n",
    "    train_loop_config={\"lr\": 1e-3, \"batch_size\": 64, \"epochs\": 10},\n",
    "    scaling_config=ScalingConfig(num_workers=8, use_gpu=True),\n",
    "    run_config=RunConfig(\n",
    "        name=\"food101_ft_resume\",\n",
    "        storage_path=\"/mnt/cluster_storage/food101_lite/results\",\n",
    "        checkpoint_config=CheckpointConfig(\n",
    "            num_to_keep=5, \n",
    "            checkpoint_frequency=1,\n",
    "            checkpoint_score_attribute=\"val_loss\",\n",
    "            checkpoint_score_order=\"min\"\n",
    "        ),\n",
    "        failure_config=FailureConfig(max_failures=3),\n",
    "    ),\n",
    ")\n",
    "\n",
    "result = trainer.fit()\n",
    "print(\"Final metrics:\", result.metrics)\n",
    "best_ckpt = result.checkpoint  # this is the one with lowest val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Plot loss curves  \n",
    "When training finishes, read the CSV history file and plot training and validation losses for every epoch. Seeing the two curves together helps you spot over-fitting or under-fitting at a glance. In this example, given enough epochs, it's expected to see some over-fitting, as indicated by the decrease in training loss but increase in validation loss. As you're saving the checkpoints of the model with the lowest validation curve, you can test inference prior to this phenomenon. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13. Plot training / validation loss curves\n",
    "history_path = \"/mnt/cluster_storage/food101_lite/results/history.csv\"\n",
    "df = pd.read_csv(history_path, names=[\"epoch\", \"train_loss\", \"val_loss\"])\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(df[\"epoch\"], df[\"train_loss\"], label=\"Train Loss\", marker=\"o\")\n",
    "plt.plot(df[\"epoch\"], df[\"val_loss\"], label=\"Val Loss\", marker=\"o\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Train/Val Loss across Epochs\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Demonstrate fault-tolerant resumption  \n",
    "To prove that checkpointing works, run `trainer.fit()` a second time without changing anything. If the earlier run crashed mid-epoch, Ray automatically picks up the latest checkpoint and continue. If it already finished, Ray simply starts a clean new experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 14. Run the trainer again to demonstrate resuming from latest checkpoint  \n",
    "\n",
    "result = trainer.fit()\n",
    "print(\"Final metrics:\", result.metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Remote inference helper  \n",
    "Define a lightweight Ray remote function that loads a chosen checkpoint into a fresh `resnet18`, runs inference on one image, and returns both the predicted and true labels. Because the function requests one GPU, Ray schedules it on an appropriate node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 15. Define batch inference function\n",
    "\n",
    "@ray.remote(num_gpus=1)\n",
    "def run_inference_from_checkpoint(checkpoint_path, parquet_path, idx=0):\n",
    "\n",
    "    # === Load model ===\n",
    "    model = resnet18(num_classes=101)\n",
    "    checkpoint = Checkpoint.from_directory(checkpoint_path)\n",
    "\n",
    "    with checkpoint.as_directory() as ckpt_dir:\n",
    "        state_dict = torch.load(os.path.join(ckpt_dir, \"model.pt\"), map_location=\"cuda\")\n",
    "\n",
    "        # Strip \"module.\" prefix from distributed data parallelism trained weights\n",
    "        state_dict = {k.replace(\"module.\", \"\", 1): v for k, v in state_dict.items()}\n",
    "\n",
    "        model.load_state_dict(state_dict)\n",
    "\n",
    "    model.eval().cuda()\n",
    "\n",
    "    # === Define transform ===\n",
    "    transform = T.Compose([\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n",
    "    ])\n",
    "\n",
    "    # === Load dataset ===\n",
    "    dataset = Food101Dataset(parquet_path, transform=transform)\n",
    "    img, label = dataset[idx]\n",
    "    img = img.unsqueeze(0).cuda()  # batch size 1\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(img)\n",
    "        pred = torch.argmax(logits, dim=1).item()\n",
    "\n",
    "    return {\"predicted_label\": pred, \"true_label\": int(label), \"index\": idx}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Run and visualize inference  \n",
    "Using the checkpoint with the lowest validation loss, invoke your remote inference helper on a validation image. Then plot the image while displaying both the model’s prediction and the ground-truth class, giving you an immediate, intuitive sense of performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 16. Perform inference with best trained model (that is, lowest validation loss for a checkpointed model)\n",
    " \n",
    "checkpoint_root = \"/mnt/cluster_storage/food101_lite/results/food101_ft_resume\"\n",
    "\n",
    "checkpoint_dirs = sorted(\n",
    "    [\n",
    "        d for d in os.listdir(checkpoint_root)\n",
    "        if d.startswith(\"checkpoint_\") and os.path.isdir(os.path.join(checkpoint_root, d))\n",
    "    ],\n",
    "    reverse=True,\n",
    ")\n",
    "\n",
    "if not checkpoint_dirs:\n",
    "    raise FileNotFoundError(\"No checkpoint directories found.\")\n",
    "\n",
    "with result.checkpoint.as_directory() as ckpt_dir:\n",
    "    print(\"Best checkpoint contents:\", os.listdir(ckpt_dir))\n",
    "    best_ckpt_path = ckpt_dir \n",
    "parquet_path = \"/mnt/cluster_storage/food101_lite/val.parquet\"\n",
    "\n",
    "# Define which image to use\n",
    "idx = 2\n",
    "\n",
    "# Launch GPU inference task with Ray\n",
    "result = ray.get(run_inference_from_checkpoint.remote(best_ckpt_path, parquet_path, idx=idx))\n",
    "print(result)\n",
    "\n",
    "# Load label map from Hugging Face\n",
    "ds = load_dataset(\"food101\", split=\"train[:1%]\")  # load just to get label names\n",
    "label_names = ds.features[\"label\"].names\n",
    "\n",
    "# Load image from the same dataset locally\n",
    "dataset = Food101Dataset(parquet_path, transform=None)  # No transform = raw image\n",
    "img, _ = dataset[idx]\n",
    "\n",
    "# Plot the image with predicted and true labels\n",
    "plt.imshow(img)\n",
    "plt.axis(\"off\")\n",
    "plt.title(f\"Pred: {label_names[result['predicted_label']]}\\nTrue: {label_names[result['true_label']]}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17. Clean up  \n",
    "Finally, tidy up by deleting temporary checkpoint folders, the metrics CSV, and any intermediate result directories. Clearing out old artifacts frees disk space and leaves your workspace clean for whatever comes next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 17. Cleanup---delete checkpoints and metrics from model training\n",
    "\n",
    "# Base directory\n",
    "BASE_DIR = \"/mnt/cluster_storage/food101_lite\"\n",
    "\n",
    "# Paths to clean\n",
    "paths_to_delete = [\n",
    "    os.path.join(BASE_DIR, \"tmp_checkpoints\"),           # custom checkpoints\n",
    "    os.path.join(BASE_DIR, \"results\", \"history.csv\"),    # metrics history file\n",
    "    os.path.join(BASE_DIR, \"results\", \"food101_ft_resume\"),  # ray trainer run dir\n",
    "    os.path.join(BASE_DIR, \"results\", \"food101_ft_run\"),\n",
    "    os.path.join(BASE_DIR, \"results\", \"food101_single_run\"),\n",
    "]\n",
    "\n",
    "# Delete each path if it exists\n",
    "for path in paths_to_delete:\n",
    "    if os.path.exists(path):\n",
    "        if os.path.isfile(path):\n",
    "            os.remove(path)\n",
    "            print(f\"Deleted file: {path}\")\n",
    "        else:\n",
    "            shutil.rmtree(path)\n",
    "            print(f\"Deleted directory: {path}\")\n",
    "    else:\n",
    "        print(f\"Not found (skipped): {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrap up and next steps\n",
    "\n",
    "You've taken a realistic computer-vision workload, from raw images all the way to distributed training and GPU inference, and run it on Ray Train with zero boilerplate around GPUs, data parallelism, or fault-tolerance. You should now feel comfortable:\n",
    "\n",
    "* Using **Ray Train’s TorchTrainer** to scale PyTorch training across multiple GPUs and nodes with minimal code changes  \n",
    "* Wrapping models and data loaders with **`prepare_model()`** and **`prepare_data_loader()`** to enable Ray-managed device placement and distributed execution  \n",
    "* Sharding data across workers using **`DistributedSampler`**, and coordinating training epochs across Ray workers  \n",
    "* Configuring **automatic checkpointing and failure recovery** using Ray Train’s built-in `Checkpoint`, `RunConfig`, and `FailureConfig` APIs  \n",
    "* Running **GPU-backed Ray tasks** for distributed inference, showing how to serve and scale model predictions across a Ray cluster  \n",
    "\n",
    "---\n",
    "\n",
    "### Where can you take this next?\n",
    "\n",
    "Below are a few directions you might explore to adapt or extend the pattern:\n",
    "\n",
    "1. **Larger or custom datasets**  \n",
    "   * Swap in the full 75 k-image Food-101 split—or your own dataset in any storage backend (S3, GCS, Azure Blob).  \n",
    "   * Add multi-file Parquet sharding and let each worker read a different shard.\n",
    "\n",
    "2. **Model architectures**  \n",
    "   * Drop in Vision Transformers (`vit_b_16`, `vit_l_32`) or ConvNeXt; the prepare helpers work exactly the same.  \n",
    "   * Experiment with transfer learning versus training from scratch.\n",
    "\n",
    "3. **Mixed precision and performance tuning**  \n",
    "   * Enable automatic mixed precision (`torch.cuda.amp`) or bfloat16 to speed up training and save memory.  \n",
    "   * Profile data-loading throughput and play with `num_workers`, prefetching, and caching.\n",
    "\n",
    "4. **Hyperparameter sweeps**  \n",
    "   * Wrap the training loop in **Ray Tune** to search over learning rates, augmentations, or optimizers.  \n",
    "   * Use Ray’s integrated reporting to schedule early stopping.\n",
    "\n",
    "5. **Data augmentation pipelines**  \n",
    "   * Integrate additional transforms inside the dataset class for image augmentation.  \n",
    "   * Compare CPU versus GPU-side augmentations for throughput.\n",
    "\n",
    "6. **Distributed validation and metrics**  \n",
    "   * Replace your simple accuracy printout with more advanced metrics (F1, top-5 accuracy, confusion matrices).  \n",
    "\n",
    "7. **Model serving**  \n",
    "   * Convert the remote inference helper into a **Ray Serve** deployment for low-latency online predictions.  \n",
    "   * Auto-scale replicas based on request volume.\n",
    "\n",
    "8. **End-to-end MLOps**  \n",
    "   * Register checkpoints in a model registry (for example, MLflow, Weights & Biases, or Ray’s built-in MLflow integration).  \n",
    "   * Schedule the notebook as a Ray Job or CI/CD pipeline for regular retraining runs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
