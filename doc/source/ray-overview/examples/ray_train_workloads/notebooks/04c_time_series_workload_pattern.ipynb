{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04c Time-Series workload pattern with Ray Train  \n",
    "In this notebook you tackle **New York City (NYC) taxi-demand forecasting** (2014 half-hourly counts) and scale a *sequence-to-sequence Transformer* across an Anyscale cluster using **Ray Train V2**.\n",
    "\n",
    "## What you learn and take away  \n",
    "- **Ray Train V2 distributed loops**: wrap a PyTorch Transformer in `TorchTrainer` and run it across 8 GPUs with a *single* `ScalingConfig` line.  \n",
    "- **Fault-tolerant checkpointing on Anyscale**: recover seamlessly from pre-emptions or node failures with automatic epoch-level checkpoints.  \n",
    "- **Remote GPU inference from checkpoints**: spin up transient GPU actors for batch forecasts without redeploying the whole trainer.  \n",
    "By the end you know exactly how to take a single-node notebook forecast and scale it—data, training, and inference—on any Anyscale cluster.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What problem are you solving? (NYC taxi demand forecasting with a Transformer)\n",
    "\n",
    "You want to predict the **next 24 hours (48 half-hour slots)** of taxi pickups in NYC, given one week of historical demand.  \n",
    "Accurate short-term forecasts help ride-hailing fleets, traffic planners, and dynamic pricing engines allocate resources efficiently.\n",
    "\n",
    "---\n",
    "\n",
    "## What's a sequence-to-sequence Transformer?\n",
    "\n",
    "A **Transformer** models the joint distribution of a sequence by stacking self-attention layers that capture long-range dependencies without recurrence.  \n",
    "Your architecture learns a function  \n",
    "\n",
    "$$\n",
    "f_\\theta : \\underbrace{\\mathbb{R}^{T\\times 1}}_{\\text{past}} \\;\\longrightarrow\\; \\underbrace{\\mathbb{R}^{F}}_{\\text{future}}\n",
    "$$\n",
    "\n",
    "where $T=168$ half-hours (one week) and $F=48$.  \n",
    "During training you use **teacher forcing**, feeding the shifted ground truth to the decoder, so the model can focus on learning residual patterns rather than inventing an initial context.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to migrate this time-series workload to a distributed multi-node setup using Ray on Anyscale\n",
    "This tutorial walks through the end-to-end process of **migrating a single-GPU PyTorch forecasting pipeline to a distributed Ray cluster running on Anyscale**.\n",
    "\n",
    "Follow these steps to make the transition:\n",
    "\n",
    "1. **Migrate local CSV data to shared Parquet**  \n",
    "   Download the NYC taxi dataset as a CSV, resample it to 30-minute intervals, normalize the values, and save it as **Parquet shards** in a shared filesystem (`/mnt/cluster_storage`)—the default storage for Anyscale clusters.\n",
    "\n",
    "2. **Create sliding window generation for Distributed Data Parallel (DDP)**  \n",
    "   Create overlapping input and output windows (past to future) to train a forecasting model. While this preprocessing is local and sequential in this tutorial, it mirrors pipelines that parallelize with **Ray Data** in large-scale settings. See other tutorials in this module that incorporate Ray Data for reference.\n",
    "\n",
    "3. **Define a vanilla PyTorch function to use distributed Ray Train**  \n",
    "   Define a `train_loop_per_worker()` function and use **Ray Train** to launch **8 GPU workers** across the cluster. Each worker loads its own Parquet shard, trains independently under Distributed Data Parallel (DDP), and reports live metrics.\n",
    "\n",
    "4. **Configure Ray for scalable cluster orchestration**  \n",
    "   Instead of managing GPUs or process groups manually, configure `ScalingConfig`, `RunConfig`, and `FailureConfig`. **Ray and Anyscale handle fault-tolerant execution across nodes.**\n",
    "\n",
    "5. **Perform offline inference of distributed forecasting with remote Ray tasks**  \n",
    "   Define a `@ray.remote` forecasting function that loads a trained checkpoint and runs prediction on the latest data window. This allows **parallel, stateless inference** on any GPU in the cluster.\n",
    "\n",
    "This pattern takes a local academic-style time-series workflow and scales it into a **cluster-resilient, fault-tolerant forecasting pipeline**, all while preserving your native PyTorch modeling code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports  \n",
    "This cell loads all required libraries for the tutorial: PyData tools for data processing, PyTorch for model building and training, and Ray Train for distributed orchestration. `TorchTrainer` is the main training engine, while `prepare_model` and `prepare_data_loader` help convert vanilla PyTorch code into Ray-aware components that scale seamlessly across nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 00. Runtime setup — install same deps and set env vars\n",
    "import os, sys, subprocess\n",
    "\n",
    "# Non-secret env var \n",
    "os.environ[\"RAY_TRAIN_V2_ENABLED\"] = \"1\"\n",
    "\n",
    "# Install Python dependencies \n",
    "subprocess.check_call([\n",
    "    sys.executable, \"-m\", \"pip\", \"install\", \"--no-cache-dir\",\n",
    "    \"torch==2.8.0\",\n",
    "    \"matplotlib==3.10.6\",\n",
    "    \"pyarrow==14.0.2\",\n",
    "    \"datasets==2.19.2\",\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 01. Imports\n",
    "import os, io, math, uuid, shutil, random\n",
    "import requests, sys\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "from datasets import load_dataset   \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "import ray\n",
    "import ray.train as train\n",
    "from ray.train import (\n",
    "    ScalingConfig, RunConfig, FailureConfig,\n",
    "    CheckpointConfig, Checkpoint, get_checkpoint, get_context\n",
    ")\n",
    "from ray.train.torch import prepare_model, prepare_data_loader, TorchTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load NYC taxi passenger counts (30-min)  \n",
    "Download and cache a lightweight NYC taxi demand dataset from GitHub. Store the file under the shared `/mnt/cluster_storage` directory so that all Ray workers can read it without duplication. Parse the timestamps and used as the DataFrame index, making the data time-series ready."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 02. Load NYC taxi passenger counts (30-min) from GitHub raw – no auth, ~1 MB\n",
    "\n",
    "DATA_DIR = \"/mnt/cluster_storage/nyc_taxi_ts\"\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/numenta/NAB/master/data/realKnownCause/nyc_taxi.csv\"\n",
    "csv_path = os.path.join(DATA_DIR, \"nyc_taxi.csv\")\n",
    "\n",
    "if not os.path.exists(csv_path):\n",
    "    print(\"Downloading nyc_taxi.csv …\")\n",
    "    df = pd.read_csv(url)\n",
    "    df.to_csv(csv_path, index=False)\n",
    "else:\n",
    "    print(\"File already present.\")\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "# Parse timestamp and tidy\n",
    "df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"])\n",
    "df = df.set_index(\"timestamp\").rename(columns={\"value\": \"passengers\"})\n",
    "\n",
    "print(\"Rows:\", len(df), \"| Time span:\", df.index.min(), \"→\", df.index.max())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Resample to hourly, then normalize  \n",
    "Resample the dataset to 30-minute intervals (if it wasn’t already), then z-score the `passengers` column to get a standardized signal. This helps with training stability, gradient scale, and ensures the model doesn’t learn absolute magnitudes too early. You reverse the normalization after inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 03. Resample to hourly, then normalize\n",
    "hourly = df.resample(\"30min\").mean()\n",
    "\n",
    "mean, std = hourly[\"passengers\"].mean(), hourly[\"passengers\"].std()\n",
    "hourly[\"norm\"] = (hourly[\"passengers\"] - mean) / std\n",
    "\n",
    "print(f\"Half-Hourly rows: {len(hourly)}  |  mean={mean:.1f}, std={std:.1f}\")\n",
    "hourly.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Quick visual sanity-check  \n",
    "Before moving to training, it’s good practice to visualise the raw data. Plot the first two weeks of half-hourly taxi demand. This helps confirm that the series exhibits strong seasonality and contains no unexpected gaps or noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 04. Quick visual sanity-check — first two weeks\n",
    "plt.figure(figsize=(10, 4))\n",
    "hourly[\"passengers\"].iloc[:24*14].plot()\n",
    "plt.title(\"NYC-Taxi passengers - first 2 weeks of 2014\")\n",
    "plt.ylabel(\"# trips in hour\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Sliding-window dataset to Parquet  \n",
    "Convert the time-series into a supervised learning format using sliding windows. Each sample consists of a fixed-length input sequence (1 week of past data) and a prediction target (next 24 hours). Write these to columnar Parquet files on shared storage to enable efficient streaming in distributed training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 05. Build sliding-window dataset and write to Parquet\n",
    "# ----------------------------------------------------\n",
    "INPUT_WINDOW = 24 * 7   # 1/2 week history (in 30-min steps = 168)\n",
    "HORIZON      = 48       # predict next 24 h\n",
    "STRIDE       = 12       # slide 6 hours at a time\n",
    "\n",
    "values = hourly[\"norm\"].to_numpy(dtype=\"float32\")  # already normalised\n",
    "\n",
    "# ---- Time-aware split to avoid leakage between train and val ----\n",
    "cut = int(0.9 * len(values))  # split by time index on the original series\n",
    "train_records, val_records = [], []\n",
    "\n",
    "for s in range(0, len(values) - INPUT_WINDOW - HORIZON + 1, STRIDE):\n",
    "    past   = values[s : s + INPUT_WINDOW]\n",
    "    future = values[s + INPUT_WINDOW : s + INPUT_WINDOW + HORIZON]\n",
    "    end    = s + INPUT_WINDOW + HORIZON  # last index consumed by this window\n",
    "\n",
    "    rec = {\n",
    "        \"series_id\": 0,\n",
    "        \"past\":  past.tolist(),\n",
    "        \"future\": future.tolist(),\n",
    "    }\n",
    "\n",
    "    if end <= cut:         # Entire window ends before the cut to train\n",
    "        train_records.append(rec)\n",
    "    elif s >= cut:         # Window starts after the cut to val\n",
    "        val_records.append(rec)\n",
    "    # else: window crosses the cut to drop to prevent leakage\n",
    "\n",
    "print(f\"Windows → train: {len(train_records)}, val: {len(val_records)}\")\n",
    "\n",
    "# Write to Parquet\n",
    "DATA_DIR     = \"/mnt/cluster_storage/nyc_taxi_ts\"\n",
    "PARQUET_DIR  = os.path.join(DATA_DIR, \"parquet\")\n",
    "os.makedirs(PARQUET_DIR, exist_ok=True)\n",
    "\n",
    "schema = pa.schema([\n",
    "    (\"series_id\", pa.int32()),\n",
    "    (\"past\",  pa.list_(pa.float32())),\n",
    "    (\"future\", pa.list_(pa.float32()))\n",
    "])\n",
    "\n",
    "def write_parquet(records, fname):\n",
    "    pq.write_table(pa.Table.from_pylist(records, schema=schema), fname, version=\"2.6\")\n",
    "\n",
    "write_parquet(train_records, os.path.join(PARQUET_DIR, \"train.parquet\"))\n",
    "write_parquet(val_records,   os.path.join(PARQUET_DIR, \"val.parquet\"))\n",
    "print(\"Parquet shards written →\", PARQUET_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. PyTorch Dataset over Parquet  \n",
    "Define a lightweight PyTorch `Dataset` class that reads each window from the Parquet shard. This makes the model training logic agnostic to how you store the data. Your DataLoader receives standard PyTorch tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 06. PyTorch Dataset that reads the Parquet shards\n",
    "\n",
    "class TaxiWindowDataset(Dataset):\n",
    "    def __init__(self, parquet_path):\n",
    "        self.table  = pq.read_table(parquet_path)\n",
    "        self.past   = self.table.column(\"past\").to_pylist()\n",
    "        self.future = self.table.column(\"future\").to_pylist()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.past)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        past   = torch.tensor(self.past[idx],   dtype=torch.float32).unsqueeze(-1)   # (T, 1)\n",
    "        future = torch.tensor(self.future[idx], dtype=torch.float32)                 # (H,)\n",
    "        return past, future"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Inspect one random batch  \n",
    "Always verify shapes before diving into training. This cell uses a basic `DataLoader` to fetch one random batch and prints the dimensions of the input and target tensors. This ensures the encoder and decoder receive tensors of the correct size and shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 07. Inspect one random batch\n",
    "loader = DataLoader(TaxiWindowDataset(os.path.join(PARQUET_DIR, \"train.parquet\")),\n",
    "                    batch_size=4, shuffle=True)\n",
    "xb, yb = next(iter(loader))\n",
    "print(\"Past:\", xb.shape, \"Future:\", yb.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Ray-prepared DataLoader  \n",
    "Ray Train provides a helper to wrap your `DataLoader` so that it integrates seamlessly with distributed training. `prepare_data_loader` takes care of sharding and worker setup, ensuring each process only loads a subset of the data and communicates correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 08. Helper to build Ray-prepared DataLoader\n",
    "from ray.train.torch import prepare_data_loader\n",
    "\n",
    "def build_dataloader(parquet_path, batch_size, shuffle=True):\n",
    "    ds = TaxiWindowDataset(parquet_path)\n",
    "    loader = DataLoader(\n",
    "        ds, batch_size=batch_size, shuffle=shuffle, num_workers=2, drop_last=False,\n",
    "    )\n",
    "    return prepare_data_loader(loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. PositionalEncoding and Transformer model  \n",
    "This is the neural architecture that you train. It consists of a standard PyTorch Transformer with encoder-decoder structure and sinusoidal positional encodings. The model accepts a sequence of past observations (and optionally decoder inputs during training) and returns predictions for the future window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 09. PositionalEncoding and Transformer model (univariate)\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.2, max_len=1024):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float32).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2, dtype=torch.float32) * -(math.log(10000.0) / d_model)\n",
    "        )\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer(\"pe\", pe.unsqueeze(0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.dropout(x + self.pe[:, : x.size(1)])\n",
    "\n",
    "class TimeSeriesTransformer(nn.Module):\n",
    "    def __init__(self, input_window, horizon, d_model=64, nhead=8, num_layers=2):\n",
    "        super().__init__()\n",
    "        self.horizon  = horizon\n",
    "        self.d_model  = d_model\n",
    "\n",
    "        self.in_proj  = nn.Linear(1, d_model)\n",
    "        self.pos_enc  = PositionalEncoding(d_model)\n",
    "        self.tr_model = nn.Transformer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            num_encoder_layers=num_layers,\n",
    "            num_decoder_layers=num_layers,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.out_proj = nn.Linear(d_model, 1)\n",
    "\n",
    "    def forward(self, past, decoder_input=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            past           : (B, T, 1)    — encoder input\n",
    "            decoder_input  : (B, F, 1)    — optional decoder input (teacher forcing)\n",
    "        Returns:\n",
    "            preds          : (B, F)       — predicted future values\n",
    "        \"\"\"\n",
    "        B = past.size(0)\n",
    "\n",
    "        # Encoder input\n",
    "        src = self.in_proj(past) * math.sqrt(self.d_model)\n",
    "        src = self.pos_enc(src)\n",
    "\n",
    "        # Decoder input\n",
    "        if decoder_input is None:\n",
    "            decoder_input = past[:, -1:, :].repeat(1, self.horizon, 1)\n",
    "\n",
    "        tgt = self.in_proj(decoder_input) * math.sqrt(self.d_model)\n",
    "        tgt = self.pos_enc(tgt)\n",
    "\n",
    "        # Transformer forward\n",
    "        output = self.tr_model(src, tgt)  # shape: (B, F, d_model)\n",
    "        return self.out_proj(output).squeeze(-1)  # shape: (B, F)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Ray Train training loop (with teacher forcing)  \n",
    "This is the heart of Ray Train. Each worker executes this loop independently, but Ray orchestrates everything from checkpointing to failure recovery. Include teacher forcing, feeding the shifted ground-truth to the decoder, which allows the model to learn more quickly than starting from zero. Also log training and validation loss per epoch and save checkpoints to the shared filesystem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. Ray Train train_loop_per_worker with checkpointing, teacher forcing, and clean structure\n",
    "\n",
    "def train_loop_per_worker(config):\n",
    "    torch.manual_seed(0)\n",
    "\n",
    "    # ─────────────────────────────────────────────────────────────\n",
    "    # 1. Instantiate and prepare the model\n",
    "    # ─────────────────────────────────────────────────────────────\n",
    "    model = TimeSeriesTransformer(\n",
    "        input_window=INPUT_WINDOW,\n",
    "        horizon=HORIZON,\n",
    "        d_model=config[\"d_model\"],\n",
    "        nhead=config[\"nhead\"],\n",
    "        num_layers=config[\"num_layers\"],\n",
    "    )\n",
    "    model = train.torch.prepare_model(model)  # wrap in DDP if needed\n",
    "\n",
    "    # ─────────────────────────────────────────────────────────────\n",
    "    # 2. Define optimizer and loss\n",
    "    # ─────────────────────────────────────────────────────────────\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config[\"lr\"])\n",
    "    loss_fn  = nn.SmoothL1Loss()\n",
    "\n",
    "    # ─────────────────────────────────────────────────────────────\n",
    "    # 3. Restore checkpoint if available\n",
    "    # ─────────────────────────────────────────────────────────────\n",
    "    start_epoch = 0\n",
    "    checkpoint = get_checkpoint()\n",
    "    if checkpoint:\n",
    "        with checkpoint.as_directory() as ckpt_dir:\n",
    "            model.load_state_dict(torch.load(os.path.join(ckpt_dir, \"model.pt\")))\n",
    "            optimizer.load_state_dict(torch.load(os.path.join(ckpt_dir, \"optim.pt\")))\n",
    "            start_epoch = torch.load(os.path.join(ckpt_dir, \"extra.pt\"))[\"epoch\"] + 1\n",
    "        print(f\"[Rank {get_context().get_world_rank()}] Resumed @ epoch {start_epoch}\")\n",
    "\n",
    "    # ─────────────────────────────────────────────────────────────\n",
    "    # 4. Load data for this worker\n",
    "    # ─────────────────────────────────────────────────────────────\n",
    "    train_loader = build_dataloader(\n",
    "        os.path.join(PARQUET_DIR, \"train.parquet\"),\n",
    "        batch_size=config[\"bs\"],\n",
    "        shuffle=True,\n",
    "    )\n",
    "    val_loader = build_dataloader(\n",
    "        os.path.join(PARQUET_DIR, \"val.parquet\"),\n",
    "        batch_size=config[\"bs\"],\n",
    "        shuffle=False,\n",
    "    )\n",
    "\n",
    "    # ─────────────────────────────────────────────────────────────\n",
    "    # 5. Epoch loop\n",
    "    # ─────────────────────────────────────────────────────────────\n",
    "    for epoch in range(start_epoch, config[\"epochs\"]):\n",
    "        model.train()\n",
    "        train_loss_sum = 0.0\n",
    "\n",
    "        # ───── Training step ─────\n",
    "        for past, future in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Teacher forcing: shift future targets to use as decoder input\n",
    "            future = future.unsqueeze(-1)                          # (B, F, 1)\n",
    "            start_token = torch.zeros_like(future[:, :1])         # (B, 1, 1)\n",
    "            decoder_input = torch.cat([start_token, future[:, :-1]], dim=1)  # (B, F, 1)\n",
    "\n",
    "            # Forward and loss\n",
    "            pred = model(past, decoder_input)                     # (B, F)\n",
    "            loss = loss_fn(pred, future.squeeze(-1))             # (B, F) vs (B, F)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss_sum += loss.item()\n",
    "\n",
    "        avg_train_loss = train_loss_sum / len(train_loader)\n",
    "\n",
    "        # ───── Validation step ─────\n",
    "        model.eval()\n",
    "        val_loss_sum = 0.0\n",
    "        with torch.no_grad():\n",
    "            for past, future in val_loader:\n",
    "                pred = model(past)                               # model inference (zeros as decoder input)\n",
    "                loss = loss_fn(pred, future)\n",
    "                val_loss_sum += loss.item()\n",
    "        avg_val_loss = val_loss_sum / len(val_loader)\n",
    "\n",
    "        # ──────────────────────────────────────────────────────────────\n",
    "        # 6. Report metrics and optionally save checkpoint (rank 0 only)\n",
    "        # ──────────────────────────────────────────────────────────────\n",
    "        metrics = {\n",
    "            \"epoch\": epoch,\n",
    "            \"train_loss\": avg_train_loss,\n",
    "            \"val_loss\": avg_val_loss,\n",
    "        }\n",
    "\n",
    "        if get_context().get_world_rank() == 0:\n",
    "            print(metrics)\n",
    "\n",
    "            # Save checkpoint\n",
    "            ckpt_dir = f\"{DATA_DIR}/tmp_ckpts/epoch_{epoch}_{uuid.uuid4().hex}\"\n",
    "            os.makedirs(ckpt_dir, exist_ok=True)\n",
    "            torch.save(model.state_dict(), os.path.join(ckpt_dir, \"model.pt\"))\n",
    "            torch.save(optimizer.state_dict(), os.path.join(ckpt_dir, \"optim.pt\"))\n",
    "            torch.save({\"epoch\": epoch}, os.path.join(ckpt_dir, \"extra.pt\"))\n",
    "            checkpoint_out = Checkpoint.from_directory(ckpt_dir)\n",
    "\n",
    "            # Save loss history\n",
    "            hist_path = os.path.join(DATA_DIR, \"results\", \"history.csv\")\n",
    "            with open(hist_path, \"a\") as f:\n",
    "                f.write(f\"{epoch},{avg_train_loss:.6f},{avg_val_loss:.6f}\\n\")\n",
    "        else:\n",
    "            checkpoint_out = None\n",
    "\n",
    "        train.report(metrics, checkpoint=checkpoint_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Launch training on 8 GPUs  \n",
    "Construct a `TorchTrainer` and run it. Ray automatically distributes the model across 8 GPUs, prepares the datasets for each worker, and starts training. Also configure checkpointing to retain the top-performing models and set failure recovery to 3 attempts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11. Launch training\n",
    "\n",
    "os.makedirs(os.path.join(DATA_DIR, \"results\"), exist_ok=True)\n",
    "\n",
    "trainer = TorchTrainer(\n",
    "    train_loop_per_worker=train_loop_per_worker,\n",
    "    train_loop_config={\"lr\": 1e-3, \"bs\": 4, \"epochs\": 20,\n",
    "                       \"d_model\": 128, \"nhead\": 4, \"num_layers\": 3},\n",
    "    scaling_config=ScalingConfig(num_workers=8, use_gpu=True),\n",
    "    run_config=RunConfig(\n",
    "        name=\"nyc_taxi_transformer\",\n",
    "        storage_path=os.path.join(DATA_DIR, \"results\"),\n",
    "        checkpoint_config=CheckpointConfig(\n",
    "            num_to_keep=4, checkpoint_frequency=1,\n",
    "            checkpoint_score_attribute=\"val_loss\", checkpoint_score_order=\"min\"),\n",
    "        failure_config=FailureConfig(max_failures=3),\n",
    "    ),\n",
    ")\n",
    "\n",
    "result = trainer.fit()\n",
    "print(\"Final metrics:\", result.metrics)\n",
    "best_ckpt = result.checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Plot training and validation loss  \n",
    "After training, visualize the saved `history.csv` to assess whether the model is over-fitting, under-fitting, or improving steadily. A healthy curve shows decreasing train and validation loss, with convergence over time. This diagnostic is especially useful when comparing different model configurations. In this tutorial, you aren't using substantial amounts of data, so you see the validation curve remains primarily stagnant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12. Plot loss curves \n",
    "\n",
    "hist_path = os.path.join(DATA_DIR, \"results\", \"history.csv\")\n",
    "\n",
    "if os.path.exists(hist_path):\n",
    "    df_hist = pd.read_csv(hist_path, names=[\"epoch\", \"train_loss\", \"val_loss\"])\n",
    "    plt.figure(figsize=(8,4))\n",
    "    plt.plot(df_hist[\"epoch\"], df_hist[\"train_loss\"], label=\"Train\", marker=\"o\")\n",
    "    plt.plot(df_hist[\"epoch\"], df_hist[\"val_loss\"], label=\"Val\",   marker=\"o\")\n",
    "    plt.xlabel(\"Epoch\"); plt.ylabel(\"MSE Loss\"); plt.grid(True); plt.legend()\n",
    "    plt.title(\"Train vs. Val Loss\"); plt.tight_layout(); plt.show()\n",
    "else:\n",
    "    print(\"No history.csv found. Make sure to log it in the training loop.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Resume training from checkpoint  \n",
    "Ray automatically resumes from the latest checkpoint if one is available. This makes fault tolerance seamless—if the system interrupts a node or it fails, training can pick up where it left off with no manual intervention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13. Demonstrate fault-tolerant resume\n",
    "result = trainer.fit()\n",
    "print(\"Metrics after resume run:\", result.metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Inference helper—remote GPU forecast  \n",
    "Define a Ray remote function that loads a model checkpoint on a spare GPU and generates a forecast given a recent input window. Ray distributes this function to any available resource and runs independently of the training workers, making it suitable for batch inference or serving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 14. Inference helper–forecast next 12 hours from best checkpoint\n",
    "\n",
    "@ray.remote(num_gpus=1)\n",
    "def forecast_from_checkpoint(ckpt_path, past_array):\n",
    "    ck = Checkpoint.from_directory(ckpt_path)\n",
    "    model = TimeSeriesTransformer(INPUT_WINDOW, HORIZON, d_model=128, nhead=4, num_layers=3)\n",
    "    with ck.as_directory() as d:\n",
    "        sd = torch.load(os.path.join(d, \"model.pt\"), map_location=\"cuda\")\n",
    "        sd = {k.replace(\"module.\", \"\", 1): v for k,v in sd.items()}\n",
    "        model.load_state_dict(sd)\n",
    "    model.eval().cuda()\n",
    "\n",
    "    past = torch.tensor(past_array, dtype=torch.float32).unsqueeze(0).unsqueeze(-1).cuda()\n",
    "    with torch.no_grad():\n",
    "        pred_norm = model(past).cpu().squeeze().numpy()\n",
    "    return pred_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Run inference and visualize prediction  \n",
    "Get the most recent week of taxi data, run it through your trained model, and plot the predicted future demand against the actual values. This gives you a visual check of model quality and allows you to verify whether the model has learned temporal patterns like daily or weekly cycles.\n",
    "\n",
    "Due to the small size of the dataset, the model in this tutorial learns the mean of the data (a constant solution). Improving these results requires more training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15. Run inference on the latest window and plot\n",
    "\n",
    "# Get last week of data\n",
    "past_norm = hourly[\"norm\"].iloc[-INPUT_WINDOW:].to_numpy()\n",
    "future_true = hourly[\"passengers\"].iloc[-HORIZON:].to_numpy()\n",
    "\n",
    "with best_ckpt.as_directory() as p:\n",
    "    pred_norm = ray.get(forecast_from_checkpoint.remote(p, past_norm))\n",
    "\n",
    "# De-normalize\n",
    "mean, std = hourly[\"passengers\"].mean(), hourly[\"passengers\"].std()\n",
    "pred = pred_norm * std + mean\n",
    "past = past_norm * std + mean\n",
    "\n",
    "# Plot\n",
    "import matplotlib.pyplot as plt\n",
    "t_past   = np.arange(-INPUT_WINDOW, 0)\n",
    "STEP_SIZE_HOURS = 0.5  # because you're now using 30min data\n",
    "t_future = np.arange(0, HORIZON) * STEP_SIZE_HOURS\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.plot(t_past, past, label=\"History\")\n",
    "plt.plot(t_future, future_true, \"--\", label=\"Ground Truth\")\n",
    "plt.plot(t_future, pred,  \"-.\", label=\"Forecast\")\n",
    "plt.axvline(0, color=\"black\"); plt.xlabel(\"Hours relative\"); plt.ylabel(\"# trips\")\n",
    "plt.title(\"NYC-Taxi 24 h Forecast\"); plt.legend(); plt.grid(); plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Cleanup: remove all training artifacts  \n",
    "Finally, tidy up by deleting temporary checkpoint folders, the metrics CSV, and any intermediate result directories. Clearing out old artifacts frees disk space and leaves your workspace clean for whatever comes next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(raylet, ip=10.0.98.167)\u001b[0m Spilled 6096 MiB, 10 objects, write throughput 742 MiB/s. Set RAY_verbose_spill_logs=0 to disable this message.\",\"component\":\"raylet\",\"filename\":\"local_object_manager.cc\",\"lineno\":259}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m WARNING: 8 PYTHON worker processes have been started on node: 9cb171e30a8b8e86a2c30d2b4a698d4e94f31d29ee391a083db570e1 with address: 10.0.82.26. This could be a result of using a large number of actors, or due to tasks blocked in ray.get() calls (see https://github.com/ray-project/ray/issues/3644 for some discussion of workarounds).\n",
      "\u001b[33m(raylet)\u001b[0m WARNING: 14 PYTHON worker processes have been started on node: 9cb171e30a8b8e86a2c30d2b4a698d4e94f31d29ee391a083db570e1 with address: 10.0.82.26. This could be a result of using a large number of actors, or due to tasks blocked in ray.get() calls (see https://github.com/ray-project/ray/issues/3644 for some discussion of workarounds).\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(autoscaler +1h23m18s)\u001b[0m Tip: use `ray status` to view detailed cluster status. To disable these messages, set RAY_SCHEDULER_EVENTS=0.\n"
     ]
    }
   ],
   "source": [
    "# 16. Cleanup – optionally remove all artifacts to free space\n",
    "if os.path.exists(DATA_DIR):\n",
    "    shutil.rmtree(DATA_DIR)\n",
    "    print(f\"Deleted {DATA_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrap up and next steps\n",
    "\n",
    "You built a robust, distributed forecasting workflow using **Ray Train on Anyscale** that:\n",
    "\n",
    "* Trains a Transformer model across **multiple GPUs** using **Ray Train with Distributed Data Parallel (DDP)**, abstracting away low-level orchestration.\n",
    "* Recovers automatically from failures with **built-in checkpointing and resume**, even across re-launches or node churn.\n",
    "* Logs and reports per-epoch metrics using **Ray Train’s reporting APIs**, enabling real-time monitoring and seamless plotting.\n",
    "* Performs inference using **Ray remote tasks**, allowing you to scale forecasting across GPUs or nodes without changing model code.\n",
    "\n",
    "---\n",
    "\n",
    "### Where can you take this next?\n",
    "\n",
    "The following are a few directions you can explore to extend or adapt this workload:\n",
    "\n",
    "1. **Hyperparameter sweeps**  \n",
    "   * Wrap the `TorchTrainer` with **Ray Tune** to search over `d_model`, `nhead`, learning rate, and window sizes.  \n",
    "\n",
    "2. **Probabilistic forecasting**  \n",
    "   * Output percentiles or fit a distribution head (for example, Gaussian) to capture prediction uncertainty.  \n",
    "\n",
    "3. **Multivariate and exogenous features**  \n",
    "   * Add weather, holidays, or ride-sharing surge multipliers as extra input channels.  \n",
    "\n",
    "4. **Early-stopping and LR scheduling**  \n",
    "   * Monitor val-loss and reduce LR on plateau, or stop when improvement < 1 %.  \n",
    "\n",
    "5. **Model compression**  \n",
    "   * Distill the large Transformer into a lightweight LSTM or Tiny-Transformer for edge deployment.  \n",
    "\n",
    "6. **Streaming and online learning**  \n",
    "   * Use **Ray Serve** to deploy the model and update weights periodically with the latest data.  \n",
    "\n",
    "7. **Interpretability**  \n",
    "   * Visualize attention maps to see which time lags the model focuses on—effective for stakeholder trust.  \n",
    "\n",
    "8. **End-to-end MLOps**  \n",
    "   * Schedule nightly retraining with **Ray jobs**, log artifacts to MLflow or Weights & Biases, and automate model promotion.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
