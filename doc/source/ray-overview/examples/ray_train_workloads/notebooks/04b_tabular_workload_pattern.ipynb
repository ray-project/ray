{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🏔️ 04b · Tabular Workload Pattern with Ray Train  \n",
    "In this tutorial you take the classic **Cover type forest-cover dataset** (580 k rows, 54 tabular features) and scale an **XGBoost** model across an Anyscale cluster using **Ray Train V2**.\n",
    "\n",
    "### What you’ll learn & take away\n",
    "\n",
    "- Ingest tabular data at scale using **Ray Data** and persist it to Parquet for reproducibility  \n",
    "- Launch a fault-tolerant, checkpoint enabled **XGBoost training loop** on multiple CPUs using **Ray Train**  \n",
    "- Resume training from checkpoints across job restarts and hardware failures  \n",
    "- Evaluate model accuracy, visualize feature importance, and scale batch inference using **Ray remote tasks**  \n",
    "- Understand how to port classic gradient boosting workflows into a **fully distributed, multi-node training setup on Anyscale**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🔢 What problem are you solving? (Forest Cover Classification with XGBoost)\n",
    "\n",
    "You're predicting which **type of forest vegetation** (For example, Lodge-pole Pine, Spruce/Fir, Aspen) is present at a given land location, using only numeric and binary cartographic features such as elevation, slope, soil type, and proximity to roads or hydrology.\n",
    "\n",
    "---\n",
    "\n",
    "### What's XGBoost?\n",
    "\n",
    "**XGBoost** (Extreme Gradient Boosting) is a fast, scalable machine learning algorithm based on **gradient-boosted decision trees**. It builds a sequence of shallow decision trees, where each new tree tries to correct the errors of the previous ensemble by minimizing a differentiable loss (like log-loss).\n",
    "\n",
    "In your case, minimize the **multi-class Softmax log-loss**, learning a function:\n",
    "\n",
    "$$\n",
    "f_\\theta: \\mathbb{R}^{54} \\rightarrow \\{0, 1, \\dots, 6\\}\n",
    "$$\n",
    "\n",
    "that maps a 54-dimensional tabular input (raw geo-spatial features) to a forest cover type. Each boosting round fits a new tree on the gradient of the loss, gradually improving accuracy over hundreds of rounds.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🧭 How you’ll migrate this tabular workload to a distributed setup using Ray on Anyscale\n",
    "\n",
    "This tutorial walks through the end-to-end process of **migrating a local XGBoost training pipeline to a distributed Ray cluster running on Anyscale**.\n",
    "\n",
    "Here’s how you make that transition:\n",
    "\n",
    "1. **Local → Remote Data**  \n",
    "   Store the raw data as Parquet in a shared cloud directory and load it using **Ray Data**, which streams and shards the dataset across workers automatically.\n",
    "\n",
    "2. **Single-process → Multi-worker Training**  \n",
    "   Define a custom `train_func`, then let **Ray Train** spin up 16 distributed training workers (1 per CPU) and run `xgb.train` in parallel, each with its own data shard.\n",
    "\n",
    "3. **Manual Checkpointing → Automated Fault Tolerance**  \n",
    "   With `RayTrainReportCallback` and `CheckpointConfig`, Ray saves checkpoints every 10 boosting rounds and can resume mid-training if any worker crashes or a job is re-launched.\n",
    "\n",
    "4. **Manual Loops → Cluster-scale Abstractions**  \n",
    "   Skip the boilerplate of manually slicing datasets, coordinating workers, or building launch scripts. Instead, declare intent (with `ScalingConfig`, `RunConfig`, and `FailureConfig`) and let **Ray + Anyscale** manage the execution.\n",
    "\n",
    "5. **Offline Inference → Remote Tasks**  \n",
    "   Batch inference can launch as **Ray remote tasks** on CPU workers, which is useful for validation, drift detection, or live scoring inside a service.\n",
    "\n",
    "This pattern turns a traditional single-node workflow into a scalable, resilient training pipeline with minimal code changes, and it works seamlessly on any cluster you provision through Anyscale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 · Imports  \n",
    "Before you touch any data, import every tool you need.  \n",
    "Alongside the standard scientific-Python stack, bring in **XGBoost** for gradient-boosted decision trees and **Ray** for distributed data loading and training. Ray Train’s helper classes (RunConfig, ScalingConfig, CheckpointConfig, FailureConfig) give you fault-tolerant, CPU training with almost no extra code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 00. Runtime setup — install same deps and set env vars\n",
    "import os, sys, subprocess\n",
    "\n",
    "# Non-secret env var \n",
    "os.environ[\"RAY_TRAIN_V2_ENABLED\"] = \"1\"\n",
    "\n",
    "# Install Python dependencies \n",
    "subprocess.check_call([\n",
    "    sys.executable, \"-m\", \"pip\", \"install\", \"--no-cache-dir\",\n",
    "    \"matplotlib==3.10.6\",\n",
    "    \"scikit-learn==1.7.2\",\n",
    "    \"pyarrow==14.0.2\",    \n",
    "    \"xgboost==3.0.5\",\n",
    "    \"seaborn==0.13.2\",\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 01. Imports\n",
    "import os, shutil, json, uuid, tempfile, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import fetch_covtype\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "\n",
    "import ray\n",
    "import ray.data as rd\n",
    "from ray.train import RunConfig, ScalingConfig, CheckpointConfig, FailureConfig, get_dataset_shard, get_checkpoint, get_context\n",
    "from ray.train.xgboost import XGBoostTrainer, RayTrainReportCallback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 · Load the University of California, Irvine (UCI) Cover type dataset  \n",
    "The Cover type dataset contains ~580 000 forest-cover observations with 54 tabular features and a 7-class label. Fetch it from `sklearn.datasets`, rename the target column to `label` (Ray’s default), and shift the classes from **1-7** to **0-6** so they're zero-indexed as XGBoost expects. A quick `value_counts` sanity-check confirms the mapping worked.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 02. Load the UCI Cover type dataset (~580k rows, 54 features)\n",
    "data = fetch_covtype(as_frame=True)\n",
    "df = data.frame\n",
    "df.rename(columns={\"Cover_Type\": \"label\"}, inplace=True)   # Ray expects \"label\"\n",
    "df[\"label\"] = df[\"label\"] - 1          # 1-7  →  0-6\n",
    "assert df[\"label\"].between(0, 6).all()\n",
    "print(df.shape, df.label.value_counts(normalize=True).head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 · Visualise class balance  \n",
    "Highly imbalanced targets can bias tree-based models, so plot the raw label counts. The cover type distribution shows skew, but not much—the bar chart lets you judge whether extra re-scaling or class-weighting is necessary (You rely on XGBoost’s built-in handling for now)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 03. Visualize class distribution\n",
    "df.label.value_counts().plot(kind=\"bar\", figsize=(6,3), title=\"Cover Type distribution\")\n",
    "plt.ylabel(\"Frequency\"); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 · Persist the dataset to Parquet  \n",
    "Storing the data-frame once on the cluster’s shared file-system keeps later steps fast and reproducible. Parquet is columnar, compressed, and lazily readable by Ray Data, which means you can stream partitions to workers without loading everything into RAM.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 04. Write to /mnt/cluster_storage/covtype/\n",
    "PARQUET_DIR = \"/mnt/cluster_storage/covtype/parquet\"\n",
    "os.makedirs(PARQUET_DIR, exist_ok=True)\n",
    "file_path = os.path.join(PARQUET_DIR, \"covtype.parquet\")\n",
    "df.to_parquet(file_path)\n",
    "print(f\"Wrote Parquet -> {file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 · Read the data as a Ray Dataset  \n",
    "`ray.data.read_parquet` gives you a **lazy, columnar dataset** and shuffles it on-the-fly. From this point on, every split, batch, or transformation executes in parallel across the cluster, so you avoid a single-node bottleneck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 05. Load dataset into a Ray Dataset (lazy, columnar)\n",
    "ds_full = rd.read_parquet(file_path).random_shuffle()      \n",
    "print(ds_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6 · Create train / validation splits  \n",
    "Perform an 80 / 20 split directly on the Ray Dataset, preserving the lazy execution plan. Each subset remains a Ray Dataset object, so they can later stream to the training workers in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 06. Split to train / validation Ray Datasets\n",
    "train_ds, val_ds = ds_full.split_proportionately([0.8])\n",
    "print(f\"Train rows: {train_ds.count()},  Val rows: {val_ds.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7 · Inspect a mini-batch  \n",
    "Taking a tiny pandas batch helps verify that feature columns and labels have the expected shapes and types. You also build `feature_columns`, a list you reuse when building XGBoost’s `DMatrix`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 07. Look into one batch to confirm feature dimensionality\n",
    "batch = train_ds.take_batch(batch_size=5, batch_format=\"pandas\")\n",
    "print(batch.head())\n",
    "feature_columns = [c for c in batch.columns if c != \"label\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8 · Custom per-worker training loop  \n",
    "Ray Train launches one copy of `train_func` on every worker (16 CPUs in your case).  \n",
    "Inside the loop you:  \n",
    "1. Pull the local shard of both the training and validation Ray datasets.  \n",
    "2. Convert each pandas shard into an XGBoost `DMatrix` (efficient Certificate Signing Request (CSR) format).  \n",
    "3. Resume from an existing checkpoint if Ray passed one in with `get_checkpoint()`.  \n",
    "4. Call `xgb.train`, handing it a `RayTrainReportCallback` so that **every boosting round automatically reports metrics**.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 08. Custom Ray Train loop for XGBoost (CPU)\n",
    "\n",
    "def train_func(config):\n",
    "    \"\"\"Per-worker training loop executed by Ray Train.\"\"\"\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # 1. Pull this worker’s data shard from Ray Datasets\n",
    "    # --------------------------------------------------------\n",
    "    label_col   = config[\"label_column\"]\n",
    "    train_df    = get_dataset_shard(\"train\").materialize().to_pandas()\n",
    "    eval_df     = get_dataset_shard(\"evaluation\").materialize().to_pandas()\n",
    "    feature_cols = [c for c in train_df.columns if c != label_col]\n",
    "\n",
    "    # Convert pandas → DMatrix (fast CSR format used by XGBoost)\n",
    "    dtrain = xgb.DMatrix(train_df[feature_cols], label=train_df[label_col])\n",
    "    deval  = xgb.DMatrix(eval_df[feature_cols],  label=eval_df[label_col])\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # 2. Train booster — RayTrainReportCallback handles:\n",
    "    #       • per-round ray.train.report(...)\n",
    "    #       • checkpoint upload to Ray storage\n",
    "    # --------------------------------------------------------\n",
    "\n",
    "    # Optional resume from checkpoint (Ray sets this automatically if resuming)\n",
    "    ckpt = get_checkpoint()\n",
    "    if ckpt:\n",
    "        with ckpt.as_directory() as d:\n",
    "            model_path = os.path.join(d, RayTrainReportCallback.CHECKPOINT_NAME)\n",
    "            booster = xgb.Booster()\n",
    "            booster.load_model(model_path)\n",
    "            print(f\"[Rank {get_context().get_world_rank()}] Resumed from checkpoint\")\n",
    "    else:\n",
    "        booster = None\n",
    "    \n",
    "    evals_result = {}  # <- XGBoost fills this with per-iteration metrics\n",
    "\n",
    "    xgb.train(\n",
    "        params          = config[\"params\"],\n",
    "        dtrain          = dtrain,\n",
    "        evals           = [(dtrain, \"train\"), (deval, \"validation\")],  # ← CHANGED label only\n",
    "        num_boost_round = config[\"num_boost_round\"],\n",
    "        xgb_model       = booster,  # <- resumes if booster is not None\n",
    "        evals_result    = evals_result,  # <- NEW: capture metrics per round\n",
    "        callbacks=[\n",
    "            RayTrainReportCallback()  # ← CHANGED: let it auto-collect metrics\n",
    "        ],\n",
    "    )\n",
    "    # --------------------------------------------------------\n",
    "    # 3. Rank-0 writes metrics JSON to the shared path\n",
    "    # --------------------------------------------------------\n",
    "\n",
    "    if get_context().get_world_rank() == 0:\n",
    "        out_json_path = config[\"out_json_path\"]\n",
    "\n",
    "        # Optionally add a quick “best” summary for convenience\n",
    "        v_hist = evals_result.get(\"validation\", {}).get(\"mlogloss\", [])\n",
    "        best_idx = int(np.argmin(v_hist)) if len(v_hist) else None\n",
    "        payload = {\n",
    "            \"evals_result\": evals_result,\n",
    "            \"best\": {\n",
    "                \"iteration\": (best_idx + 1) if best_idx is not None else None,\n",
    "                \"validation-mlogloss\": (float(v_hist[best_idx]) if best_idx is not None else None),\n",
    "            },\n",
    "        }\n",
    "\n",
    "        os.makedirs(os.path.dirname(out_json_path), exist_ok=True)\n",
    "        with open(out_json_path, \"w\") as f:\n",
    "            json.dump(payload, f)\n",
    "        print(f\"[Rank 0] Wrote metrics JSON → {out_json_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9 · Configure XGBoost and build the Trainer  \n",
    "Here you define all model hyper-parameters (objective, number of classes, CPU tree method, etc.) and wrap `train_func` inside an `XGBoostTrainer`.  \n",
    "* `ScalingConfig(num_workers=16, use_gpu=False)` allocates one CPU per worker.  \n",
    "* `CheckpointConfig(checkpoint_frequency=10, num_to_keep=3)` keeps the three most recent checkpoints.  \n",
    "* `FailureConfig(max_failures=1)` tells Ray to retry training up to one time if a worker crashes.  \n",
    "Because you pass the Ray Datasets directly, Ray takes care of sharding them evenly across workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 09. XGBoost config + Trainer (uses train_func above)\n",
    "xgb_params = {\n",
    "    \"objective\": \"multi:softprob\",\n",
    "    \"num_class\": 7,\n",
    "    \"eval_metric\": \"mlogloss\",\n",
    "    \"tree_method\": \"hist\",  # CPU histogram algorithm \n",
    "    \"eta\": 0.3,\n",
    "    \"max_depth\": 8,\n",
    "}\n",
    "\n",
    "trainer = XGBoostTrainer(\n",
    "    train_func,                \n",
    "    scaling_config   = ScalingConfig(num_workers=16, use_gpu=False),\n",
    "    datasets         = {\"train\": train_ds, \"evaluation\": val_ds},\n",
    "    train_loop_config={\n",
    "        \"label_column\": \"label\",\n",
    "        \"params\": xgb_params,\n",
    "        \"num_boost_round\": 50,  # Increase or decrease to adjust training iterations\n",
    "        \"out_json_path\": \"/mnt/cluster_storage/covtype/results/covtype_xgb_cpu/metrics.json\",\n",
    "    },\n",
    "    run_config       = RunConfig(\n",
    "        name=\"covtype_xgb_cpu\",\n",
    "        storage_path=\"/mnt/cluster_storage/covtype/results\",\n",
    "        checkpoint_config=CheckpointConfig(checkpoint_frequency=10, num_to_keep=1),\n",
    "        failure_config=FailureConfig(max_failures=1),  # resume up to 3 times\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10 · Start distributed training  \n",
    "`trainer.fit()` blocks until all boosting rounds finish (or until Ray exhausts retries).  The result object contains the last reported metrics and the best checkpoint found so far. Print the final validation log-loss and keep a handle to the checkpoint for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. Fit the trainer (reports eval metrics every boosting round)\n",
    "result = trainer.fit()\n",
    "best_ckpt = result.checkpoint            # saved automatically by Trainer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11 · Plot log-loss over boosting rounds  \n",
    "During training you captured the full per-round evaluation history using XGBoost’s built-in `evals_result` and saved it to JSON. Reloading that JSON now gives you both training and validation log-loss values for each boosting round. Plotting these lists against their round index shows how the model converges: training loss decrease steadily, while validation loss follows, maintaining a small gap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11. Plot evaluation history from saved JSON\n",
    "\n",
    "with open(\"/mnt/cluster_storage/covtype/results/covtype_xgb_cpu/metrics.json\") as f:\n",
    "    payload = json.load(f)\n",
    "\n",
    "hist = payload[\"evals_result\"]\n",
    "train = hist[\"train\"][\"mlogloss\"]\n",
    "val   = hist[\"validation\"][\"mlogloss\"]\n",
    "\n",
    "xs = np.arange(1, len(val) + 1)\n",
    "plt.figure(figsize=(7,4))\n",
    "plt.plot(xs, train, label=\"Train\")\n",
    "plt.plot(xs, val,   label=\"Val\")\n",
    "plt.xlabel(\"Boosting round\"); plt.ylabel(\"Log-loss\"); plt.title(\"XGBoost log-loss\")\n",
    "plt.grid(True); plt.legend(); plt.tight_layout(); plt.show()\n",
    "\n",
    "best = payload[\"best\"][\"validation-mlogloss\"]\n",
    "print(\"Best validation log-loss:\", best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12 · Evaluate the trained model  \n",
    "Pull the XGBoost `Booster` back from the checkpoint, run predictions on the entire validation set, and compute overall accuracy. Converting the Ray Dataset to pandas keeps the example short; in production you could stream batches instead of materialising the whole frame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12. Retrieve Booster object from Ray Checkpoint\n",
    "booster = RayTrainReportCallback.get_model(best_ckpt)\n",
    "\n",
    "# Convert Ray Dataset → pandas for quick local scoring\n",
    "val_pd = val_ds.to_pandas()\n",
    "dmatrix = xgb.DMatrix(val_pd[feature_columns])\n",
    "pred_prob = booster.predict(dmatrix)\n",
    "pred_labels = np.argmax(pred_prob, axis=1)\n",
    "\n",
    "acc = accuracy_score(val_pd.label, pred_labels)\n",
    "print(f\"Validation accuracy: {acc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13 · Confusion matrix visualisation  \n",
    "Raw counts and row-normalised ratios highlight which cover types the model confuses most often. Diagonal dominance indicates good performance; off-diagonal hot spots may suggest a need for more data or feature engineering for those specific classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13. Confusion matrix\n",
    "\n",
    "cm = confusion_matrix(val_pd.label, pred_labels)  # or sample_batch.label if used\n",
    "\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"viridis\")\n",
    "plt.title(\"Confusion Matrix with Counts\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.show()\n",
    "\n",
    "cm_norm = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis]\n",
    "sns.heatmap(cm_norm, annot=True, fmt=\".2f\", cmap=\"viridis\")\n",
    "plt.title(\"Normalized Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14 · CPU batch inference with Ray remote tasks  \n",
    "To demonstrate scalable inference, send a 1024-row pandas batch to a **single CPU worker**.  The remote function loads the model once per task, converts the batch to `DMatrix`, and returns class indices. Measure accuracy on the fly to confirm that out-of-process inference matches earlier results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 14. Example: Run batch inference using Ray remote task on a CPU worker\n",
    "\n",
    "# This remote function is scheduled on a CPU-enabled Ray worker.\n",
    "# It loads a trained XGBoost model from a Ray checkpoint and runs predictions on a pandas DataFrame.\n",
    "@ray.remote(num_cpus=1)\n",
    "def predict_batch(ckpt, batch_pd):\n",
    "    # Load the trained XGBoost Booster model from the checkpoint.\n",
    "    model = RayTrainReportCallback.get_model(ckpt)\n",
    "\n",
    "    # Convert the input batch (pandas DataFrame) to DMatrix, required by XGBoost for inference.\n",
    "    dmatrix = xgb.DMatrix(batch_pd[feature_columns])\n",
    "\n",
    "    # Predict class probabilities for each row in the batch.\n",
    "    preds = model.predict(dmatrix)\n",
    "\n",
    "    # Select the class with highest predicted probability for each row.\n",
    "    return np.argmax(preds, axis=1)\n",
    "\n",
    "# Take a random sample of 1024 rows from the validation set to use as input.\n",
    "sample_batch = val_pd.sample(1024, random_state=0)\n",
    "\n",
    "# Submit the batch inference task to a Ray worker and block until it finishes.\n",
    "preds = ray.get(predict_batch.remote(best_ckpt, sample_batch))\n",
    "\n",
    "# Compute and print classification accuracy by comparing predictions to true labels.\n",
    "print(\"Sample batch accuracy:\", accuracy_score(sample_batch.label, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15 · Feature-importance diagnostics  \n",
    "XGBoost’s built-in `get_score(importance_type=\"gain\")` ranks each feature by its average gain across all splits. Visualising the top-15 helps connect model behaviour back to domain knowledge. For example, elevation, and soil-type often dominate forest-cover prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15. Gain‑based feature importance\n",
    "importances = booster.get_score(importance_type=\"gain\")\n",
    "keys, gains = zip(*sorted(importances.items(), key=lambda kv: kv[1], reverse=True)[:15])\n",
    "\n",
    "plt.barh(range(len(gains)), gains)\n",
    "plt.yticks(range(len(gains)), keys)\n",
    "plt.gca().invert_yaxis()\n",
    "plt.title(\"Top-15 Feature Importances (gain)\"); plt.xlabel(\"Average gain\"); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 16 · Continue training from the latest checkpoint  \n",
    "Because `train_func` always checks for `get_checkpoint()`, re-invoking `trainer.fit()` automatically resumes boosting from where you left off. Simply call `fit()` a second time and print the new best validation log-loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 16. Run 50 more training iterations from the last saved checkpoint\n",
    "result = trainer.fit()\n",
    "best_ckpt = result.checkpoint            # saved automatically by Trainer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 17 · Verify post-training inference \n",
    "Rerun the same batch-inference helper to confirm that extra boosting rounds improved accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 17. Rerun example batch inference from before to verify improved accuracy:\n",
    "\n",
    "# Take a random sample of 1024 rows from the validation set to use as input.\n",
    "sample_batch = val_pd.sample(1024, random_state=0)\n",
    "\n",
    "# Submit the batch inference task to a Ray worker and block until it finishes.\n",
    "preds = ray.get(predict_batch.remote(best_ckpt, sample_batch))\n",
    "\n",
    "# Compute and print classification accuracy by comparing predictions to true labels.\n",
    "print(\"Sample batch accuracy:\", accuracy_score(sample_batch.label, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 18 · Cleanup  \n",
    "Finally, tidy up by deleting temporary checkpoint folders, the metrics CSV, and any intermediate result directories. Clearing out old artefacts frees disk space and leaves your workspace clean for whatever comes next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 18. Optional clean‑up to free space\n",
    "ARTIFACT_DIR = \"/mnt/cluster_storage/covtype\"\n",
    "if os.path.exists(ARTIFACT_DIR):\n",
    "    shutil.rmtree(ARTIFACT_DIR)\n",
    "    print(f\"Deleted {ARTIFACT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎉 Wrapping Up & Next Steps\n",
    "\n",
    "Awesome work making it to the end. You’ve built a fast and fault-tolerant XGBoost training loop that runs on real data, scales across CPUs, recovers from worker failures, and supports batch inference, all inside a single notebook.\n",
    "\n",
    "You should now feel confident:\n",
    "\n",
    "* Using **Ray Data** to ingest, shuffle, and shard large tabular datasets across a cluster  \n",
    "* Defining custom `train_func`s that run on **Ray Train** workers and resume seamlessly from checkpoints  \n",
    "* Tracking per-round metrics and saving checkpoints with **RayTrainReportCallback**  \n",
    "* Leveraging **Ray’s distributed execution model** to evaluate and monitor models without manual orchestration  \n",
    "* Launching remote CPU-powered inference tasks using **Ray remote functions** for scalable batch scoring\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### 🚀 Where can you take this next?\n",
    "\n",
    "Below are a few directions you might explore to adapt or extend the pattern:\n",
    "\n",
    "1. **Early Stopping & Best Iteration Tracking**  \n",
    "   * Add `early_stopping_rounds=10` to `xgb.train` and log the best round.  \n",
    "   * Track performance delta across resumed runs.\n",
    "\n",
    "2. **Hyperparameter Sweeps**  \n",
    "   * Wrap the trainer with **Ray Tune** and search over `eta`, `max_depth`, or `subsample`.  \n",
    "   * Use Tune’s built-in checkpoint pruning and log callbacks.\n",
    "\n",
    "3. **Feature Engineering at Scale**  \n",
    "   * Create new features using `Ray Dataset.map_batches`, such as terrain interactions or log-scaled distances.  \n",
    "   * Materialize multiple Parquet shards and benchmark load time.\n",
    "\n",
    "4. **Model Interpretability**  \n",
    "   * Use XGBoost’s built-in `Booster.get_score` for feature attributions.  \n",
    "   * Rank features by importance and validate with domain knowledge.\n",
    "\n",
    "5. **Serving the Model**  \n",
    "   * Package the Booster as a Ray task or **Ray Serve** endpoint.  \n",
    "   * Deploy an API that takes a feature vector and returns the predicted cover type.\n",
    "\n",
    "6. **Real-Time Logging**  \n",
    "   * Integrate with MLflow or Weights & Biases to store logs, plots, and checkpoints.  \n",
    "   * Use tags and metadata to track experiments over time.\n",
    "\n",
    "7. **Alternative Objectives**  \n",
    "   * Try a binary objective (For example, presence vs. absence of a species) or regression target (For example, canopy height).  \n",
    "   * Fine-tune loss functions for specific ecological tasks.\n",
    "\n",
    "8. **End-to-End MLOps**  \n",
    "   * Schedule retraining with Ray Jobs or Anyscale Jobs.  \n",
    "   * Upload new data snapshots and trigger daily training runs with automatic checkpoint cleanup.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
