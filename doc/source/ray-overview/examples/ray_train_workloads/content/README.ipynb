{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e18da277",
   "metadata": {},
   "source": [
    "# Ray Train workloads tutorial series\n",
    "\n",
    "<div align=\"left\">\n",
    "<a target=\"_blank\" href=\"https://console.anyscale.com/\"><img src=\"https://img.shields.io/badge/ðŸš€ Run_on-Anyscale-9hf\"></a>&nbsp;\n",
    "<a href=\"https://github.com/ray-project/ray\" role=\"button\"><img src=\"https://img.shields.io/static/v1?label=&amp;message=View%20On%20GitHub&amp;color=586069&amp;logo=github&amp;labelColor=2f363d\"></a>&nbsp;\n",
    "</div>\n",
    "\n",
    "This tutorial series provides hands-on learning for **Ray Train V2** and its ecosystem (Ray Data, Ray Tune, Anyscale Workspaces).  \n",
    "The tutorials walk through common ML workload patternsâ€”vision, tabular, time series, generative, policy learning, and recommendationâ€”showing how to scale them **from single-node to fully distributed training and inference** with minimal code changes.\n",
    "\n",
    "---\n",
    "\n",
    "## Tutorial index\n",
    "\n",
    "### 1. Getting started\n",
    "- [**Introduction to Ray Train**](https://docs.ray.io/en/latest/ray-overview/examples/ray_train_workloads/content/getting-started/01_02_03_intro_to_ray_train.html)  \n",
    "  Your starting point. Learn the basics of distributed training with PyTorch and Ray Train:\n",
    "  - Why and when to use Ray Train vs. raw Distributed Data Parallel (DDP)  \n",
    "  - Wrapping models/data loaders with [`prepare_model`](https://docs.ray.io/en/latest/train/api/doc/ray.train.torch.prepare_model.html) / [`prepare_data_loader`](https://docs.ray.io/en/latest/train/api/doc/ray.train.torch.prepare_data_loader.html)  \n",
    "  - Using [`ScalingConfig`](https://docs.ray.io/en/latest/train/api/doc/ray.train.ScalingConfig.html) and [`RunConfig`](https://docs.ray.io/en/latest/train/api/doc/ray.train.RunConfig.html) for scale and checkpointing  \n",
    "  - Reporting metrics, saving checkpoints, and inspecting results with [`train.report`](https://docs.ray.io/en/latest/train/api/doc/ray.train.report.html)  \n",
    "  - Running fully distributed end-to-end training on Anyscale\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Workload patterns (independent, can be taken in any order)\n",
    "\n",
    "- [**Vision workloads**](https://docs.ray.io/en/latest/ray-overview/examples/ray_train_workloads/content/workload-patterns/04a_vision_pattern.html)  \n",
    "  Real-world computer vision with Food-101, preprocessing with Ray Data, fault-tolerant ResNet training, and scalable inference tasks.  \n",
    "\n",
    "- [**Tabular workloads**](https://docs.ray.io/en/latest/ray-overview/examples/ray_train_workloads/content/workload-patterns/04b_tabular_workload_pattern.html)  \n",
    "  Tabular ML with CoverType dataset, XGBoost + Ray Train, checkpoint-aware training, feature importance, and distributed inference.  \n",
    "\n",
    "- [**Time series workloads**](https://docs.ray.io/en/latest/ray-overview/examples/ray_train_workloads/content/workload-patterns/04c_time_series_workload_pattern.html)  \n",
    "  New York City taxi demand forecasting with a Transformer model, scaling across GPUs, epoch-level fault tolerance, and remote inference from checkpoints.  \n",
    "\n",
    "- [**Generative computer vision workloads**](https://docs.ray.io/en/latest/ray-overview/examples/ray_train_workloads/content/workload-patterns/04d1_generative_cv_pattern.html)  \n",
    "  A mini diffusion pipeline (Food-101-Lite), showcasing Ray Data preprocessing, PyTorch Lightning integration, checkpointing, and image generation.  \n",
    "\n",
    "- [**Policy learning workloads**](https://docs.ray.io/en/latest/ray-overview/examples/ray_train_workloads/content/workload-patterns/04d2_policy_learning_pattern.html)  \n",
    "  Diffusion-policy pipeline on Gymnasiumâ€™s Pendulum-v1 dataset, scaling across GPUs, checkpoint-per-epoch, and direct policy rollout in-notebook.  \n",
    "\n",
    "- [**Recommendation system workloads**](https://docs.ray.io/en/latest/ray-overview/examples/ray_train_workloads/content/workload-patterns/04e_rec_sys_workload_pattern.html)  \n",
    "  Matrix-factorization recommendation system with MovieLens 100K, streaming batches with [`iter_torch_batches`](https://docs.ray.io/en/latest/data/api/doc/ray.data.DataIterator.iter_torch_batches.html), custom training loop with checkpointing, and modular separation of training/eval/inference.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Hyperparameter tuning\n",
    "\n",
    "- [**Hyperparameter tuning with Ray Tune**](https://docs.ray.io/en/latest/ray-overview/examples/ray_train_workloads/content/hyperparameter-tuning/05a_ray_tune_pytorch_example.html)  \n",
    "  Introduces **Ray Tune** for PyTorch hyperparameter optimization:  \n",
    "  - Define search spaces  \n",
    "  - Run parallel trials with various schedulers  \n",
    "  - Add checkpoint/resume logic with [`tune.get_checkpoint`](https://docs.ray.io/en/latest/tune/api/doc/ray.tune.get_checkpoint.html)  \n",
    "  - Retrieve and evaluate the best model checkpoint  \n",
    "  This tutorial is standalone, but naturally extends the Ray Train patterns preceding.\n",
    "\n",
    "---\n",
    "\n",
    "## Key benefits\n",
    "- **Unified abstraction:** One training loop works across CPU, GPU, and multi-node clusters.\n",
    "- **Fault tolerance:** Resume from checkpoints on failures or preemptions.\n",
    "- **Scalability:** Move from laptop prototyping to cluster-scale training without code changes.\n",
    "- **Observability:** Access metrics, logs, and checkpoints through Ray and Anyscale tooling.\n",
    "- **Flexibility:** Mix and match workload patterns for real-world ML pipelines.  \n",
    "\n",
    "```{toctree}\n",
    ":hidden:\n",
    "\n",
    "getting-started/01_02_03_intro_to_ray_train.ipynb\n",
    "workload-patterns/04a_vision_pattern.ipynb\n",
    "workload-patterns/04b_tabular_workload_pattern.ipynb\n",
    "workload-patterns/04c_time_series_workload_pattern.ipynb\n",
    "workload-patterns/04d1_generative_cv_pattern.ipynb\n",
    "workload-patterns/04d2_policy_learning_pattern.ipynb\n",
    "workload-patterns/04e_rec_sys_workload_pattern.ipynb\n",
    "hyperparameter-tuning/05a_ray_tune_pytorch_example.ipynb\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
