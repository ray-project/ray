{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e18da277",
   "metadata": {},
   "source": [
    "# Ray Train Workloads Tutorial Series\n",
    "\n",
    "<div align=\"left\">\n",
    "<a target=\"_blank\" href=\"https://console.anyscale.com/\"><img src=\"https://img.shields.io/badge/ðŸš€ Run_on-Anyscale-9hf\"></a>&nbsp;\n",
    "<a href=\"https://github.com/ray-project/ray\" role=\"button\"><img src=\"https://img.shields.io/static/v1?label=&amp;message=View%20On%20GitHub&amp;color=586069&amp;logo=github&amp;labelColor=2f363d\"></a>&nbsp;\n",
    "</div>\n",
    "\n",
    "This repository contains a hands-on tutorial series for learning **Ray Train V2** and its ecosystem (Ray Data, Ray Tune, Anyscale Workspaces).  \n",
    "The tutorials walk through common ML workload patternsâ€”vision, tabular, time-series, generative, policy learning, and recommendationâ€”showing how to scale them **from single-node to fully distributed training and inference** with minimal code changes.\n",
    "\n",
    "---a\n",
    "\n",
    "## Tutorial Index\n",
    "\n",
    "### 1. Getting Started\n",
    "- [**`01_02_03_intro_to_ray_train.ipynb`**](https://github.com/ray-project/ray/blob/master/doc/source/ray-overview/examples/ray_train_workloads/notebooks/01_02_03_intro_to_ray_train.ipynb)  \n",
    "  Your starting point. Learn the basics of distributed training with PyTorch and Ray Train:\n",
    "  - Why and when to use Ray Train vs. raw Distributed Data Parallel (DDP)  \n",
    "  - Wrapping models/data loaders with [`prepare_model`](https://docs.ray.io/en/latest/train/api/doc/ray.train.torch.prepare_model.html) / [`prepare_data_loader`](https://docs.ray.io/en/latest/train/api/doc/ray.train.torch.prepare_data_loader.html)  \n",
    "  - Using [`ScalingConfig`](https://docs.ray.io/en/latest/train/api/doc/ray.train.ScalingConfig.html) and [`RunConfig`](https://docs.ray.io/en/latest/train/api/doc/ray.train.RunConfig.html) for scale and checkpointing  \n",
    "  - Reporting metrics, saving checkpoints, and inspecting results with [`train.report`](https://docs.ray.io/en/latest/train/api/doc/ray.train.report.html)  \n",
    "  - Running fully distributed end-to-end training on Anyscale\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Workload Patterns (independent, can be taken in any order)\n",
    "\n",
    "- [**`04a_vision_pattern.ipynb`**](https://github.com/ray-project/ray/blob/master/doc/source/ray-overview/examples/ray_train_workloads/notebooks/04a_vision_pattern.ipynb)  \n",
    "  Real-world computer vision with Food-101, preprocessing with Ray Data, fault-tolerant ResNet training, and scalable inference tasks.  \n",
    "\n",
    "- [**`04b_tabular_workload_pattern.ipynb`**](https://github.com/ray-project/ray/blob/master/doc/source/ray-overview/examples/ray_train_workloads/notebooks/04b_tabular_workload_pattern.ipynb)  \n",
    "  Tabular ML with CoverType dataset, XGBoost + Ray Train, checkpoint-aware training, feature importance, and distributed inference.  \n",
    "\n",
    "- [**`04c_time_series_workload_pattern.ipynb`**](https://github.com/ray-project/ray/blob/master/doc/source/ray-overview/examples/ray_train_workloads/notebooks/04c_time_series_workload_pattern.ipynb)  \n",
    "  New York City taxi demand forecasting with a Transformer model, scaling across GPUs, epoch-level fault tolerance, and remote inference from checkpoints.  \n",
    "\n",
    "- [**`04d1_generative_cv_pattern.ipynb`**](https://github.com/ray-project/ray/blob/master/doc/source/ray-overview/examples/ray_train_workloads/notebooks/04d1_generative_cv_pattern.ipynb)  \n",
    "  A mini diffusion pipeline (Food-101-Lite), showcasing Ray Data preprocessing, PyTorch Lightning integration, checkpointing, and image generation.  \n",
    "\n",
    "- [**`04d2_policy_learning_pattern.ipynb`**](https://github.com/ray-project/ray/blob/master/doc/source/ray-overview/examples/ray_train_workloads/notebooks/04d2_policy_learning_pattern.ipynb)  \n",
    "  Diffusion-policy pipeline on Gymnasiumâ€™s Pendulum-v1 dataset, scaling across GPUs, checkpoint-per-epoch, and direct policy rollout in-notebook.  \n",
    "\n",
    "- [**`04e_rec_sys_workload_pattern.ipynb`**](https://github.com/ray-project/ray/blob/master/doc/source/ray-overview/examples/ray_train_workloads/notebooks/04e_rec_sys_workload_pattern.ipynb)  \n",
    "  Matrix-factorization recommendation system with MovieLens 100K, streaming batches with [`iter_torch_batches`](https://docs.ray.io/en/latest/data/api/doc/ray.data.DataIterator.iter_torch_batches.html), custom training loop with checkpointing, and modular separation of training/eval/inference.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Hyperparameter Tuning\n",
    "\n",
    "- [**`05_ray_tune_pytorch_example.ipynb`**](https://github.com/ray-project/ray/blob/master/doc/source/ray-overview/examples/ray_train_workloads/notebooks/05_ray_tune_pytorch_example.ipynb)  \n",
    "  Introduces **Ray Tune** for PyTorch hyperparameter optimization:  \n",
    "  - Define search spaces  \n",
    "  - Run parallel trials with various schedulers  \n",
    "  - Add checkpoint/resume logic with [`tune.get_checkpoint`](https://docs.ray.io/en/latest/tune/api/doc/ray.tune.get_checkpoint.html)  \n",
    "  - Retrieve and evaluate the best model checkpoint  \n",
    "  This tutorial is standalone, but naturally extends the Ray Train patterns preceding.\n",
    "\n",
    "---\n",
    "\n",
    "## Key Benefits\n",
    "- **Unified abstraction:** One training loop, portable across CPU, GPU, and multi-node clusters  \n",
    "- **Fault tolerance:** Seamless resume from checkpoints on failures or pre-emptions  \n",
    "- **Scalability:** Move from laptop prototyping to cluster-scale training without code changes  \n",
    "- **Observability:** Metrics, logs, and checkpoints accessible through Ray + Anyscale tooling  \n",
    "- **Flexibility:** Mix and match workload patterns for real-world ML pipelines  \n",
    "\n",
    "```{toctree}\n",
    ":hidden:\n",
    "\n",
    "notebooks/01_02_03_intro_to_ray_train.ipynb\n",
    "notebooks/04a_vision_pattern.ipynb\n",
    "notebooks/04b_tabular_workload_pattern.ipynb\n",
    "notebooks/04c_time_series_workload_pattern.ipynb\n",
    "notebooks/04d1_generative_cv_pattern.ipynb\n",
    "notebooks/04d2_policy_learning_pattern.ipynb\n",
    "notebooks/04e_rec_sys_workload_pattern.ipynb\n",
    "notebooks/05_ray_tune_pytorch_example.ipynb\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef3f9c0",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
