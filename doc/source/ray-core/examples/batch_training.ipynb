{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Training with Ray Core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{tip}\n",
    "We strongly recommend using [Ray Datasets](data_user_guide) and [AIR Trainers](air-trainers) to develop batch training, which will enable you to build it faster and more easily, and get the built-in benefits like auto-scaling actor pool. If you think your use case cannot be supported by Ray Datasets or AIR, we'd love to get your feedback e.g. through a [Ray GitHub issue](https://github.com/ray-project/ray/issues).\n",
    "```\n",
    "\n",
    "Batch training and tuning are common tasks in simple machine learning use-cases such as time series forecasting. They require fitting of simple models on multiple data batches corresponding to locations, products, etc. This notebook showcases how to conduct batch training on the [NYC Taxi Dataset](https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page) using only Ray Core and stateless Ray tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch training in the context of this notebook is understood as creating the same model(s) for different and separate datasets or subsets of a dataset. This task is naively parallelizable and can be easily scaled with Ray.\n",
    "\n",
    "![Batch training diagram](./images/batch-training.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Walkthrough\n",
    "\n",
    "Our task is to create separate time series models for each pickup location. We can use the `pickup_location_id` column in the dataset to group the dataset into data batches. We will then fit models for each batch and choose the best one.\n",
    "\n",
    "Letâ€™s start by importing Ray and initializing a local Ray cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Optional, List, Union, Tuple, Iterable\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "import pyarrow as pa\n",
    "from pyarrow import fs\n",
    "from pyarrow import dataset as ds\n",
    "from pyarrow import parquet as pq\n",
    "import pyarrow.compute as pc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "    <div style=\"margin-left: 50px;display: flex;flex-direction: row;align-items: center\">\n",
       "        <h3 style=\"color: var(--jp-ui-font-color0)\">Ray</h3>\n",
       "        <svg version=\"1.1\" id=\"ray\" width=\"3em\" viewBox=\"0 0 144.5 144.6\" style=\"margin-left: 3em;margin-right: 3em\">\n",
       "            <g id=\"layer-1\">\n",
       "                <path fill=\"#00a2e9\" class=\"st0\" d=\"M97.3,77.2c-3.8-1.1-6.2,0.9-8.3,5.1c-3.5,6.8-9.9,9.9-17.4,9.6S58,88.1,54.8,81.2c-1.4-3-3-4-6.3-4.1\n",
       "                    c-5.6-0.1-9.9,0.1-13.1,6.4c-3.8,7.6-13.6,10.2-21.8,7.6C5.2,88.4-0.4,80.5,0,71.7c0.1-8.4,5.7-15.8,13.8-18.2\n",
       "                    c8.4-2.6,17.5,0.7,22.3,8c1.3,1.9,1.3,5.2,3.6,5.6c3.9,0.6,8,0.2,12,0.2c1.8,0,1.9-1.6,2.4-2.8c3.5-7.8,9.7-11.8,18-11.9\n",
       "                    c8.2-0.1,14.4,3.9,17.8,11.4c1.3,2.8,2.9,3.6,5.7,3.3c1-0.1,2,0.1,3,0c2.8-0.5,6.4,1.7,8.1-2.7s-2.3-5.5-4.1-7.5\n",
       "                    c-5.1-5.7-10.9-10.8-16.1-16.3C84,38,81.9,37.1,78,38.3C66.7,42,56.2,35.7,53,24.1C50.3,14,57.3,2.8,67.7,0.5\n",
       "                    C78.4-2,89,4.7,91.5,15.3c0.1,0.3,0.1,0.5,0.2,0.8c0.7,3.4,0.7,6.9-0.8,9.8c-1.7,3.2-0.8,5,1.5,7.2c6.7,6.5,13.3,13,19.8,19.7\n",
       "                    c1.8,1.8,3,2.1,5.5,1.2c9.1-3.4,17.9-0.6,23.4,7c4.8,6.9,4.6,16.1-0.4,22.9c-5.4,7.2-14.2,9.9-23.1,6.5c-2.3-0.9-3.5-0.6-5.1,1.1\n",
       "                    c-6.7,6.9-13.6,13.7-20.5,20.4c-1.8,1.8-2.5,3.2-1.4,5.9c3.5,8.7,0.3,18.6-7.7,23.6c-7.9,5-18.2,3.8-24.8-2.9\n",
       "                    c-6.4-6.4-7.4-16.2-2.5-24.3c4.9-7.8,14.5-11,23.1-7.8c3,1.1,4.7,0.5,6.9-1.7C91.7,98.4,98,92.3,104.2,86c1.6-1.6,4.1-2.7,2.6-6.2\n",
       "                    c-1.4-3.3-3.8-2.5-6.2-2.6C99.8,77.2,98.9,77.2,97.3,77.2z M72.1,29.7c5.5,0.1,9.9-4.3,10-9.8c0-0.1,0-0.2,0-0.3\n",
       "                    C81.8,14,77,9.8,71.5,10.2c-5,0.3-9,4.2-9.3,9.2c-0.2,5.5,4,10.1,9.5,10.3C71.8,29.7,72,29.7,72.1,29.7z M72.3,62.3\n",
       "                    c-5.4-0.1-9.9,4.2-10.1,9.7c0,0.2,0,0.3,0,0.5c0.2,5.4,4.5,9.7,9.9,10c5.1,0.1,9.9-4.7,10.1-9.8c0.2-5.5-4-10-9.5-10.3\n",
       "                    C72.6,62.3,72.4,62.3,72.3,62.3z M115,72.5c0.1,5.4,4.5,9.7,9.8,9.9c5.6-0.2,10-4.8,10-10.4c-0.2-5.4-4.6-9.7-10-9.7\n",
       "                    c-5.3-0.1-9.8,4.2-9.9,9.5C115,72.1,115,72.3,115,72.5z M19.5,62.3c-5.4,0.1-9.8,4.4-10,9.8c-0.1,5.1,5.2,10.4,10.2,10.3\n",
       "                    c5.6-0.2,10-4.9,9.8-10.5c-0.1-5.4-4.5-9.7-9.9-9.6C19.6,62.3,19.5,62.3,19.5,62.3z M71.8,134.6c5.9,0.2,10.3-3.9,10.4-9.6\n",
       "                    c0.5-5.5-3.6-10.4-9.1-10.8c-5.5-0.5-10.4,3.6-10.8,9.1c0,0.5,0,0.9,0,1.4c-0.2,5.3,4,9.8,9.3,10\n",
       "                    C71.6,134.6,71.7,134.6,71.8,134.6z\"/>\n",
       "            </g>\n",
       "        </svg>\n",
       "        <table>\n",
       "            <tr>\n",
       "                <td style=\"text-align: left\"><b>Python version:</b></td>\n",
       "                <td style=\"text-align: left\"><b>3.8.5</b></td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                <td style=\"text-align: left\"><b>Ray version:</b></td>\n",
       "                <td style=\"text-align: left\"><b> 2.0.0</b></td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "    <td style=\"text-align: left\"><b>Dashboard:</b></td>\n",
       "    <td style=\"text-align: left\"><b><a href=\"http://console.anyscale-staging.com/api/v2/sessions/ses_ZmHebxHaZpYkw9x9efJ5wBVX/services?redirect_to=dashboard\" target=\"_blank\">http://console.anyscale-staging.com/api/v2/sessions/ses_ZmHebxHaZpYkw9x9efJ5wBVX/services?redirect_to=dashboard</a></b></td>\n",
       "</tr>\n",
       "\n",
       "        </table>\n",
       "    </div>\n",
       "</div>\n"
      ],
      "text/plain": [
       "RayContext(dashboard_url='console.anyscale-staging.com/api/v2/sessions/ses_ZmHebxHaZpYkw9x9efJ5wBVX/services?redirect_to=dashboard', python_version='3.8.5', ray_version='2.0.0', ray_commit='cba26cc83f6b5b8a2ff166594a65cb74c0ec8740', address_info={'node_ip_address': '172.31.93.111', 'raylet_ip_address': '172.31.93.111', 'redis_address': None, 'object_store_address': '/tmp/ray/session_2022-10-03_05-35-53_173961_160/sockets/plasma_store', 'raylet_socket_name': '/tmp/ray/session_2022-10-03_05-35-53_173961_160/sockets/raylet', 'webui_url': 'console.anyscale-staging.com/api/v2/sessions/ses_ZmHebxHaZpYkw9x9efJ5wBVX/services?redirect_to=dashboard', 'session_dir': '/tmp/ray/session_2022-10-03_05-35-53_173961_160', 'metrics_export_port': 61748, 'gcs_address': '172.31.93.111:9031', 'address': '172.31.93.111:9031', 'dashboard_agent_listen_port': 52365, 'node_id': '456d522a97ae4eaeca7ff7526c4aeb46a7f85d64550c604195de78f6'})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ray\n",
    "\n",
    "ray.init(ignore_reinit_error=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For benchmarking purposes, we can print the times of various operations. In order to reduce clutter in the output, this is set to False by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRINT_TIMES = False\n",
    "\n",
    "\n",
    "def print_time(msg: str):\n",
    "    if PRINT_TIMES:\n",
    "        print(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To speed things up, we'll only use a small subset of the full dataset consisting of two last months of 2019. You can choose to use the full dataset for 2018-2019 by setting the `SMOKE_TEST` variable to False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SMOKE_TEST = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we will be using the NYC Taxi dataset, we define a simple batch transformation function to set correct data types, calculate the trip duration and fill missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A Pandas DataFrame UDF for transforming the underlying blocks of a Dataset in parallel.\n",
    "def transform_batch(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df[\"pickup_at\"] = pd.to_datetime(df[\"pickup_at\"], format=\"%Y-%m-%d %H:%M:%S\")\n",
    "    df[\"dropoff_at\"] = pd.to_datetime(df[\"dropoff_at\"], format=\"%Y-%m-%d %H:%M:%S\")\n",
    "    df[\"trip_duration\"] = (df[\"dropoff_at\"] - df[\"pickup_at\"]).dt.seconds\n",
    "    df[\"pickup_location_id\"] = df[\"pickup_location_id\"].fillna(-1)\n",
    "    df[\"dropoff_location_id\"] = df[\"dropoff_location_id\"].fillna(-1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be fitting scikit-learn models on data batches. We define a Ray task `fit_and_score_sklearn` that fits the model and calculates mean absolute error on the validation set. We will be treating this as a simple regression problem where we want to predict the relationship between the drop-off location and the trip duration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ray task to fit and score a scikit-learn model.\n",
    "@ray.remote\n",
    "def fit_and_score_sklearn(\n",
    "    train: pd.DataFrame, test: pd.DataFrame, model: BaseEstimator\n",
    ") -> Tuple[BaseEstimator, float]:\n",
    "    train_X = train[[\"dropoff_location_id\"]]\n",
    "    train_y = train[\"trip_duration\"]\n",
    "    test_X = test[[\"dropoff_location_id\"]]\n",
    "    test_y = test[\"trip_duration\"]\n",
    "\n",
    "    # Start training.\n",
    "    model = model.fit(train_X, train_y)\n",
    "    pred_y = model.predict(test_X)\n",
    "    error = mean_absolute_error(test_y, pred_y)\n",
    "    return model, error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `train_and_evaluate` function contains the logic for train-test splitting and fitting of multiple models in parallel on each data batch, for purposes of comparison. Thanks to this, we can evaluate several models and choose the best one for each data batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(\n",
    "    df: pd.DataFrame, models: List[BaseEstimator], i: int = 0\n",
    ") -> List[Tuple[BaseEstimator, float]]:\n",
    "    # We need at least 4 rows to create a train / test split.\n",
    "    if len(df) < 4:\n",
    "        print_time(f\"Dataframe for LocID: {i} is empty or smaller than 4\")\n",
    "        return None\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    # Train / test split.\n",
    "    train, test = train_test_split(df)\n",
    "\n",
    "    # We put the train & test dataframes into Ray object store\n",
    "    # so that they can be reused by all models fitted here.\n",
    "    # https://docs.ray.io/en/latest/ray-core/tips-for-first-time.html#tip-3-avoid-passing-same-object-repeatedly-to-remote-tasks\n",
    "    train_ref = ray.put(train)\n",
    "    test_ref = ray.put(test)\n",
    "\n",
    "    # Launch a fit and score task for each model.\n",
    "    results = ray.get(\n",
    "        [fit_and_score_sklearn.remote(train_ref, test_ref, model) for model in models]\n",
    "    )\n",
    "    results.sort(key=lambda x: x[1])  # sort by error\n",
    "\n",
    "    time_taken = time.time() - start\n",
    "    print_time(f\"Training time for LocID: {i}: {time_taken}\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `read_data` function reads a Parquet file and uses a push-down predicate to extract the data batch we want to fit a model on using the provided index to group the rows. By having each task read the data and extract batches separately, we ensure that memory utilization is minimal - as opposed to requiring each task to load the entire partition into memory first.\n",
    "\n",
    "We are using PyArrow to read the file, as it supports push-down predicates to be applied during file reading. This lets us avoid having to load an entire file into memory, which could cause an OOM error with a large dataset. After the dataset is loaded, we convert it to pandas so that it can be used for training with scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(file: str, i: int) -> pd.DataFrame:\n",
    "    return pq.read_table(\n",
    "        file,\n",
    "        filters=[(\"pickup_location_id\", \"=\", i)],\n",
    "        columns=[\n",
    "            \"pickup_at\",\n",
    "            \"dropoff_at\",\n",
    "            \"pickup_location_id\",\n",
    "            \"dropoff_location_id\",\n",
    "        ],\n",
    "    ).to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `task` Ray task contains all logic necessary to load a data batch, transform it and fit and evaluate models on it.\n",
    "\n",
    "You may notice that we have previously defined `fit_and_score_sklearn` as a Ray task as well and set it to be executed from inside `task`. This allows us to dynamically create a [tree of tasks](task-pattern-tree-of-tasks), ensuring that the cluster resources are fully utillized. Without this pattern, each `task` would need to be assigned several CPU cores for the model fitting, meaning that if certain models finish faster, then those CPU cores would stil stay occupied. Thankfully, Ray is able to deal with nested parallelism in tasks without the need for any extra logic, allowing us to simplify the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def task(\n",
    "    data: Union[str, pd.DataFrame],\n",
    "    file_name: str,\n",
    "    i: int,\n",
    "    models: List[BaseEstimator],\n",
    "    load_data_func: Optional[Callable] = None,\n",
    ") -> List[Tuple[BaseEstimator, float]]:\n",
    "    if load_data_func:\n",
    "        start_time = time.time()\n",
    "        data = load_data_func(data, i)\n",
    "        data_loading_time = time.time() - start_time\n",
    "        print_time(f\"Data loading time for LocID: {i}: {data_loading_time}\")\n",
    "\n",
    "    # Cast PyArrow scalar to Python if needed.\n",
    "    try:\n",
    "        i = i.as_py()\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # Perform transformation\n",
    "    start_time = time.time()\n",
    "    data = transform_batch(data)\n",
    "    transform_time = time.time() - start_time\n",
    "    print_time(f\"Data transform time for LocID: {i}: {transform_time}\")\n",
    "\n",
    "    return file_name, i, train_and_evaluate(data, models, i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `task_generator` generator dispatches tasks and yields references to them. Each task will be ran in parallel on a separate batch as determined by the `pickup_location_id` column in the provided file. Ray will handle scheduling automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def task_generator(files: List[str], models: List[BaseEstimator]) -> ray.ObjectRef:\n",
    "    for file in files:\n",
    "        try:\n",
    "            locdf = pq.read_table(file, columns=[\"pickup_location_id\"])\n",
    "        except Exception:\n",
    "            continue\n",
    "        loc_list = locdf[\"pickup_location_id\"].unique()\n",
    "\n",
    "        for i in loc_list:\n",
    "            yield task.remote(file, file, i, models, read_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the `run` driver function generates tasks for each Parquet file it recieves (with each file corresponding to one month). We define the function to take in a list of models, so that we can evaluate them all and choose the best one for each batch. The function blocks when it reaches `ray.get()` and waits for tasks to return their results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(files: List[str], task_generator: Callable, models: List[BaseEstimator]):\n",
    "    print(\"Starting run...\")\n",
    "    start = time.time()\n",
    "\n",
    "    task_refs = list(task_generator(files, models))\n",
    "    results = ray.get(task_refs)\n",
    "\n",
    "    taken = time.time() - start\n",
    "    count = len(results)\n",
    "    results_not_none = [x for x in results if x is not None]\n",
    "    count_not_none = len(results_not_none)\n",
    "\n",
    "    # Sleep a moment for nicer output\n",
    "    time.sleep(1)\n",
    "    print(\"\", flush=True)\n",
    "    print(f\"Total number of models (all tasks): {count_not_none} ({count})\")\n",
    "    print(f\"TOTAL TIME TAKEN: {taken:.2f} seconds\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtain the partitions of the dataset from an S3 bucket so that we can pass them to `run`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtained 2 files!\n"
     ]
    }
   ],
   "source": [
    "# Obtain the dataset. Each month is a separate file.\n",
    "dataset = ds.dataset(\n",
    "    \"s3://anonymous@air-example-data/ursa-labs-taxi-data/by_year/\",\n",
    "    partitioning=[\"year\", \"month\"],\n",
    ")\n",
    "starting_idx = -2 if SMOKE_TEST else 0\n",
    "files = [f\"s3://{file}\" for file in dataset.files][starting_idx:]\n",
    "print(f\"Obtained {len(files)} files!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now run our script. The output is a list of tuples in the following format: `(file name, partition id, list of models and their MAE scores)`. For brevity, we will print out the first 10 tuples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting run...\n",
      "\n",
      "Total number of models (all tasks): 522 (522)\n",
      "TOTAL TIME TAKEN: 21.19 seconds\n",
      "[('s3://air-example-data/ursa-labs-taxi-data/by_year/2019/05/data.parquet/359c21b3e28f40328e68cf66f7ba40e2_000000.parquet', 145, [(LinearRegression(), 851.3091289442241)]), ('s3://air-example-data/ursa-labs-taxi-data/by_year/2019/05/data.parquet/359c21b3e28f40328e68cf66f7ba40e2_000000.parquet', 161, [(LinearRegression(), 763.587971487081)]), ('s3://air-example-data/ursa-labs-taxi-data/by_year/2019/05/data.parquet/359c21b3e28f40328e68cf66f7ba40e2_000000.parquet', 163, [(LinearRegression(), 742.3122613593824)]), ('s3://air-example-data/ursa-labs-taxi-data/by_year/2019/05/data.parquet/359c21b3e28f40328e68cf66f7ba40e2_000000.parquet', 193, [(LinearRegression(), 899.5440269877245)]), ('s3://air-example-data/ursa-labs-taxi-data/by_year/2019/05/data.parquet/359c21b3e28f40328e68cf66f7ba40e2_000000.parquet', 260, [(LinearRegression(), 741.1232150739363)]), ('s3://air-example-data/ursa-labs-taxi-data/by_year/2019/05/data.parquet/359c21b3e28f40328e68cf66f7ba40e2_000000.parquet', 56, [(LinearRegression(), 860.3183412585847)]), ('s3://air-example-data/ursa-labs-taxi-data/by_year/2019/05/data.parquet/359c21b3e28f40328e68cf66f7ba40e2_000000.parquet', 79, [(LinearRegression(), 728.9143263092787)]), ('s3://air-example-data/ursa-labs-taxi-data/by_year/2019/05/data.parquet/359c21b3e28f40328e68cf66f7ba40e2_000000.parquet', 90, [(LinearRegression(), 649.3464235594931)]), ('s3://air-example-data/ursa-labs-taxi-data/by_year/2019/05/data.parquet/359c21b3e28f40328e68cf66f7ba40e2_000000.parquet', 162, [(LinearRegression(), 723.9509168205005)]), ('s3://air-example-data/ursa-labs-taxi-data/by_year/2019/05/data.parquet/359c21b3e28f40328e68cf66f7ba40e2_000000.parquet', 50, [(LinearRegression(), 671.7616933026658)])]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "results = run(files, task_generator, models=[LinearRegression()])\n",
    "print(results[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also provide multiple scikit-learn models to our `run` function and the best one will be chosen for each batch. A common use-case here would be to define several models of the same type with different hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting run...\n",
      "\n",
      "Total number of models (all tasks): 522 (522)\n",
      "TOTAL TIME TAKEN: 18.51 seconds\n",
      "[('s3://air-example-data/ursa-labs-taxi-data/by_year/2019/05/data.parquet/359c21b3e28f40328e68cf66f7ba40e2_000000.parquet', 145, [(DecisionTreeRegressor(), 619.9080145718), (DecisionTreeRegressor(splitter='random'), 620.9351656841662), (LinearRegression(), 894.9093613150645)]), ('s3://air-example-data/ursa-labs-taxi-data/by_year/2019/05/data.parquet/359c21b3e28f40328e68cf66f7ba40e2_000000.parquet', 161, [(DecisionTreeRegressor(), 585.1303154215874), (DecisionTreeRegressor(splitter='random'), 585.1334584269538), (LinearRegression(), 746.3996639952683)]), ('s3://air-example-data/ursa-labs-taxi-data/by_year/2019/05/data.parquet/359c21b3e28f40328e68cf66f7ba40e2_000000.parquet', 163, [(DecisionTreeRegressor(), 590.8829340940193), (DecisionTreeRegressor(splitter='random'), 591.0654550332006), (LinearRegression(), 758.3602607590221)]), ('s3://air-example-data/ursa-labs-taxi-data/by_year/2019/05/data.parquet/359c21b3e28f40328e68cf66f7ba40e2_000000.parquet', 193, [(DecisionTreeRegressor(), 739.1724549207835), (DecisionTreeRegressor(splitter='random'), 739.5002953972328), (LinearRegression(), 906.5242773055481)]), ('s3://air-example-data/ursa-labs-taxi-data/by_year/2019/05/data.parquet/359c21b3e28f40328e68cf66f7ba40e2_000000.parquet', 260, [(DecisionTreeRegressor(), 593.1233945510796), (DecisionTreeRegressor(splitter='random'), 593.1233945510796), (LinearRegression(), 709.558440515228)]), ('s3://air-example-data/ursa-labs-taxi-data/by_year/2019/05/data.parquet/359c21b3e28f40328e68cf66f7ba40e2_000000.parquet', 56, [(DecisionTreeRegressor(splitter='random'), 1302.8135501217532), (DecisionTreeRegressor(), 1308.5687584550865), (LinearRegression(), 1400.7256036944598)]), ('s3://air-example-data/ursa-labs-taxi-data/by_year/2019/05/data.parquet/359c21b3e28f40328e68cf66f7ba40e2_000000.parquet', 79, [(DecisionTreeRegressor(), 573.3767209185635), (DecisionTreeRegressor(splitter='random'), 573.3853566498115), (LinearRegression(), 711.9296171689957)]), ('s3://air-example-data/ursa-labs-taxi-data/by_year/2019/05/data.parquet/359c21b3e28f40328e68cf66f7ba40e2_000000.parquet', 90, [(DecisionTreeRegressor(splitter='random'), 483.88298667156215), (DecisionTreeRegressor(), 484.1489956504658), (LinearRegression(), 638.507610810801)]), ('s3://air-example-data/ursa-labs-taxi-data/by_year/2019/05/data.parquet/359c21b3e28f40328e68cf66f7ba40e2_000000.parquet', 162, [(DecisionTreeRegressor(splitter='random'), 546.0548872824131), (DecisionTreeRegressor(), 546.0673940127546), (LinearRegression(), 687.9393358281769)]), ('s3://air-example-data/ursa-labs-taxi-data/by_year/2019/05/data.parquet/359c21b3e28f40328e68cf66f7ba40e2_000000.parquet', 50, [(DecisionTreeRegressor(splitter='random'), 529.9439816747014), (DecisionTreeRegressor(), 530.0687930367063), (LinearRegression(), 681.5231361774709)])]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "results = run(\n",
    "    files,\n",
    "    task_generator,\n",
    "    models=[\n",
    "        LinearRegression(),\n",
    "        DecisionTreeRegressor(),\n",
    "        DecisionTreeRegressor(splitter=\"random\"),\n",
    "    ],\n",
    ")\n",
    "print(results[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data once into Ray object store\n",
    "\n",
    "In order to ensure that the data can always fit in memory, each task reads the files independently and extracts the desired data batch. This, however, negatively impacts the runtime. If we have sufficient memory in our Ray cluster, we can instead load each partition once, extract the batches, and save them in the [Ray object store](objects-in-ray), reducing time required dramatically at a cost of higher memory usage.\n",
    "\n",
    "Notice we do not call `ray.get()` on the references of the `read_into_object_store`. Instead, we pass the reference itself as the argument to the `task.remote` dispatch, [allowing for the data to stay in the object store until it is actually needed](ray-pass-large-arg-by-value). This avoids a situation where all the data would be loaded into the memory of the process calling `ray.get()`.\n",
    "\n",
    "You can use the Ray Dashboard to compare the memory usage between the previous approach and this one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.util.placement_group import placement_group, remove_placement_group\n",
    "from ray.util.scheduling_strategies import PlacementGroupSchedulingStrategy\n",
    "\n",
    "\n",
    "@ray.remote\n",
    "def read_into_object_store(file: str) -> List[ray.ObjectRef]:\n",
    "    print(f\"Loading {file}\")\n",
    "    # Read the entire file into memory.\n",
    "    try:\n",
    "        locdf = pq.read_table(\n",
    "            file,\n",
    "            columns=[\n",
    "                \"pickup_at\",\n",
    "                \"dropoff_at\",\n",
    "                \"pickup_location_id\",\n",
    "                \"dropoff_location_id\",\n",
    "            ],\n",
    "        )\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "    loc_list = locdf[\"pickup_location_id\"].unique()\n",
    "\n",
    "    group_refs = []\n",
    "    for i in loc_list:\n",
    "        # Put each data batch as a separate dataframe into Ray object store.\n",
    "        group_refs.append(\n",
    "            (i, ray.put(locdf.filter(pc.field(\"pickup_location_id\") == i).to_pandas()))\n",
    "        )\n",
    "\n",
    "    return group_refs\n",
    "\n",
    "\n",
    "def task_generator_with_object_store(\n",
    "    files: List[str], models: List[BaseEstimator]\n",
    ") -> ray.ObjectRef:\n",
    "    # Use a placement group with a SPREAD strategy to load each\n",
    "    # file on a separate node as an OOM safeguard.\n",
    "    # This is not foolproof though! We can also specify a resource\n",
    "    # requirement for memory, if we know what is the maximum\n",
    "    # memory requirement for a single file.\n",
    "    pg = placement_group([{\"CPU\": 1}] * len(files), strategy=\"SPREAD\")\n",
    "    ray.get(pg.ready())\n",
    "\n",
    "    read_into_object_store_pg = read_into_object_store.options(\n",
    "        scheduling_strategy=PlacementGroupSchedulingStrategy(placement_group=pg)\n",
    "    )\n",
    "    load_tasks = [read_into_object_store_pg.remote(file) for file in files]\n",
    "    group_refs = {}\n",
    "    for i, refs in enumerate(ray.get(load_tasks)):\n",
    "        group_refs[files[i]] = refs\n",
    "    remove_placement_group(pg)\n",
    "\n",
    "    for file, refs in group_refs.items():\n",
    "        for i, ref in refs:\n",
    "            yield task.remote(ref, file, i, models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting run...\n",
      "(read_into_object_store pid=3170, ip=172.31.93.1) Loading s3://air-example-data/ursa-labs-taxi-data/by_year/2019/05/data.parquet/359c21b3e28f40328e68cf66f7ba40e2_000000.parquet\n",
      "(read_into_object_store pid=3077, ip=172.31.68.44) Loading s3://air-example-data/ursa-labs-taxi-data/by_year/2019/06/data.parquet/ab5b9d2b8cc94be19346e260b543ec35_000000.parquet\n",
      "(scheduler +59s) Tip: use `ray status` to view detailed cluster status. To disable these messages, set RAY_SCHEDULER_EVENTS=0.\n",
      "(scheduler +59s) Warning: The following resource request cannot be scheduled right now: {'CPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.\n",
      "\n",
      "Total number of models (all tasks): 522 (522)\n",
      "TOTAL TIME TAKEN: 15.78 seconds\n",
      "[('s3://air-example-data/ursa-labs-taxi-data/by_year/2019/05/data.parquet/359c21b3e28f40328e68cf66f7ba40e2_000000.parquet', 145, [(LinearRegression(), 852.9429209323498)]), ('s3://air-example-data/ursa-labs-taxi-data/by_year/2019/05/data.parquet/359c21b3e28f40328e68cf66f7ba40e2_000000.parquet', 161, [(LinearRegression(), 756.4310964446844)]), ('s3://air-example-data/ursa-labs-taxi-data/by_year/2019/05/data.parquet/359c21b3e28f40328e68cf66f7ba40e2_000000.parquet', 163, [(LinearRegression(), 759.0581689980796)]), ('s3://air-example-data/ursa-labs-taxi-data/by_year/2019/05/data.parquet/359c21b3e28f40328e68cf66f7ba40e2_000000.parquet', 193, [(LinearRegression(), 811.8705198797737)]), ('s3://air-example-data/ursa-labs-taxi-data/by_year/2019/05/data.parquet/359c21b3e28f40328e68cf66f7ba40e2_000000.parquet', 260, [(LinearRegression(), 669.7161874214457)]), ('s3://air-example-data/ursa-labs-taxi-data/by_year/2019/05/data.parquet/359c21b3e28f40328e68cf66f7ba40e2_000000.parquet', 56, [(LinearRegression(), 1388.4215954337024)]), ('s3://air-example-data/ursa-labs-taxi-data/by_year/2019/05/data.parquet/359c21b3e28f40328e68cf66f7ba40e2_000000.parquet', 79, [(LinearRegression(), 715.368673359218)]), ('s3://air-example-data/ursa-labs-taxi-data/by_year/2019/05/data.parquet/359c21b3e28f40328e68cf66f7ba40e2_000000.parquet', 90, [(LinearRegression(), 644.6049120675258)]), ('s3://air-example-data/ursa-labs-taxi-data/by_year/2019/05/data.parquet/359c21b3e28f40328e68cf66f7ba40e2_000000.parquet', 162, [(LinearRegression(), 695.9343158694874)]), ('s3://air-example-data/ursa-labs-taxi-data/by_year/2019/05/data.parquet/359c21b3e28f40328e68cf66f7ba40e2_000000.parquet', 50, [(LinearRegression(), 717.3705726378896)])]\n"
     ]
    }
   ],
   "source": [
    "results = run(files, task_generator_with_object_store, models=[LinearRegression()])\n",
    "print(results[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that this approach allowed us to finish training much faster, but it would not have been possible if the dataset was too large to fit into our cluster memory. Therefore, this pattern is only recommended if the data you are working with is small. Otherwise, it is recommended to load the data inside the tasks right before its used."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3c0d54d489a08ae47a06eae2fd00ff032d6cddb527c382959b7b2575f6a8167f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
