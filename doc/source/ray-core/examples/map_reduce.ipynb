{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "999dbcc2",
   "metadata": {},
   "source": [
    "# Running a Simple MapReduce Example with Ray Core\n",
    "\n",
    "In this example we're discussing how easy it can be to use Ray to implement MapReduce, a significant milestone in distributed computing.\n",
    "Many popular big data technologies, such as Hadoop, are built upon this programming model, and it is worth discussing in relation to Ray.\n",
    "To illustrate its use, we will use a simple example of counting word occurrences across multiple documents.\n",
    "While this may seem like a straightforward task when working with a small number of documents,\n",
    "it becomes more complex when dealing with a large corpus, requiring the use of multiple compute nodes to process the data.\n",
    "This is a commonly used example in distributed computing and is worth learning about.\n",
    "The approach involves three simple steps:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae37e0a",
   "metadata": {},
   "source": [
    "1. Use a set of documents as the input and apply a specified function to transform or \"map\" each element within them (such as the words contained in the documents).\n",
    "   This map phase will produce key-value pairs, where the key represents an element in the document and the value is a metric calculated for that element.\n",
    "   In this particular case, the goal is to count the number of times each word appears in a document,\n",
    "   so the map function will output the pair `(word, 1)` every time a word is encountered to show that it has been found once.\n",
    "2. All the outputs from the map phase are collected and organized based on their key.\n",
    "   This may involve transferring data between different nodes, as the same key could potentially be found on multiple compute nodes.\n",
    "   This process is commonly referred to as the shuffle phase.\n",
    "   As an example, if the map phase produces four occurrences of the pair `(word, 1)`, the shuffle phase will ensure that all occurrences of\n",
    "   the same word are located on the same node.\n",
    "3. The reduce phase is so named because it aggregates or combines the elements from the shuffle step.\n",
    "   Using the example provided, the final count of a word's occurrences is obtained by adding up all the occurrences on each node.\n",
    "   For example, four instances of `(word, 1)` would be combined to result in a final count of `word: 4`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87bd5e5e",
   "metadata": {},
   "source": [
    "MapReduce is named after the first and last stages of the process, but the middle stage is just as crucial.\n",
    "These phases may appear straightforward, but their strength lies in the ability to run them concurrently on multiple machines.\n",
    "An example of using the three MapReduce phases on a set of documents divided into three parts is shown in this figure:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0b1af6",
   "metadata": {},
   "source": [
    "\n",
    "![Simple Map Reduce](https://raw.githubusercontent.com/maxpumperla/learning_ray/main/notebooks/images/chapter_02/map_reduce.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2225ae60",
   "metadata": {},
   "source": [
    "## Loading Data\n",
    "\n",
    "We will be using Python to implement the MapReduce algorithm for our word-count purpose and utilizing Ray to parallelize the computation.\n",
    "To better understand what we are working with, we will begin by loading some example data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "91c6ddc0",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "zen_of_python = subprocess.check_output([\"python\", \"-c\", \"import this\"])\n",
    "corpus = zen_of_python.split()\n",
    "\n",
    "num_partitions = 3\n",
    "chunk = len(corpus) // num_partitions\n",
    "partitions = [\n",
    "    corpus[i * chunk: (i + 1) * chunk] for i in range(num_partitions)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc96c181",
   "metadata": {},
   "source": [
    "We will be using the Zen of Python, a collection of guidelines from the Python community, as our data for this exercise.\n",
    "The Zen of Python can be accessed by typing \"import this\" in a Python session and is traditionally hidden as an \"Easter egg.\"\n",
    "While it is beneficial for Python programmers to read these guidelines, for the purposes of this exercise,\n",
    "we will only be counting the number of words contained within them.\n",
    "To do this, we will divide the Zen of Python into three separate \"documents\" by treating each line as a separate entity\n",
    "and then splitting it into these partitions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1357e924",
   "metadata": {},
   "source": [
    "## Mapping our Data\n",
    "\n",
    "To determine the map phase, we require a map function that we will utilize on each document.\n",
    "In this particular scenario, we want to output the pair `(word, 1)` for every word found in a document.\n",
    "For basic text documents that are loaded as Python strings, this process appears as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "742193e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_function(document):\n",
    "    for word in document.lower().split():\n",
    "        yield word, 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52009879",
   "metadata": {},
   "source": [
    "We will use the apply_map function on a large collection of documents by marking it as a task in Ray using the `@ray.remote` decorator.\n",
    "When we call `apply_map`, it will be applied to three sets of document data (`num_partitions=3`).\n",
    "The `apply_map` function will return three lists, one for each partition.\n",
    "We do this so that Ray can rearrange the results of the map phase and distribute them to the appropriate nodes for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a2fed469",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "\n",
    "@ray.remote\n",
    "def apply_map(corpus, num_partitions=3):\n",
    "    map_results = [list() for _ in range(num_partitions)]\n",
    "    for document in corpus:\n",
    "        for result in map_function(document):\n",
    "            first_letter = result[0].decode(\"utf-8\")[0]\n",
    "            word_index = ord(first_letter) % num_partitions\n",
    "            map_results[word_index].append(result)\n",
    "    return map_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ba13f8",
   "metadata": {},
   "source": [
    "For text corpora that can be stored on a single machine, it is unnecessary to use the map phase.\n",
    "However, when the data needs to be divided across multiple nodes, the map phase becomes useful.\n",
    "In order to apply the map phase to our corpus in parallel, we use a remote call on `apply_map`, just like we have done in previous examples.\n",
    "The main difference now is that we also specify that we want three results returned (one for each partition) using the num_returns argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "360b19b8",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapper 0, return value 0: [(b'of', 1), (b'is', 1)]\n",
      "Mapper 0, return value 1: [(b'python,', 1), (b'peters', 1)]\n",
      "Mapper 0, return value 2: [(b'the', 1), (b'zen', 1)]\n",
      "Mapper 1, return value 0: [(b'unless', 1), (b'in', 1)]\n",
      "Mapper 1, return value 1: [(b'although', 1), (b'practicality', 1)]\n",
      "Mapper 1, return value 2: [(b'beats', 1), (b'errors', 1)]\n",
      "Mapper 2, return value 0: [(b'is', 1), (b'is', 1)]\n",
      "Mapper 2, return value 1: [(b'although', 1), (b'a', 1)]\n",
      "Mapper 2, return value 2: [(b'better', 1), (b'than', 1)]\n"
     ]
    }
   ],
   "source": [
    "map_results = [\n",
    "    apply_map.options(num_returns=num_partitions)\n",
    "    .remote(data, num_partitions)\n",
    "    for data in partitions\n",
    "]\n",
    "\n",
    "for i in range(num_partitions):\n",
    "    mapper_results = ray.get(map_results[i])\n",
    "    for j, result in enumerate(mapper_results):\n",
    "        print(f\"Mapper {i}, return value {j}: {result[:2]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171744b1",
   "metadata": {},
   "source": [
    "## Shuffling and Reducing our Data\n",
    "\n",
    "We can make it so that all pairs from the `j`-th return value end up on the same node for the reduce phase.\n",
    "Let’s discuss this phase next.\n",
    "In the reduce phase we can create a dictionary that sums up all word occurrences on each partition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5891b2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def apply_reduce(*results):\n",
    "    reduce_results = dict()\n",
    "    for res in results:\n",
    "        for key, value in res:\n",
    "            if key not in reduce_results:\n",
    "                reduce_results[key] = 0\n",
    "            reduce_results[key] += value\n",
    "\n",
    "    return reduce_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ee3c55",
   "metadata": {},
   "source": [
    "We can take the j-th return value from each mapper and send it to the j-th reducer using the following method.\n",
    "It's important to note that this code works for larger datasets that don't fit on one machine because we are passing references\n",
    "to the data using Ray objects rather than the actual data itself.\n",
    "Both the map and reduce phases can be run on any Ray cluster and the data shuffling is also handled by Ray."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a395a7f9",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is: 10\n",
      "better: 8\n",
      "than: 8\n",
      "the: 6\n",
      "to: 5\n",
      "of: 3\n",
      "although: 3\n",
      "be: 3\n",
      "unless: 2\n",
      "one: 2\n",
      "if: 2\n",
      "implementation: 2\n",
      "idea.: 2\n",
      "special: 2\n",
      "should: 2\n",
      "do: 2\n",
      "may: 2\n",
      "a: 2\n",
      "never: 2\n",
      "way: 2\n",
      "explain,: 2\n",
      "ugly.: 1\n",
      "implicit.: 1\n",
      "complex.: 1\n",
      "complex: 1\n",
      "complicated.: 1\n",
      "flat: 1\n",
      "readability: 1\n",
      "counts.: 1\n",
      "cases: 1\n",
      "rules.: 1\n",
      "in: 1\n",
      "face: 1\n",
      "refuse: 1\n",
      "one--: 1\n",
      "only: 1\n",
      "--obvious: 1\n",
      "it.: 1\n",
      "obvious: 1\n",
      "first: 1\n",
      "often: 1\n",
      "*right*: 1\n",
      "it's: 1\n",
      "it: 1\n",
      "idea: 1\n",
      "--: 1\n",
      "let's: 1\n",
      "python,: 1\n",
      "peters: 1\n",
      "simple: 1\n",
      "sparse: 1\n",
      "dense.: 1\n",
      "aren't: 1\n",
      "practicality: 1\n",
      "purity.: 1\n",
      "pass: 1\n",
      "silently.: 1\n",
      "silenced.: 1\n",
      "ambiguity,: 1\n",
      "guess.: 1\n",
      "and: 1\n",
      "preferably: 1\n",
      "at: 1\n",
      "you're: 1\n",
      "dutch.: 1\n",
      "good: 1\n",
      "are: 1\n",
      "great: 1\n",
      "more: 1\n",
      "zen: 1\n",
      "by: 1\n",
      "tim: 1\n",
      "beautiful: 1\n",
      "explicit: 1\n",
      "nested.: 1\n",
      "enough: 1\n",
      "break: 1\n",
      "beats: 1\n",
      "errors: 1\n",
      "explicitly: 1\n",
      "temptation: 1\n",
      "there: 1\n",
      "that: 1\n",
      "not: 1\n",
      "now: 1\n",
      "never.: 1\n",
      "now.: 1\n",
      "hard: 1\n",
      "bad: 1\n",
      "easy: 1\n",
      "namespaces: 1\n",
      "honking: 1\n",
      "those!: 1\n"
     ]
    }
   ],
   "source": [
    "outputs = []\n",
    "for i in range(num_partitions):\n",
    "    outputs.append(\n",
    "        apply_reduce.remote(*[partition[i] for partition in map_results])\n",
    "    )\n",
    "\n",
    "counts = {k: v for output in ray.get(outputs) for k, v in output.items()}\n",
    "\n",
    "sorted_counts = sorted(counts.items(), key=lambda item: item[1], reverse=True)\n",
    "for count in sorted_counts:\n",
    "    print(f\"{count[0].decode('utf-8')}: {count[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57bd93b",
   "metadata": {},
   "source": [
    "To gain a thorough understanding of how to scale MapReduce tasks across multiple nodes using Ray,\n",
    "including memory management, we suggest reading this [insightful blog post on the topic](https://medium.com/distributed-computing-with-ray/executing-adistributed-shuffle-without-a-mapreduce-system-d5856379426c).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20346f17",
   "metadata": {},
   "source": [
    "## Wrapping up\n",
    "\n",
    "The important part about this MapReduce example is to realize how flexible Ray’s programming model really is.\n",
    "Of course, a production-grade MapReduce implementation takes a bit more effort.\n",
    "But being able to reproduce common algorithms like this one _quickly_ goes a long way.\n",
    "Keep in mind that in the earlier phases of MapReduce, say around 2010, this paradigm was often the only thing\n",
    "you had to express your workloads.\n",
    "With Ray, a whole range of interesting distributed computing patterns\n",
    "become accessible to any intermediate Python programmer.\n",
    "\n",
    "If you want to learn more about Ray, and Ray Core and particular, check out the [Ray Core Examples Gallery](./overview.rst),\n",
    "or some of the ML workloads in our [Use Case Gallery](../../ray-overview/use-cases.rst).\n",
    "This simple MapReduce example can also be found in [\"Learning Ray\"](https://maxpumperla.com/learning_ray/),\n",
    "which contains more examples like this one."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
