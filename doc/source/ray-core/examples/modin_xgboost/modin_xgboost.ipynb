{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66806cbf",
   "metadata": {},
   "source": [
    "# XGBoost-Ray with Modin\n",
    "\n",
    "This notebook includes an example workflow using\n",
    "[XGBoost-Ray](https://docs.ray.io/en/latest/xgboost-ray.html) and\n",
    "[Modin](https://modin.readthedocs.io/en/latest/) for distributed model\n",
    "training and prediction.\n",
    "\n",
    "## Cluster Setup\n",
    "\n",
    "First, we'll set up our Ray Cluster. The provided [modin_xgboost.yaml](https://raw.githubusercontent.com/ray-project/ray/master/doc/source/ray-core/examples/modin_xgboost/modin_xgboost.yaml)\n",
    "cluster config can be used to set up an AWS cluster with 64 CPUs.\n",
    "\n",
    "The following steps assume you are in a directory with both\n",
    "``modin_xgboost.yaml`` and this file saved as ``modin_xgboost.ipynb``.\n",
    "\n",
    "**Step 1:** Bring up the Ray cluster.\n",
    "\n",
    "```bash\n",
    "pip install ray boto3\n",
    "ray up modin_xgboost.yaml\n",
    "```\n",
    "\n",
    "**Step 2:** Move ``modin_xgboost.ipynb`` to the cluster and start Jupyter.\n",
    "\n",
    "```bash\n",
    "ray rsync_up modin_xgboost.yaml \"./modin_xgboost.ipynb\" \\\n",
    "  \"~/modin_xgboost.ipynb\"\n",
    "ray exec modin_xgboost.yaml --port-forward=9999 \"jupyter notebook \\\n",
    "  --port=9999\"\n",
    "```\n",
    "\n",
    "You can then access this notebook at the URL that is output:\n",
    "``http://localhost:9999/?token=<token>``\n",
    "\n",
    "## Python Setup\n",
    "\n",
    "First, we'll import all the libraries we'll be using. This step also helps us\n",
    "verify that the environment is configured correctly. If any of the imports\n",
    "are missing, an exception will be raised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7da4af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import time\n",
    "\n",
    "import modin.pandas as pd\n",
    "from modin.experimental.sklearn.model_selection import train_test_split\n",
    "from xgboost_ray import RayDMatrix, RayParams, train, predict\n",
    "\n",
    "import ray"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b07fdf9",
   "metadata": {},
   "source": [
    "Next, let's parse some arguments. This will be used for executing the ``.py``\n",
    "file, but not for the ``.ipynb``. If you are using the interactive notebook,\n",
    "you can directly override the arguments manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2303e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\n",
    "    \"--address\", type=str, default=\"auto\", help=\"The address to use for Ray.\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--smoke-test\",\n",
    "    action=\"store_true\",\n",
    "    help=\"Read a smaller dataset for quick testing purposes.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--num-actors\", type=int, default=4, help=\"Sets number of actors for training.\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--cpus-per-actor\",\n",
    "    type=int,\n",
    "    default=8,\n",
    "    help=\"The number of CPUs per actor for training.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--num-actors-inference\",\n",
    "    type=int,\n",
    "    default=16,\n",
    "    help=\"Sets number of actors for inference.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--cpus-per-actor-inference\",\n",
    "    type=int,\n",
    "    default=2,\n",
    "    help=\"The number of CPUs per actor for inference.\",\n",
    ")\n",
    "# Ignore -f from ipykernel_launcher\n",
    "args, _ = parser.parse_known_args()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2d56be",
   "metadata": {},
   "source": [
    " Override these arguments as needed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a19350e",
   "metadata": {},
   "outputs": [],
   "source": [
    "address = args.address\n",
    "smoke_test = args.smoke_test\n",
    "num_actors = args.num_actors\n",
    "cpus_per_actor = args.cpus_per_actor\n",
    "num_actors_inference = args.num_actors_inference\n",
    "cpus_per_actor_inference = args.cpus_per_actor_inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8aaf46",
   "metadata": {},
   "source": [
    "## Connecting to the Ray cluster\n",
    "\n",
    "Now, let's connect our Python script to this newly deployed Ray cluster!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6f836d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not ray.is_initialized():\n",
    "    ray.init(address=address)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29910376",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "We will use the [HIGGS dataset from the UCI Machine Learning dataset\n",
    "repository](https://archive.ics.uci.edu/ml/datasets/HIGGS). The HIGGS\n",
    "dataset consists of 11,000,000 samples and 28 attributes, which is large\n",
    "enough size to show the benefits of distributed computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b46f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL_COLUMN = \"label\"\n",
    "if smoke_test:\n",
    "    # Test dataset with only 10,000 records.\n",
    "    FILE_URL = \"https://ray-ci-higgs.s3.us-west-2.amazonaws.com/simpleHIGGS\" \".csv\"\n",
    "else:\n",
    "    # Full dataset. This may take a couple of minutes to load.\n",
    "    FILE_URL = (\n",
    "        \"https://archive.ics.uci.edu/ml/machine-learning-databases\"\n",
    "        \"/00280/HIGGS.csv.gz\"\n",
    "    )\n",
    "\n",
    "colnames = [LABEL_COLUMN] + [\"feature-%02d\" % i for i in range(1, 29)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6151f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_data_start_time = time.time()\n",
    "\n",
    "df = pd.read_csv(FILE_URL, names=colnames)\n",
    "\n",
    "load_data_end_time = time.time()\n",
    "load_data_duration = load_data_end_time - load_data_start_time\n",
    "print(f\"Dataset loaded in {load_data_duration} seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347d0479",
   "metadata": {},
   "source": [
    "Split data into training and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875dff40",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_validation = train_test_split(df)\n",
    "print(df_train, df_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe601729",
   "metadata": {},
   "source": [
    "## Distributed Training\n",
    "\n",
    "The ``train_xgboost`` function contains all the logic necessary for\n",
    "training using XGBoost-Ray.\n",
    "\n",
    "Distributed training can not only speed up the process, but also allow you\n",
    "to use datasets that are too large to fit in memory of a single node. With\n",
    "distributed training, the dataset is sharded across different actors\n",
    "running on separate nodes. Those actors communicate with each other to\n",
    "create the final model.\n",
    "\n",
    "First, the dataframes are wrapped in ``RayDMatrix`` objects, which handle\n",
    "data sharding across the cluster. Then, the ``train`` function is called.\n",
    "The evaluation scores will be saved to ``evals_result`` dictionary. The\n",
    "function returns a tuple of the trained model (booster) and the evaluation\n",
    "scores.\n",
    "\n",
    "The ``ray_params`` variable expects a ``RayParams`` object that contains\n",
    "Ray-specific settings, such as the number of workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c13aff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_xgboost(config, train_df, test_df, target_column, ray_params):\n",
    "    train_set = RayDMatrix(train_df, target_column)\n",
    "    test_set = RayDMatrix(test_df, target_column)\n",
    "\n",
    "    evals_result = {}\n",
    "\n",
    "    train_start_time = time.time()\n",
    "\n",
    "    # Train the classifier\n",
    "    bst = train(\n",
    "        params=config,\n",
    "        dtrain=train_set,\n",
    "        evals=[(test_set, \"eval\")],\n",
    "        evals_result=evals_result,\n",
    "        verbose_eval=False,\n",
    "        num_boost_round=100,\n",
    "        ray_params=ray_params,\n",
    "    )\n",
    "\n",
    "    train_end_time = time.time()\n",
    "    train_duration = train_end_time - train_start_time\n",
    "    print(f\"Total time taken: {train_duration} seconds.\")\n",
    "\n",
    "    model_path = \"model.xgb\"\n",
    "    bst.save_model(model_path)\n",
    "    print(\"Final validation error: {:.4f}\".format(evals_result[\"eval\"][\"error\"][-1]))\n",
    "\n",
    "    return bst, evals_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a499531",
   "metadata": {},
   "source": [
    "We can now pass our Modin dataframes and run the function. We will use\n",
    "``RayParams`` to specify that the number of actors and CPUs to train with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e0d9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard XGBoost config for classification\n",
    "config = {\n",
    "    \"tree_method\": \"approx\",\n",
    "    \"objective\": \"binary:logistic\",\n",
    "    \"eval_metric\": [\"logloss\", \"error\"],\n",
    "}\n",
    "\n",
    "bst, evals_result = train_xgboost(\n",
    "    config,\n",
    "    df_train,\n",
    "    df_validation,\n",
    "    LABEL_COLUMN,\n",
    "    RayParams(cpus_per_actor=cpus_per_actor, num_actors=num_actors),\n",
    ")\n",
    "print(f\"Results: {evals_result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556575be",
   "metadata": {},
   "source": [
    "## Prediction\n",
    "\n",
    "With the model trained, we can now predict on unseen data. For the\n",
    "purposes of this example, we will use the same dataset for prediction as\n",
    "for training.\n",
    "\n",
    "Since prediction is naively parallelizable, distributing it over multiple\n",
    "actors can measurably reduce the amount of time needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0170516b",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_df = RayDMatrix(df, ignore=[LABEL_COLUMN, \"partition\"])\n",
    "results = predict(\n",
    "    bst,\n",
    "    inference_df,\n",
    "    ray_params=RayParams(\n",
    "        cpus_per_actor=cpus_per_actor_inference, num_actors=num_actors_inference\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
