(serving-llms)=

# Serving LLMs

Ray Serve LLM provides a high-performance, scalable framework for deploying Large Language Models (LLMs) in production. It specializes Ray Serve primitives for distributed LLM serving workloads, offering enterprise-grade features with OpenAI API compatibility.

## Why Ray Serve LLM?

Ray Serve LLM excels at highly distributed multi-node inference workloads:

- **Advanced parallelism strategies**: Seamlessly combine pipeline parallelism, tensor parallelism, expert parallelism, and data parallel attention for models of any size.
- **Prefill-decode disaggregation**: Separates and optimizes prefill and decode phases independently for better resource utilization and cost efficiency.
- **Custom request routing**: Implements prefix-aware, session-aware, or custom routing logic to maximize cache hits and reduce latency.
- **Multi-node deployments**: Serves massive models that span multiple nodes with automatic placement and coordination.
- **Production-ready**: Has built-in autoscaling, monitoring, fault tolerance, and observability.

## Features

- âš¡ï¸ Automatic scaling and load balancing
- ğŸŒ Unified multi-node multi-model deployment
- ğŸ”Œ OpenAI-compatible API
- ğŸ”„ Multi-LoRA support with shared base models
- ğŸš€ Engine-agnostic architecture (vLLM, SGLang, etc.)
- ğŸ“Š Built-in metrics and Grafana dashboards
- ğŸ¯ Advanced serving patterns (PD disaggregation, data parallel attention)

## Requirements

```bash
pip install ray[serve,llm]
```

### Cloud storage (optional)

If you need to load models from cloud storage, install the appropriate extras for your cloud provider:

```bash
# For AWS S3
pip install ray[serve,llm-s3]

# For Google Cloud Storage
pip install ray[serve,llm-gcp]

# For Azure Blob Storage
pip install ray[serve,llm-azure]
```

```{toctree}
:hidden:

Quickstart <quick-start>
Examples <examples>
User Guides <user-guides/index>
Architecture <architecture/index>
Benchmarks <benchmarks>
Troubleshooting <troubleshooting>
```

## Next steps

- {doc}`Quickstart <quick-start>` - Deploy your first LLM with Ray Serve
- {doc}`Examples <examples>` - Production-ready deployment tutorials
- {doc}`User Guides <user-guides/index>` - Practical guides for advanced features
- {doc}`Architecture <architecture/index>` - Technical design and implementation details
- {doc}`Troubleshooting <troubleshooting>` - Common issues and solutions