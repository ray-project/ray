text: Below are tutorials for exploring Ray Serve capabilities and learning how to integrate different modeling frameworks.
examples:
  - title: Serve ML Models
    skill_level: beginner
    frameworks:
      - pytorch
    link: tutorials/serve-ml-models
  - title: Serve a Stable Diffusion Model
    skill_level: beginner
    use_cases:
      - computer vision
    link: tutorials/stable-diffusion
  - title: Serve a Text Classification Model
    skill_level: beginner
    use_cases:
      - natural language processing
    link: tutorials/text-classification
  - title: Serve an Object Detection Model
    skill_level: beginner
    use_cases:
      - computer vision
    link: tutorials/object-detection

  - title: Serve an Inference Model on AWS NeuronCores Using FastAPI
    skill_level: intermediate
    link: tutorials/aws-neuron-core-inference
  - title: Serve an Inference with Stable Diffusion Model on AWS NeuronCores Using FastAPI
    skill_level: intermediate
    link: tutorials/aws-neuron-core-inference-stable-diffusion
  - title: Serve a model on Intel Habana Gaudi
    skill_level: intermediate
    frameworks:
      - pytorch
    use_cases:
      - generative ai
      - large language models
    link: tutorials/intel-habana-gaudi-inference
  - title: Scale a Gradio App with Ray Serve
    skill_level: intermediate
    link: tutorials/gradio-integration
  - title: Serve a Text Generator with Request Batching
    skill_level: intermediate
    link: tutorials/batch
  - title: Serve a Chatbot with Request and Response Streaming
    skill_level: intermediate
    link: tutorials/streaming
  - title: Serving models with Triton Server in Ray Serve
    skill_level: intermediate
    link: tutorials/triton-server-integration

  - title: Serve a Java App
    skill_level: advanced
    link: tutorials/java
