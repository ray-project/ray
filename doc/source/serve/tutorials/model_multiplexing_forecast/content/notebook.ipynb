{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model multiplexing with forecasting models\n",
        "\n",
        "<div align=\"left\">\n",
        "<a target=\"_blank\" href=\"https://console.anyscale.com/template-preview/model_multiplexing_forecast\"><img src=\"https://img.shields.io/badge/ðŸš€ Run_on-Anyscale-9hf\"></a>&nbsp;\n",
        "<a href=\"https://github.com/ray-project/ray/tree/master/doc/source/serve/tutorials/model_multiplexing_forecast/content\" role=\"button\"><img src=\"https://img.shields.io/static/v1?label=&message=View%20On%20GitHub&color=586069&logo=github&labelColor=2f363d\"></a>&nbsp;\n",
        "</div>\n",
        "\n",
        "This tutorial shows you how to efficiently serve multiple forecasting models using Ray Serve's model multiplexing pattern. Model multiplexing lets you serve dozens or thousands of models from a shared pool of replicas, optimizing cost and resources.\n",
        "\n",
        "## Why model multiplexing for forecasting?\n",
        "\n",
        "Forecasting applications often require separate models per customer, team, or region. Standing up separate deployments for each model is expensive and wasteful. Model multiplexing solves this by:\n",
        "\n",
        "- **Sharing resources**: Multiple models use the same replica pool.\n",
        "- **Lazy loading**: Models load on-demand when first requested.\n",
        "- **Automatic caching**: Frequently used models stay in memory using Least Recently Used (LRU) policy.\n",
        "- **Intelligent routing**: Requests route to replicas that already have the model loaded.\n",
        "\n",
        "See [Model Multiplexing](https://docs.ray.io/en/latest/serve/model-multiplexing.html) for the core concepts and API reference.\n",
        "\n",
        "Use model multiplexing when you have:\n",
        "- Many models that can fit in memory.\n",
        "- Sparse access patterns (not all models queried constantly).\n",
        "- Similar input/output formats across models.\n",
        "- Limited infrastructure budget.\n",
        "\n",
        "## Write a multiplexed deployment\n",
        "\n",
        "Assume you have multiple forecasting models stored in cloud storage with the following structure:\n",
        "```\n",
        "/customer_123/model.pkl\n",
        "/customer_456/model.pkl\n",
        "/customer_789/model.pkl\n",
        "...\n",
        "```\n",
        "\n",
        "Define a multiplexed deployment:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# serve_forecast_multiplex.py\n",
        "import asyncio\n",
        "import numpy as np\n",
        "import pickle\n",
        "from ray import serve\n",
        "from starlette.requests import Request\n",
        "\n",
        "\n",
        "# Simple forecasting model\n",
        "class ForecastModel:\n",
        "    \"\"\"A customer-specific forecasting model.\"\"\"\n",
        "    \n",
        "    def __init__(self, customer_id: str):\n",
        "        self.customer_id = customer_id\n",
        "        # Each customer has different model parameters\n",
        "        np.random.seed(hash(customer_id) % 1000)\n",
        "        self.trend = np.random.uniform(-1, 3)\n",
        "        self.base_level = np.random.uniform(90, 110)\n",
        "    \n",
        "    def predict(self, sequence_data: list) -> list:\n",
        "        \"\"\"Generate a 7-day forecast.\"\"\"\n",
        "        last_value = sequence_data[-1] if sequence_data else self.base_level\n",
        "        forecast = []\n",
        "        for i in range(7):\n",
        "            # Simple forecast: last value + trend\n",
        "            value = last_value + self.trend * (i + 1)\n",
        "            forecast.append(round(value, 2))\n",
        "        return forecast\n",
        "\n",
        "\n",
        "@serve.deployment\n",
        "class ForecastingService:\n",
        "    def __init__(self):\n",
        "        # In production, this is your cloud storage path or model registry\n",
        "        self.model_storage_path = \"/customer-models\"\n",
        "    \n",
        "    @serve.multiplexed(max_num_models_per_replica=4)\n",
        "    async def get_model(self, customer_id: str):\n",
        "        \"\"\"Load a customer's forecasting model.\n",
        "        \n",
        "        In production, this function downloads from cloud storage or loads from a database.\n",
        "        For this example, the code mocks the I/O with asyncio.sleep().\n",
        "        \"\"\"\n",
        "        # Simulate downloading model from remote storage\n",
        "        await asyncio.sleep(0.1)  # Mock network I/O delay\n",
        "        \n",
        "        # In production:\n",
        "        # model_bytes = await download_from_storage(f\"{self.model_storage_path}/{customer_id}/model.pkl\")\n",
        "        # return pickle.loads(model_bytes)\n",
        "        \n",
        "        # For this example, create a mock model\n",
        "        return ForecastModel(customer_id)\n",
        "    \n",
        "    async def __call__(self, request: Request):\n",
        "        \"\"\"Generate forecast for a customer.\"\"\"\n",
        "        # Get the serve_multiplexed_model_id from the request header\n",
        "        customer_id = serve.get_multiplexed_model_id()\n",
        "        \n",
        "        # Load the model (cached if already loaded)\n",
        "        model = await self.get_model(customer_id)\n",
        "        \n",
        "        # Get input data\n",
        "        data = await request.json()\n",
        "        sequence_data = data.get(\"sequence_data\", [])\n",
        "        \n",
        "        # Generate forecast\n",
        "        forecast = model.predict(sequence_data)\n",
        "        \n",
        "        return {\"customer_id\": customer_id, \"forecast\": forecast}\n",
        "\n",
        "\n",
        "app = ForecastingService.bind()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `@serve.multiplexed` decorator enables automatic caching with LRU eviction. The `max_num_models_per_replica` parameter controls how many models to cache per replica. When the cache fills, Ray Serve evicts the least recently used model.\n",
        "\n",
        "See [Model Multiplexing](https://docs.ray.io/en/latest/serve/model-multiplexing.html) for details on how the caching and routing work.\n",
        "\n",
        "## Deploy locally\n",
        "\n",
        "Test your multiplexed deployment on your local machine before moving to production.\n",
        "\n",
        "### Launch\n",
        "\n",
        "In a terminal, run:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "shellscript"
        }
      },
      "outputs": [],
      "source": [
        "!serve run serve_forecast_multiplex:app --non-blocking"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Note:** When running in a notebook, the `--non-blocking` flag returns control immediately so you can continue executing cells. Without it, `serve run` blocks the notebook. In a terminal, you can omit this flag to stream logs to the console.\n",
        "\n",
        "Ray Serve logs the endpoint of your application once the service is deployed:\n",
        "```console\n",
        "INFO 2025-12-04 01:46:12,733 serve 5085 -- Application 'default' is ready at http://0.0.0.0:8000/\n",
        "```\n",
        "\n",
        "### Send requests\n",
        "\n",
        "To send a request to a specific customer's model, include the `serve_multiplexed_model_id` header:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# client.py\n",
        "import requests\n",
        "\n",
        "# time series data\n",
        "sequence_data = [100, 102, 98, 105, 110, 108, 112, 115, 118, 120]\n",
        "\n",
        "# Send request with customer_id in header\n",
        "response = requests.post(\n",
        "    \"http://localhost:8000\",\n",
        "    headers={\"serve_multiplexed_model_id\": \"customer_123\"},\n",
        "    json={\"sequence_data\": sequence_data}\n",
        ")\n",
        "\n",
        "print(response.json())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Output:\n",
        "```json\n",
        "{\n",
        "  \"customer_id\": \"customer_123\",\n",
        "  \"forecast\": [121.45, 122.90, 124.35, 125.80, 127.25, 128.70, 130.15]\n",
        "}\n",
        "```\n",
        "\n",
        "The first request to a model triggers loading that you can track in the logs:\n",
        "\n",
        "```\n",
        "INFO 2025-12-04 00:50:18,097 default_ForecastingService -- Loading model 'customer_123'.\n",
        "INFO 2025-12-04 00:50:18,197 default_ForecastingService -- Successfully loaded model 'customer_123' in 100.1ms.\n",
        "```\n",
        "\n",
        "Subsequent requests for the same model use the cache instead, unless the model has been evicted, which you can also track in the logs:\n",
        "```\n",
        "INFO 2025-12-04 01:59:08,141 default_ForecastingService -- Unloading model 'customer_def'.\n",
        "INFO 2025-12-04 01:59:08,142 default_ForecastingService -- Successfully unloaded model 'customer_abc' in 0.2ms.\n",
        "```\n",
        "\n",
        "### Test multiple customers\n",
        "\n",
        "Send requests for different customers to see multiplexing in action:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# client_multiple_requests.py\n",
        "import random\n",
        "import requests\n",
        "\n",
        "customer_ids = [\"customer_123\", \"customer_456\", \"customer_789\", \"customer_abc\", \"customer_def\", \"customer_hij\"]\n",
        "\n",
        "# Create a random list of 100 requests from those 3 customers\n",
        "random_requests = [random.choice(customer_ids) for _ in range(100)]\n",
        "\n",
        "# Send all 100 requests\n",
        "for i, customer_id in enumerate(random_requests):\n",
        "    # Generate random \"live\" data for each request\n",
        "    live_sequence_data = [random.uniform(90, 110) for _ in range(10)]\n",
        "    \n",
        "    response = requests.post(\n",
        "        \"http://localhost:8000\",\n",
        "        headers={\"serve_multiplexed_model_id\": customer_id},\n",
        "        json={\"sequence_data\": live_sequence_data}\n",
        "    )\n",
        "    forecast = response.json()[\"forecast\"]\n",
        "    print(f\"Request {i+1} - {customer_id}: {forecast[:3]}...\")\n",
        "\n",
        "print(f\"\\nSent {len(random_requests)} requests total\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "What happens:\n",
        "1. First request for each customer triggers model loading (~100&nbsp;ms).\n",
        "2. Subsequent requests use the cached model (<5&nbsp;ms).\n",
        "3. When cache fills (>4 models per replica), least recently used models evict.\n",
        "\n",
        "You can also send requests using the deployment handle:\n",
        "\n",
        "```python\n",
        "handle = serve.get_deployment_handle(\"ForecastingService\", \"default\")\n",
        "result = await handle.options(multiplexed_model_id=\"customer_123\").remote(request)\n",
        "```\n",
        "\n",
        "### Shutdown\n",
        "\n",
        "Shutdown your service:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "shellscript"
        }
      },
      "outputs": [],
      "source": [
        "!serve shutdown -y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Deploy to production with Anyscale Services\n",
        "\n",
        "For production deployment, use Anyscale Services to deploy the application to a dedicated cluster.\n",
        "\n",
        "Create a `service.yaml` file:\n",
        "\n",
        "```yaml\n",
        "# service.yaml\n",
        "name: my-forecast-multiplexing-service\n",
        "image_uri: anyscale/ray:2.52.1-slim-py312\n",
        "compute_config:\n",
        "  auto_select_worker_config: true\n",
        "working_dir: .\n",
        "applications:\n",
        "  - import_path: serve_forecast_multiplex:app\n",
        "\n",
        "```\n",
        "\n",
        "### Launch\n",
        "\n",
        "Deploy your Anyscale Service:\n",
        "```bash\n",
        "anyscale service deploy -f service.yaml\n",
        "```\n",
        "\n",
        "The output shows your endpoint URL and authentication token.\n",
        "```console\n",
        "(anyscale +6.6s) Query the service once it's running using the following curl command (add the path you want to query):\n",
        "(anyscale +6.6s) curl -H \"Authorization: Bearer <YOUR-TOKEN>\" <YOUR-ENDPOINT>\n",
        "```\n",
        "\n",
        "You can also retrieve them from your console. Go to your Anyscale Service page, then click the **Query** button at the top.\n",
        "\n",
        "### Send requests\n",
        "\n",
        "Use the endpoint and token from the deployment output:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# client_anyscale_service.py\n",
        "import requests\n",
        "\n",
        "ENDPOINT = \"<YOUR-ENDPOINT>\"  # From the deployment output\n",
        "TOKEN = \"<YOUR-TOKEN>\"  # From the deployment output\n",
        "\n",
        "response = requests.post(\n",
        "    ENDPOINT,\n",
        "    headers={\n",
        "        \"Authorization\": f\"Bearer {TOKEN}\",\n",
        "        \"serve_multiplexed_model_id\": \"customer_123\"\n",
        "    },\n",
        "    json={\"sequence_data\": [100, 102, 98, 105, 110]}\n",
        ")\n",
        "\n",
        "print(response.json())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Shutdown\n",
        "\n",
        "Terminate your Anyscale Service:\n",
        "```bash\n",
        "anyscale service terminate -n my-forecast-multiplexing-service\n",
        "```\n",
        "\n",
        "## Monitor your deployment\n",
        "\n",
        "Ray Serve exposes metrics for model multiplexing:\n",
        "\n",
        "| Metric | Description |\n",
        "|--------|-------------|\n",
        "| `ray_serve_num_multiplexed_models` | Number of models loaded per replica |\n",
        "| `ray_serve_multiplexed_model_load_latency_ms` | Model load time |\n",
        "| `ray_serve_multiplexed_models_load_counter_total` | Total models loaded |\n",
        "| `ray_serve_multiplexed_models_unload_counter_total` | Total models unloaded |\n",
        "\n",
        "See [Monitoring and Debugging](https://docs.ray.io/en/latest/serve/monitoring.html) for more details on monitoring a Serve application.\n",
        "\n",
        "### Access the dashboard on Anyscale\n",
        "\n",
        "To view metrics in an Anyscale Service or Workspace:\n",
        "\n",
        "1. From your console, navigate to your Anyscale Service or Workspace page.\n",
        "2. Go to the **Metrics** tab, then **Serve Deployment Dashboard**.\n",
        "\n",
        "From there, you can also open Grafana by clicking **View tab in Grafana**.\n",
        "\n",
        "## Performance tips\n",
        "\n",
        "Optimize your multiplexed deployment for better cache hit and lower latency.\n",
        "\n",
        "### Use co-routines for blocking operations during model loading\n",
        "\n",
        "Use async operations for network or I/O operations:\n",
        "\n",
        "```python\n",
        "\n",
        "@serve.multiplexed(max_num_models_per_replica=10)\n",
        "async def get_model(self, customer_id: str):\n",
        "    ...\n",
        "    model = await long_download_model()\n",
        "    await long_io_operation()\n",
        "    ...\n",
        "    return model\n",
        "```\n",
        "\n",
        "### Tune cache size\n",
        "\n",
        "Set `max_num_models_per_replica` based on your memory and access patterns:\n",
        "\n",
        "```python\n",
        "# For small models (10MB each) on 4GB nodes\n",
        "@serve.multiplexed(max_num_models_per_replica=100)\n",
        "\n",
        "# For larger models (100MB each)\n",
        "@serve.multiplexed(max_num_models_per_replica=20)\n",
        "```\n",
        "\n",
        "### Scale replicas\n",
        "\n",
        "Increase the number of replicas if you want more models to be warm:\n",
        "\n",
        "```python\n",
        "@serve.deployment(num_replicas=5)  # 5 Ã— 10 = 50 warm models\n",
        "```\n",
        "\n",
        "### Pre-warm models\n",
        "\n",
        "Consider pre-loading important models in the cache during initialization to avoid costly cold-starts:\n",
        "\n",
        "```python\n",
        "async def _prewarm(self):\n",
        "    ...\n",
        "    for customer_id in [\"customer_vip_1\", \"customer_vip_2\"]:\n",
        "        await self.get_model(customer_id)\n",
        "```\n",
        "\n",
        "### Calculate cache hit rate\n",
        "\n",
        "Monitor the ratio of model loads to model requests by looking at metrics such as `ray_serve_multiplexed_models_load_counter_total` or `ray_serve_multiplexed_get_model_requests_counter_total`.\n",
        "\n",
        "A high cache hit rate (>90%) means most requests use cached models. A low rate suggests:\n",
        "- Too many unique models for cache size\n",
        "- Need to increase `max_num_models_per_replica`\n",
        "- Consider adding more replicas\n",
        "\n",
        "### Implement resource cleanup\n",
        "\n",
        "If your models hold resources (GPU memory, database connections), implement `__del__` in the object returned by your loading model function (the one decorated by `serve.multiplexed()`):\n",
        "\n",
        "```python\n",
        "@serve.deployment\n",
        "class ForecastingService:\n",
        "    ...\n",
        "    @serve.multiplexed()\n",
        "    async def load_model(self, model_id: str):\n",
        "        ...\n",
        "        return ForecastModel()\n",
        "\n",
        "class ForecastModel:\n",
        "    def __init__(self, model_id: str):\n",
        "        self.model_id = model_id\n",
        "        self.db_connection = connect_to_db()\n",
        "        self.get_lock()\n",
        "    \n",
        "    def __del__(self):\n",
        "        \"\"\"Clean up resources when model is evicted.\"\"\"\n",
        "        if hasattr(self, 'db_connection'):\n",
        "            self.db_connection.close()\n",
        "        self.release_lock()\n",
        "        self.clean_gpu_memory()\n",
        "        ...\n",
        "```\n",
        "\n",
        "Ray Serve automatically calls `__del__` when evicting models from the cache.\n",
        "\n",
        "## Troubleshooting\n",
        "\n",
        "Common issues you might encounter when using model multiplexing.\n",
        "\n",
        "### Missing model ID in request\n",
        "\n",
        "```\n",
        "ValueError: The model ID cannot be empty.\n",
        "```\n",
        "\n",
        "Make sure you pass `serve_multiplexed_model_id` in the header of your request, or in the handle options:\n",
        "```python\n",
        "# Forward `serve_multiplexed_model_id` in the request header\n",
        "response = requests.post(\n",
        "    \"http://localhost:8000\",\n",
        "    headers={\"serve_multiplexed_model_id\": \"customer_123\"},\n",
        "    json={\"sequence_data\": sequence_data}\n",
        ")\n",
        "# Or\n",
        "# Forward `serve_multiplexed_model_id` in the handle options\n",
        "await self.forecaster.options(\n",
        "    multiplexed_model_id=request.model_id\n",
        ").remote(request)\n",
        "```\n",
        "\n",
        "### TypeError: object can't be used in 'await' expression\n",
        "\n",
        "```\n",
        "TypeError: object <...> can't be used in 'await' expression\n",
        "```\n",
        "\n",
        "Make sure you define your model loading function, decorated with `@serve.multiplexed()`, as an async function with `async def`:\n",
        "\n",
        "```python\n",
        "@serve.deployment\n",
        "class ForecastingService:\n",
        "    ...\n",
        "    @serve.multiplexed(max_num_models_per_replica=10)\n",
        "    async def my_loading_model_function(self, my_model_id: str):\n",
        "        ...\n",
        "```\n",
        "\n",
        "### Slow first requests\n",
        "\n",
        "When you send the first request for a model, the system loads the model into the cache. Subsequent requests are fast until the system evicts the model from the cache according to the LRU policy.\n",
        "\n",
        "If you wish to avoid cold-starts, consider [awaiting blocking IO/network operations](#use-co-routines-for-blocking-operations-during-model-loading), [pre-warming important models](#pre-warm-models), [increasing the cache size](#tune-cache-size), or [add more replicas](#scale-replicas) to your deployment.\n",
        "\n",
        "### Cache thrashing\n",
        "\n",
        "If you observe high latency variance and your models are constantly loading and unloading, you might have too many models for the cache size.\n",
        "\n",
        "For example:\n",
        "- 100 different models requested per minute\n",
        "- Cache size: 10 models per replica\n",
        "- Result: Constant eviction and reloading\n",
        "\n",
        "See [Performance tips](#performance-tips) for guidance on how to optimize your deployment.\n",
        "\n",
        "### Out of memory errors\n",
        "\n",
        "If the system loads your models simultaneously, they might exceed available memory, especially when some models require GPU memory. Reduce `max_num_models_per_replica` to limit the size of your cache. \n",
        "\n",
        "## Summary\n",
        "\n",
        "This tutorial shows you how to use model multiplexing to serve multiple forecasting models from shared replicas, configure deployments with the `@serve.multiplexed` decorator and cache tuning, deploy both locally and in production, route requests with model IDs, monitor performance, optimize your deployment, and troubleshoot common issues."
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "orphan": true
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
