{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model multiplexing with forecasting models\n",
    "\n",
    "<div align=\"left\">\n",
    "<a target=\"_blank\" href=\"https://console.anyscale.com/template-preview/model_multiplexing_forecast\"><img src=\"https://img.shields.io/badge/ðŸš€ Run_on-Anyscale-9hf\"></a>&nbsp;\n",
    "<a href=\"https://github.com/ray-project/ray/tree/master/doc/source/serve/tutorials/model_multiplexing_forecast/content\" role=\"button\"><img src=\"https://img.shields.io/static/v1?label=&message=View%20On%20GitHub&color=586069&logo=github&labelColor=2f363d\"></a>&nbsp;\n",
    "</div>\n",
    "\n",
    "This tutorial shows you how to efficiently serve multiple forecasting models using Ray Serve's model multiplexing pattern. Model multiplexing lets you serve dozens or thousands of models from a shared pool of replicas, optimizing cost and resources.\n",
    "\n",
    "## Why model multiplexing for forecasting?\n",
    "\n",
    "Forecasting applications often require separate models per customer, team, or region. Standing up separate deployments for each model is expensive and wasteful. Model multiplexing solves this by:\n",
    "\n",
    "- **Sharing resources**: Multiple models use the same replica pool.\n",
    "- **Lazy loading**: Models load on-demand when first requested.\n",
    "- **Automatic caching**: Frequently used models stay in memory using Least Recently Used (LRU) policy.\n",
    "- **Intelligent routing**: Requests route to replicas that already have the model loaded.\n",
    "\n",
    "See [Model Multiplexing](https://docs.ray.io/en/latest/serve/model-multiplexing.html) for the core concepts and API reference.\n",
    "\n",
    "## Configure a multiplexed deployment\n",
    "\n",
    "Assume you have multiple forecasting models stored in cloud storage with the following structure:\n",
    "```\n",
    "/customer_123/model.pkl\n",
    "/customer_456/model.pkl\n",
    "/customer_789/model.pkl\n",
    "...\n",
    "```\n",
    "\n",
    "Define a multiplexed deployment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# serve_forecast_multiplex.py\n",
    "import asyncio\n",
    "import numpy as np\n",
    "import pickle\n",
    "from ray import serve\n",
    "from starlette.requests import Request\n",
    "\n",
    "\n",
    "class ForecastModel:\n",
    "    \"\"\"A customer-specific forecasting model.\n",
    "    \n",
    "    Note: If your models hold resources (GPU memory, database connections),\n",
    "    implement __del__ to clean up when Ray Serve evicts the model from cache.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, customer_id: str):\n",
    "        self.customer_id = customer_id\n",
    "        # Each customer has different model parameters\n",
    "        np.random.seed(hash(customer_id) % 1000)\n",
    "        self.trend = np.random.uniform(-1, 3)\n",
    "        self.base_level = np.random.uniform(90, 110)\n",
    "    \n",
    "    def predict(self, sequence_data: list) -> list:\n",
    "        \"\"\"Generate a 7-day forecast.\"\"\"\n",
    "        last_value = sequence_data[-1] if sequence_data else self.base_level\n",
    "        forecast = []\n",
    "        for i in range(7):\n",
    "            # Simple forecast: last value + trend\n",
    "            value = last_value + self.trend * (i + 1)\n",
    "            forecast.append(round(value, 2))\n",
    "        return forecast\n",
    "    \n",
    "    def __del__(self):\n",
    "        \"\"\"Clean up resources when model is evicted from cache.\"\"\"\n",
    "        # Example: close database connections, release GPU memory, etc.\n",
    "        pass\n",
    "\n",
    "\n",
    "@serve.deployment\n",
    "class ForecastingService:\n",
    "    def __init__(self):\n",
    "        # In production, this is your cloud storage path or model registry\n",
    "        self.model_storage_path = \"/customer-models\"\n",
    "    \n",
    "    @serve.multiplexed(max_num_models_per_replica=4)\n",
    "    async def get_model(self, customer_id: str):\n",
    "        \"\"\"Load a customer's forecasting model.\n",
    "        \n",
    "        In production, this function downloads from cloud storage or loads from a database.\n",
    "        For this example, the code mocks the I/O with asyncio.sleep().\n",
    "        \"\"\"\n",
    "        # Simulate downloading model from remote storage\n",
    "        await asyncio.sleep(0.1)  # Mock network I/O delay\n",
    "        \n",
    "        # In production:\n",
    "        # model_bytes = await download_from_storage(f\"{self.model_storage_path}/{customer_id}/model.pkl\")\n",
    "        # return pickle.loads(model_bytes)\n",
    "        \n",
    "        # For this example, create a mock model\n",
    "        return ForecastModel(customer_id)\n",
    "    \n",
    "    async def __call__(self, request: Request):\n",
    "        \"\"\"Generate forecast for a customer.\"\"\"\n",
    "        # Get the serve_multiplexed_model_id from the request header\n",
    "        customer_id = serve.get_multiplexed_model_id()\n",
    "        \n",
    "        # Load the model (cached if already loaded)\n",
    "        model = await self.get_model(customer_id)\n",
    "        \n",
    "        # Get input data\n",
    "        data = await request.json()\n",
    "        sequence_data = data.get(\"sequence_data\", [])\n",
    "        \n",
    "        # Generate forecast\n",
    "        forecast = model.predict(sequence_data)\n",
    "        \n",
    "        return {\"customer_id\": customer_id, \"forecast\": forecast}\n",
    "\n",
    "\n",
    "app = ForecastingService.bind()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `@serve.multiplexed` decorator enables automatic caching with LRU eviction. The `max_num_models_per_replica` parameter controls how many models to cache per replica. When the cache fills, Ray Serve evicts the least recently used model.\n",
    "\n",
    "See [Model Multiplexing](https://docs.ray.io/en/latest/serve/model-multiplexing.html) for details on how the caching and routing work.\n",
    "\n",
    "## Deploy locally\n",
    "\n",
    "Test your multiplexed deployment on your local machine before moving to production.\n",
    "\n",
    "### Launch\n",
    "\n",
    "Deploy your application:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "serve.run(app)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ray Serve logs the endpoint of your application once the service is deployed:\n",
    "```console\n",
    "INFO 2025-12-04 01:46:12,733 serve 5085 -- Application 'default' is ready at http://0.0.0.0:8000/\n",
    "```\n",
    "\n",
    "### Send requests\n",
    "\n",
    "To send a request to a specific customer's model, include the `serve_multiplexed_model_id` header:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# client.py\n",
    "import requests\n",
    "\n",
    "# time series data\n",
    "sequence_data = [100, 102, 98, 105, 110, 108, 112, 115, 118, 120]\n",
    "\n",
    "# Send request with customer_id in header\n",
    "response = requests.post(\n",
    "    \"http://localhost:8000\",\n",
    "    headers={\"serve_multiplexed_model_id\": \"customer_123\"},\n",
    "    json={\"sequence_data\": sequence_data}\n",
    ")\n",
    "\n",
    "print(response.json())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:\n",
    "```json\n",
    "{\n",
    "  \"customer_id\": \"customer_123\",\n",
    "  \"forecast\": [121.45, 122.90, 124.35, 125.80, 127.25, 128.70, 130.15]\n",
    "}\n",
    "```\n",
    "\n",
    "The first request to a model triggers loading that you can track in the logs:\n",
    "\n",
    "```\n",
    "INFO 2025-12-04 00:50:18,097 default_ForecastingService -- Loading model 'customer_123'.\n",
    "INFO 2025-12-04 00:50:18,197 default_ForecastingService -- Successfully loaded model 'customer_123' in 100.1ms.\n",
    "```\n",
    "\n",
    "Subsequent requests for the same model use the cache instead, unless the model has been evicted, which you can also track in the logs:\n",
    "```\n",
    "INFO 2025-12-04 01:59:08,141 default_ForecastingService -- Unloading model 'customer_def'.\n",
    "INFO 2025-12-04 01:59:08,142 default_ForecastingService -- Successfully unloaded model 'customer_abc' in 0.2ms.\n",
    "```\n",
    "\n",
    "You can also send the `multiplexed_model_id` using the deployment handle:\n",
    "\n",
    "```python\n",
    "handle = serve.get_deployment_handle(\"ForecastingService\", \"default\")\n",
    "result = await handle.options(multiplexed_model_id=\"customer_123\").remote(request)\n",
    "```\n",
    "\n",
    "### Test concurrent requests\n",
    "\n",
    "Send concurrent requests to see the LRU caching and model loading in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# client_concurrent_requests.py\n",
    "import random\n",
    "import requests\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "customer_ids = [\"customer_123\", \"customer_456\", \"customer_789\", \"customer_abc\", \"customer_def\", \"customer_hij\"]\n",
    "\n",
    "def send_request(customer_id):\n",
    "    live_sequence_data = [random.uniform(90, 110) for _ in range(10)]\n",
    "    response = requests.post(\n",
    "        \"http://localhost:8000\",\n",
    "        headers={\"serve_multiplexed_model_id\": customer_id},\n",
    "        json={\"sequence_data\": live_sequence_data}\n",
    "    )\n",
    "    return customer_id, response.json()[\"forecast\"]\n",
    "\n",
    "# Create 100 random requests across 6 customers\n",
    "random_customers = [random.choice(customer_ids) for _ in range(100)]\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "    futures = [executor.submit(send_request, cid) for cid in random_customers]\n",
    "    for future in as_completed(futures):\n",
    "        customer_id, forecast = future.result()\n",
    "        print(f\"{customer_id}: {forecast[:3]}...\")\n",
    "\n",
    "print(\"\\nSent 100 concurrent requests\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under concurrent load with 6 customers and a cache size of 4, the LRU policy evicts the least recently used models. Watch the logs to see models loading and unloading as requests arrive.\n",
    "\n",
    "**Note:** If you see frequent model loading/unloading (cache thrashing), increase `max_num_models_per_replica` or add more replicas to warm more models.\n",
    "\n",
    "### Shutdown\n",
    "\n",
    "Shutdown your service:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "serve.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy to production with Anyscale Services\n",
    "\n",
    "For production deployment, use Anyscale Services to deploy the application to a dedicated cluster.\n",
    "\n",
    "Create a `service.yaml` file:\n",
    "\n",
    "```yaml\n",
    "# service.yaml\n",
    "name: my-forecast-multiplexing-service\n",
    "image_uri: anyscale/ray:2.52.1-slim-py312\n",
    "compute_config:\n",
    "  auto_select_worker_config: true\n",
    "working_dir: .\n",
    "applications:\n",
    "  - import_path: serve_forecast_multiplex:app\n",
    "\n",
    "```\n",
    "\n",
    "### Launch\n",
    "\n",
    "Deploy your Anyscale Service:\n",
    "```bash\n",
    "anyscale service deploy -f service.yaml\n",
    "```\n",
    "\n",
    "The output shows your endpoint URL and authentication token.\n",
    "```console\n",
    "(anyscale +6.6s) Query the service once it's running using the following curl command (add the path you want to query):\n",
    "(anyscale +6.6s) curl -H \"Authorization: Bearer <YOUR-TOKEN>\" <YOUR-ENDPOINT>\n",
    "```\n",
    "\n",
    "You can also retrieve them from your console. Go to your Anyscale Service page, then click the **Query** button at the top.\n",
    "\n",
    "### Send requests\n",
    "\n",
    "Use the endpoint and token from the deployment output:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# client_anyscale_service.py\n",
    "import requests\n",
    "\n",
    "ENDPOINT = \"<YOUR-ENDPOINT>\"  # From the deployment output\n",
    "TOKEN = \"<YOUR-TOKEN>\"  # From the deployment output\n",
    "\n",
    "response = requests.post(\n",
    "    ENDPOINT,\n",
    "    headers={\n",
    "        \"Authorization\": f\"Bearer {TOKEN}\",\n",
    "        \"serve_multiplexed_model_id\": \"customer_123\"\n",
    "    },\n",
    "    json={\"sequence_data\": [100, 102, 98, 105, 110]}\n",
    ")\n",
    "\n",
    "print(response.json())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shutdown\n",
    "\n",
    "Terminate your Anyscale Service:\n",
    "```bash\n",
    "anyscale service terminate -n my-forecast-multiplexing-service\n",
    "```\n",
    "\n",
    "## Monitor your deployment\n",
    "\n",
    "Ray Serve exposes metrics for model multiplexing:\n",
    "\n",
    "| Metric | Description |\n",
    "|--------|-------------|\n",
    "| `ray_serve_num_multiplexed_models` | Number of models loaded per replica |\n",
    "| `ray_serve_multiplexed_model_load_latency_ms` | Model load time |\n",
    "| `ray_serve_multiplexed_models_load_counter_total` | Total models loaded |\n",
    "| `ray_serve_multiplexed_models_unload_counter_total` | Total models unloaded |\n",
    "\n",
    "Track the ratio of cache hits to total requests using `ray_serve_multiplexed_models_load_counter_total` and `ray_serve_multiplexed_get_model_requests_counter_total`. \n",
    "A high cache hit rate (>90%) indicates most requests use cached models. A low rate suggests you need to increase `max_num_models_per_replica` or add more replicas.\n",
    "\n",
    "See [Monitoring and Debugging](https://docs.ray.io/en/latest/serve/monitoring.html) for more details on monitoring a Serve application.\n",
    "\n",
    "### Access the dashboard on Anyscale\n",
    "\n",
    "To view metrics in an Anyscale Service or Workspace:\n",
    "\n",
    "1. From your console, navigate to your Anyscale Service or Workspace page.\n",
    "2. Go to the **Metrics** tab, then **Serve Deployment Dashboard**.\n",
    "\n",
    "From there, you can also open Grafana by clicking **View tab in Grafana**.\n",
    "\n",
    "## Optimize cache performance\n",
    "\n",
    "Tune your multiplexed deployment for better cache hit rates and lower latency.\n",
    "\n",
    "### Increase cache size\n",
    "\n",
    "Set `max_num_models_per_replica` based on your memory and access patterns to increase the cache size per replica:\n",
    "\n",
    "```python\n",
    "# For small models (10MB each) on 4GB nodes\n",
    "@serve.multiplexed(max_num_models_per_replica=100)\n",
    "\n",
    "# For larger models (100MB each)\n",
    "@serve.multiplexed(max_num_models_per_replica=20)\n",
    "```\n",
    "\n",
    "You can also scale the number of replicas to keep more models loaded in memory across the cluster:\n",
    "\n",
    "```python\n",
    "@serve.deployment(num_replicas=5)  # 5 replicas Ã— 10 models = 50 warm models\n",
    "```\n",
    "\n",
    "**Note:** If you observe high latency variance with constant model loading/unloading (cache thrashing), your cache is too small for your access pattern. Increase `max_num_models_per_replica` or add more replicas.\n",
    "\n",
    "### Pre-warm models at initialization\n",
    "\n",
    "Avoid cold-starts for high-priority models by pre-loading during initialization:\n",
    "\n",
    "```python\n",
    "async def _prewarm(self):\n",
    "    for customer_id in [\"customer_vip_1\", \"customer_vip_2\"]:\n",
    "        await self.get_model(customer_id)\n",
    "```\n",
    "\n",
    "**Note:** First requests for a model are slow because they trigger model loading. Pre-warming eliminates this latency for important models.\n",
    "\n",
    "## Troubleshooting\n",
    "\n",
    "### Missing model ID in request\n",
    "\n",
    "```\n",
    "ValueError: The model ID cannot be empty.\n",
    "```\n",
    "\n",
    "Make sure you pass `serve_multiplexed_model_id` in the header of your request, or in the handle options:\n",
    "```python\n",
    "response = requests.post(\n",
    "    \"http://localhost:8000\",\n",
    "    headers={\"serve_multiplexed_model_id\": \"customer_123\"},\n",
    "    json={\"sequence_data\": sequence_data}\n",
    ")\n",
    "# Or use handle options with `multiplexed_model_id`:\n",
    "await self.forecaster.options(multiplexed_model_id=request.model_id).remote(request)\n",
    "```\n",
    "\n",
    "### TypeError: object can't be used in 'await' expression\n",
    "\n",
    "Make sure you define your model loading function with `async def`:\n",
    "\n",
    "```python\n",
    "@serve.multiplexed(max_num_models_per_replica=10)\n",
    "async def get_model(self, model_id: str):  # Must be async\n",
    "    ...\n",
    "```\n",
    "\n",
    "### Out of memory errors\n",
    "\n",
    "If models exceed available memory, reduce `max_num_models_per_replica` to limit cache size.\n",
    "\n",
    "## Summary\n",
    "\n",
    "This tutorial demonstrated how to use model multiplexing to serve multiple forecasting models. The key concepts covered include:\n",
    "\n",
    "- Serving multiple models from shared replicas with `@serve.multiplexed`\n",
    "- Configuring LRU cache size with `max_num_models_per_replica`\n",
    "- Routing requests with the `serve_multiplexed_model_id` header\n",
    "- Deploying locally and to production with Anyscale Services\n",
    "- Monitoring cache usage and model load metrics"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  },
  "orphan": true
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
