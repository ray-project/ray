(serve-streaming-tutorial)=

# Streaming Tutorial

This guide walks you through deploying a chatbot that streams output back to the
user. We show

* How to stream outputs from your Serve deployments
* How to add WebSockets to your Serve deployments
* How to batch requests when streaming output

This tutorial should help you with following use cases:

* You want to serve a large language model that should stream results back token-by-token.
* You want to serve a chatbot that must accept a stream of requests from the user.

This tutorial serves the [DialoGPT](https://huggingface.co/microsoft/DialoGPT-small) language model. Install the HuggingFace library to access it:

```
pip install transformers
```

# Create a Streaming Deployment

Open a new Python file called `textbot.py`. First, add the imports and the Serve logger.

```{literalinclude} ../doc_code/streaming_tutorial.py
:language: python
:start-after: __textbot_setup_start__
:end-before: __textbot_setup_end__
```

Create a FastAPI deployment, and initialize the model and the tokenizer in the
constructor:

```{literalinclude} ../doc_code/streaming_tutorial.py
:language: python
:start-after: __textbot_constructor_start__
:end-before: __textbot_constructor_end__
```

Note that the constructor also caches an `asyncio` loop. This will come in handy when we need to run a model and concurrently stream its tokens back to the user.

Add the following logic to handle requests sent to the `Textbot`:

```{literalinclude} ../doc_code/streaming_tutorial.py
:language: python
:start-after: __textbot_logic_start__
:end-before: __textbot_logic_end__
```

`Textbot` uses three methods to handler requests:

* `handle_request`: the entrypoint for HTTP requests. FastAPI automatically unpacks the `prompt` query parameter and passes it into `handle_request`. This method then creates a `TextIteratorStreamer`. HuggingFace provides this streamer as a convenient interface to access tokens generated by a language model. `handle_request` then kicks off the model in a background thread using `self.loop.run_in_executor`. This lets the model generate tokens while `handle_request` concurrently calls `self.consume_streamer` to stream the tokens back to the user. `self.consume_streamer` is a generator that yields tokens one by one from the streamer. Last, `handle_request` passes the `self.consume_streamer` generator into a Starlette `StreamingResponse` and returns the response. Serve unpacks the Starlette `StreamingResponse` and yields the contents of the generator back to the user one by one.
* `generate_text`: the method that runs the model. This method runs in a background thread kicked off by `handle_request`. It pushes generated tokens into the streamer constructed by `handle_request`.
* `consume_streamer`: a generator method that consumes the streamer constructed by `handle_request`. This method keeps yielding tokens from the streamer until the model in `generate_text` closes the streamer. This method avoids blocking the event loop by calling `asyncio.sleep` with a brief timeout whenever the streamer is empty and waiting for a new token.

Bind the `Textbot` to a language model. For this tutorial, use the `"microsoft/DialoGPT-small"` model:

```{literalinclude} ../doc_code/streaming_tutorial.py
:language: python
:start-after: __textbot_bind_start__
:end-before: __textbot_bind_end__
```

Run the model with `serve run textbot:app`, and query it from another terminal window with this script:

```{literalinclude} ../doc_code/streaming_tutorial.py
:language: python
:start-after: __stream_client_start__
:end-before: __stream_client_end__
```

You should see the output printed token by token.

# Add WebSocket Support

WebSockets let you stream input into a deployment and stream output back to the client. You can use this to create a chatbot that stores a conversation with a user.

Create a Python file called `chatbot.py`. First add the imports:

```{literalinclude} ../doc_code/streaming_tutorial.py
:language: python
:start-after: __chatbot_setup_start__
:end-before: __chatbot_setup_end__
```

Create a FastAPI deployment, and initialize the model and the tokenizer in the
constructor:

```{literalinclude} ../doc_code/streaming_tutorial.py
:language: python
:start-after: __chatbot_constructor_start__
:end-before: __chatbot_constructor_end__
```

Add the following logic to handle requests sent to the `Chatbot`:

```{literalinclude} ../doc_code/streaming_tutorial.py
:language: python
:start-after: __chatbot_logic_start__
:end-before: __chatbot_logic_end__
```

The `generate_text` and `consume_streamer` methods are the same as they were for the `Textbot`. The `handle_request` method has been updated to handle WebSocket requests.

The `handle_request` method is decorated with a `fastapi_app.websocket` decorator, which lets it accept WebSocket requests. First it `awaits` to accept the client's WebSocket request. Then, until the client disconnects, it does the following:

* gets the prompt from the client with `ws.receive_text`
* starts a new `TextIteratorStreamer` to access generated tokens
* runs the model in a background thread on the conversation so far
* streams the model's output back using `ws.send_text`
* stores the prompt and the response in the `conversation` string

Each time `handle_request` gets a new prompt from a client, it runs the whole conversation– with the new prompt appended– through the model. When it the model is finished generating tokens, `handle_request` sends the `"<<Response Finished>>"` string to inform the client that all tokens have been generated. `handle_request` continues to run until the client explicitly disconnects. This raises a `WebSocketDisconnect` exception, which ends the call.

Read more about WebSockets in the [FastAPI documentation](https://fastapi.tiangolo.com/advanced/websockets/).

Bind the `Chatbot` to a language model. For this tutorial, use the `"microsoft/DialoGPT-small"` model:

```{literalinclude} ../doc_code/streaming_tutorial.py
:language: python
:start-after: __chatbot_bind_start__
:end-before: __chatbot_bind_end__
```

Run the model with `serve run chatbot:app`. Query it using the `websockets` package (`pip install websockets`):

```{literalinclude} ../doc_code/streaming_tutorial.py
:language: python
:start-after: __ws_client_start__
:end-before: __ws_client_end__
```

You should see the outputs printed token by token.
