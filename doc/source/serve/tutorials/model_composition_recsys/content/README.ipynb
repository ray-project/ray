{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model composition for recommendation systems\n",
    "\n",
    "<div align=\"left\">\n",
    "<a target=\"_blank\" href=\"https://console.anyscale.com/template-preview/model_composition_recsys\"><img src=\"https://img.shields.io/badge/ðŸš€ Run_on-Anyscale-9hf\"></a>&nbsp;\n",
    "<a href=\"https://github.com/ray-project/ray/tree/master/doc/source/serve/tutorials/model_composition_recsys/content\" role=\"button\"><img src=\"https://img.shields.io/static/v1?label=&message=View%20On%20GitHub&color=586069&logo=github&labelColor=2f363d\"></a>&nbsp;\n",
    "</div>\n",
    "\n",
    "This tutorial shows you how to build a recommendation system using Ray Serve's model composition pattern. Model composition breaks complex ML pipelines into independent deployments that you can scale and update separately.\n",
    "\n",
    "## Why model composition for recommendation systems?\n",
    "\n",
    "Recommendation systems typically involve multiple stages: feature extraction, candidate generation, ranking, and filtering. Model composition solves common challenges by:\n",
    "\n",
    "- **Independent scaling**: Scale feature extractors separately from ranking models based on traffic patterns.\n",
    "- **Flexible updates**: Update one component without redeploying the entire pipeline.\n",
    "- **Resource optimization**: Allocate different resources (CPU/GPU) to each component.\n",
    "\n",
    "See [Model Composition](https://docs.ray.io/en/latest/serve/model-composition.html) for the core concepts and patterns.\n",
    "\n",
    "## Configure a composed deployment\n",
    "\n",
    "Build a recommendation system with three components:\n",
    "1. **UserFeatureExtractor**: Extracts user features (demographics, history, preferences).\n",
    "2. **ItemRankingModel**: Scores items based on user features.\n",
    "3. **RecommendationService**: Orchestrates the pipeline and returns top recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# serve_recommendation_pipeline.py\n",
    "import asyncio\n",
    "import numpy as np\n",
    "from typing import List, Dict\n",
    "from ray import serve\n",
    "from ray.serve.handle import DeploymentHandle\n",
    "from starlette.requests import Request\n",
    "\n",
    "\n",
    "# Component 1: User Feature Extractor\n",
    "@serve.deployment(num_replicas=2)\n",
    "class UserFeatureExtractor:\n",
    "    \"\"\"Extracts user features from user ID.\n",
    "    \n",
    "    In production, this queries a database or feature store.\n",
    "    For this example, the code generates mock features.\n",
    "    \"\"\"\n",
    "    \n",
    "    async def extract_features(self, user_id: str) -> Dict[str, float]:\n",
    "        \"\"\"Extract user features.\"\"\"\n",
    "        # Simulate database lookup\n",
    "        await asyncio.sleep(0.01)\n",
    "        \n",
    "        # In production:\n",
    "        # features = await db.query(\"SELECT * FROM user_features WHERE user_id = ?\", user_id)\n",
    "        # return features\n",
    "        \n",
    "        # Mock features based on user_id hash\n",
    "        np.random.seed(hash(user_id) % 10000)\n",
    "        return {\n",
    "            \"age_group\": float(np.random.randint(18, 65)),\n",
    "            \"avg_session_duration\": float(np.random.uniform(5, 60)),\n",
    "            \"total_purchases\": float(np.random.randint(0, 100)),\n",
    "            \"engagement_score\": float(np.random.uniform(0, 1)),\n",
    "        }\n",
    "\n",
    "\n",
    "# Component 2: Item Ranking Model\n",
    "@serve.deployment(\n",
    "    autoscaling_config={\n",
    "        \"min_replicas\": 1,\n",
    "        \"max_replicas\": 5,\n",
    "        \"target_ongoing_requests\": 3\n",
    "    },\n",
    "    ray_actor_options={\"num_cpus\": 2}\n",
    ")\n",
    "class ItemRankingModel:\n",
    "    \"\"\"Ranks items for a user based on features.\n",
    "    \n",
    "    In production, this runs a trained ML model (XGBoost, neural network, etc.).\n",
    "    For this example, the code uses a simple scoring function.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Mock item catalog. In production, this comes from a database query.\n",
    "    CANDIDATE_ITEMS = [f\"item_{i}\" for i in range(1000)]\n",
    "    \n",
    "    def __init__(self):\n",
    "        # In production, this is your cloud storage path or model registry\n",
    "        # self.model = load_model(\"/models/ranking_model.pkl\")\n",
    "        pass\n",
    "    \n",
    "    def _score_items(self, user_features: Dict[str, float]) -> List[Dict[str, any]]:\n",
    "        \"\"\"Score and rank items for a single user.\"\"\"\n",
    "        ranked_items = []\n",
    "        for item_id in self.CANDIDATE_ITEMS:\n",
    "            item_popularity = (hash(item_id) % 100) / 100.0\n",
    "            score = (\n",
    "                user_features[\"engagement_score\"] * 0.6 + \n",
    "                item_popularity * 0.4\n",
    "            )\n",
    "            ranked_items.append({\n",
    "                \"item_id\": item_id,\n",
    "                \"score\": round(score, 3)\n",
    "            })\n",
    "        ranked_items.sort(key=lambda x: x[\"score\"], reverse=True)\n",
    "        return ranked_items\n",
    "    \n",
    "    @serve.batch(max_batch_size=32, batch_wait_timeout_s=0.01)\n",
    "    async def rank_items(\n",
    "        self, \n",
    "        user_features_batch: List[Dict[str, float]]\n",
    "    ) -> List[List[Dict[str, any]]]:\n",
    "        \"\"\"Rank candidate items for a batch of users.\"\"\"\n",
    "        # Simulate model inference time\n",
    "        await asyncio.sleep(0.05)\n",
    "        \n",
    "        # In production, use vectorized batch inference:\n",
    "        # return self.model.batch_predict(user_features_batch, self.CANDIDATE_ITEMS)\n",
    "        \n",
    "        return [self._score_items(features) for features in user_features_batch]\n",
    "\n",
    "\n",
    "# Component 3: Recommendation Service (Orchestrator)\n",
    "@serve.deployment\n",
    "class RecommendationService:\n",
    "    \"\"\"Orchestrates the recommendation pipeline.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        user_feature_extractor: DeploymentHandle,\n",
    "        ranking_model: DeploymentHandle\n",
    "    ):\n",
    "        self.user_feature_extractor = user_feature_extractor\n",
    "        self.ranking_model = ranking_model\n",
    "    \n",
    "    async def __call__(self, request: Request) -> Dict:\n",
    "        \"\"\"Generate recommendations for a user.\"\"\"\n",
    "        data = await request.json()\n",
    "        user_id = data[\"user_id\"]\n",
    "        top_k = data.get(\"top_k\", 5)\n",
    "        \n",
    "        # Step 1: Extract user features\n",
    "        user_features = await self.user_feature_extractor.extract_features.remote(user_id)\n",
    "        \n",
    "        # Step 2: Rank candidate items (batched automatically by @serve.batch)\n",
    "        ranked_items = await self.ranking_model.rank_items.remote(user_features)\n",
    "        \n",
    "        # Step 3: Return top-k recommendations\n",
    "        return {\n",
    "            \"user_id\": user_id,\n",
    "            \"recommendations\": ranked_items[:top_k]\n",
    "        }\n",
    "\n",
    "\n",
    "# Build the application\n",
    "app = RecommendationService.bind(\n",
    "    user_feature_extractor=UserFeatureExtractor.bind(),\n",
    "    ranking_model=ItemRankingModel.bind()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each deployment in the composition can scale independently based on its resource needs and traffic patterns. The `RecommendationService` orchestrates calls to the other deployments using deployment handles.\n",
    "\n",
    "**Note:** The `ItemRankingModel` uses several performance optimizations:\n",
    "- **Autoscaling**: Automatically scales replicas based on traffic via `autoscaling_config`. See [Autoscaling](https://docs.ray.io/en/latest/serve/autoscaling-guide.html).\n",
    "- **Request batching**: The `@serve.batch` decorator groups concurrent requests for efficient batch inference. See [Dynamic Request Batching](https://docs.ray.io/en/latest/serve/advanced-guides/dyn-req-batch.html).\n",
    "\n",
    "**Warning:** When calling deployment handles inside a deployment, always use `await` instead of `.result()`. The `.result()` method blocks the replica from processing other requests while waiting. Using `await` enables the deployment to handle other requests concurrently.\n",
    "\n",
    "See [Model Composition](https://docs.ray.io/en/latest/serve/model-composition.html) for details on deployment handles and orchestration patterns.\n",
    "\n",
    "## Deploy locally\n",
    "\n",
    "Test your composed pipeline on your local machine before moving to production.\n",
    "\n",
    "### Launch\n",
    "\n",
    "Deploy your application:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "serve.run(app)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ray Serve logs the endpoint of your application once the service is deployed:\n",
    "```console\n",
    "INFO 2025-12-04 03:15:42,123 serve 8923 -- Application 'default' is ready at http://0.0.0.0:8000/\n",
    "```\n",
    "\n",
    "### Send requests\n",
    "\n",
    "Send a recommendation request for a user:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# client.py\n",
    "import requests\n",
    "\n",
    "response = requests.post(\n",
    "    \"http://localhost:8000\",\n",
    "    json={\n",
    "        \"user_id\": \"user_42\",\n",
    "        \"top_k\": 5\n",
    "    }\n",
    ")\n",
    "\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:\n",
    "```json\n",
    "{\n",
    "  \"user_id\": \"user_42\",\n",
    "  \"recommendations\": [\n",
    "    {\"item_id\": \"item_108\", \"score\": 0.847},\n",
    "    {\"item_id\": \"item_103\", \"score\": 0.792},\n",
    "    {\"item_id\": \"item_110\", \"score\": 0.756},\n",
    "    {\"item_id\": \"item_101\", \"score\": 0.723},\n",
    "    {\"item_id\": \"item_105\", \"score\": 0.689}\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "The request flows through the pipeline:\n",
    "1. `RecommendationService` receives the request.\n",
    "2. `UserFeatureExtractor` extracts user features (~10&nbsp;ms).\n",
    "3. `ItemRankingModel` scores all candidate items from its catalog (~50&nbsp;ms).\n",
    "4. `RecommendationService` returns top-k items.\n",
    "\n",
    "### Test concurrent requests\n",
    "\n",
    "Send concurrent requests to trigger autoscaling and see the pipeline handle load:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# client_concurrent_requests.py\n",
    "import requests\n",
    "import random\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "def send_request(user_id):\n",
    "    response = requests.post(\n",
    "        \"http://localhost:8000\",\n",
    "        json={\"user_id\": user_id, \"top_k\": 3}\n",
    "    )\n",
    "    return user_id, response.json()\n",
    "\n",
    "user_ids = [f\"user_{random.randint(1, 1000)}\" for _ in range(100)]\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "    futures = [executor.submit(send_request, uid) for uid in user_ids]\n",
    "    for future in as_completed(futures):\n",
    "        user_id, result = future.result()\n",
    "        top_items = [rec[\"item_id\"] for rec in result[\"recommendations\"]]\n",
    "        print(f\"{user_id}: {top_items}\")\n",
    "\n",
    "print(\"\\nSent 100 concurrent requests\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under concurrent load, Ray Serve automatically scales the `ItemRankingModel` replicas based on the `autoscaling_config`. When traffic exceeds `target_ongoing_requests` per replica, new replicas spin up (up to `max_replicas`). When traffic drops, replicas scale back down to `min_replicas`.\n",
    "\n",
    "### Shutdown\n",
    "\n",
    "Shutdown your service:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "serve.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy to production with Anyscale Services\n",
    "\n",
    "For production deployment, use Anyscale Services to deploy the application to a dedicated cluster.\n",
    "\n",
    "Create a `service.yaml` file:\n",
    "\n",
    "```yaml\n",
    "# service.yaml\n",
    "name: my-recommendation-service\n",
    "image_uri: anyscale/ray:2.52.1-slim-py312\n",
    "compute_config:\n",
    "  auto_select_worker_config: true\n",
    "working_dir: .\n",
    "applications:\n",
    "  - import_path: serve_recommendation_pipeline:app\n",
    "\n",
    "```\n",
    "\n",
    "### Launch\n",
    "\n",
    "Deploy your Anyscale Service:\n",
    "```bash\n",
    "anyscale service deploy -f service.yaml\n",
    "```\n",
    "\n",
    "The output shows your endpoint URL and authentication token.\n",
    "```console\n",
    "(anyscale +5.2s) Query the service once it's running using the following curl command:\n",
    "(anyscale +5.2s) curl -H \"Authorization: Bearer <YOUR-TOKEN>\" <YOUR-ENDPOINT>\n",
    "```\n",
    "\n",
    "You can also retrieve them from your console. Go to your Anyscale Service page, then click the **Query** button at the top.\n",
    "\n",
    "### Send requests\n",
    "\n",
    "Use the endpoint and token from the deployment output:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# client_anyscale_service.py\n",
    "import requests\n",
    "\n",
    "ENDPOINT = \"<YOUR-ENDPOINT>\"  # From the deployment output\n",
    "TOKEN = \"<YOUR-TOKEN>\"  # From the deployment output\n",
    "\n",
    "response = requests.post(\n",
    "    ENDPOINT,\n",
    "    headers={\"Authorization\": f\"Bearer {TOKEN}\"},\n",
    "    json={\n",
    "        \"user_id\": \"user_42\",\n",
    "        \"top_k\": 5\n",
    "    }\n",
    ")\n",
    "\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shutdown\n",
    "\n",
    "Terminate your Anyscale Service:\n",
    "```bash\n",
    "anyscale service terminate -n my-recommendation-service\n",
    "```\n",
    "\n",
    "## Monitor your deployment\n",
    "\n",
    "Ray Serve exposes per-deployment metrics that help you understand pipeline performance:\n",
    "\n",
    "| Metric | Description |\n",
    "|--------|-------------|\n",
    "| `ray_serve_deployment_request_counter` | Total requests per deployment |\n",
    "| `ray_serve_deployment_processing_latency_ms` | Processing time per replica |\n",
    "| `ray_serve_num_deployment_http_error_requests_total` | Error rate per deployment |\n",
    "| `ray_serve_deployment_queued_queries` | Queue depth per replica |\n",
    "\n",
    "See [Monitoring and Debugging](https://docs.ray.io/en/latest/serve/monitoring.html) for more details on monitoring a Serve application.\n",
    "\n",
    "### Access the dashboard on Anyscale\n",
    "\n",
    "To view metrics in an Anyscale Service or Workspace:\n",
    "\n",
    "1. From your console, navigate to your Anyscale Service or Workspace page.\n",
    "2. Go to the **Metrics** tab, then **Serve Deployment Dashboard**.\n",
    "\n",
    "From there, you can also open Grafana by clicking **View tab in Grafana**.\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this tutorial, you learned how how to build a recommendation system with Ray Serve using a model composition pattern. You learned to create independent deployments for each pipeline stage, configure autoscaling for changing traffic, orchestrate multi-stage workflows with deployment handles, deploy to production with Anyscale Services, and monitor per-component metrics."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  },
  "orphan": true
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
