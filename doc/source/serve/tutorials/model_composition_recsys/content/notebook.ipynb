{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model composition for recommendation systems\n",
        "\n",
        "<div align=\"left\">\n",
        "<a target=\"_blank\" href=\"https://console.anyscale.com/template-preview/model_composition_recsys\"><img src=\"https://img.shields.io/badge/ðŸš€ Run_on-Anyscale-9hf\"></a>&nbsp;\n",
        "<a href=\"https://github.com/ray-project/ray/tree/master/doc/source/serve/tutorials/model_composition_recsys/content\" role=\"button\"><img src=\"https://img.shields.io/static/v1?label=&message=View%20On%20GitHub&color=586069&logo=github&labelColor=2f363d\"></a>&nbsp;\n",
        "</div>\n",
        "\n",
        "This tutorial shows you how to build a recommendation system using Ray Serve's model composition pattern. Model composition lets you break complex ML pipelines into independent deployments that scale and update separately.\n",
        "\n",
        "## Why model composition for recommendation systems?\n",
        "\n",
        "Recommendation systems typically involve multiple stages: feature extraction, candidate generation, ranking, and filtering. Model composition solves common challenges by:\n",
        "\n",
        "- **Independent scaling**: Scale feature extractors separately from ranking models based on traffic patterns.\n",
        "- **Team ownership**: Different teams can own and deploy their models independently.\n",
        "- **Flexible updates**: Update one component without redeploying the entire pipeline.\n",
        "- **Resource optimization**: Allocate different resources (CPU/GPU) to each component.\n",
        "\n",
        "See [Model Composition](https://docs.ray.io/en/latest/serve/model-composition.html) for the core concepts and patterns.\n",
        "\n",
        "Use model composition when you have:\n",
        "- Multi-stage ML pipelines with distinct models.\n",
        "- Components that scale differently (for example lightweight feature extraction vs. heavy ranking).\n",
        "- Multiple teams contributing models to the same system.\n",
        "- Need to A/B test individual components.\n",
        "\n",
        "## Write a composed recommendation pipeline\n",
        "\n",
        "Build a recommendation system with three components:\n",
        "1. **UserFeatureExtractor**: Extracts user features (demographics, history, preferences).\n",
        "2. **ItemRankingModel**: Scores items based on user features.\n",
        "3. **RecommendationService**: Orchestrates the pipeline and returns top recommendations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# serve_recommendation_pipeline.py\n",
        "import asyncio\n",
        "import numpy as np\n",
        "from typing import List, Dict\n",
        "from ray import serve\n",
        "from ray.serve.handle import DeploymentHandle\n",
        "from starlette.requests import Request\n",
        "\n",
        "\n",
        "# Component 1: User Feature Extractor\n",
        "@serve.deployment(num_replicas=2)\n",
        "class UserFeatureExtractor:\n",
        "    \"\"\"Extracts user features from user ID.\n",
        "    \n",
        "    In production, this queries a database or feature store.\n",
        "    For this example, the code generates mock features.\n",
        "    \"\"\"\n",
        "    \n",
        "    async def extract_features(self, user_id: str) -> Dict[str, float]:\n",
        "        \"\"\"Extract user features.\"\"\"\n",
        "        # Simulate database lookup\n",
        "        await asyncio.sleep(0.01)\n",
        "        \n",
        "        # In production:\n",
        "        # features = await db.query(\"SELECT * FROM user_features WHERE user_id = ?\", user_id)\n",
        "        # return features\n",
        "        \n",
        "        # Mock features based on user_id hash\n",
        "        np.random.seed(hash(user_id) % 10000)\n",
        "        return {\n",
        "            \"age_group\": float(np.random.randint(18, 65)),\n",
        "            \"avg_session_duration\": float(np.random.uniform(5, 60)),\n",
        "            \"total_purchases\": float(np.random.randint(0, 100)),\n",
        "            \"engagement_score\": float(np.random.uniform(0, 1)),\n",
        "        }\n",
        "\n",
        "\n",
        "# Component 2: Item Ranking Model\n",
        "@serve.deployment(\n",
        "    num_replicas=3,\n",
        "    ray_actor_options={\"num_cpus\": 2}\n",
        ")\n",
        "class ItemRankingModel:\n",
        "    \"\"\"Ranks items for a user based on features.\n",
        "    \n",
        "    In production, this runs a trained ML model (XGBoost, neural network, etc.).\n",
        "    For this example, the code uses a simple scoring function.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        # In production: Load model weights\n",
        "        # self.model = load_model(\"s3://models/ranking_model.pkl\")\n",
        "        pass\n",
        "    \n",
        "    async def rank_items(\n",
        "        self, \n",
        "        user_features: Dict[str, float], \n",
        "        candidate_items: List[str]\n",
        "    ) -> List[Dict[str, any]]:\n",
        "        \"\"\"Rank candidate items for the user.\"\"\"\n",
        "        # Simulate model inference time\n",
        "        await asyncio.sleep(0.05)\n",
        "        \n",
        "        # In production:\n",
        "        # scores = self.model.predict(user_features, candidate_items)\n",
        "        \n",
        "        # Mock scoring: combine user engagement with item popularity\n",
        "        ranked_items = []\n",
        "        for item_id in candidate_items:\n",
        "            # Simple mock scoring based on user engagement and item hash\n",
        "            item_popularity = (hash(item_id) % 100) / 100.0\n",
        "            score = (\n",
        "                user_features[\"engagement_score\"] * 0.6 + \n",
        "                item_popularity * 0.4\n",
        "            )\n",
        "            ranked_items.append({\n",
        "                \"item_id\": item_id,\n",
        "                \"score\": round(score, 3)\n",
        "            })\n",
        "        \n",
        "        # Sort by score descending\n",
        "        ranked_items.sort(key=lambda x: x[\"score\"], reverse=True)\n",
        "        return ranked_items\n",
        "\n",
        "\n",
        "# Component 3: Recommendation Service (Orchestrator)\n",
        "@serve.deployment\n",
        "class RecommendationService:\n",
        "    \"\"\"Orchestrates the recommendation pipeline.\"\"\"\n",
        "    \n",
        "    def __init__(\n",
        "        self,\n",
        "        user_feature_extractor: DeploymentHandle,\n",
        "        ranking_model: DeploymentHandle\n",
        "    ):\n",
        "        self.user_feature_extractor = user_feature_extractor\n",
        "        self.ranking_model = ranking_model\n",
        "    \n",
        "    async def __call__(self, request: Request) -> Dict:\n",
        "        \"\"\"Generate recommendations for a user.\"\"\"\n",
        "        data = await request.json()\n",
        "        user_id = data[\"user_id\"]\n",
        "        candidate_items = data.get(\"candidate_items\", [])\n",
        "        top_k = data.get(\"top_k\", 5)\n",
        "        \n",
        "        # Step 1: Extract user features\n",
        "        user_features = await self.user_feature_extractor.extract_features.remote(user_id)\n",
        "        \n",
        "        # Step 2: Rank candidate items\n",
        "        ranked_items = await self.ranking_model.rank_items.remote(\n",
        "            user_features, \n",
        "            candidate_items\n",
        "        )\n",
        "        \n",
        "        # Step 3: Return top-k recommendations\n",
        "        return {\n",
        "            \"user_id\": user_id,\n",
        "            \"recommendations\": ranked_items[:top_k],\n",
        "            \"total_candidates\": len(candidate_items)\n",
        "        }\n",
        "\n",
        "\n",
        "# Build the application\n",
        "app = RecommendationService.bind(\n",
        "    user_feature_extractor=UserFeatureExtractor.bind(),\n",
        "    ranking_model=ItemRankingModel.bind()\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Each deployment in the composition can scale independently based on its resource needs and traffic patterns. The `RecommendationService` orchestrates calls to the other deployments using deployment handles.\n",
        "\n",
        "See [Model Composition](https://docs.ray.io/en/latest/serve/model-composition.html) for details on deployment handles and orchestration patterns.\n",
        "\n",
        "## Deploy locally\n",
        "\n",
        "Test your composed pipeline on your local machine before moving to production.\n",
        "\n",
        "### Launch\n",
        "\n",
        "In a terminal, run:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!serve run serve_recommendation_pipeline:app --non-blocking"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Note:** When running in a notebook, the `--non-blocking` flag returns control immediately so you can continue executing cells. Without it, `serve run` blocks the notebook. In a terminal, you can omit this flag to stream logs to the console.\n",
        "\n",
        "Ray Serve logs the endpoint of your application once the service is deployed:\n",
        "```console\n",
        "INFO 2025-12-04 03:15:42,123 serve 8923 -- Application 'default' is ready at http://0.0.0.0:8000/\n",
        "```\n",
        "\n",
        "### Send requests\n",
        "\n",
        "Send a recommendation request for a user:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# client.py\n",
        "import requests\n",
        "\n",
        "response = requests.post(\n",
        "    \"http://localhost:8000\",\n",
        "    json={\n",
        "        \"user_id\": \"user_42\",\n",
        "        \"candidate_items\": [\n",
        "            \"item_101\", \"item_102\", \"item_103\", \n",
        "            \"item_104\", \"item_105\", \"item_106\",\n",
        "            \"item_107\", \"item_108\", \"item_109\", \"item_110\"\n",
        "        ],\n",
        "        \"top_k\": 5\n",
        "    }\n",
        ")\n",
        "\n",
        "print(response.json())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Output:\n",
        "```json\n",
        "{\n",
        "  \"user_id\": \"user_42\",\n",
        "  \"recommendations\": [\n",
        "    {\"item_id\": \"item_108\", \"score\": 0.847},\n",
        "    {\"item_id\": \"item_103\", \"score\": 0.792},\n",
        "    {\"item_id\": \"item_110\", \"score\": 0.756},\n",
        "    {\"item_id\": \"item_101\", \"score\": 0.723},\n",
        "    {\"item_id\": \"item_105\", \"score\": 0.689}\n",
        "  ],\n",
        "  \"total_candidates\": 10\n",
        "}\n",
        "```\n",
        "\n",
        "The request flows through the pipeline:\n",
        "1. `RecommendationService` receives the request.\n",
        "2. `UserFeatureExtractor` extracts user features (~10&nbsp;ms).\n",
        "3. `ItemRankingModel` scores all candidate items (~50&nbsp;ms).\n",
        "4. `RecommendationService` returns top-k items.\n",
        "\n",
        "### Test with multiple users\n",
        "\n",
        "Send requests for different users to see the pipeline in action:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# client_multiple_requests.py\n",
        "import requests\n",
        "import random\n",
        "\n",
        "# Generate sample candidate items\n",
        "all_items = [f\"item_{i}\" for i in range(1000)]\n",
        "\n",
        "# Test with multiple users\n",
        "for i in range(100):\n",
        "    user_id = f\"user_{random.randint(1, 1000)}\"\n",
        "    # Each user gets a random subset of candidate items\n",
        "    candidate_items = random.sample(all_items, k=50)\n",
        "    \n",
        "    response = requests.post(\n",
        "        \"http://localhost:8000\",\n",
        "        json={\n",
        "            \"user_id\": user_id,\n",
        "            \"candidate_items\": candidate_items,\n",
        "            \"top_k\": 3\n",
        "        }\n",
        "    )\n",
        "    \n",
        "    result = response.json()\n",
        "    top_items = [rec[\"item_id\"] for rec in result[\"recommendations\"]]\n",
        "    print(f\"Request {i+1} - {user_id}: {top_items}\")\n",
        "\n",
        "print(f\"\\nSent 100 requests total\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Each component processes requests independently and can scale based on load. For example:\n",
        "- `UserFeatureExtractor` (2 replicas) handles feature extraction.\n",
        "- `ItemRankingModel` (3 replicas with 2 CPUs each) handles compute-intensive ranking.\n",
        "- `RecommendationService` (1 replica) orchestrates the pipeline.\n",
        "\n",
        "### Shutdown\n",
        "\n",
        "Shutdown your service:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!serve shutdown -y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Deploy to production with Anyscale Services\n",
        "\n",
        "For production deployment, use Anyscale Services to deploy the application to a dedicated cluster.\n",
        "\n",
        "Create a `service.yaml` file:\n",
        "\n",
        "```yaml\n",
        "# service.yaml\n",
        "name: my-recommendation-service\n",
        "image_uri: anyscale/ray:2.52.1-slim-py312\n",
        "compute_config:\n",
        "  auto_select_worker_config: true\n",
        "working_dir: .\n",
        "applications:\n",
        "  - import_path: serve_recommendation_pipeline:app\n",
        "\n",
        "```\n",
        "\n",
        "### Launch\n",
        "\n",
        "Deploy your Anyscale Service:\n",
        "```bash\n",
        "anyscale service deploy -f service.yaml\n",
        "```\n",
        "\n",
        "The output shows your endpoint URL and authentication token.\n",
        "```console\n",
        "(anyscale +5.2s) Query the service once it's running using the following curl command:\n",
        "(anyscale +5.2s) curl -H \"Authorization: Bearer <YOUR-TOKEN>\" <YOUR-ENDPOINT>\n",
        "```\n",
        "\n",
        "You can also retrieve them from your console. Go to your Anyscale Service page, then click the **Query** button at the top.\n",
        "\n",
        "### Send requests\n",
        "\n",
        "Use the endpoint and token from the deployment output:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# client_anyscale_service.py\n",
        "import requests\n",
        "\n",
        "ENDPOINT = \"<YOUR-ENDPOINT>\"  # From the deployment output\n",
        "TOKEN = \"<YOUR-TOKEN>\"  # From the deployment output\n",
        "\n",
        "response = requests.post(\n",
        "    ENDPOINT,\n",
        "    headers={\"Authorization\": f\"Bearer {TOKEN}\"},\n",
        "    json={\n",
        "        \"user_id\": \"user_42\",\n",
        "        \"candidate_items\": [f\"item_{i}\" for i in range(100, 120)],\n",
        "        \"top_k\": 5\n",
        "    }\n",
        ")\n",
        "\n",
        "print(response.json())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Shutdown\n",
        "\n",
        "Terminate your Anyscale Service:\n",
        "```bash\n",
        "anyscale service terminate -n my-recommendation-service\n",
        "```\n",
        "\n",
        "## Monitor your deployment\n",
        "\n",
        "Ray Serve exposes per-deployment metrics that help you understand pipeline performance:\n",
        "\n",
        "| Metric | Description |\n",
        "|--------|-------------|\n",
        "| `ray_serve_deployment_request_counter` | Total requests per deployment |\n",
        "| `ray_serve_deployment_processing_latency_ms` | Processing time per replica |\n",
        "| `ray_serve_num_deployment_http_error_requests_total` | Error rate per deployment |\n",
        "| `ray_serve_deployment_queued_queries` | Queue depth per replica |\n",
        "\n",
        "See [Monitoring and Debugging](https://docs.ray.io/en/latest/serve/monitoring.html) for more details on monitoring a Serve application.\n",
        "\n",
        "### Access the dashboard on Anyscale\n",
        "\n",
        "To view metrics in an Anyscale Service or Workspace:\n",
        "\n",
        "1. From your console, navigate to your Anyscale Service or Workspace page.\n",
        "2. Go to the **Metrics** tab, then **Serve Deployment Dashboard**.\n",
        "\n",
        "From there, you can also open Grafana by clicking **View tab in Grafana**.\n",
        "\n",
        "## Performance tips\n",
        "\n",
        "Optimize your composed pipeline for better throughput and lower latency.\n",
        "\n",
        "### Scale deployments independently\n",
        "\n",
        "Identify bottlenecks and scale specific components:\n",
        "\n",
        "```python\n",
        "# Scale feature extraction for high traffic\n",
        "@serve.deployment(num_replicas=5)\n",
        "class UserFeatureExtractor:\n",
        "    ...\n",
        "\n",
        "# Scale ranking model with more resources\n",
        "@serve.deployment(\n",
        "    num_replicas=10,\n",
        "    ray_actor_options={\"num_cpus\": 4, \"num_gpus\": 1}\n",
        ")\n",
        "class ItemRankingModel:\n",
        "    ...\n",
        "```\n",
        "\n",
        "### Use autoscaling\n",
        "\n",
        "Configure autoscaling for components with variable load:\n",
        "\n",
        "```python\n",
        "@serve.deployment(\n",
        "    autoscaling_config={\n",
        "        \"min_replicas\": 2,\n",
        "        \"max_replicas\": 10,\n",
        "        \"target_ongoing_requests\": 5\n",
        "    }\n",
        ")\n",
        "class ItemRankingModel:\n",
        "    ...\n",
        "```\n",
        "\n",
        "### Batch requests for ranking models\n",
        "\n",
        "Improve throughput by batching ranking requests:\n",
        "\n",
        "```python\n",
        "@serve.deployment\n",
        "class ItemRankingModel:\n",
        "    @serve.batch(max_batch_size=32, batch_wait_timeout_s=0.01)\n",
        "    async def rank_items(self, requests):\n",
        "        # Process batch of requests together\n",
        "        ...\n",
        "```\n",
        "\n",
        "### Use co-routines for blocking operations\n",
        "\n",
        "Use async operations for blocking operations:\n",
        "\n",
        "```python\n",
        "@serve.deployment\n",
        "class UserFeatureExtractor:\n",
        "    async def extract_features(self, user_id: str):\n",
        "        # Use async database client\n",
        "        features = await db.query_async(user_id)\n",
        "        return features\n",
        "```\n",
        "\n",
        ":::warning\n",
        "When calling deployment handles inside a deployment, always use `await` instead of `.result()`. The `.result()` method blocks the deployment from processing other requests while waiting for the remote call to finish. Using `await` lets the deployment handle other requests concurrently:\n",
        "\n",
        "```python\n",
        "@serve.deployment\n",
        "class RecommendationService:\n",
        "    def __init__(self, user_feature_extractor: DeploymentHandle):\n",
        "        self.user_feature_extractor = user_feature_extractor\n",
        "    \n",
        "    async def __call__(self, request):\n",
        "        user_id = (await request.json())[\"user_id\"]\n",
        "        \n",
        "        # Correct: Non-blocking - allows other requests to be processed\n",
        "        features = await self.user_feature_extractor.extract_features.remote(user_id)\n",
        "        \n",
        "        # Avoid: Blocks the replica from handling other requests\n",
        "        # features = self.user_feature_extractor.extract_features.remote(user_id).result()\n",
        "        \n",
        "        return features\n",
        "```\n",
        ":::\n",
        "\n",
        "## Troubleshooting\n",
        "\n",
        "Common issues you might encounter when using model composition.\n",
        "\n",
        "### Missing deployment handle\n",
        "\n",
        "```\n",
        "TypeError: __init__() missing 1 required positional argument: 'ranking_model'\n",
        "```\n",
        "\n",
        "Make sure you bind all deployment handles when building the application:\n",
        "\n",
        "```python\n",
        "# Correct: All handles bound\n",
        "app = RecommendationService.bind(\n",
        "    user_feature_extractor=UserFeatureExtractor.bind(),\n",
        "    ranking_model=ItemRankingModel.bind()\n",
        ")\n",
        "\n",
        "# Incorrect: Missing ranking_model handle\n",
        "app = RecommendationService.bind(\n",
        "    user_feature_extractor=UserFeatureExtractor.bind()\n",
        ")\n",
        "```\n",
        "\n",
        "### AttributeError: 'DeploymentHandle' object has no attribute 'X'\n",
        "\n",
        "```\n",
        "AttributeError: 'DeploymentHandle' object has no attribute 'extract_features'\n",
        "```\n",
        "\n",
        "Make sure you call methods on the handle using `.remote()`:\n",
        "\n",
        "```python\n",
        "# Correct\n",
        "features = await self.user_feature_extractor.extract_features.remote(user_id)\n",
        "\n",
        "# Incorrect\n",
        "features = await self.user_feature_extractor.extract_features(user_id)\n",
        "```\n",
        "\n",
        "### Timeout errors\n",
        "\n",
        "```\n",
        "RayTaskError: Task timed out after 30 seconds\n",
        "```\n",
        "\n",
        "Check for deadlocks or increase timeout for slow components such as model inference or database operations:\n",
        "\n",
        "```python\n",
        "# Set `max_ongoing_requests` to monitor which replica is not receiving responses fast enough\n",
        "@serve.deployment(max_ongoing_requests=10)\n",
        "class ItemRankingModel:\n",
        "    ...\n",
        "\n",
        "# Configure timeout per call\n",
        "ranked = await self.ranking_model.rank_items.options(\n",
        "    timeout_s=60\n",
        ").remote(features, items)\n",
        "```\n",
        "\n",
        "### High latency between components\n",
        "\n",
        "If you observe high latency and low throughput between deployments:\n",
        "\n",
        "1. **Check for `.result()` usage**: Make sure you're using `await` instead of `.result()` when calling deployment handles. Using `.result()` blocks the replica from processing other requests, which severely impacts performance. See [Use async operations](#use-async-operations) for the correct pattern.\n",
        "2. **Check replica placement**: Ensure replicas are on the same nodes when possible.\n",
        "3. **Monitor queue depth**: High queue depth indicates insufficient replicas.\n",
        "4. **Profile each component**: Identify which stage is the bottleneck.\n",
        "5. **Consider batching**: Batch requests to improve throughput.\n",
        "\n",
        "## Summary\n",
        "\n",
        "This tutorial shows you how to use model composition to build recommendation systems with independent deployments, configure each component with different resources and scaling policies, orchestrate multi-stage pipelines with deployment handles, deploy both locally and in production, monitor per-component metrics, optimize pipeline performance, and troubleshoot common issues.\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "orphan": true
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
