{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c105c497",
   "metadata": {},
   "source": [
    "# Deploy a reasoning LLM\n",
    "\n",
    "A reasoning LLM handles tasks that require deeper analysis or step-by-step thought. It generates intermediate reasoning before arriving at a final answer, making it better suited for situations where careful logic or structured problem-solving is more important than speed or efficiency.\n",
    "\n",
    "This tutorial deploys a reasoning LLM using Ray Serve LLM.  \n",
    "\n",
    "---\n",
    "\n",
    "## Compare reasoning and non-reasoning models\n",
    "\n",
    "Reasoning models simulate step-by-step, structured thought processes to solve complex tasks like math, multi-hop QA, or code generation. In contrast, non-reasoning models provide fast, direct responses and focus on fluency or instruction following without explicit intermediate reasoning. The key distinction lies in whether the model attempts to \"think through\" the problem before answering.\n",
    "\n",
    "| **Model type**          | **Core behavior**                    | **Use case examples**                                    | **Limitation**                                        |\n",
    "| ----------------------- | ------------------------------------ | -------------------------------------------------------- | ----------------------------------------------------- |\n",
    "| **Reasoning model**     | Explicit multi-step thinking process | Math, coding, logic puzzles, multi-hop QA, CoT prompting | Slower response time, more tokens used.                |\n",
    "| **Non-reasoning model** | Direct answer generation             | Casual queries, short instructions, single-step answers  | May struggle with complex reasoning or interpretability. |\n",
    "\n",
    "Many reasoning-capable models structure their outputs with special markers such as `<think>` tags, or expose reasoning traces inside dedicated fields like `reasoning_content` in the OpenAI API response. Always check the model's documentation for how to structure and control thinking.\n",
    "\n",
    "**Note:** Reasoning LLMs often benefit from long context windows (32K up to +1M tokens), high token throughput, low-temperature decoding (greedy sampling), and strong instruction tuning or scratchpad-style reasoning.\n",
    "\n",
    "---\n",
    "\n",
    "### Choose when to use reasoning models\n",
    "\n",
    "Whether you should use a reasoning model depends on how much information your prompt already provides.\n",
    "\n",
    "If your input is clear and complete, a standard model is usually faster and more efficient. If your input is ambiguous or complex, a reasoning model works better because it can work through the problem step by step and fill in gaps through intermediate reasoning.\n",
    "\n",
    "---\n",
    "\n",
    "## Parse reasoning outputs\n",
    "\n",
    "Reasoning models often separate *reasoning* from the *final answer* using tags like `<think>...</think>`. Without a proper parser, this reasoning may end up in the `content` field instead of the dedicated `reasoning_content` field.\n",
    "\n",
    "To extract reasoning correctly, configure a `reasoning_parser` in your Ray Serve deployment. This tells vLLM how to isolate the model’s thought process from the rest of the output.\n",
    "**Note:** For example, *QwQ* uses the `deepseek-r1` parser. Other models may require different parsers. See the [vLLM docs](https://docs.vllm.ai/en/stable/features/reasoning_outputs.html#supported-models) or your model's documentation to find a supported parser, or [build your own](https://docs.vllm.ai/en/stable/features/reasoning_outputs.html#how-to-support-a-new-reasoning-model) if needed.\n",
    "\n",
    "```yaml\n",
    "applications:\n",
    "- name: reasoning-llm-app\n",
    "  ...\n",
    "  args:\n",
    "    llm_configs:\n",
    "      - model_loading_config:\n",
    "          model_id: my-qwq-32B\n",
    "          model_source: Qwen/QwQ-32B\n",
    "        ...\n",
    "        engine_kwargs:\n",
    "          ...\n",
    "          reasoning_parser: deepseek_r1 # <-- for QwQ models\n",
    "```\n",
    "\n",
    "See [Configure Ray Serve LLM](#configure-ray-serve-llm) for a complete example.\n",
    "\n",
    "**Example response**  \n",
    "When using a reasoning parser, the response is typically structured like this:\n",
    "\n",
    "```python\n",
    "ChatCompletionMessage(\n",
    "    content=\"The temperature is...\",\n",
    "    ...,\n",
    "    reasoning_content=\"Okay, the user is asking for the temperature today and tomorrow...\"\n",
    ")\n",
    "```\n",
    "And you can extract the content and reasoning as follows:\n",
    "```python\n",
    "response = client.chat.completions.create(\n",
    "  ...\n",
    ")\n",
    "\n",
    "print(f\"Content: {response.choices[0].message.content}\")\n",
    "print(f\"Reasoning: {response.choices[0].message.reasoning_content}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Configure Ray Serve LLM\n",
    "\n",
    "Set your Hugging Face token in the config file to access gated models.\n",
    "\n",
    "Ray Serve LLM provides multiple [Python APIs](https://docs.ray.io/en/latest/serve/api/index.html#llm-api) for defining your application. Use [`build_openai_app`](https://docs.ray.io/en/latest/serve/api/doc/ray.serve.llm.build_openai_app.html#ray.serve.llm.build_openai_app) to build a full application from your [`LLMConfig`](https://docs.ray.io/en/latest/serve/api/doc/ray.serve.llm.LLMConfig.html#ray.serve.llm.LLMConfig) object.\n",
    "\n",
    "Set `tensor_parallel_size=8` to distribute the model's weights among 8 GPUs in the node. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ae0ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# serve_qwq_32b.py\n",
    "from ray.serve.llm import LLMConfig, build_openai_app\n",
    "import os\n",
    "\n",
    "llm_config = LLMConfig(\n",
    "    model_loading_config=dict(\n",
    "        model_id=\"my-qwq-32B\",\n",
    "        model_source=\"Qwen/QwQ-32B\",\n",
    "    ),\n",
    "    accelerator_type=\"A100-40G\",\n",
    "    deployment_config=dict(\n",
    "        autoscaling_config=dict(\n",
    "            min_replicas=1,\n",
    "            max_replicas=2,\n",
    "        )\n",
    "    ),\n",
    "    ### Uncomment if your model is gated and needs your Hugging Face token to access it\n",
    "    # runtime_env=dict(env_vars={\"HF_TOKEN\": os.environ.get(\"HF_TOKEN\")}),\n",
    "    engine_kwargs=dict(\n",
    "        tensor_parallel_size=8, max_model_len=32768, reasoning_parser=\"deepseek_r1\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "app = build_openai_app({\"llm_configs\": [llm_config]})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d515e268",
   "metadata": {},
   "source": [
    "**Note:** Before moving to a production setup, migrate to a [Serve config file](https://docs.ray.io/en/latest/serve/production-guide/config.html) to make your deployment version-controlled, reproducible, and easier to maintain for CI/CD pipelines. See [Serving LLMs: production guide](https://docs.ray.io/en/latest/serve/llm/serving-llms.html#production-deployment) for an example.\n",
    "\n",
    "---\n",
    "\n",
    "## Deploy locally\n",
    "\n",
    "**Prerequisites**\n",
    "\n",
    "* Access to GPU compute.\n",
    "* (Optional) A **Hugging Face token** if using gated models like Meta’s Llama. Store it in `export HF_TOKEN=<YOUR-TOKEN-HERE>`\n",
    "\n",
    "**Note:** Depending on the organization, you can usually request access on the model's Hugging Face page. For example, Meta’s Llama models approval can take anywhere from a few hours to several weeks.\n",
    "\n",
    "**Dependencies:**  \n",
    "```bash\n",
    "pip install \"ray[serve,llm]\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Launch the service\n",
    "\n",
    "Follow the instructions at [Configure Ray Serve LLM](#configure-ray-serve-llm) to define your app in a Python module `serve_qwq_32b.py`.  \n",
    "\n",
    "In a terminal, run:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d6a307",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "serve run serve_qwq_32b:app --non-blocking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646f1272",
   "metadata": {},
   "source": [
    "Deployment typically takes a few minutes as the cluster is provisioned, the vLLM server starts, and the model is downloaded. \n",
    "\n",
    "---\n",
    "\n",
    "### Send requests\n",
    "\n",
    "Your endpoint is available locally at `http://localhost:8000` and you can use a placeholder authentication token for the OpenAI client, for example `\"FAKE_KEY\"`.\n",
    "\n",
    "Example curl:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a53387",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "curl -X POST http://localhost:8000/v1/chat/completions \\\n",
    "  -H \"Authorization: Bearer FAKE_KEY\" \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -d '{ \\\n",
    "        \"model\": \"my-qwq-32B\", \\\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": \"Pick three random words with 3 syllables each and count the number of R'\\''s in each of them\"}] \\\n",
    "      }'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942e675c",
   "metadata": {},
   "source": [
    "Example python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5005dde7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#client.py\n",
    "from urllib.parse import urljoin\n",
    "from openai import OpenAI\n",
    "\n",
    "api_key = \"FAKE_KEY\"\n",
    "base_url = \"http://localhost:8000\"\n",
    "\n",
    "client = OpenAI(base_url=urljoin(base_url, \"v1\"), api_key=api_key)\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"my-qwq-32B\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"What is the sum of all even numbers between 1 and 100?\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(f\"Reasoning: \\n{response.choices[0].message.reasoning_content}\\n\\n\")\n",
    "print(f\"Answer: \\n {response.choices[0].message.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e04db4e",
   "metadata": {},
   "source": [
    "If you configure a valid reasoning parser, the reasoning output should appear in the `reasoning_content` field of the response message. Otherwise, it may be included in the main `content` field, typically wrapped in `<think>...</think>` tags. See [Parse reasoning outputs](#parse-reasoning-outputs) for more information.\n",
    "\n",
    "---\n",
    "\n",
    "### Shutdown\n",
    "\n",
    "Shutdown your LLM service:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1f3edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "serve shutdown -y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc9e8eb",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## Deploy to production with Anyscale services\n",
    "\n",
    "For production, use Anyscale services to deploy your Ray Serve app on a dedicated cluster without code changes. Anyscale provides scalability, fault tolerance, and load balancing, ensuring resilience against node failures, high traffic, and rolling updates. See [Deploying a medium size LLM](https://docs.ray.io/en/latest/serve/tutorials/deployment-serve-llm/medium-size-llm/README.html#deploy-to-production-with-anyscale-services) for an example with a medium-sized model like the *QwQ-32&nbsp;B* used here.\n",
    "\n",
    "---\n",
    "\n",
    "## Stream reasoning content\n",
    "\n",
    "Reasoning models may take longer to begin generating the main content. You can stream their intermediate reasoning output in the same way as the main content.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02472f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#client_streaming.py\n",
    "from urllib.parse import urljoin\n",
    "from openai import OpenAI\n",
    "\n",
    "api_key = <YOUR-TOKEN-HERE>\n",
    "base_url = <YOUR-ENDPOINT-HERE>\n",
    "\n",
    "client = OpenAI(base_url=urljoin(base_url, \"v1\"), api_key=api_key)\n",
    "\n",
    "# Example: Complex query with thinking process\n",
    "response = client.chat.completions.create(\n",
    "    model=\"my-qwq-32B\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"I need to plan a trip to Paris from Seattle. Can you help me research flight costs, create an itinerary for 3 days, and suggest restaurants based on my dietary restrictions (vegetarian)?\"}\n",
    "    ],\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "# Stream\n",
    "for chunk in response:\n",
    "    # Stream reasoning content\n",
    "    if hasattr(chunk.choices[0].delta, \"reasoning_content\"):\n",
    "        data_reasoning = chunk.choices[0].delta.reasoning_content\n",
    "        if data_reasoning:\n",
    "            print(data_reasoning, end=\"\", flush=True)\n",
    "    # Later, stream the final answer\n",
    "    if hasattr(chunk.choices[0].delta, \"content\"):\n",
    "        data_content = chunk.choices[0].delta.content\n",
    "        if data_content:\n",
    "            print(data_content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70455ea2",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this tutorial, you deployed a reasoning LLM with Ray Serve LLM, from development to production. You learned how to configure Ray Serve LLM with the right reasoning parser, deploy your service on your Ray cluster, send requests, and parse reasoning outputs in the response."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "repo_ray_docs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
