{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc12c0d2",
   "metadata": {},
   "source": [
    "# Quickstarts for LLM serving\n",
    "\n",
    "These guides provide a fast path to serving LLMs using Ray Serve on Anyscale, with focused tutorials for different deployment scales, from single-GPU setups to multi-node clusters.\n",
    "\n",
    "Each tutorial includes development and production setups, tips for configuring your cluster, and guidance on monitoring and scaling with Ray Serve.\n",
    "\n",
    "## Tutorial categories\n",
    "\n",
    "**[Small-sized LLM deployment](https://docs.ray.io/en/latest/ray-overview/examples/deployment-serve-llm/small-size-llm/README.html)**  \n",
    "Deploy small-sized models on a single GPU, such as Llama 3 8&nbsp;B, Mistral 7&nbsp;B, or Phi-2.  \n",
    "\n",
    "---\n",
    "\n",
    "**[Medium-sized LLM deployment](https://docs.ray.io/en/latest/ray-overview/examples/deployment-serve-llm/medium-size-llm/README.html)**  \n",
    "Deploy medium-sized models using tensor parallelism across 4â€”8 GPUs on a single node, such as Llama 3 70&nbsp;B, Qwen 14&nbsp;B, Mixtral 8x7&nbsp;B.  \n",
    "\n",
    "---\n",
    "\n",
    "**[Large-sized LLM deployment](https://docs.ray.io/en/latest/ray-overview/examples/deployment-serve-llm/large-size-llm/README.html)**  \n",
    "Deploy massive models using pipeline parallelism across a multi-node cluster, such as Deepseek-R1 or Llama-Nemotron-253&nbsp;B.  \n",
    "\n",
    "---\n",
    "\n",
    "**[Vision LLM deployment](https://docs.ray.io/en/latest/ray-overview/examples/deployment-serve-llm/vision-llm/README.html)**  \n",
    "Deploy models with image and text input such as Qwen 2.5-VL-7&nbsp;B-Instruct, MiniGPT-4, or Pixtral-12&nbsp;B.  \n",
    "\n",
    "---\n",
    "\n",
    "**[Reasoning LLM deployment](https://docs.ray.io/en/latest/ray-overview/examples/deployment-serve-llm/reasoning-llm/README.html)**  \n",
    "Deploy models with reasoning capabilities designed for long-context tasks, coding, or tool use, such as QwQ-32&nbsp;B.  \n",
    "\n",
    "---\n",
    "\n",
    "**[Hybrid thinking LLM deployment](https://docs.ray.io/en/latest/ray-overview/examples/deployment-serve-llm/hybrid-reasoning-llm/README.html)**  \n",
    "Deploy models that can switch between reasoning and non-reasoning modes for flexible usage, such as Qwen-3."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "myst": {
   "front_matter": {
    "orphan": true
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
