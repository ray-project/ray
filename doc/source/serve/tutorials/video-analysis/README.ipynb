{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video analysis inference pipeline with Ray Serve\n",
    "\n",
    "This notebook demonstrates how to build a production-grade video analysis pipeline with [Ray Serve](https://docs.ray.io/en/latest/serve/). The pipeline processes videos from S3 and extracts:\n",
    "\n",
    "- **Tags**: [Zero-shot classification](https://huggingface.co/tasks/zero-shot-image-classification) labels (such as \"kitchen\", \"office\", \"park\")\n",
    "- **Captions**: Retrieval-based descriptions matching the video content\n",
    "- **Scene changes**: Detected transitions using [exponential moving average (EMA)](https://en.wikipedia.org/wiki/Exponential_smoothing) analysis\n",
    "\n",
    "The system uses [**SigLIP**](https://huggingface.co/google/siglip-so400m-patch14-384) (`google/siglip-so400m-patch14-384`) as the vision-language backbone. SigLIP provides a unified embedding space for both images and text, enabling zero-shot classification and retrieval without task-specific fine-tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture\n",
    "\n",
    "The pipeline splits work across three [Ray Serve deployments](https://docs.ray.io/en/latest/serve/key-concepts.html#deployment), each optimized for its workload:\n",
    "\n",
    "```\n",
    "                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "                         â”Œâ”€â”€â”€â–¶â”‚  VideoEncoder       â”‚\n",
    "                         â”‚    â”‚      (GPU)          â”‚\n",
    "                         â”‚    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚    â”‚ â€¢ SigLIP embedding  â”‚\n",
    "â”‚  VideoAnalyzer      â”‚â”€â”€â”¤    â”‚ â€¢ 16 frames/chunk   â”‚\n",
    "â”‚     (Ingress)       â”‚  â”‚    â”‚ â€¢ L2 normalization  â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "â”‚ â€¢ S3 download       â”‚  â”‚          num_gpus=1\n",
    "â”‚ â€¢ FFmpeg chunking   â”‚  â”‚\n",
    "â”‚ â€¢ Request routing   â”‚  â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â–¶â”‚  MultiDecoder       â”‚\n",
    "      num_cpus=6              â”‚      (CPU)          â”‚\n",
    "                              â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "                              â”‚ â€¢ Tag classificationâ”‚\n",
    "                              â”‚ â€¢ Caption retrieval â”‚\n",
    "                              â”‚ â€¢ Scene detection   â”‚\n",
    "                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                                    num_cpus=1\n",
    "```\n",
    "\n",
    "**Request lifecycle:**\n",
    "1. `VideoAnalyzer` receives HTTP request with S3 video URI\n",
    "2. Downloads video from S3, splits into fixed-duration chunks using FFmpeg\n",
    "3. Sends all chunks to `VideoEncoder` concurrently\n",
    "4. Encoder returns embedding references (stored in Ray object store)\n",
    "5. `VideoAnalyzer` sends embeddings to `MultiDecoder` serially (for EMA state continuity)\n",
    "6. Aggregates results and returns tags, captions, and scene changes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Ray Serve?\n",
    "\n",
    "This pipeline has three distinct workloads: a GPU-bound encoder running SigLIP, a CPU-bound decoder doing cosine similarity, and a CPU-heavy ingress running FFmpeg. Traditional serving frameworks force you to bundle these into a single container with fixed resources, wasting GPU when the decoder runs or starving FFmpeg when the encoder dominates.\n",
    "\n",
    "Ray Serve solves this with **heterogeneous [resource allocation](https://docs.ray.io/en/latest/serve/configure-serve-deployment.html#configure-ray-serve-deployments) per deployment**. The encoder requests 1 GPU, the decoder requests 1 CPU, and the ingress requests 6 CPUs for parallel FFmpeg. Each deployment [scales independently](https://docs.ray.io/en/latest/serve/autoscaling-guide.html) based on its own queue depthâ€”GPU replicas scale with encoding demand while CPU replicas scale separately with decoding demand. The load test demonstrates this: throughput scales near-linearly from 2.4 to 67.5 requests/second as the system provisions replicas to match load.\n",
    "\n",
    "The pipeline also benefits from **zero-copy data transfer**. The ingress passes encoder results directly to the decoder as unawaited [DeploymentResponse](https://docs.ray.io/en/latest/serve/api/doc/ray.serve.handle.DeploymentResponse.html) references rather than serialized data. Ray stores the embeddings in its [object store](https://docs.ray.io/en/latest/ray-core/objects.html), and the decoder retrieves them directly without routing through the ingress. When encoder and decoder land on the same node, this transfer is zero-copy.\n",
    "\n",
    "**Request pipelining** keeps the GPU saturated. By allowing two concurrent requests per encoder replica via `max_ongoing_requests`, one request prepares data on CPU while another computes on GPU. This achieves 100% GPU utilization without batching, which would add latency from waiting for requests to accumulate.\n",
    "\n",
    "Finally, **[deployment composition](https://docs.ray.io/en/latest/serve/model_composition.html)** lets you define the encoder, decoder, and ingress as separate classes, then wire them together with `.bind()`. Ray Serve handles deployment ordering, health checks, and request routing. The ingress maintains explicit state (EMA for scene detection) across chunks, which works correctly even when autoscaling routes requests to different decoder replicasâ€”no sticky sessions required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prerequisites\n",
    "\n",
    "Before running this notebook, ensure you have:\n",
    "\n",
    "| Requirement | Purpose |\n",
    "|-------------|---------|\n",
    "| **Pexels API key** | Download sample video (free at https://www.pexels.com/api/) |\n",
    "| **S3 bucket** | Store videos and text embeddings |\n",
    "| **AWS credentials** | Read/write access to your S3 bucket |\n",
    "| **ffmpeg** | Video processing and frame extraction |\n",
    "| **GPU** | Run SigLIP model for encoding (1 GPU minimum) |\n",
    "\n",
    "Set these environment variables before running:\n",
    "\n",
    "```bash\n",
    "export PEXELS_API_KEY=\"your-pexels-api-key\"\n",
    "export S3_BUCKET=\"your-bucket-name\"\n",
    "export AWS_ACCESS_KEY_ID=\"...\"\n",
    "export AWS_SECRET_ACCESS_KEY=\"...\"\n",
    "```\n",
    "\n",
    "> **Note on GPU type**: The benchmarks, design choices, and hyperparameters in this notebook were tuned for **NVIDIA L4 GPUs**. Different GPU types (A10G, T4, A100, etc.) have different memory bandwidth, compute throughput, and batch characteristics. You may need to adjust `max_ongoing_requests`, chunk sizes, and concurrency limits for optimal performance on other hardware.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "PEXELS_API_KEY = os.environ.get(\"PEXELS_API_KEY\")  # Or set directly: \"your-api-key\"\n",
    "S3_BUCKET = os.environ.get(\"S3_BUCKET\")  # Or set directly: \"your-bucket\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download sample video\n",
    "\n",
    "Before running the pipeline, we need a sample video in S3. This section downloads a video from Pexels, normalizes it, and uploads to S3.\n",
    "\n",
    "**Why normalize videos?** We re-encode all videos to a consistent format:\n",
    "- **384Ã—384 resolution**: Matches SigLIP's input size exactly, eliminating resize during inference\n",
    "- **30 fps**: Standardizes frame timing for consistent chunk boundaries\n",
    "- **[H.264](https://en.wikipedia.org/wiki/Advanced_Video_Coding) codec (libx264)**: Fast seekingâ€”FFmpeg can jump directly to any timestamp without decoding preceding frames. Some source codecs (VP9, HEVC) require sequential decoding, adding latency for chunk extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "try:\n",
    "    asyncio.get_running_loop()\n",
    "except RuntimeError:\n",
    "    loop = asyncio.new_event_loop()\n",
    "    asyncio.set_event_loop(loop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… S3 bucket 'anyscale-example-video-analysis-test-bucket' accessible\n",
      "âœ… Found existing manifest with 1 videos in S3\n",
      "   Skipping Pexels API download\n",
      "\n",
      "Sample video ready: s3://anyscale-example-video-analysis-test-bucket/anyscale-example/stock-videos/kitchen_cooking_35395675_00.mp4\n"
     ]
    }
   ],
   "source": [
    "from scripts.download_stock_videos import download_sample_videos\n",
    "\n",
    "# Download sample videos (checks for existing manifest first, skips Pexels API if found)\n",
    "S3_PREFIX = \"anyscale-example/stock-videos/\"\n",
    "video_paths = asyncio.get_event_loop().run_until_complete(download_sample_videos(\n",
    "    api_key=PEXELS_API_KEY,\n",
    "    bucket=S3_BUCKET,\n",
    "    total=1,  # Just need one sample video\n",
    "    s3_prefix=S3_PREFIX,\n",
    "    overwrite=False\n",
    "))\n",
    "\n",
    "if not video_paths:\n",
    "    raise RuntimeError(\"No videos downloaded\")\n",
    "\n",
    "SAMPLE_VIDEO_URI = video_paths[0]\n",
    "print(f\"\\nSample video ready: {SAMPLE_VIDEO_URI}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate embeddings for text bank\n",
    "\n",
    "The decoder matches video embeddings against precomputed **text embeddings** for tags and descriptions. We define the text banks here and use a [Ray task](https://docs.ray.io/en/latest/ray-core/tasks.html) to compute embeddings on GPU and upload to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tags: ['kitchen', 'living room', 'office', 'meeting room', 'classroom', 'restaurant', 'cafe', 'grocery store', 'gym', 'warehouse', 'parking lot', 'city street', 'park', 'shopping mall', 'beach', 'sports field', 'hallway', 'lobby', 'bathroom', 'bedroom']\n",
      "Descriptions: ['A person cooking in a kitchen', 'Someone preparing food on a counter', 'A chef working in a professional kitchen', 'People eating at a dining table', 'A group having a meal together', 'A person working at a desk', 'Someone typing on a laptop', 'A business meeting in progress', 'A presentation being given', 'People collaborating in an office', 'A teacher lecturing in a classroom', 'Students sitting at desks', 'A person giving a speech', 'Someone writing on a whiteboard', 'A customer shopping in a store', 'People browsing products on shelves', 'A cashier at a checkout counter', 'A person exercising at a gym', 'Someone lifting weights', 'A person running on a treadmill', 'People walking on a city sidewalk', 'Pedestrians crossing a street', 'Traffic moving through an intersection', 'Cars driving on a road', 'A vehicle parked in a lot', 'People walking through a park', 'Someone jogging outdoors', 'A group having a conversation', 'Two people talking face to face', 'A person on a phone call', 'Someone reading a book', 'A person watching television', 'People waiting in line', 'A crowded public space', 'An empty hallway or corridor', 'A person entering a building', 'Someone opening a door', 'A delivery being made', 'A person carrying boxes', 'Workers in a warehouse']\n"
     ]
    }
   ],
   "source": [
    "from textbanks import TAGS, DESCRIPTIONS\n",
    "\n",
    "print(f\"Tags: {TAGS}\")\n",
    "print(f\"Descriptions: {DESCRIPTIONS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-06 08:27:08,101\tINFO worker.py:1821 -- Connecting to existing Ray cluster at address: 10.0.45.10:6379...\n",
      "2026-01-06 08:27:08,114\tINFO worker.py:1998 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32mhttps://session-lqu9h8iu3cpgv59j74p498djis.i.anyscaleuserdata-staging.com \u001b[39m\u001b[22m\n",
      "2026-01-06 08:27:08,160\tINFO packaging.py:463 -- Pushing file package 'gcs://_ray_pkg_56a868c6743600cdc03ad7fedef93ccaf9011e05.zip' (10.59MiB) to Ray cluster...\n",
      "2026-01-06 08:27:08,200\tINFO packaging.py:476 -- Successfully pushed file package 'gcs://_ray_pkg_56a868c6743600cdc03ad7fedef93ccaf9011e05.zip'.\n",
      "/home/ray/anaconda3/lib/python3.12/site-packages/ray/_private/worker.py:2046: FutureWarning: Tip: In future versions of Ray, Ray will no longer override accelerator visible devices env var if num_gpus=0 or num_gpus=None (default). To enable this behavior and turn off this error message, set RAY_ACCEL_ENV_VAR_OVERRIDE_ON_ZERO=0\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(generate_embeddings_task pid=6626, ip=10.0.222.50)\u001b[0m ============================================================\n",
      "\u001b[36m(generate_embeddings_task pid=6626, ip=10.0.222.50)\u001b[0m Starting text embedding generation\n",
      "\u001b[36m(generate_embeddings_task pid=6626, ip=10.0.222.50)\u001b[0m ============================================================\n",
      "\u001b[36m(generate_embeddings_task pid=6626, ip=10.0.222.50)\u001b[0m \n",
      "\u001b[36m(generate_embeddings_task pid=6626, ip=10.0.222.50)\u001b[0m ðŸ“š Loading text banks...\n",
      "\u001b[36m(generate_embeddings_task pid=6626, ip=10.0.222.50)\u001b[0m    Tags: 20\n",
      "\u001b[36m(generate_embeddings_task pid=6626, ip=10.0.222.50)\u001b[0m    Descriptions: 40\n",
      "\u001b[36m(generate_embeddings_task pid=6626, ip=10.0.222.50)\u001b[0m \n",
      "\u001b[36m(generate_embeddings_task pid=6626, ip=10.0.222.50)\u001b[0m ðŸ¤– Loading SigLIP model: google/siglip-so400m-patch14-384\n",
      "\u001b[36m(generate_embeddings_task pid=6626, ip=10.0.222.50)\u001b[0m    Device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(generate_embeddings_task pid=6626, ip=10.0.222.50)\u001b[0m Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(generate_embeddings_task pid=6626, ip=10.0.222.50)\u001b[0m    Model loaded in 2.6s\n",
      "\u001b[36m(generate_embeddings_task pid=6626, ip=10.0.222.50)\u001b[0m \n",
      "\u001b[36m(generate_embeddings_task pid=6626, ip=10.0.222.50)\u001b[0m ðŸ·ï¸  Generating tag embeddings...\n",
      "\u001b[36m(generate_embeddings_task pid=6626, ip=10.0.222.50)\u001b[0m    Shape: (20, 1152)\n",
      "\u001b[36m(generate_embeddings_task pid=6626, ip=10.0.222.50)\u001b[0m    Time: 0.21s\n",
      "\u001b[36m(generate_embeddings_task pid=6626, ip=10.0.222.50)\u001b[0m \n",
      "\u001b[36m(generate_embeddings_task pid=6626, ip=10.0.222.50)\u001b[0m ðŸ“ Generating description embeddings...\n",
      "\u001b[36m(generate_embeddings_task pid=6626, ip=10.0.222.50)\u001b[0m    Shape: (40, 1152)\n",
      "\u001b[36m(generate_embeddings_task pid=6626, ip=10.0.222.50)\u001b[0m    Time: 0.25s\n",
      "\u001b[36m(generate_embeddings_task pid=6626, ip=10.0.222.50)\u001b[0m \n",
      "\u001b[36m(generate_embeddings_task pid=6626, ip=10.0.222.50)\u001b[0m â˜ï¸  Uploading to S3 bucket: anyscale-example-video-analysis-test-bucket\n",
      "\n",
      "Text embeddings ready:\n",
      "  Tags: s3://anyscale-example-video-analysis-test-bucket/anyscale-example/embeddings/tag_embeddings.npz\n",
      "  Descriptions: s3://anyscale-example-video-analysis-test-bucket/anyscale-example/embeddings/description_embeddings.npz\n"
     ]
    }
   ],
   "source": [
    "import ray\n",
    "\n",
    "from jobs.generate_text_embeddings import generate_embeddings_task\n",
    "\n",
    "S3_EMBEDDINGS_PREFIX = \"anyscale-example/embeddings/\"\n",
    "\n",
    "# Run the Ray task (uses TAGS and DESCRIPTIONS from textbanks module)\n",
    "# Note: runtime_env ships local modules to worker nodes (job working_dir only applies to driver)\n",
    "ray.init(runtime_env={\"working_dir\": \".\"}, ignore_reinit_error=True)\n",
    "result = ray.get(generate_embeddings_task.remote(S3_BUCKET, S3_EMBEDDINGS_PREFIX))\n",
    "\n",
    "print(f\"\\nText embeddings ready:\")\n",
    "print(f\"  Tags: {result['tag_embeddings']['s3_uri']}\")\n",
    "print(f\"  Descriptions: {result['description_embeddings']['s3_uri']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Build the Ray Serve application\n",
    "\n",
    "This section walks through building the video analysis pipeline step by step, introducing Ray Serve concepts as we go.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU encoder\n",
    "\n",
    "The `VideoEncoder` deployment runs on GPU and converts video frames to embeddings using SigLIP. Key configuration:\n",
    "\n",
    "- `num_gpus=1`: Each replica requires a dedicated GPU\n",
    "- `max_ongoing_requests=2`: Allows pipeliningâ€”while one request computes on GPU, another prepares data on CPU\n",
    "\n",
    "**Why no request batching?** A single chunk (16 frames @ 384Ã—384) already saturates GPU compute. Batching multiple requests would require holding them until a batch forms, adding latency without throughput gain. Instead, we use `max_ongoing_requests=2` to pipeline preparation and computation.\n",
    "\n",
    "![GPU Utilization](assets/gpu_utilization.png)\n",
    "\n",
    "**Why [`asyncio.to_thread`](https://docs.python.org/3/library/asyncio-task.html#asyncio.to_thread)?** Ray Serve deployments run in an async event loop. The `encode_frames` method is CPU/GPU-bound (PyTorch inference), which would block the event loop and prevent concurrent request handling. Wrapping it in `asyncio.to_thread` offloads the blocking work to a thread pool, keeping the event loop free to accept new requests.\n",
    "\n",
    "**Why pass [`DeploymentResponse`](https://docs.ray.io/en/latest/serve/api/doc/ray.serve.handle.DeploymentResponse.html) to decoder?** Instead of awaiting the encoder result in `VideoAnalyzer` and passing raw data to the decoder, we pass the unawaited `DeploymentResponse` directly. Ray Serve automatically resolves this reference when the decoder needs it, storing the embeddings in the [object store](https://docs.ray.io/en/latest/ray-core/objects.html#objects-in-ray). This avoids an unnecessary serialize/deserialize round-trip through `VideoAnalyzer`â€”the decoder retrieves data directly from the object store, enabling zero-copy transfer if encoder and decoder are on the same node.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(generate_embeddings_task pid=6626, ip=10.0.222.50)\u001b[0m    Tags: s3://anyscale-example-video-analysis-test-bucket/anyscale-example/embeddings/tag_embeddings.npz\n",
      "\u001b[36m(generate_embeddings_task pid=6626, ip=10.0.222.50)\u001b[0m    Descriptions: s3://anyscale-example-video-analysis-test-bucket/anyscale-example/embeddings/description_embeddings.npz\n",
      "\u001b[36m(generate_embeddings_task pid=6626, ip=10.0.222.50)\u001b[0m \n",
      "\u001b[36m(generate_embeddings_task pid=6626, ip=10.0.222.50)\u001b[0m âœ… Done!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL_NAME: google/siglip-so400m-patch14-384\n"
     ]
    }
   ],
   "source": [
    "from constants import MODEL_NAME\n",
    "\n",
    "print(f\"MODEL_NAME: {MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@serve.deployment(\n",
      "    num_replicas=\"auto\",\n",
      "    ray_actor_options={\"num_gpus\": 1, \"num_cpus\": 2},\n",
      "    # GPU utilization is at 100% when this is set to 2. with L4\n",
      "    # aka number on ongoing chunks that can be processed at once.\n",
      "    max_ongoing_requests=2,\n",
      "    autoscaling_config={\n",
      "        \"min_replicas\": 1,\n",
      "        \"max_replicas\": 10,\n",
      "        \"target_num_ongoing_requests\": 2,\n",
      "    },\n",
      ")\n",
      "class VideoEncoder:\n",
      "    \"\"\"\n",
      "    Encodes video frames into embeddings using SigLIP.\n",
      "    \n",
      "    Returns both per-frame embeddings and pooled embedding.\n",
      "    \"\"\"\n",
      "    \n",
      "    def __init__(self):\n",
      "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "        print(f\"VideoEncoder initializing on {self.device}\")\n",
      "        \n",
      "        # Load SigLIP model and processor\n",
      "        self.processor = AutoProcessor.from_pretrained(MODEL_NAME)\n",
      "        self.model = AutoModel.from_pretrained(MODEL_NAME).to(self.device)\n",
      "        self.model.eval()\n",
      "        \n",
      "        # Get embedding dimension\n",
      "        self.embedding_dim = self.model.config.vision_config.hidden_size\n",
      "        \n",
      "        print(f\"VideoEncoder ready (embedding_dim={self.embedding_dim})\")\n",
      "    \n",
      "    def encode_frames(self, frames: np.ndarray) -> np.ndarray:\n",
      "        \"\"\"\n",
      "        Encode frames and return per-frame embeddings.\n",
      "        \n",
      "        Args:\n",
      "            frames: np.ndarray of shape (T, H, W, 3) uint8 RGB\n",
      "        \n",
      "        Returns:\n",
      "            np.ndarray of shape (T, D) float32, L2-normalized per-frame embeddings\n",
      "        \"\"\"\n",
      "        \n",
      "        # Convert to PIL images\n",
      "        pil_images = frames_to_pil_list(frames)\n",
      "        \n",
      "        # Process images\n",
      "        inputs = self.processor(images=pil_images, return_tensors=\"pt\").to(self.device)\n",
      "        # inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
      "\n",
      "        start = torch.cuda.Event(enable_timing=True)\n",
      "        end = torch.cuda.Event(enable_timing=True)\n",
      "\n",
      "        # Get embeddings\n",
      "        with torch.no_grad():\n",
      "            with torch.amp.autocast(device_type=self.device, enabled=self.device == \"cuda\"):\n",
      "                outputs = self.model.get_image_features(**inputs)\n",
      "\n",
      "                # L2 normalize on GPU (faster than CPU numpy)\n",
      "                frame_embeddings = torch.nn.functional.normalize(outputs, p=2, dim=1)\n",
      "        \n",
      "        # Move to CPU and convert to numpy\n",
      "        result = frame_embeddings.cpu().numpy().astype(np.float32)\n",
      "        return result\n",
      "    \n",
      "    async def encode_unbatched(self, frames: np.ndarray) -> dict:\n",
      "        \"\"\"\n",
      "        Unbatched entry point - processes single request directly.\n",
      "        \n",
      "        Args:\n",
      "            frames: np.ndarray of shape (T, H, W, 3)\n",
      "        \n",
      "        Returns:\n",
      "            dict with 'frame_embeddings' and 'embedding_dim'\n",
      "        \"\"\"\n",
      "        print(f\"Unbatched: {frames.shape[0]} frames\")\n",
      "        \n",
      "        frame_embeddings = await asyncio.to_thread(self.encode_frames, frames)\n",
      "        \n",
      "        return {\n",
      "            \"frame_embeddings\": frame_embeddings,\n",
      "            \"embedding_dim\": self.embedding_dim,\n",
      "        }\n",
      "    \n",
      "    @serve.batch(max_batch_size=2, batch_wait_timeout_s=0.1)\n",
      "    async def encode_batched(self, frames_batch: List[np.ndarray]) -> List[dict]:\n",
      "        \"\"\"\n",
      "        Batched entry point - collects multiple requests into single GPU call.\n",
      "        \n",
      "        Args:\n",
      "            frames_batch: List of frame arrays, each of shape (T, H, W, 3)\n",
      "        \n",
      "        Returns:\n",
      "            List of dicts, each with 'frame_embeddings' and 'embedding_dim'\n",
      "        \"\"\"\n",
      "        frame_counts = [f.shape[0] for f in frames_batch]\n",
      "        total_frames = sum(frame_counts)\n",
      "        \n",
      "        print(f\"Batched: {len(frames_batch)} requests ({total_frames} total frames)\")\n",
      "        \n",
      "        # Concatenate all frames into single batch\n",
      "        all_frames = np.concatenate(frames_batch, axis=0)\n",
      "        \n",
      "        # Single forward pass for all frames\n",
      "        all_embeddings = await asyncio.to_thread(self.encode_frames, all_frames)\n",
      "        \n",
      "        # Split results back per request\n",
      "        results = []\n",
      "        offset = 0\n",
      "        for n_frames in frame_counts:\n",
      "            chunk_embeddings = all_embeddings[offset:offset + n_frames]\n",
      "            results.append({\n",
      "                \"frame_embeddings\": chunk_embeddings,\n",
      "                \"embedding_dim\": self.embedding_dim,\n",
      "            })\n",
      "            offset += n_frames\n",
      "        \n",
      "        return results\n",
      "    \n",
      "    async def __call__(self, frames: np.ndarray, use_batching: bool = False) -> dict:\n",
      "        \"\"\"\n",
      "        Main entry point. Set use_batching=False for direct comparison.\n",
      "        \"\"\"\n",
      "        if use_batching:\n",
      "            return await self.encode_batched(frames)\n",
      "        else:\n",
      "            return await self.encode_unbatched(frames)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from deployments.encoder import VideoEncoder\n",
    "import inspect\n",
    "\n",
    "print(inspect.getsource(VideoEncoder.func_or_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CPU decoder\n",
    "\n",
    "The `MultiDecoder` deployment runs on CPU and performs three tasks:\n",
    "\n",
    "1. **Tag classification**: [Cosine similarity](https://en.wikipedia.org/wiki/Cosine_similarity) between video embedding and precomputed tag embeddings\n",
    "2. **Caption retrieval**: Find the best-matching description from a text bank\n",
    "3. **Scene detection**: EMA-based anomaly detection comparing each frame to recent history\n",
    "\n",
    "The decoder loads precomputed text embeddings from S3 at startup.\n",
    "\n",
    "**Why separate GPU/CPU deployments?** The encoder needs GPU for neural network inference; the decoder only does numpy dot products. Separating them allows independent scalingâ€”encoders are limited by GPU count, decoders scale cheaply with CPU cores. This avoids tying expensive GPU resources to lightweight CPU work.\n",
    "\n",
    "**Why EMA for scene detection?** Exponential Moving Average reuses existing SigLIP embeddings without an extra model. The algorithm computes `score = 1 - cosine(frame, EMA)` where EMA updates as `ema = 0.9*ema + 0.1*frame`. A simple threshold (`score > 0.15`) detects abrupt scene changes while smoothing noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@serve.deployment(\n",
      "    num_replicas=\"auto\",\n",
      "    ray_actor_options={\"num_cpus\": 1},\n",
      "    max_ongoing_requests=4, # can be set higher than 4, but since the encoder is limited to 4, we need to keep it at 4.\n",
      "    autoscaling_config={\n",
      "        \"min_replicas\": 1,\n",
      "        \"max_replicas\": 10,\n",
      "        \"target_num_ongoing_requests\": 2,\n",
      "    },\n",
      ")\n",
      "class MultiDecoder:\n",
      "    \"\"\"\n",
      "    Decodes video embeddings into tags, captions, and scene changes.\n",
      "    \n",
      "    Uses precomputed text embeddings loaded from S3.\n",
      "    This deployment is stateless - EMA state for scene detection is passed\n",
      "    in and returned with each call, allowing the caller to maintain state\n",
      "    continuity across multiple replicas.\n",
      "    \"\"\"\n",
      "    \n",
      "    async def __init__(self, bucket: str, s3_prefix: str = S3_EMBEDDINGS_PREFIX):\n",
      "        \"\"\"Initialize decoder with text embeddings from S3.\"\"\"\n",
      "        self.bucket = bucket\n",
      "        self.ema_alpha = EMA_ALPHA\n",
      "        self.scene_threshold = SCENE_CHANGE_THRESHOLD\n",
      "        self.s3_prefix = s3_prefix\n",
      "        logger.info(f\"MultiDecoder initializing (bucket={self.bucket}, ema_alpha={self.ema_alpha}, threshold={self.scene_threshold})\")\n",
      "        \n",
      "        await self._load_embeddings()\n",
      "        \n",
      "        logger.info(f\"MultiDecoder ready (tags={len(self.tag_texts)}, descriptions={len(self.desc_texts)})\")\n",
      "    \n",
      "    async def _load_embeddings(self):\n",
      "        \"\"\"Load precomputed text embeddings from S3.\"\"\"\n",
      "        session = aioboto3.Session(region_name=get_s3_region(self.bucket))\n",
      "        \n",
      "        async with session.client(\"s3\") as s3:\n",
      "            # Load tag embeddings\n",
      "            tag_key = f\"{self.s3_prefix}tag_embeddings.npz\"\n",
      "            response = await s3.get_object(Bucket=self.bucket, Key=tag_key)\n",
      "            tag_data = await response[\"Body\"].read()\n",
      "            tag_npz = np.load(io.BytesIO(tag_data), allow_pickle=True)\n",
      "            self.tag_embeddings = tag_npz[\"embeddings\"]\n",
      "            self.tag_texts = tag_npz[\"texts\"].tolist()\n",
      "            \n",
      "            # Load description embeddings\n",
      "            desc_key = f\"{self.s3_prefix}description_embeddings.npz\"\n",
      "            response = await s3.get_object(Bucket=self.bucket, Key=desc_key)\n",
      "            desc_data = await response[\"Body\"].read()\n",
      "            desc_npz = np.load(io.BytesIO(desc_data), allow_pickle=True)\n",
      "            self.desc_embeddings = desc_npz[\"embeddings\"]\n",
      "            self.desc_texts = desc_npz[\"texts\"].tolist()\n",
      "    \n",
      "    def _cosine_similarity(self, embedding: np.ndarray, bank: np.ndarray) -> np.ndarray:\n",
      "        \"\"\"Compute cosine similarity between embedding and all vectors in bank.\"\"\"\n",
      "        return bank @ embedding\n",
      "    \n",
      "    def _get_top_tags(self, embedding: np.ndarray, top_k: int = 5) -> list[dict]:\n",
      "        \"\"\"Get top-k matching tags with scores.\"\"\"\n",
      "        scores = self._cosine_similarity(embedding, self.tag_embeddings)\n",
      "        top_indices = np.argsort(scores)[::-1][:top_k]\n",
      "        return [\n",
      "            {\"text\": self.tag_texts[i], \"score\": float(scores[i])}\n",
      "            for i in top_indices\n",
      "        ]\n",
      "    \n",
      "    def _get_retrieval_caption(self, embedding: np.ndarray) -> dict:\n",
      "        \"\"\"Get best matching description.\"\"\"\n",
      "        scores = self._cosine_similarity(embedding, self.desc_embeddings)\n",
      "        best_idx = np.argmax(scores)\n",
      "        return {\n",
      "            \"text\": self.desc_texts[best_idx],\n",
      "            \"score\": float(scores[best_idx]),\n",
      "        }\n",
      "    \n",
      "    def _detect_scene_changes(\n",
      "        self,\n",
      "        frame_embeddings: np.ndarray,\n",
      "        chunk_index: int,\n",
      "        chunk_start_time: float,\n",
      "        chunk_duration: float,\n",
      "        ema_state: np.ndarray | None = None,\n",
      "    ) -> tuple[list[dict], np.ndarray]:\n",
      "        \"\"\"\n",
      "        Detect scene changes using EMA-based scoring.\n",
      "        \n",
      "        score_t = 1 - cosine(E_t, ema_t)\n",
      "        ema_t = Î± * ema_{t-1} + (1-Î±) * E_t\n",
      "        \n",
      "        Args:\n",
      "            frame_embeddings: (T, D) normalized embeddings\n",
      "            chunk_index: Index of this chunk in the video\n",
      "            chunk_start_time: Start time of chunk in video (seconds)\n",
      "            chunk_duration: Duration of chunk (seconds)\n",
      "            ema_state: EMA state from previous chunk, or None for first chunk\n",
      "        \n",
      "        Returns:\n",
      "            Tuple of (scene_changes list, updated ema_state)\n",
      "        \"\"\"\n",
      "        num_frames = len(frame_embeddings)\n",
      "        if num_frames == 0:\n",
      "            # Return empty changes and unchanged state (or zeros if no state)\n",
      "            return [], ema_state if ema_state is not None else np.zeros(0)\n",
      "        \n",
      "        # Initialize EMA from first frame if no prior state\n",
      "        ema = ema_state.copy() if ema_state is not None else frame_embeddings[0].copy()\n",
      "        scene_changes = []\n",
      "        \n",
      "        for frame_idx, embedding in enumerate(frame_embeddings):\n",
      "            # Compute score: how different is current frame from recent history\n",
      "            similarity = float(np.dot(embedding, ema))\n",
      "            score = max(0.0, 1.0 - similarity)\n",
      "            \n",
      "            # Detect scene change if score exceeds threshold\n",
      "            if score >= self.scene_threshold:\n",
      "                # Calculate timestamp within video\n",
      "                frame_offset = (frame_idx / max(1, num_frames - 1)) * chunk_duration\n",
      "                timestamp = chunk_start_time + frame_offset\n",
      "                \n",
      "                scene_changes.append({\n",
      "                    \"timestamp\": round(timestamp, 3),\n",
      "                    \"score\": round(score, 4),\n",
      "                    \"chunk_index\": chunk_index,\n",
      "                    \"frame_index\": frame_idx,\n",
      "                })\n",
      "            \n",
      "            # Update EMA\n",
      "            ema = self.ema_alpha * ema + (1 - self.ema_alpha) * embedding\n",
      "            # Re-normalize\n",
      "            ema = ema / np.linalg.norm(ema)\n",
      "        \n",
      "        return scene_changes, ema\n",
      "    \n",
      "    def __call__(\n",
      "        self,\n",
      "        encoder_output: dict,\n",
      "        chunk_index: int,\n",
      "        chunk_start_time: float,\n",
      "        chunk_duration: float,\n",
      "        top_k_tags: int = 5,\n",
      "        ema_state: np.ndarray | None = None,\n",
      "    ) -> dict:\n",
      "        \"\"\"\n",
      "        Decode embeddings into tags, caption, and scene changes.\n",
      "        \n",
      "        Args:\n",
      "            encoder_output: Dict with 'frame_embeddings' and 'embedding_dim'\n",
      "            chunk_index: Index of this chunk in the video\n",
      "            chunk_start_time: Start time of chunk (seconds)\n",
      "            chunk_duration: Duration of chunk (seconds)\n",
      "            top_k_tags: Number of top tags to return\n",
      "            ema_state: EMA state from previous chunk for scene detection continuity.\n",
      "                Pass None for the first chunk of a stream.\n",
      "        \n",
      "        Returns:\n",
      "            Dict containing tags, retrieval_caption, scene_changes, and updated ema_state.\n",
      "            The caller should pass the returned ema_state to the next chunk's call.\n",
      "        \"\"\"\n",
      "        # Get frame embeddings from encoder output\n",
      "        frame_embeddings = encoder_output[\"frame_embeddings\"]\n",
      "        \n",
      "        # Calculate pooled embedding (mean across frames, normalized)\n",
      "        pooled_embedding = frame_embeddings.mean(axis=0)\n",
      "        pooled_embedding = pooled_embedding / np.linalg.norm(pooled_embedding)\n",
      "\n",
      "        # Classification and retrieval on pooled embedding\n",
      "        tags = self._get_top_tags(pooled_embedding, top_k=top_k_tags)\n",
      "        caption = self._get_retrieval_caption(pooled_embedding)\n",
      "        \n",
      "        # Scene change detection on frame embeddings\n",
      "        scene_changes, new_ema_state = self._detect_scene_changes(\n",
      "            frame_embeddings=frame_embeddings,\n",
      "            chunk_index=chunk_index,\n",
      "            chunk_start_time=chunk_start_time,\n",
      "            chunk_duration=chunk_duration,\n",
      "            ema_state=ema_state,\n",
      "        )\n",
      "        \n",
      "        return {\n",
      "            \"tags\": tags,\n",
      "            \"retrieval_caption\": caption,\n",
      "            \"scene_changes\": scene_changes,\n",
      "            \"ema_state\": new_ema_state,\n",
      "        }\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from deployments.decoder import MultiDecoder\n",
    "\n",
    "print(inspect.getsource(MultiDecoder.func_or_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video chunking\n",
    "\n",
    "Before we can process a video, we need to split it into fixed-duration chunks and extract frames. The chunking process:\n",
    "\n",
    "1. **Get video duration** using [`ffprobe`](https://ffmpeg.org/ffprobe.html)\n",
    "2. **Define chunk boundaries** (e.g., 0-10s, 10-20s, 20-30s for a 30s video)\n",
    "3. **Extract frames in parallel** using multiple concurrent [`ffmpeg`](https://ffmpeg.org/) processes\n",
    "4. **Limit concurrency** with [`asyncio.Semaphore`](https://docs.python.org/3/library/asyncio-sync.html#asyncio.Semaphore) to avoid CPU oversubscription\n",
    "\n",
    "Each chunk extracts 16 frames uniformly sampled across its duration, resized to 384Ã—384 (matching SigLIP's input size).\n",
    "\n",
    "#### Design choices\n",
    "\n",
    "**Direct S3 download vs presigned URLs**: We download the video to local disk before processing. An alternative is generating a presigned S3 URL and passing it directly to FFmpeg. Benchmarks show direct download is fasterâ€”FFmpeg's HTTP client doesn't handle S3's chunked responses as efficiently as `aioboto3`, and network latency compounds across multiple seeks.\n",
    "\n",
    "![Presigned vs Direct S3](assets/presigned_vs_direct_s3.png)\n",
    "\n",
    "**Single FFmpeg vs parallel FFmpeg**: Two approaches for extracting frames from multiple chunks:\n",
    "- **Single FFmpeg**: One process reads the entire video, using `select` filter to pick frames at specific timestamps\n",
    "- **Parallel FFmpeg**: Multiple concurrent processes, each extracting one chunk\n",
    "\n",
    "Chose the Single FFmpeg approach since it outperforms parallel FFmpeg on longer videos and yields similar performance for typical 10s chunks. This method is both efficient and scalable as chunk counts grow.\n",
    "\n",
    "![Single vs Multi FFmpeg](assets/single_vs_multi_ffmpeg.png)\n",
    "\n",
    "**Chunk duration**: We use 10-second chunks. Shorter chunks increase overhead (more FFmpeg calls, more encoder/decoder round-trips). Longer chunks increases processing efficiency but **degrade inference quality**â€”SigLIP processes exactly 16 frames per chunk, so a 60-second chunk samples one frame every 3.75 seconds, missing fast scene changes. The 10-second sweet spot balances throughput with temporal resolution (~1.6 fps sampling).\n",
    "\n",
    "![Chunk Video Analysis](assets/chunk_video_analysis.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deployment composition\n",
    "\n",
    "The `VideoAnalyzer` ingress deployment orchestrates the encoder and decoder. It uses [FastAPI](https://fastapi.tiangolo.com/) integration with [`@serve.ingress`](https://docs.ray.io/en/latest/serve/http-guide.html#fastapi-http-deployments) for HTTP endpoints.\n",
    "\n",
    "#### Design choices\n",
    "\n",
    "**Why `num_cpus=6`?** The analyzer runs FFmpeg for frame extraction. Each FFmpeg process uses `FFMPEG_THREADS=2`, and we run up to `NUM_WORKERS=3` concurrent processes. So `2 Ã— 3 = 6` CPUs ensures FFmpeg doesn't contend for CPU during parallel chunk extraction.\n",
    "\n",
    "**Why `max_ongoing_requests=4`?** The encoder has `max_ongoing_requests=2`. We want the analyzer to stay ahead: while 2 videos are encoding, we download and chunk 2 more videos so they're ready when encoder slots free up. This keeps the GPU pipeline saturated without excessive memory from queued requests.\n",
    "\n",
    "**Why cache the S3 client?** Creating a new `aioboto3` client per request adds overhead (connection setup, credential resolution). Caching the client in `__init__` and reusing it across requests amortizes this cost. The client is thread-safe and handles connection pooling internally.\n",
    "\n",
    "**Why encode in parallel but decode serially?** Encoding is statelessâ€”each chunk's frames go through SigLIP independently, so we fire all chunks concurrently with [`asyncio.gather`](https://docs.python.org/3/library/asyncio-task.html#asyncio.gather). Decoding requires temporal orderingâ€”the EMA for scene detection must process chunks in order (chunk 0's EMA state feeds into chunk 1). The `VideoAnalyzer` calls the decoder serially, passing EMA state from each response to the next request. This explicit state passing ensures correct scene detection even when multiple decoder replicas exist under autoscaling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@serve.deployment(\n",
      "    # setting this to twice that of the encoder. So that requests can complete the\n",
      "    # upfront CPU work and be queued for GPU processing.\n",
      "    num_replicas=\"auto\",\n",
      "    ray_actor_options={\"num_cpus\": FFMPEG_THREADS},\n",
      "    max_ongoing_requests=4,\n",
      "    autoscaling_config={\n",
      "        \"min_replicas\": 2,\n",
      "        \"max_replicas\": 20,\n",
      "        \"target_num_ongoing_requests\": 2,\n",
      "    },\n",
      ")\n",
      "@serve.ingress(fastapi_app)\n",
      "class VideoAnalyzer:\n",
      "    \"\"\"\n",
      "    Main ingress deployment that orchestrates VideoEncoder and MultiDecoder.\n",
      "    \n",
      "    Encoder refs are passed directly to decoder; Ray Serve resolves dependencies.\n",
      "    Downloads video from S3 to temp file for fast local processing.\n",
      "    \"\"\"\n",
      "    \n",
      "    def __init__(self, encoder: VideoEncoder, decoder: MultiDecoder):\n",
      "        self.encoder = encoder\n",
      "        self.decoder = decoder\n",
      "        self._s3_session = aioboto3.Session()\n",
      "        self._s3_client = None  # Cached client for reuse across requests\n",
      "        logger.info(\"VideoAnalyzer ready\")\n",
      "    \n",
      "    async def _get_s3_client(self):\n",
      "        \"\"\"Get or create a reusable S3 client.\"\"\"\n",
      "        if self._s3_client is None:\n",
      "            self._s3_client = await self._s3_session.client(\"s3\").__aenter__()\n",
      "        return self._s3_client\n",
      "    \n",
      "    async def _download_video(self, s3_uri: str) -> Path:\n",
      "        \"\"\"Download video from S3 to temp file. Returns local path.\"\"\"\n",
      "        bucket, key = parse_s3_uri(s3_uri)\n",
      "        \n",
      "        # Create temp file with video extension\n",
      "        suffix = Path(key).suffix or \".mp4\"\n",
      "        temp_file = tempfile.NamedTemporaryFile(suffix=suffix, delete=False)\n",
      "        temp_path = Path(temp_file.name)\n",
      "        temp_file.close()\n",
      "        \n",
      "        s3 = await self._get_s3_client()\n",
      "        await s3.download_file(bucket, key, str(temp_path))\n",
      "        \n",
      "        return temp_path\n",
      "\n",
      "    def _aggregate_results(\n",
      "        self,\n",
      "        chunk_results: list[dict],\n",
      "        top_k_tags: int = 5,\n",
      "    ) -> dict:\n",
      "        \"\"\"\n",
      "        Aggregate results from multiple chunks.\n",
      "        \n",
      "        Strategy:\n",
      "        - Tags: Average scores across chunks, return top-k\n",
      "        - Caption: Return the one with highest score across all chunks\n",
      "        \"\"\"\n",
      "        # Aggregate tag scores\n",
      "        tag_scores = defaultdict(list)\n",
      "        for result in chunk_results:\n",
      "            for tag in result[\"tags\"]:\n",
      "                tag_scores[tag[\"text\"]].append(tag[\"score\"])\n",
      "        \n",
      "        # Average tag scores and sort\n",
      "        aggregated_tags = [\n",
      "            {\"text\": text, \"score\": np.mean(scores)}\n",
      "            for text, scores in tag_scores.items()\n",
      "        ]\n",
      "        aggregated_tags.sort(key=lambda x: x[\"score\"], reverse=True)\n",
      "        top_tags = aggregated_tags[:top_k_tags]\n",
      "        \n",
      "        # Best caption across all chunks\n",
      "        best_caption = max(\n",
      "            (r[\"retrieval_caption\"] for r in chunk_results),\n",
      "            key=lambda x: x[\"score\"],\n",
      "        )\n",
      "        \n",
      "        return {\n",
      "            \"tags\": top_tags,\n",
      "            \"retrieval_caption\": best_caption,\n",
      "        }\n",
      "    \n",
      "    def _encode_chunk(self, frames: np.ndarray, use_batching: bool = False) -> DeploymentResponse:\n",
      "        \"\"\"Encode a single chunk's frames to embeddings. Returns DeploymentResponse ref.\"\"\"\n",
      "        return self.encoder.remote(frames, use_batching=use_batching)\n",
      "\n",
      "    async def _decode_chunk(\n",
      "        self,\n",
      "        encoder_output: dict,\n",
      "        chunk_index: int,\n",
      "        chunk_start_time: float,\n",
      "        chunk_duration: float,\n",
      "        ema_state=None,\n",
      "    ) -> dict:\n",
      "        \"\"\"Decode embeddings to tags, caption, scene changes.\"\"\"\n",
      "        return await self.decoder.remote(\n",
      "            encoder_output=encoder_output,\n",
      "            chunk_index=chunk_index,\n",
      "            chunk_start_time=chunk_start_time,\n",
      "            chunk_duration=chunk_duration,\n",
      "            ema_state=ema_state,\n",
      "        )\n",
      "    \n",
      "    @fastapi_app.post(\"/analyze\", response_model=AnalyzeResponse)\n",
      "    async def analyze(self, request: AnalyzeRequest) -> AnalyzeResponse:\n",
      "        \"\"\"\n",
      "        Analyze a video from S3 and return tags, caption, and scene changes.\n",
      "        \n",
      "        Downloads video to temp file for fast local processing.\n",
      "        Chunks the entire video and aggregates results.\n",
      "        Encoder refs are passed directly to decoder for dependency resolution.\n",
      "        \"\"\"\n",
      "        total_start = time.perf_counter()\n",
      "        temp_path = None\n",
      "\n",
      "        try:\n",
      "            # Download video from S3 to temp file\n",
      "            download_start = time.perf_counter()\n",
      "            try:\n",
      "                temp_path = await self._download_video(request.video_path)\n",
      "            except Exception as e:\n",
      "                raise HTTPException(status_code=400, detail=f\"Cannot download S3 video: {e}\")\n",
      "            s3_download_ms = (time.perf_counter() - download_start) * 1000\n",
      "\n",
      "            # Chunk video with PARALLEL frame extraction from local file\n",
      "            decode_start = time.perf_counter()\n",
      "            try:\n",
      "                chunks = await chunk_video_async(\n",
      "                    str(temp_path),\n",
      "                    chunk_duration=request.chunk_duration,\n",
      "                    num_frames_per_chunk=request.num_frames,\n",
      "                    ffmpeg_threads=FFMPEG_THREADS,\n",
      "                    use_single_ffmpeg=True,\n",
      "                )\n",
      "            except Exception as e:\n",
      "                raise HTTPException(status_code=400, detail=f\"Cannot process video: {e}\")\n",
      "\n",
      "            decode_video_ms = (time.perf_counter() - decode_start) * 1000\n",
      "            \n",
      "            if not chunks:\n",
      "                raise HTTPException(status_code=400, detail=\"No chunks extracted from video\")\n",
      "            \n",
      "            # Calculate video duration from chunks\n",
      "            video_duration = chunks[-1].start_time + chunks[-1].duration\n",
      "            \n",
      "            # Fire off all encoder calls (returns refs, not awaited)\n",
      "            encode_start = time.perf_counter()\n",
      "            encode_refs = [\n",
      "                self._encode_chunk(chunk.frames, use_batching=request.use_batching) \n",
      "                for chunk in chunks\n",
      "            ]\n",
      "            encode_ms = (time.perf_counter() - encode_start) * 1000\n",
      "            \n",
      "            # Decode chunks SERIALLY, passing encoder refs directly.\n",
      "            # Ray Serve resolves the encoder result when decoder needs it.\n",
      "            # EMA state is tracked here (not in decoder) to ensure continuity\n",
      "            # even when autoscaling routes requests to different replicas.\n",
      "            decode_start = time.perf_counter()\n",
      "            decode_results = []\n",
      "            ema_state = None  # Will be initialized from first chunk's first frame\n",
      "            for chunk, enc_ref in zip(chunks, encode_refs):\n",
      "                dec_result = await self._decode_chunk(\n",
      "                    encoder_output=enc_ref,\n",
      "                    chunk_index=chunk.index,\n",
      "                    chunk_start_time=chunk.start_time,\n",
      "                    chunk_duration=chunk.duration,\n",
      "                    ema_state=ema_state,\n",
      "                )\n",
      "                decode_results.append(dec_result)\n",
      "                ema_state = dec_result[\"ema_state\"]  # Carry forward for next chunk\n",
      "            decode_ms = (time.perf_counter() - decode_start) * 1000\n",
      "            \n",
      "            # Collect results\n",
      "            chunk_results = []\n",
      "            per_chunk_results = []\n",
      "            all_scene_changes = []\n",
      "            \n",
      "            for chunk, decoder_result in zip(chunks, decode_results):\n",
      "                chunk_results.append(decoder_result)\n",
      "                \n",
      "                # Scene changes come directly from decoder\n",
      "                chunk_scene_changes = [\n",
      "                    SceneChange(**sc) for sc in decoder_result[\"scene_changes\"]\n",
      "                ]\n",
      "                all_scene_changes.extend(chunk_scene_changes)\n",
      "                \n",
      "                per_chunk_results.append(ChunkResult(\n",
      "                    chunk_index=chunk.index,\n",
      "                    start_time=chunk.start_time,\n",
      "                    duration=chunk.duration,\n",
      "                    tags=[TagResult(**t) for t in decoder_result[\"tags\"]],\n",
      "                    retrieval_caption=CaptionResult(**decoder_result[\"retrieval_caption\"]),\n",
      "                    scene_changes=chunk_scene_changes,\n",
      "                ))\n",
      "            \n",
      "            # Aggregate results\n",
      "            aggregated = self._aggregate_results(chunk_results)\n",
      "            \n",
      "            total_ms = (time.perf_counter() - total_start) * 1000\n",
      "            \n",
      "            return AnalyzeResponse(\n",
      "                stream_id=request.stream_id,\n",
      "                tags=[TagResult(**t) for t in aggregated[\"tags\"]],\n",
      "                retrieval_caption=CaptionResult(**aggregated[\"retrieval_caption\"]),\n",
      "                scene_changes=all_scene_changes,\n",
      "                num_scene_changes=len(all_scene_changes),\n",
      "                chunks=per_chunk_results,\n",
      "                num_chunks=len(chunks),\n",
      "                video_duration=video_duration,\n",
      "                timing_ms=TimingResult(\n",
      "                    s3_download_ms=round(s3_download_ms, 2),\n",
      "                    decode_video_ms=round(decode_video_ms, 2),\n",
      "                    encode_ms=round(encode_ms, 2),\n",
      "                    decode_ms=round(decode_ms, 2),\n",
      "                    total_ms=round(total_ms, 2),\n",
      "                ),\n",
      "            )\n",
      "        finally:\n",
      "            # Clean up temp file\n",
      "            if temp_path and temp_path.exists():\n",
      "                temp_path.unlink(missing_ok=True)\n",
      "    \n",
      "    @fastapi_app.get(\"/health\")\n",
      "    async def health(self):\n",
      "        \"\"\"Health check endpoint.\"\"\"\n",
      "        return {\"status\": \"healthy\"}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from app import VideoAnalyzer\n",
    "\n",
    "print(inspect.getsource(VideoAnalyzer.func_or_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = VideoEncoder.bind()\n",
    "decoder = MultiDecoder.bind(bucket=S3_BUCKET, s3_prefix=S3_EMBEDDINGS_PREFIX)\n",
    "app = VideoAnalyzer.bind(encoder=encoder, decoder=decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ProxyActor pid=251250)\u001b[0m INFO 2026-01-06 08:27:23,294 proxy 10.0.45.10 -- Proxy starting on node 7beb9ebc3808dac027f36a0400592d7a27cc9e90f0aab0a2578c328b (HTTP port: 8000).\n",
      "INFO 2026-01-06 08:27:23,369 serve 250708 -- Started Serve in namespace \"serve\".\n",
      "\u001b[36m(ServeController pid=251181)\u001b[0m INFO 2026-01-06 08:27:23,394 controller 251181 -- Registering autoscaling state for deployment Deployment(name='VideoEncoder', app='video-analyzer')\n",
      "\u001b[36m(ServeController pid=251181)\u001b[0m INFO 2026-01-06 08:27:23,395 controller 251181 -- Deploying new version of Deployment(name='VideoEncoder', app='video-analyzer') (initial target replicas: 1).\n",
      "\u001b[36m(ServeController pid=251181)\u001b[0m INFO 2026-01-06 08:27:23,397 controller 251181 -- Registering autoscaling state for deployment Deployment(name='MultiDecoder', app='video-analyzer')\n",
      "\u001b[36m(ServeController pid=251181)\u001b[0m INFO 2026-01-06 08:27:23,397 controller 251181 -- Deploying new version of Deployment(name='MultiDecoder', app='video-analyzer') (initial target replicas: 1).\n",
      "\u001b[36m(ServeController pid=251181)\u001b[0m INFO 2026-01-06 08:27:23,398 controller 251181 -- Registering autoscaling state for deployment Deployment(name='VideoAnalyzer', app='video-analyzer')\n",
      "\u001b[36m(ServeController pid=251181)\u001b[0m INFO 2026-01-06 08:27:23,398 controller 251181 -- Deploying new version of Deployment(name='VideoAnalyzer', app='video-analyzer') (initial target replicas: 2).\n",
      "\u001b[36m(ProxyActor pid=251250)\u001b[0m INFO 2026-01-06 08:27:23,365 proxy 10.0.45.10 -- Got updated endpoints: {}.\n",
      "\u001b[36m(ProxyActor pid=251250)\u001b[0m INFO 2026-01-06 08:27:23,403 proxy 10.0.45.10 -- Got updated endpoints: {Deployment(name='VideoAnalyzer', app='video-analyzer'): EndpointInfo(route='/', app_is_cross_language=False, route_patterns=None)}.\n",
      "\u001b[36m(ServeController pid=251181)\u001b[0m INFO 2026-01-06 08:27:23,508 controller 251181 -- Adding 1 replica to Deployment(name='VideoEncoder', app='video-analyzer').\n",
      "\u001b[36m(ServeController pid=251181)\u001b[0m INFO 2026-01-06 08:27:23,510 controller 251181 -- Adding 1 replica to Deployment(name='MultiDecoder', app='video-analyzer').\n",
      "\u001b[36m(ServeController pid=251181)\u001b[0m INFO 2026-01-06 08:27:23,512 controller 251181 -- Adding 2 replicas to Deployment(name='VideoAnalyzer', app='video-analyzer').\n",
      "\u001b[36m(ProxyActor pid=251250)\u001b[0m INFO 2026-01-06 08:27:23,416 proxy 10.0.45.10 -- Started <ray.serve._private.router.SharedRouterLongPollClient object at 0x72d0e5f4f710>.\n",
      "\u001b[36m(ProxyActor pid=251250)\u001b[0m INFO 2026-01-06 08:27:27,576 proxy 10.0.45.10 -- Got updated endpoints: {Deployment(name='VideoAnalyzer', app='video-analyzer'): EndpointInfo(route='/', app_is_cross_language=False, route_patterns=[RoutePattern(methods=['POST'], path='/analyze'), RoutePattern(methods=['GET', 'HEAD'], path='/docs'), RoutePattern(methods=['GET', 'HEAD'], path='/docs/oauth2-redirect'), RoutePattern(methods=['GET'], path='/health'), RoutePattern(methods=['GET', 'HEAD'], path='/openapi.json'), RoutePattern(methods=['GET', 'HEAD'], path='/redoc')])}.\n",
      "\u001b[36m(ProxyActor pid=67913, ip=10.0.239.104)\u001b[0m INFO 2026-01-06 08:27:28,373 proxy 10.0.239.104 -- Proxy starting on node 6954457300d89edbf779f01ef0bfcfc5d109d927fca0e3b4f80974ba (HTTP port: 8000).\u001b[32m [repeated 2x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[36m(ProxyActor pid=6627, ip=10.0.222.50)\u001b[0m INFO 2026-01-06 08:27:26,287 proxy 10.0.222.50 -- Got updated endpoints: {Deployment(name='VideoAnalyzer', app='video-analyzer'): EndpointInfo(route='/', app_is_cross_language=False, route_patterns=None)}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ServeReplica:video-analyzer:VideoEncoder pid=6736, ip=10.0.222.50)\u001b[0m VideoEncoder initializing on cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ServeReplica:video-analyzer:VideoEncoder pid=6736, ip=10.0.222.50)\u001b[0m Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "\u001b[36m(ProxyActor pid=67913, ip=10.0.239.104)\u001b[0m INFO 2026-01-06 08:27:28,444 proxy 10.0.239.104 -- Started <ray.serve._private.router.SharedRouterLongPollClient object at 0x7dd178a4d190>.\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ServeReplica:video-analyzer:VideoEncoder pid=6736, ip=10.0.222.50)\u001b[0m VideoEncoder ready (embedding_dim=1152)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO 2026-01-06 08:27:31,518 serve 250708 -- Application 'video-analyzer' is ready at http://0.0.0.0:8000/.\n",
      "INFO 2026-01-06 08:27:31,534 serve 250708 -- Started <ray.serve._private.router.SharedRouterLongPollClient object at 0x78edac0dbef0>.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DeploymentHandle(deployment='VideoAnalyzer')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ray import serve\n",
    "\n",
    "serve.run(app, name=\"video-analyzer\", route_prefix=\"/\", blocking=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for deployment to be healthy...\n",
      "Deployment ready in 0.0s\n"
     ]
    }
   ],
   "source": [
    "import httpx\n",
    "import time\n",
    "\n",
    "# Wait for deployment to be ready\n",
    "print(\"Waiting for deployment to be healthy...\")\n",
    "start = time.time()\n",
    "while True:\n",
    "    try:\n",
    "        with httpx.Client(timeout=5.0) as client:\n",
    "            response = client.get(\"http://localhost:8000/health\")\n",
    "            if response.status_code == 200:\n",
    "                print(f\"Deployment ready in {time.time() - start:.1f}s\")\n",
    "                break\n",
    "    except httpx.RequestError:\n",
    "        pass\n",
    "    time.sleep(1.0)\n",
    "    if time.time() - start > 120:\n",
    "        raise TimeoutError(\"Deployment did not become healthy within 120s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ServeReplica:video-analyzer:VideoAnalyzer pid=67729, ip=10.0.239.104)\u001b[0m INFO 2026-01-06 08:27:31,893 video-analyzer_VideoAnalyzer npdr89ay b9cd7535-7acc-4fa4-a54b-a0e789ca9ce7 -- GET /health 200 1.5ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing video: s3://anyscale-example-video-analysis-test-bucket/anyscale-example/stock-videos/kitchen_cooking_35395675_00.mp4\n",
      "Stream ID: 766cc773\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ServeReplica:video-analyzer:VideoAnalyzer pid=67728, ip=10.0.239.104)\u001b[0m INFO 2026-01-06 08:27:32,683 video-analyzer_VideoAnalyzer ntukkwkd 9326cc34-5b87-491c-8025-a7db9a2806f4 -- Started <ray.serve._private.router.SharedRouterLongPollClient object at 0x788bd83d2120>.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ServeReplica:video-analyzer:VideoEncoder pid=6736, ip=10.0.222.50)\u001b[0m Unbatched: 16 frames\n",
      "============================================================\n",
      "Video duration: 8.3s\n",
      "Chunks processed: 1\n",
      "\n",
      "Top Tags:\n",
      "  0.082  kitchen\n",
      "  0.070  cafe\n",
      "  0.058  living room\n",
      "  0.057  bedroom\n",
      "  0.053  office\n",
      "\n",
      "Best Caption:\n",
      "  0.101  Someone preparing food on a counter\n",
      "\n",
      "Scene Changes: 0\n",
      "\n",
      "Timing:\n",
      "  S3 download:    238.8 ms\n",
      "  Video decode:   100.6 ms\n",
      "  Encode (GPU):    18.3 ms\n",
      "  Decode (CPU):   855.0 ms\n",
      "  Total server:  1212.9 ms\n",
      "  Round-trip:    1235.5 ms\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ServeReplica:video-analyzer:MultiDecoder pid=6737, ip=10.0.222.50)\u001b[0m /home/ray/anaconda3/lib/python3.12/site-packages/ray/serve/_private/replica.py:1640: UserWarning: Calling sync method '__call__' directly on the asyncio loop. In a future version, sync methods will be run in a threadpool by default. Ensure your sync methods are thread safe or keep the existing behavior by making them `async def`. Opt into the new behavior by setting RAY_SERVE_RUN_SYNC_IN_THREADPOOL=1.\n",
      "\u001b[36m(ServeReplica:video-analyzer:MultiDecoder pid=6737, ip=10.0.222.50)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(ServeReplica:video-analyzer:MultiDecoder pid=6737, ip=10.0.222.50)\u001b[0m INFO 2026-01-06 08:27:33,539 video-analyzer_MultiDecoder 1carxuil 9326cc34-5b87-491c-8025-a7db9a2806f4 -- CALL __call__ OK 2.1ms\n",
      "\u001b[36m(ServeReplica:video-analyzer:VideoEncoder pid=6736, ip=10.0.222.50)\u001b[0m INFO 2026-01-06 08:27:33,533 video-analyzer_VideoEncoder pxjowb4x 9326cc34-5b87-491c-8025-a7db9a2806f4 -- CALL __call__ OK 804.7ms\n",
      "\u001b[36m(ServeReplica:video-analyzer:VideoAnalyzer pid=67728, ip=10.0.239.104)\u001b[0m INFO 2026-01-06 08:27:33,541 video-analyzer_VideoAnalyzer ntukkwkd 9326cc34-5b87-491c-8025-a7db9a2806f4 -- POST /analyze 200 1217.9ms\n"
     ]
    }
   ],
   "source": [
    "import httpx\n",
    "import time\n",
    "import uuid\n",
    "\n",
    "# Send a sample request to the deployed service\n",
    "payload = {\n",
    "    \"stream_id\": uuid.uuid4().hex[:8],\n",
    "    \"video_path\": SAMPLE_VIDEO_URI,\n",
    "    \"num_frames\": 16,\n",
    "    \"chunk_duration\": 10.0,\n",
    "}\n",
    "\n",
    "print(f\"Analyzing video: {SAMPLE_VIDEO_URI}\")\n",
    "print(f\"Stream ID: {payload['stream_id']}\\n\")\n",
    "\n",
    "start = time.perf_counter()\n",
    "with httpx.Client(timeout=300.0) as client:\n",
    "    response = client.post(\"http://localhost:8000/analyze\", json=payload)\n",
    "latency_ms = (time.perf_counter() - start) * 1000\n",
    "\n",
    "result = response.json()\n",
    "\n",
    "# Print results\n",
    "print(\"=\" * 60)\n",
    "print(f\"Video duration: {result['video_duration']:.1f}s\")\n",
    "print(f\"Chunks processed: {result['num_chunks']}\")\n",
    "print()\n",
    "\n",
    "print(\"Top Tags:\")\n",
    "for tag in result[\"tags\"]:\n",
    "    print(f\"  {tag['score']:.3f}  {tag['text']}\")\n",
    "print()\n",
    "\n",
    "print(\"Best Caption:\")\n",
    "print(f\"  {result['retrieval_caption']['score']:.3f}  {result['retrieval_caption']['text']}\")\n",
    "print()\n",
    "\n",
    "print(f\"Scene Changes: {result['num_scene_changes']}\")\n",
    "for sc in result[\"scene_changes\"][:5]:  # Show first 5\n",
    "    print(f\"  {sc['timestamp']:6.2f}s  score={sc['score']:.3f}\")\n",
    "print()\n",
    "\n",
    "print(\"Timing:\")\n",
    "timing = result[\"timing_ms\"]\n",
    "print(f\"  S3 download:  {timing['s3_download_ms']:7.1f} ms\")\n",
    "print(f\"  Video decode: {timing['decode_video_ms']:7.1f} ms\")\n",
    "print(f\"  Encode (GPU): {timing['encode_ms']:7.1f} ms\")\n",
    "print(f\"  Decode (CPU): {timing['decode_ms']:7.1f} ms\")\n",
    "print(f\"  Total server: {timing['total_ms']:7.1f} ms\")\n",
    "print(f\"  Round-trip:   {latency_ms:7.1f} ms\")\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "serve.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Load Test Results\n",
    "\n",
    "To evaluate the pipeline's performance under realistic conditions, we ran load tests using the `client/load_test.py` script with **concurrency levels ranging from 2 to 64 concurrent requests**. The Ray Serve application was configured with autoscaling enabled, allowing replicas to scale dynamically based on demand.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methodology\n",
    "\n",
    "The load test was executed using:\n",
    "\n",
    "```bash\n",
    "python -m client.load_test --video s3://bucket/video.mp4 --concurrency <N>\n",
    "```\n",
    "\n",
    "**Test parameters:**\n",
    "- Concurrency levels: 2, 4, 8, 16, 32, 64\n",
    "- Chunk duration: 10 seconds\n",
    "- Frames per chunk: 16\n",
    "- Autoscaling: Enabled (replicas scale based on `target_num_ongoing_requests`)\n",
    "\n",
    "The charts below show autoscaling in action during the concurrency=2 to 64 load test:\n",
    "\n",
    "![Ongoing Requests](assets/ongoing.png)\n",
    "\n",
    "![Replica Count](assets/num_replicas.png)\n",
    "\n",
    "![Queue Length](assets/queue_len.png)\n",
    "\n",
    "As load increases, replicas scale up to maintain target queue depth. The system reaches steady state once enough replicas are provisioned to handle the request rate.\n",
    "\n",
    "To ensure fair comparison, we discarded the first half of each test run to exclude the autoscaling warm-up period where latencies are elevated due to replica initialization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results Summary\n",
    "\n",
    "| Concurrency | Requests | P50 (ms) | P95 (ms) | P99 (ms) | Throughput (req/s) |\n",
    "|-------------|----------|----------|----------|----------|-------------------|\n",
    "| 2           | 152      | 838      | 843      | 844      | 2.37              |\n",
    "| 4           | 590      | 858      | 984      | 1,025    | 4.68              |\n",
    "| 8           | 1,103    | 885      | 1,065    | 1,120    | 8.97              |\n",
    "| 16          | 2,240    | 900      | 1,074    | 1,128    | 17.78             |\n",
    "| 32          | 4,141    | 928      | 1,095    | 1,151    | 34.75             |\n",
    "| 64          | 17,283   | 967      | 1,128    | 1,188    | 67.55             |\n",
    "\n",
    "**Key findings:**\n",
    "\n",
    "- **100% success rate** across all 25,509 requests analyzed\n",
    "- **Near-linear throughput scaling**: Throughput increased from 2.37 req/s at concurrency 2 to **67.55 req/s at concurrency 64** (~28x improvement)\n",
    "- **Stable latencies under load**: P95 latency remained between 843ms and 1,128ms across all concurrency levels, demonstrating effective autoscaling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing Time Breakdown\n",
    "\n",
    "The chart below shows how processing time is distributed across pipeline stages at each concurrency level:\n",
    "\n",
    "![Processing Time Breakdown](assets/processing_time_breakdown.png)\n",
    "\n",
    "**At concurrency 64 (best throughput):**\n",
    "- **S3 Download**: 77ms (8%)\n",
    "- **Video Decode (FFmpeg)**: 98ms (10%)\n",
    "- **Encode (GPU)**: <1ms (<1%) â€” Note: This number is artificially low due to how timing is measured. Because the output of VideoEncoder is passed directly into MultiDecoder as an argument, the instrumentation mistakenly attributes most of the computational time to the downstream stage. In reality, VideoEncoder performs the bulk of the work, so its true processing time is significantly higher than reported here.\n",
    "- **Decode (CPU)**: 757ms (81%)\n",
    "\n",
    "The CPU decoding stage (tag/caption generation) dominates processing time. S3 download and video decoding are relatively fast due to local caching and optimized FFmpeg settings.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Throughput Analysis\n",
    "\n",
    "The charts below show throughput scaling and the latency-throughput tradeoff:\n",
    "\n",
    "![Throughput Analysis](assets/throughput_analysis.png)\n",
    "\n",
    "**Observations:**\n",
    "\n",
    "1. **Linear scaling**: Throughput scales almost linearly with concurrency, indicating that autoscaling successfully provisions enough replicas to handle increased load.\n",
    "\n",
    "2. **Latency-throughput tradeoff**: The right chart shows that P95 latency increases slightly as throughput grows (from ~843ms to ~1,128ms), but remains within acceptable bounds. This ~34% latency increase enables a ~28x throughput improvement.\n",
    "\n",
    "3. **No saturation point**: Even at concurrency 64, throughput continues to scale. The system could likely handle higher concurrency with additional GPU resources.\n",
    "\n",
    "The near-linear scaling demonstrates that the pipeline architectureâ€”with separate GPU encoder, CPU decoder, and CPU-bound ingressâ€”allows each component to scale independently based on its resource requirements.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
