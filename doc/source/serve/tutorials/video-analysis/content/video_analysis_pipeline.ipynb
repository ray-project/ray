{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video analysis pipeline with Ray Serve\n",
    "\n",
    "This notebook demonstrates how to build a production-grade video analysis pipeline with [Ray Serve](https://docs.ray.io/en/latest/serve/). The pipeline processes videos from S3 and extracts:\n",
    "\n",
    "- **Tags**: [Zero-shot classification](https://huggingface.co/tasks/zero-shot-image-classification) labels (such as \"kitchen\", \"office\", \"park\")\n",
    "- **Captions**: Retrieval-based descriptions matching the video content\n",
    "- **Scene changes**: Detected transitions using [exponential moving average (EMA)](https://en.wikipedia.org/wiki/Exponential_smoothing) analysis\n",
    "\n",
    "The system uses [**SigLIP**](https://huggingface.co/google/siglip-so400m-patch14-384) (`google/siglip-so400m-patch14-384`) as the vision-language backbone. SigLIP provides a unified embedding space for both images and text, enabling zero-shot classification and retrieval without task-specific fine-tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture\n",
    "\n",
    "The pipeline splits work across three [Ray Serve deployments](https://docs.ray.io/en/latest/serve/key-concepts.html#deployment), each optimized for its workload:\n",
    "\n",
    "```\n",
    "                              ┌─────────────────────┐\n",
    "                         ┌───▶│  VideoEncoder       │\n",
    "                         │    │      (GPU)          │\n",
    "                         │    ├─────────────────────┤\n",
    "┌─────────────────────┐  │    │ • SigLIP embedding  │\n",
    "│  VideoAnalyzer      │──┤    │ • 16 frames/chunk   │\n",
    "│     (Ingress)       │  │    │ • L2 normalization  │\n",
    "├─────────────────────┤  │    └─────────────────────┘\n",
    "│ • S3 download       │  │          num_gpus=1\n",
    "│ • FFmpeg chunking   │  │\n",
    "│ • Request routing   │  │    ┌─────────────────────┐\n",
    "└─────────────────────┘  └───▶│  MultiDecoder       │\n",
    "      num_cpus=6              │      (CPU)          │\n",
    "                              ├─────────────────────┤\n",
    "                              │ • Tag classification│\n",
    "                              │ • Caption retrieval │\n",
    "                              │ • Scene detection   │\n",
    "                              └─────────────────────┘\n",
    "                                    num_cpus=1\n",
    "```\n",
    "\n",
    "**Request lifecycle:**\n",
    "1. `VideoAnalyzer` receives HTTP request with S3 video URI\n",
    "2. Downloads video from S3, splits into fixed-duration chunks using FFmpeg\n",
    "3. Sends all chunks to `VideoEncoder` concurrently (`asyncio.gather`)\n",
    "4. Encoder returns embedding references (stored in Ray object store)\n",
    "5. `VideoAnalyzer` sends embeddings to `MultiDecoder` serially (for EMA state continuity)\n",
    "6. Aggregates results and returns tags, captions, and scene changes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prerequisites\n",
    "\n",
    "Before running this notebook, ensure you have:\n",
    "\n",
    "| Requirement | Purpose |\n",
    "|-------------|---------|\n",
    "| **Pexels API key** | Download sample video (free at https://www.pexels.com/api/) |\n",
    "| **S3 bucket** | Store videos and text embeddings |\n",
    "| **AWS credentials** | Read/write access to your S3 bucket |\n",
    "| **ffmpeg** | Video processing and frame extraction |\n",
    "| **GPU** | Run SigLIP model for encoding (1 GPU minimum) |\n",
    "\n",
    "Set these environment variables before running:\n",
    "\n",
    "```bash\n",
    "export PEXELS_API_KEY=\"your-pexels-api-key\"\n",
    "export S3_BUCKET=\"your-bucket-name\"\n",
    "export AWS_ACCESS_KEY_ID=\"...\"\n",
    "export AWS_SECRET_ACCESS_KEY=\"...\"\n",
    "```\n",
    "\n",
    "> **Note on GPU type**: The benchmarks, design choices, and hyperparameters in this notebook were tuned for **NVIDIA L4 GPUs**. Different GPU types (A10G, T4, A100, etc.) have different memory bandwidth, compute throughput, and batch characteristics. You may need to adjust `max_ongoing_requests`, chunk sizes, and concurrency limits for optimal performance on other hardware.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "\n",
    "PEXELS_API_KEY = os.environ.get(\"PEXELS_API_KEY\")  # Or set directly: \"your-api-key\"\n",
    "S3_BUCKET = os.environ.get(\"S3_BUCKET\")  # Or set directly: \"your-bucket\"\n",
    "\n",
    "# Get the region of the S3 bucket\n",
    "s3 = boto3.client(\"s3\")\n",
    "response = s3.get_bucket_location(Bucket=S3_BUCKET)\n",
    "# AWS returns None for us-east-1, otherwise returns the region name\n",
    "S3_REGION = response[\"LocationConstraint\"] or \"us-east-1\"\n",
    "print(f\"Bucket '{S3_BUCKET}' is in region: {S3_REGION}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download sample video\n",
    "\n",
    "Before running the pipeline, we need a sample video in S3. This section downloads a video from Pexels, normalizes it, and uploads to S3.\n",
    "\n",
    "**Why normalize videos?** We re-encode all videos to a consistent format:\n",
    "- **384×384 resolution**: Matches SigLIP's input size exactly, eliminating resize during inference\n",
    "- **30 fps**: Standardizes frame timing for consistent chunk boundaries\n",
    "- **[H.264](https://en.wikipedia.org/wiki/Advanced_Video_Coding) codec (libx264)**: Fast seeking—FFmpeg can jump directly to any timestamp without decoding preceding frames. Some source codecs (VP9, HEVC) require sequential decoding, adding latency for chunk extraction\n",
    "\n",
    "> **Note:**  \n",
    "> The code below is a trimmed-down version of `scripts/download_stock_videos.py`.  \n",
    "> For bulk downloads or custom video sets, run:  \n",
    "> ```bash\n",
    "> python scripts/download_stock_videos.py --api-key YOUR_PEXELS_API_KEY --bucket YOUR_S3_BUCKET\n",
    "> ```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "S3_PREFIX = \"anyscale-example/stock-videos/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading video 35510474...\n",
      "Normalizing to 384x384@30fps...\n",
      "Uploaded to s3://abrar-test-bucket-123/anyscale-example/stock-videos/sample_35510474.mp4\n",
      "\n",
      "Sample video ready: s3://abrar-test-bucket-123/anyscale-example/stock-videos/sample_35510474.mp4\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "\n",
    "import httpx\n",
    "\n",
    "\n",
    "def download_sample_video() -> str:\n",
    "    \"\"\"Download one sample video from Pexels, normalize it, and upload to S3.\n",
    "    \n",
    "    If a video already exists in S3, return that instead of downloading.\n",
    "    \"\"\"\n",
    "    # Check if any videos already exist in S3\n",
    "    response = s3.list_objects_v2(\n",
    "        Bucket=S3_BUCKET,\n",
    "        Prefix=S3_PREFIX,\n",
    "        MaxKeys=1,\n",
    "    )\n",
    "    existing_files = response.get(\"Contents\", [])\n",
    "    \n",
    "    if existing_files:\n",
    "        # Use the first existing video\n",
    "        existing_key = existing_files[0][\"Key\"]\n",
    "        s3_uri = f\"s3://{S3_BUCKET}/{existing_key}\"\n",
    "        print(f\"Found existing video in S3: {s3_uri}\")\n",
    "        return s3_uri\n",
    "\n",
    "    # No existing video found, download from Pexels\n",
    "    print(\"No existing video in S3, downloading from Pexels...\")\n",
    "    \n",
    "    # Search for a video\n",
    "    with httpx.Client() as client:\n",
    "        response = client.get(\n",
    "            \"https://api.pexels.com/videos/search\",\n",
    "            headers={\"Authorization\": PEXELS_API_KEY},\n",
    "            params={\"query\": \"kitchen cooking\", \"per_page\": 1, \"orientation\": \"landscape\"},\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        videos = response.json().get(\"videos\", [])\n",
    "\n",
    "    if not videos:\n",
    "        raise RuntimeError(\"No videos found\")\n",
    "\n",
    "    video = videos[0]\n",
    "    video_files = video.get(\"video_files\", [])\n",
    "    # Pick HD quality\n",
    "    video_file = next((vf for vf in video_files if vf.get(\"quality\") == \"hd\"), video_files[0])\n",
    "    download_url = video_file[\"link\"]\n",
    "    video_id = video[\"id\"]\n",
    "\n",
    "    print(f\"Downloading video {video_id}...\")\n",
    "\n",
    "    with tempfile.TemporaryDirectory() as temp_dir:\n",
    "        raw_path = Path(temp_dir) / \"raw.mp4\"\n",
    "        normalized_path = Path(temp_dir) / f\"sample_{video_id}.mp4\"\n",
    "\n",
    "        # Download\n",
    "        with httpx.Client() as client:\n",
    "            with client.stream(\"GET\", download_url, timeout=120.0) as resp:\n",
    "                resp.raise_for_status()\n",
    "                with open(raw_path, \"wb\") as f:\n",
    "                    for chunk in resp.iter_bytes(8192):\n",
    "                        f.write(chunk)\n",
    "\n",
    "        print(\"Normalizing to 384x384@30fps...\")\n",
    "\n",
    "        # Normalize with ffmpeg (384x384 matches SigLIP input size)\n",
    "        subprocess.run([\n",
    "            \"ffmpeg\", \"-y\", \"-i\", str(raw_path),\n",
    "            \"-vf\", \"scale=384:384:force_original_aspect_ratio=decrease,pad=384:384:(ow-iw)/2:(oh-ih)/2,fps=30\",\n",
    "            \"-c:v\", \"libx264\", \"-preset\", \"fast\", \"-an\",\n",
    "            str(normalized_path),\n",
    "        ], capture_output=True, check=True)\n",
    "\n",
    "        # Upload to S3\n",
    "        s3_key = f\"{S3_PREFIX}sample_{video_id}.mp4\"\n",
    "        s3.upload_file(str(normalized_path), S3_BUCKET, s3_key)\n",
    "\n",
    "        s3_uri = f\"s3://{S3_BUCKET}/{s3_key}\"\n",
    "        print(f\"Uploaded to {s3_uri}\")\n",
    "        return s3_uri\n",
    "\n",
    "\n",
    "# Run the download\n",
    "SAMPLE_VIDEO_URI = download_sample_video()\n",
    "print(f\"\\nSample video ready: {SAMPLE_VIDEO_URI}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate embeddings for text bank\n",
    "\n",
    "The decoder matches video embeddings against precomputed **text embeddings** for tags and descriptions. We define the text banks here and use a [Ray task](https://docs.ray.io/en/latest/ray-core/tasks.html) to compute embeddings on GPU and upload to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tags: 20\n",
      "Descriptions: 20\n"
     ]
    }
   ],
   "source": [
    "# Text banks for zero-shot classification and retrieval\n",
    "\n",
    "TAGS = [\n",
    "    \"kitchen\", \"living room\", \"office\", \"meeting room\", \"classroom\",\n",
    "    \"restaurant\", \"cafe\", \"grocery store\", \"gym\", \"warehouse\",\n",
    "    \"parking lot\", \"city street\", \"park\", \"shopping mall\", \"beach\",\n",
    "    \"sports field\", \"hallway\", \"lobby\", \"bathroom\", \"bedroom\",\n",
    "]\n",
    "\n",
    "DESCRIPTIONS = [\n",
    "    \"A person cooking in a kitchen\",\n",
    "    \"Someone preparing food on a counter\",\n",
    "    \"A chef working in a professional kitchen\",\n",
    "    \"People eating at a dining table\",\n",
    "    \"A person working at a desk\",\n",
    "    \"Someone typing on a laptop\",\n",
    "    \"A business meeting in progress\",\n",
    "    \"A presentation being given\",\n",
    "    \"A teacher lecturing in a classroom\",\n",
    "    \"Students sitting at desks\",\n",
    "    \"A customer shopping in a store\",\n",
    "    \"People browsing products on shelves\",\n",
    "    \"A person exercising at a gym\",\n",
    "    \"Someone lifting weights\",\n",
    "    \"People walking on a city sidewalk\",\n",
    "    \"Traffic moving through an intersection\",\n",
    "    \"Cars driving on a road\",\n",
    "    \"People walking through a park\",\n",
    "    \"A group having a conversation\",\n",
    "    \"A person on a phone call\",\n",
    "]\n",
    "\n",
    "print(f\"Tags: {len(TAGS)}\")\n",
    "print(f\"Descriptions: {len(DESCRIPTIONS)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"google/siglip-so400m-patch14-384\"\n",
    "S3_EMBEDDINGS_PREFIX = \"anyscale-example/embeddings/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-04 04:04:42,994\tINFO worker.py:1821 -- Connecting to existing Ray cluster at address: 10.0.62.211:6379...\n",
      "2026-01-04 04:04:43,007\tINFO worker.py:1998 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32mhttps://session-lqu9h8iu3cpgv59j74p498djis.i.anyscaleuserdata-staging.com \u001b[39m\u001b[22m\n",
      "2026-01-04 04:04:43,048\tINFO packaging.py:463 -- Pushing file package 'gcs://_ray_pkg_062afcc0148d0f0e3896130fe2baff02fcfc420c.zip' (8.63MiB) to Ray cluster...\n",
      "2026-01-04 04:04:43,080\tINFO packaging.py:476 -- Successfully pushed file package 'gcs://_ray_pkg_062afcc0148d0f0e3896130fe2baff02fcfc420c.zip'.\n",
      "/home/ray/anaconda3/lib/python3.12/site-packages/ray/_private/worker.py:2046: FutureWarning: Tip: In future versions of Ray, Ray will no longer override accelerator visible devices env var if num_gpus=0 or num_gpus=None (default). To enable this behavior and turn off this error message, set RAY_ACCEL_ENV_VAR_OVERRIDE_ON_ZERO=0\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(autoscaler +6s)\u001b[0m Tip: use `ray status` to view detailed cluster status. To disable these messages, set RAY_SCHEDULER_EVENTS=0.\n",
      "\u001b[36m(generate_text_embeddings pid=2379, ip=10.0.25.139)\u001b[0m Loading SigLIP on cuda...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(generate_text_embeddings pid=2379, ip=10.0.25.139)\u001b[0m Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(generate_text_embeddings pid=2379, ip=10.0.25.139)\u001b[0m Computing tag embeddings...\n",
      "\u001b[36m(generate_text_embeddings pid=2379, ip=10.0.25.139)\u001b[0m Computing description embeddings...\n",
      "\n",
      "Text embeddings ready:\n",
      "  Tags: s3://abrar-test-bucket-123/anyscale-example/embeddings/tag_embeddings.npz\n",
      "  Descriptions: s3://abrar-test-bucket-123/anyscale-example/embeddings/description_embeddings.npz\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import io\n",
    "\n",
    "import aioboto3\n",
    "import numpy as np\n",
    "import ray\n",
    "import torch\n",
    "from transformers import AutoModel, AutoProcessor\n",
    "\n",
    "\n",
    "@ray.remote(num_gpus=1)\n",
    "def generate_text_embeddings(tags: list[str], descriptions: list[str], bucket: str) -> dict:\n",
    "    \"\"\"Ray task: compute text embeddings on GPU and upload to S3.\"\"\"\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"Loading SigLIP on {device}...\")\n",
    "\n",
    "    processor = AutoProcessor.from_pretrained(MODEL_NAME)\n",
    "    model = AutoModel.from_pretrained(MODEL_NAME).to(device)\n",
    "    model.eval()\n",
    "\n",
    "    def embed_texts(texts: list[str]) -> np.ndarray:\n",
    "        inputs = processor(text=texts, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model.get_text_features(**inputs)\n",
    "        embeddings = outputs.cpu().numpy()\n",
    "        return (embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)).astype(np.float32)\n",
    "\n",
    "    print(\"Computing tag embeddings...\")\n",
    "    tag_embeddings = embed_texts(tags)\n",
    "\n",
    "    print(\"Computing description embeddings...\")\n",
    "    desc_embeddings = embed_texts(descriptions)\n",
    "\n",
    "    async def upload():\n",
    "        session = aioboto3.Session(region_name=S3_REGION)\n",
    "\n",
    "        async def save_npz(embeddings, texts, key):\n",
    "            buffer = io.BytesIO()\n",
    "            np.savez_compressed(buffer, embeddings=embeddings, texts=np.array(texts, dtype=object))\n",
    "            buffer.seek(0)\n",
    "            async with session.client(\"s3\") as s3:\n",
    "                await s3.put_object(Bucket=bucket, Key=key, Body=buffer.getvalue())\n",
    "            return f\"s3://{bucket}/{key}\"\n",
    "\n",
    "        return await asyncio.gather(\n",
    "            save_npz(tag_embeddings, tags, f\"{S3_EMBEDDINGS_PREFIX}tag_embeddings.npz\"),\n",
    "            save_npz(desc_embeddings, descriptions, f\"{S3_EMBEDDINGS_PREFIX}description_embeddings.npz\"),\n",
    "        )\n",
    "\n",
    "    tag_uri, desc_uri = asyncio.run(upload())\n",
    "    print(f\"Uploaded: {tag_uri}, {desc_uri}\")\n",
    "\n",
    "    return {\"tags\": tag_uri, \"descriptions\": desc_uri}\n",
    "\n",
    "\n",
    "# Run the Ray task\n",
    "ray.init(ignore_reinit_error=True)\n",
    "result = ray.get(generate_text_embeddings.remote(TAGS, DESCRIPTIONS, S3_BUCKET))\n",
    "print(f\"\\nText embeddings ready:\")\n",
    "print(f\"  Tags: {result['tags']}\")\n",
    "print(f\"  Descriptions: {result['descriptions']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Build the Ray Serve application\n",
    "\n",
    "This section walks through building the video analysis pipeline step by step, introducing Ray Serve concepts as we go.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU encoder\n",
    "\n",
    "The `VideoEncoder` deployment runs on GPU and converts video frames to embeddings using SigLIP. Key configuration:\n",
    "\n",
    "- `num_gpus=1`: Each replica requires a dedicated GPU\n",
    "- `max_ongoing_requests=2`: Allows pipelining—while one request computes on GPU, another prepares data on CPU\n",
    "\n",
    "**Why no request batching?** A single chunk (16 frames @ 384×384) already saturates GPU compute. Batching multiple requests would require holding them until a batch forms, adding latency without throughput gain. Instead, we use `max_ongoing_requests=2` to pipeline preparation and computation.\n",
    "\n",
    "![GPU Utilization](assets/gpu_utilization.png)\n",
    "\n",
    "**Why [`asyncio.to_thread`](https://docs.python.org/3/library/asyncio-task.html#asyncio.to_thread)?** Ray Serve deployments run in an async event loop. The `encode_frames` method is CPU/GPU-bound (PyTorch inference), which would block the event loop and prevent concurrent request handling. Wrapping it in `asyncio.to_thread` offloads the blocking work to a thread pool, keeping the event loop free to accept new requests.\n",
    "\n",
    "**Why [`ray.put`](https://docs.ray.io/en/latest/ray-core/objects.html)?** Embeddings are large numpy arrays (~75KB per chunk). Without `ray.put`, the encoder would return raw arrays to `VideoAnalyzer`, which deserializes them—only to pass them to the decoder, requiring another serialize/deserialize cycle. With `ray.put`, the encoder stores arrays in the [object store](https://docs.ray.io/en/latest/ray-core/objects.html#objects-in-ray) and returns lightweight references. `VideoAnalyzer` just forwards the references (no deserialization), and the decoder calls `ray.get` once to retrieve the data—zero-copy if on the same node.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(generate_text_embeddings pid=2379, ip=10.0.25.139)\u001b[0m Uploaded: s3://abrar-test-bucket-123/anyscale-example/embeddings/tag_embeddings.npz, s3://abrar-test-bucket-123/anyscale-example/embeddings/description_embeddings.npz\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import numpy as np\n",
    "import ray\n",
    "import torch\n",
    "from ray import serve\n",
    "from transformers import AutoModel, AutoProcessor\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "@serve.deployment(\n",
    "    num_replicas=\"auto\",\n",
    "    ray_actor_options={\"num_gpus\": 1, \"num_cpus\": 2},\n",
    "    max_ongoing_requests=2,\n",
    "    autoscaling_config={\n",
    "        \"min_replicas\": 1,\n",
    "        \"max_replicas\": 10,\n",
    "        \"target_num_ongoing_requests\": 2,\n",
    "    },\n",
    ")\n",
    "class VideoEncoder:\n",
    "    \"\"\"Encodes video frames into embeddings using SigLIP.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "        # Load SigLIP model and processor\n",
    "        self.processor = AutoProcessor.from_pretrained(MODEL_NAME)\n",
    "        self.model = AutoModel.from_pretrained(MODEL_NAME).to(self.device)\n",
    "        self.model.eval()\n",
    "\n",
    "        self.embedding_dim = self.model.config.vision_config.hidden_size\n",
    "\n",
    "    def encode_frames(self, frames: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Encode frames and return L2-normalized embeddings.\"\"\"\n",
    "        # Convert numpy frames to PIL images\n",
    "        pil_images = [Image.fromarray(frame) for frame in frames]\n",
    "\n",
    "        # Process images\n",
    "        inputs = self.processor(images=pil_images, return_tensors=\"pt\").to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            with torch.amp.autocast(device_type=self.device, enabled=self.device == \"cuda\"):\n",
    "                outputs = self.model.get_image_features(**inputs)\n",
    "                # L2 normalize\n",
    "                frame_embeddings = torch.nn.functional.normalize(outputs, p=2, dim=1)\n",
    "\n",
    "        return frame_embeddings.cpu().numpy().astype(np.float32)\n",
    "\n",
    "    async def __call__(self, frames: np.ndarray) -> dict:\n",
    "        \"\"\"Process frames and return embeddings reference.\"\"\"\n",
    "        frame_embeddings = await asyncio.to_thread(self.encode_frames, frames)\n",
    "\n",
    "        # Store embeddings in object store to avoid serialization\n",
    "        frame_embeddings_ref = ray.put(frame_embeddings)\n",
    "\n",
    "        return {\n",
    "            \"frame_embeddings_ref\": frame_embeddings_ref,\n",
    "            \"embedding_dim\": self.embedding_dim,\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CPU decoder\n",
    "\n",
    "The `MultiDecoder` deployment runs on CPU and performs three tasks:\n",
    "\n",
    "1. **Tag classification**: [Cosine similarity](https://en.wikipedia.org/wiki/Cosine_similarity) between video embedding and precomputed tag embeddings\n",
    "2. **Caption retrieval**: Find the best-matching description from a text bank\n",
    "3. **Scene detection**: EMA-based anomaly detection comparing each frame to recent history\n",
    "\n",
    "The decoder loads precomputed text embeddings from S3 at startup.\n",
    "\n",
    "**Why separate GPU/CPU deployments?** The encoder needs GPU for neural network inference; the decoder only does numpy dot products. Separating them allows independent scaling—encoders are limited by GPU count, decoders scale cheaply with CPU cores. This avoids tying expensive GPU resources to lightweight CPU work.\n",
    "\n",
    "**Why EMA for scene detection?** Exponential Moving Average reuses existing SigLIP embeddings without an extra model. The algorithm computes `score = 1 - cosine(frame, EMA)` where EMA updates as `ema = 0.9*ema + 0.1*frame`. EMA state persists across chunks, making it streaming-compatible. A simple threshold (`score > 0.15`) detects abrupt scene changes while smoothing noise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "\n",
    "import aioboto3\n",
    "import numpy as np\n",
    "import ray\n",
    "from ray import serve\n",
    "\n",
    "SCENE_CHANGE_THRESHOLD = 0.15\n",
    "EMA_ALPHA = 0.9\n",
    "\n",
    "\n",
    "@serve.deployment(\n",
    "    num_replicas=\"auto\",\n",
    "    ray_actor_options={\"num_cpus\": 1},\n",
    "    max_ongoing_requests=4,\n",
    "    autoscaling_config={\n",
    "        \"min_replicas\": 1,\n",
    "        \"max_replicas\": 10,\n",
    "        \"target_num_ongoing_requests\": 2,\n",
    "    },\n",
    ")\n",
    "class MultiDecoder:\n",
    "    \"\"\"Decodes video embeddings into tags, captions, and scene changes.\"\"\"\n",
    "\n",
    "    async def __init__(self):\n",
    "        self.bucket = S3_BUCKET\n",
    "        self.ema_alpha = EMA_ALPHA\n",
    "        self.scene_threshold = SCENE_CHANGE_THRESHOLD\n",
    "\n",
    "        # Per-stream EMA state for scene detection\n",
    "        self.stream_ema: dict[str, np.ndarray] = {}\n",
    "\n",
    "        await self._load_embeddings()\n",
    "\n",
    "    async def _load_embeddings(self):\n",
    "        \"\"\"Load precomputed text embeddings from S3.\"\"\"\n",
    "        session = aioboto3.Session(region_name=S3_REGION)\n",
    "\n",
    "        async with session.client(\"s3\") as s3:\n",
    "            # Load tag embeddings\n",
    "            tag_key = f\"{S3_EMBEDDINGS_PREFIX}tag_embeddings.npz\"\n",
    "            response = await s3.get_object(Bucket=self.bucket, Key=tag_key)\n",
    "            tag_data = await response[\"Body\"].read()\n",
    "            tag_npz = np.load(io.BytesIO(tag_data), allow_pickle=True)\n",
    "            self.tag_embeddings = tag_npz[\"embeddings\"]\n",
    "            self.tag_texts = tag_npz[\"texts\"].tolist()\n",
    "\n",
    "            # Load description embeddings\n",
    "            desc_key = f\"{S3_EMBEDDINGS_PREFIX}description_embeddings.npz\"\n",
    "            response = await s3.get_object(Bucket=self.bucket, Key=desc_key)\n",
    "            desc_data = await response[\"Body\"].read()\n",
    "            desc_npz = np.load(io.BytesIO(desc_data), allow_pickle=True)\n",
    "            self.desc_embeddings = desc_npz[\"embeddings\"]\n",
    "            self.desc_texts = desc_npz[\"texts\"].tolist()\n",
    "\n",
    "    def _cosine_similarity(self, embedding: np.ndarray, bank: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Compute cosine similarity between embedding and all vectors in bank.\"\"\"\n",
    "        return bank @ embedding\n",
    "\n",
    "    def _get_top_tags(self, embedding: np.ndarray, top_k: int = 5) -> list[dict]:\n",
    "        \"\"\"Get top-k matching tags with scores.\"\"\"\n",
    "        scores = self._cosine_similarity(embedding, self.tag_embeddings)\n",
    "        top_indices = np.argsort(scores)[::-1][:top_k]\n",
    "        return [{\"text\": self.tag_texts[i], \"score\": float(scores[i])} for i in top_indices]\n",
    "\n",
    "    def _get_retrieval_caption(self, embedding: np.ndarray) -> dict:\n",
    "        \"\"\"Get best matching description.\"\"\"\n",
    "        scores = self._cosine_similarity(embedding, self.desc_embeddings)\n",
    "        best_idx = np.argmax(scores)\n",
    "        return {\"text\": self.desc_texts[best_idx], \"score\": float(scores[best_idx])}\n",
    "\n",
    "    def _detect_scene_changes(\n",
    "        self,\n",
    "        frame_embeddings: np.ndarray,\n",
    "        stream_id: str,\n",
    "        chunk_index: int,\n",
    "        chunk_start_time: float,\n",
    "        chunk_duration: float,\n",
    "    ) -> list[dict]:\n",
    "        \"\"\"Detect scene changes using EMA-based scoring.\"\"\"\n",
    "        num_frames = len(frame_embeddings)\n",
    "        if num_frames == 0:\n",
    "            return []\n",
    "\n",
    "        if stream_id not in self.stream_ema:\n",
    "            self.stream_ema[stream_id] = frame_embeddings[0].copy()\n",
    "\n",
    "        ema = self.stream_ema[stream_id]\n",
    "        scene_changes = []\n",
    "\n",
    "        for frame_idx, embedding in enumerate(frame_embeddings):\n",
    "            similarity = float(np.dot(embedding, ema))\n",
    "            score = max(0.0, 1.0 - similarity)\n",
    "\n",
    "            if score >= self.scene_threshold:\n",
    "                frame_offset = (frame_idx / max(1, num_frames - 1)) * chunk_duration\n",
    "                timestamp = chunk_start_time + frame_offset\n",
    "                scene_changes.append({\n",
    "                    \"timestamp\": round(timestamp, 3),\n",
    "                    \"score\": round(score, 4),\n",
    "                    \"chunk_index\": chunk_index,\n",
    "                    \"frame_index\": frame_idx,\n",
    "                })\n",
    "\n",
    "            ema = self.ema_alpha * ema + (1 - self.ema_alpha) * embedding\n",
    "            ema = ema / np.linalg.norm(ema)\n",
    "\n",
    "        self.stream_ema[stream_id] = ema\n",
    "        return scene_changes\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        encoder_output: dict,\n",
    "        stream_id: str,\n",
    "        chunk_index: int,\n",
    "        chunk_start_time: float,\n",
    "        chunk_duration: float,\n",
    "        top_k_tags: int = 5,\n",
    "        end_stream: bool = False,\n",
    "    ) -> dict:\n",
    "        \"\"\"Decode embeddings into tags, caption, and scene changes.\"\"\"\n",
    "        frame_embeddings = ray.get(encoder_output[\"frame_embeddings_ref\"])\n",
    "\n",
    "        pooled_embedding = frame_embeddings.mean(axis=0)\n",
    "        pooled_embedding = pooled_embedding / np.linalg.norm(pooled_embedding)\n",
    "\n",
    "        tags = self._get_top_tags(pooled_embedding, top_k=top_k_tags)\n",
    "        caption = self._get_retrieval_caption(pooled_embedding)\n",
    "        scene_changes = self._detect_scene_changes(\n",
    "            frame_embeddings, stream_id, chunk_index, chunk_start_time, chunk_duration\n",
    "        )\n",
    "\n",
    "        if end_stream and stream_id in self.stream_ema:\n",
    "            del self.stream_ema[stream_id]\n",
    "\n",
    "        return {\"tags\": tags, \"retrieval_caption\": caption, \"scene_changes\": scene_changes}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video chunking\n",
    "\n",
    "Before we can process a video, we need to split it into fixed-duration chunks and extract frames. The chunking process:\n",
    "\n",
    "1. **Get video duration** using [`ffprobe`](https://ffmpeg.org/ffprobe.html)\n",
    "2. **Define chunk boundaries** (e.g., 0-10s, 10-20s, 20-30s for a 30s video)\n",
    "3. **Extract frames in parallel** using multiple concurrent [`ffmpeg`](https://ffmpeg.org/) processes\n",
    "4. **Limit concurrency** with [`asyncio.Semaphore`](https://docs.python.org/3/library/asyncio-sync.html#asyncio.Semaphore) to avoid CPU oversubscription\n",
    "\n",
    "Each chunk extracts 16 frames uniformly sampled across its duration, resized to 384×384 (matching SigLIP's input size).\n",
    "\n",
    "#### Design choices\n",
    "\n",
    "**Direct S3 download vs presigned URLs**: We download the video to local disk before processing. An alternative is generating a presigned S3 URL and passing it directly to FFmpeg. Benchmarks show direct download is faster—FFmpeg's HTTP client doesn't handle S3's chunked responses as efficiently as `aioboto3`, and network latency compounds across multiple seeks.\n",
    "\n",
    "![Presigned vs Direct S3](assets/presigned_vs_direct_s3.png)\n",
    "\n",
    "**Single FFmpeg vs parallel FFmpeg**: Two approaches for extracting frames from multiple chunks:\n",
    "- **Single FFmpeg**: One process reads the entire video, using `select` filter to pick frames at specific timestamps\n",
    "- **Parallel FFmpeg**: Multiple concurrent processes, each extracting one chunk\n",
    "\n",
    "Parallel wins for local files (better CPU utilization).\n",
    "\n",
    "![Single vs Multi FFmpeg](assets/single_vs_multi_ffmpeg.png)\n",
    "\n",
    "**Sequential vs parallel chunk processing**: Even with parallel FFmpeg, we limit concurrency with `asyncio.Semaphore(NUM_WORKERS)`. Too many concurrent FFmpeg processes thrash CPU and memory. Benchmarks show 3-4 workers is optimal.\n",
    "\n",
    "![Sync vs Async Comparison](assets/sync_vs_async_comparison.png)\n",
    "\n",
    "**Chunk duration**: We use 10-second chunks. Shorter chunks increase overhead (more FFmpeg calls, more encoder/decoder round-trips). Longer chunks increases processing efficiency but **degrade inference quality**—SigLIP processes exactly 16 frames per chunk, so a 60-second chunk samples one frame every 3.75 seconds, missing fast scene changes. The 10-second sweet spot balances throughput with temporal resolution (~1.6 fps sampling).\n",
    "\n",
    "![Chunk Video Analysis](assets/chunk_video_analysis.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import json\n",
    "import subprocess\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "NUM_WORKERS = 3  # Max concurrent ffmpeg processes\n",
    "FFMPEG_THREADS = 2  # Threads per ffmpeg process\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class VideoChunk:\n",
    "    index: int\n",
    "    start_time: float\n",
    "    duration: float\n",
    "    frames: Optional[np.ndarray] = None\n",
    "\n",
    "\n",
    "def get_video_metadata(video_path: str) -> dict:\n",
    "    \"\"\"Get video duration using ffprobe.\"\"\"\n",
    "    cmd = [\"ffprobe\", \"-v\", \"error\", \"-show_entries\", \"format=duration\", \"-of\", \"json\", video_path]\n",
    "    result = subprocess.run(cmd, capture_output=True, text=True, check=True)\n",
    "    return {\"duration\": float(json.loads(result.stdout)[\"format\"][\"duration\"])}\n",
    "\n",
    "\n",
    "def extract_frames_ffmpeg(\n",
    "    video_path: str, start: float, duration: float, num_frames: int, size: int = 384, threads: int = 0\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Extract frames from a video segment using ffmpeg.\"\"\"\n",
    "    fps = num_frames / duration if duration > 0 else num_frames\n",
    "    cmd = [\n",
    "        \"ffmpeg\", \"-threads\", str(threads),\n",
    "        \"-ss\", str(start), \"-t\", str(duration), \"-i\", video_path,\n",
    "        \"-vf\", f\"fps={fps},scale={size}:{size}\", \"-pix_fmt\", \"rgb24\", \"-f\", \"rawvideo\", \"-\",\n",
    "    ]\n",
    "    result = subprocess.run(cmd, capture_output=True, check=True)\n",
    "    frame_size = size * size * 3\n",
    "    frames = np.frombuffer(result.stdout, dtype=np.uint8).reshape(-1, size, size, 3)\n",
    "    # Pad if fewer frames than requested\n",
    "    if len(frames) < num_frames:\n",
    "        padding = np.tile(frames[-1:], (num_frames - len(frames), 1, 1, 1))\n",
    "        frames = np.concatenate([frames, padding])\n",
    "    return frames[:num_frames]\n",
    "\n",
    "\n",
    "async def chunk_video_async(\n",
    "    video_path: str, chunk_duration: float, num_frames: int, target_size: int = 384\n",
    ") -> list[VideoChunk]:\n",
    "    \"\"\"Split video into chunks with PARALLEL frame extraction.\"\"\"\n",
    "    metadata = await asyncio.to_thread(get_video_metadata, video_path)\n",
    "\n",
    "    # Build chunk definitions\n",
    "    chunk_defs = []\n",
    "    start = 0.0\n",
    "    idx = 0\n",
    "    while start < metadata[\"duration\"]:\n",
    "        dur = min(chunk_duration, metadata[\"duration\"] - start)\n",
    "        if dur < 0.5:\n",
    "            break\n",
    "        chunk_defs.append((idx, start, dur))\n",
    "        start += chunk_duration\n",
    "        idx += 1\n",
    "\n",
    "    if not chunk_defs:\n",
    "        return []\n",
    "\n",
    "    # Extract frames in PARALLEL, limited by semaphore\n",
    "    semaphore = asyncio.Semaphore(NUM_WORKERS)\n",
    "\n",
    "    async def extract_with_limit(idx, start, duration):\n",
    "        async with semaphore:\n",
    "            return await asyncio.to_thread(\n",
    "                extract_frames_ffmpeg, video_path, start, duration, num_frames, target_size, FFMPEG_THREADS\n",
    "            )\n",
    "\n",
    "    frame_results = await asyncio.gather(*[extract_with_limit(*c) for c in chunk_defs])\n",
    "\n",
    "    return [\n",
    "        VideoChunk(index=idx, start_time=start, duration=dur, frames=frames)\n",
    "        for (idx, start, dur), frames in zip(chunk_defs, frame_results)\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deployment composition\n",
    "\n",
    "The `VideoAnalyzer` ingress deployment orchestrates the encoder and decoder. It uses [FastAPI](https://fastapi.tiangolo.com/) integration with [`@serve.ingress`](https://docs.ray.io/en/latest/serve/http-guide.html#fastapi-http-deployments) for HTTP endpoints.\n",
    "\n",
    "Deployments receive handles to other deployments through constructor injection using `.bind()`:\n",
    "\n",
    "```python\n",
    "encoder = VideoEncoder.bind()\n",
    "decoder = MultiDecoder.bind()\n",
    "app = VideoAnalyzer.bind(encoder=encoder, decoder=decoder)\n",
    "```\n",
    "\n",
    "#### Design choices\n",
    "\n",
    "**Why `num_cpus=6`?** The analyzer runs FFmpeg for frame extraction. Each FFmpeg process uses `FFMPEG_THREADS=2`, and we run up to `NUM_WORKERS=3` concurrent processes. So `2 × 3 = 6` CPUs ensures FFmpeg doesn't contend for CPU during parallel chunk extraction.\n",
    "\n",
    "**Why `max_ongoing_requests=4`?** The encoder has `max_ongoing_requests=2`. We want the analyzer to stay ahead: while 2 videos are encoding, we download and chunk 2 more videos so they're ready when encoder slots free up. This keeps the GPU pipeline saturated without excessive memory from queued requests.\n",
    "\n",
    "**Why cache the S3 client?** Creating a new `aioboto3` client per request adds overhead (connection setup, credential resolution). Caching the client in `__init__` and reusing it across requests amortizes this cost. The client is thread-safe and handles connection pooling internally.\n",
    "\n",
    "**Why encode in parallel but decode serially?** Encoding is stateless—each chunk's frames go through SigLIP independently, so we fire all chunks concurrently with [`asyncio.gather`](https://docs.python.org/3/library/asyncio-task.html#asyncio.gather). Decoding is stateful—the EMA for scene detection must process chunks in order (chunk 0's EMA state feeds into chunk 1). Serial decoding preserves this temporal dependency.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class AnalyzeRequest(BaseModel):\n",
    "    stream_id: str\n",
    "    video_path: str  # S3 URI: s3://bucket/key\n",
    "    num_frames: int = 16\n",
    "    chunk_duration: float = 10.0\n",
    "\n",
    "\n",
    "# Response models\n",
    "class TagScore(BaseModel):\n",
    "    text: str\n",
    "    score: float\n",
    "\n",
    "\n",
    "class Caption(BaseModel):\n",
    "    text: str\n",
    "    score: float\n",
    "\n",
    "\n",
    "class SceneChange(BaseModel):\n",
    "    timestamp: float\n",
    "    score: float\n",
    "    chunk_index: int\n",
    "    frame_index: int\n",
    "\n",
    "\n",
    "class TimingMs(BaseModel):\n",
    "    s3_download_ms: float\n",
    "    decode_video_ms: float\n",
    "    encode_ms: float\n",
    "    decode_ms: float\n",
    "    total_ms: float\n",
    "\n",
    "\n",
    "class ChunkResult(BaseModel):\n",
    "    chunk_index: int\n",
    "    start_time: float\n",
    "    duration: float\n",
    "    tags: list[TagScore]\n",
    "    retrieval_caption: Caption\n",
    "    scene_changes: list[SceneChange]\n",
    "\n",
    "\n",
    "class AnalyzeResponse(BaseModel):\n",
    "    stream_id: str\n",
    "    embedding_dim: int\n",
    "    tags: list[TagScore]\n",
    "    retrieval_caption: Caption\n",
    "    scene_changes: list[SceneChange]\n",
    "    num_scene_changes: int\n",
    "    chunks: list[ChunkResult]\n",
    "    num_chunks: int\n",
    "    video_duration: float\n",
    "    timing_ms: TimingMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import tempfile\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "import aioboto3\n",
    "import numpy as np\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from ray import serve\n",
    "\n",
    "fastapi_app = FastAPI(title=\"Video Embedding API\")\n",
    "\n",
    "\n",
    "# --- VideoAnalyzer deployment ---\n",
    "\n",
    "@serve.deployment(\n",
    "    num_replicas=\"auto\",\n",
    "    ray_actor_options={\"num_cpus\": 6},\n",
    "    max_ongoing_requests=4,\n",
    "    autoscaling_config={\n",
    "        \"min_replicas\": 2,\n",
    "        \"max_replicas\": 20,\n",
    "        \"target_num_ongoing_requests\": 2,\n",
    "    },\n",
    ")\n",
    "@serve.ingress(fastapi_app)\n",
    "class VideoAnalyzer:\n",
    "    \"\"\"Ingress deployment that orchestrates VideoEncoder and MultiDecoder.\"\"\"\n",
    "\n",
    "    def __init__(self, encoder, decoder):\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self._s3_session = aioboto3.Session(region_name=S3_REGION)\n",
    "        self._s3_client = None  # Cached client for reuse across requests\n",
    "\n",
    "    async def _get_s3_client(self):\n",
    "        \"\"\"Get or create a reusable S3 client.\"\"\"\n",
    "        if self._s3_client is None:\n",
    "            self._s3_client = await self._s3_session.client(\"s3\").__aenter__()\n",
    "        return self._s3_client\n",
    "\n",
    "    async def _download_video(self, s3_uri: str) -> Path:\n",
    "        \"\"\"Download video from S3 to temp file.\"\"\"\n",
    "        parsed = urlparse(s3_uri)\n",
    "        bucket, key = parsed.netloc, parsed.path.lstrip(\"/\")\n",
    "        suffix = Path(key).suffix or \".mp4\"\n",
    "        temp_file = tempfile.NamedTemporaryFile(suffix=suffix, delete=False)\n",
    "        temp_path = Path(temp_file.name)\n",
    "        temp_file.close()\n",
    "\n",
    "        s3 = await self._get_s3_client()\n",
    "        await s3.download_file(bucket, key, str(temp_path))\n",
    "        return temp_path\n",
    "\n",
    "    def _aggregate_results(self, chunk_results: list[dict], top_k: int = 5) -> dict:\n",
    "        \"\"\"Aggregate tags and caption across chunks.\"\"\"\n",
    "        tag_scores = defaultdict(list)\n",
    "        for r in chunk_results:\n",
    "            for tag in r[\"tags\"]:\n",
    "                tag_scores[tag[\"text\"]].append(tag[\"score\"])\n",
    "        agg_tags = sorted(\n",
    "            [{\"text\": t, \"score\": np.mean(s)} for t, s in tag_scores.items()],\n",
    "            key=lambda x: x[\"score\"], reverse=True\n",
    "        )[:top_k]\n",
    "        best_caption = max((r[\"retrieval_caption\"] for r in chunk_results), key=lambda x: x[\"score\"])\n",
    "        return {\"tags\": agg_tags, \"retrieval_caption\": best_caption}\n",
    "\n",
    "    @fastapi_app.post(\"/analyze\", response_model=AnalyzeResponse)\n",
    "    async def analyze(self, request: AnalyzeRequest) -> AnalyzeResponse:\n",
    "        \"\"\"Analyze a video from S3.\"\"\"\n",
    "        total_start = time.perf_counter()\n",
    "        temp_path = None\n",
    "\n",
    "        try:\n",
    "            # Download video\n",
    "            download_start = time.perf_counter()\n",
    "            temp_path = await self._download_video(request.video_path)\n",
    "            s3_download_ms = (time.perf_counter() - download_start) * 1000\n",
    "\n",
    "            # Chunk video (parallel frame extraction)\n",
    "            decode_start = time.perf_counter()\n",
    "            chunks = await chunk_video_async(str(temp_path), request.chunk_duration, request.num_frames)\n",
    "            decode_video_ms = (time.perf_counter() - decode_start) * 1000\n",
    "\n",
    "            if not chunks:\n",
    "                raise HTTPException(status_code=400, detail=\"No chunks extracted\")\n",
    "\n",
    "            video_duration = chunks[-1].start_time + chunks[-1].duration\n",
    "\n",
    "            # Encode all chunks CONCURRENTLY\n",
    "            encode_start = time.perf_counter()\n",
    "            encode_results = await asyncio.gather(*[self.encoder.remote(c.frames) for c in chunks])\n",
    "            encode_ms = (time.perf_counter() - encode_start) * 1000\n",
    "\n",
    "            embedding_dim = encode_results[0][\"embedding_dim\"] if encode_results else 0\n",
    "\n",
    "            # Decode chunks SERIALLY (EMA state)\n",
    "            decode_start = time.perf_counter()\n",
    "            decode_results = []\n",
    "            for i, (chunk, enc) in enumerate(zip(chunks, encode_results)):\n",
    "                dec = await self.decoder.remote(\n",
    "                    encoder_output=enc,\n",
    "                    stream_id=request.stream_id,\n",
    "                    chunk_index=chunk.index,\n",
    "                    chunk_start_time=chunk.start_time,\n",
    "                    chunk_duration=chunk.duration,\n",
    "                    end_stream=(i == len(chunks) - 1),\n",
    "                )\n",
    "                decode_results.append(dec)\n",
    "            decode_ms = (time.perf_counter() - decode_start) * 1000\n",
    "\n",
    "            # Aggregate and build response\n",
    "            aggregated = self._aggregate_results(decode_results)\n",
    "            all_scene_changes = [\n",
    "                SceneChange(**sc) for r in decode_results for sc in r[\"scene_changes\"]\n",
    "            ]\n",
    "\n",
    "            per_chunk = [\n",
    "                ChunkResult(\n",
    "                    chunk_index=c.index,\n",
    "                    start_time=c.start_time,\n",
    "                    duration=c.duration,\n",
    "                    tags=[TagScore(**t) for t in r[\"tags\"]],\n",
    "                    retrieval_caption=Caption(**r[\"retrieval_caption\"]),\n",
    "                    scene_changes=[SceneChange(**sc) for sc in r[\"scene_changes\"]],\n",
    "                )\n",
    "                for c, r in zip(chunks, decode_results)\n",
    "            ]\n",
    "\n",
    "            total_ms = (time.perf_counter() - total_start) * 1000\n",
    "\n",
    "            return AnalyzeResponse(\n",
    "                stream_id=request.stream_id,\n",
    "                embedding_dim=embedding_dim,\n",
    "                tags=[TagScore(**t) for t in aggregated[\"tags\"]],\n",
    "                retrieval_caption=Caption(**aggregated[\"retrieval_caption\"]),\n",
    "                scene_changes=all_scene_changes,\n",
    "                num_scene_changes=len(all_scene_changes),\n",
    "                chunks=per_chunk,\n",
    "                num_chunks=len(chunks),\n",
    "                video_duration=video_duration,\n",
    "                timing_ms=TimingMs(\n",
    "                    s3_download_ms=round(s3_download_ms, 2),\n",
    "                    decode_video_ms=round(decode_video_ms, 2),\n",
    "                    encode_ms=round(encode_ms, 2),\n",
    "                    decode_ms=round(decode_ms, 2),\n",
    "                    total_ms=round(total_ms, 2),\n",
    "                ),\n",
    "            )\n",
    "        finally:\n",
    "            if temp_path and temp_path.exists():\n",
    "                temp_path.unlink(missing_ok=True)\n",
    "\n",
    "    @fastapi_app.get(\"/health\")\n",
    "    async def health(self):\n",
    "        return {\"status\": \"healthy\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = VideoEncoder.bind()\n",
    "decoder = MultiDecoder.bind()\n",
    "app = VideoAnalyzer.bind(encoder=encoder, decoder=decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO 2026-01-04 04:05:04,480 serve 135398 -- Connecting to existing Serve app in namespace \"serve\". New http options will not be applied.\n",
      "\u001b[36m(ServeController pid=134537)\u001b[0m INFO 2026-01-04 04:05:04,591 controller 134537 -- Registering autoscaling state for deployment Deployment(name='VideoEncoder', app='video-analyzer')\n",
      "\u001b[36m(ServeController pid=134537)\u001b[0m INFO 2026-01-04 04:05:04,592 controller 134537 -- Deploying new version of Deployment(name='VideoEncoder', app='video-analyzer') (initial target replicas: 1).\n",
      "\u001b[36m(ServeController pid=134537)\u001b[0m INFO 2026-01-04 04:05:04,593 controller 134537 -- Registering autoscaling state for deployment Deployment(name='MultiDecoder', app='video-analyzer')\n",
      "\u001b[36m(ServeController pid=134537)\u001b[0m INFO 2026-01-04 04:05:04,594 controller 134537 -- Deploying new version of Deployment(name='MultiDecoder', app='video-analyzer') (initial target replicas: 1).\n",
      "\u001b[36m(ServeController pid=134537)\u001b[0m INFO 2026-01-04 04:05:04,594 controller 134537 -- Registering autoscaling state for deployment Deployment(name='VideoAnalyzer', app='video-analyzer')\n",
      "\u001b[36m(ServeController pid=134537)\u001b[0m INFO 2026-01-04 04:05:04,595 controller 134537 -- Deploying new version of Deployment(name='VideoAnalyzer', app='video-analyzer') (initial target replicas: 2).\n",
      "\u001b[36m(ServeController pid=134537)\u001b[0m INFO 2026-01-04 04:05:04,700 controller 134537 -- Adding 1 replica to Deployment(name='VideoEncoder', app='video-analyzer').\n",
      "\u001b[36m(ServeController pid=134537)\u001b[0m INFO 2026-01-04 04:05:04,702 controller 134537 -- Adding 1 replica to Deployment(name='MultiDecoder', app='video-analyzer').\n",
      "\u001b[36m(ServeController pid=134537)\u001b[0m INFO 2026-01-04 04:05:04,706 controller 134537 -- Stopping 1 replicas of Deployment(name='VideoAnalyzer', app='video-analyzer') with outdated versions.\n",
      "\u001b[36m(ServeController pid=134537)\u001b[0m INFO 2026-01-04 04:05:04,706 controller 134537 -- Adding 1 replica to Deployment(name='VideoAnalyzer', app='video-analyzer').\n",
      "\u001b[36m(ServeController pid=134537)\u001b[0m INFO 2026-01-04 04:05:06,770 controller 134537 -- Replica(id='ndldcpkc', deployment='VideoAnalyzer', app='video-analyzer') is stopped.\n",
      "\u001b[36m(ServeController pid=134537)\u001b[0m INFO 2026-01-04 04:05:08,135 controller 134537 -- Stopping 1 replicas of Deployment(name='VideoAnalyzer', app='video-analyzer') with outdated versions.\n",
      "\u001b[36m(ServeController pid=134537)\u001b[0m INFO 2026-01-04 04:05:08,135 controller 134537 -- Adding 1 replica to Deployment(name='VideoAnalyzer', app='video-analyzer').\n",
      "\u001b[36m(ServeController pid=134537)\u001b[0m INFO 2026-01-04 04:05:10,142 controller 134537 -- Replica(id='ie8o7dry', deployment='VideoAnalyzer', app='video-analyzer') is stopped.\n",
      "\u001b[36m(ServeReplica:video-analyzer:VideoEncoder pid=2548, ip=10.0.25.139)\u001b[0m Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(autoscaler +32s)\u001b[0m [autoscaler] [16CPU-64GB] Attempting to add 1 node to the cluster (increasing from 1 to 2).\n",
      "\u001b[36m(autoscaler +32s)\u001b[0m [autoscaler] [16CPU-64GB|m8i.4xlarge] [us-west-2a] [on-demand] Launched 1 instance.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ProxyActor pid=2607, ip=10.0.25.139)\u001b[0m INFO 2026-01-04 04:05:11,437 proxy 10.0.25.139 -- Proxy starting on node d4cb8459fbe780df3989d0b4736d9e58b3f58e690fc0f0fead60c58d (HTTP port: 8000).\n",
      "\u001b[36m(ProxyActor pid=2607, ip=10.0.25.139)\u001b[0m INFO 2026-01-04 04:05:11,551 proxy 10.0.25.139 -- Got updated endpoints: {Deployment(name='VideoAnalyzer', app='video-analyzer'): EndpointInfo(route='/', app_is_cross_language=False, route_patterns=[RoutePattern(methods=['POST'], path='/analyze'), RoutePattern(methods=['GET', 'HEAD'], path='/docs'), RoutePattern(methods=['GET', 'HEAD'], path='/docs/oauth2-redirect'), RoutePattern(methods=['GET'], path='/health'), RoutePattern(methods=['GET', 'HEAD'], path='/openapi.json'), RoutePattern(methods=['GET', 'HEAD'], path='/redoc')])}.\n",
      "\u001b[36m(ProxyActor pid=2607, ip=10.0.25.139)\u001b[0m INFO 2026-01-04 04:05:11,565 proxy 10.0.25.139 -- Started <ray.serve._private.router.SharedRouterLongPollClient object at 0x7afd189d4080>.\n",
      "INFO 2026-01-04 04:05:13,515 serve 135398 -- Application 'video-analyzer' is ready at http://0.0.0.0:8000/.\n",
      "INFO 2026-01-04 04:05:13,528 serve 135398 -- Started <ray.serve._private.router.SharedRouterLongPollClient object at 0x73478063f6b0>.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DeploymentHandle(deployment='VideoAnalyzer')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "serve.run(app, name=\"video-analyzer\", route_prefix=\"/\", blocking=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for deployment to be healthy...\n",
      "Deployment ready in 0.0s\n"
     ]
    }
   ],
   "source": [
    "import httpx\n",
    "import time\n",
    "\n",
    "# Wait for deployment to be ready\n",
    "print(\"Waiting for deployment to be healthy...\")\n",
    "start = time.time()\n",
    "while True:\n",
    "    try:\n",
    "        with httpx.Client(timeout=5.0) as client:\n",
    "            response = client.get(\"http://localhost:8000/health\")\n",
    "            if response.status_code == 200:\n",
    "                print(f\"Deployment ready in {time.time() - start:.1f}s\")\n",
    "                break\n",
    "    except httpx.RequestError:\n",
    "        pass\n",
    "    time.sleep(1.0)\n",
    "    if time.time() - start > 120:\n",
    "        raise TimeoutError(\"Deployment did not become healthy within 120s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing video: s3://abrar-test-bucket-123/anyscale-example/stock-videos/sample_35510474.mp4\n",
      "Stream ID: 43b5fd0e\n",
      "\n",
      "============================================================\n",
      "Video duration: 12.0s\n",
      "Chunks processed: 2\n",
      "\n",
      "Top Tags:\n",
      "  0.055  kitchen\n",
      "  0.033  cafe\n",
      "  0.015  bathroom\n",
      "  0.008  living room\n",
      "  0.007  bedroom\n",
      "\n",
      "Best Caption:\n",
      "  0.100  A person cooking in a kitchen\n",
      "\n",
      "Scene Changes: 0\n",
      "\n",
      "Timing:\n",
      "  S3 download:    200.6 ms\n",
      "  Video decode:   136.6 ms\n",
      "  Encode (GPU):   869.5 ms\n",
      "  Decode (CPU):    12.7 ms\n",
      "  Total server:  1219.5 ms\n",
      "  Round-trip:    1237.7 ms\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ServeReplica:video-analyzer:VideoEncoder pid=2548, ip=10.0.25.139)\u001b[0m INFO 2026-01-04 04:05:36,175 video-analyzer_VideoEncoder g2pni5ln 726f6fd6-771d-4626-9495-8f30bcd8143a -- CALL __call__ OK 752.4ms\n",
      "\u001b[36m(ServeReplica:video-analyzer:VideoEncoder pid=2548, ip=10.0.25.139)\u001b[0m INFO 2026-01-04 04:05:36,270 video-analyzer_VideoEncoder g2pni5ln 726f6fd6-771d-4626-9495-8f30bcd8143a -- CALL __call__ OK 836.0ms\n",
      "\u001b[36m(ServeReplica:video-analyzer:MultiDecoder pid=20831, ip=10.0.26.32)\u001b[0m INFO 2026-01-04 04:05:36,278 video-analyzer_MultiDecoder vsd0e8e9 726f6fd6-771d-4626-9495-8f30bcd8143a -- CALL __call__ OK 4.0ms\n",
      "\u001b[36m(ServeReplica:video-analyzer:MultiDecoder pid=20831, ip=10.0.26.32)\u001b[0m INFO 2026-01-04 04:05:36,283 video-analyzer_MultiDecoder vsd0e8e9 726f6fd6-771d-4626-9495-8f30bcd8143a -- CALL __call__ OK 3.1ms\n",
      "\u001b[36m(ServeReplica:video-analyzer:VideoAnalyzer pid=20921, ip=10.0.26.32)\u001b[0m INFO 2026-01-04 04:05:36,284 video-analyzer_VideoAnalyzer z5pw9lpz 726f6fd6-771d-4626-9495-8f30bcd8143a -- POST /analyze 200 1223.1ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(autoscaler +1m7s)\u001b[0m [autoscaler] Cluster upscaled to {40 CPU, 2 GPU}.\n"
     ]
    }
   ],
   "source": [
    "import httpx\n",
    "import time\n",
    "import uuid\n",
    "\n",
    "# Send a sample request to the deployed service\n",
    "payload = {\n",
    "    \"stream_id\": uuid.uuid4().hex[:8],\n",
    "    \"video_path\": SAMPLE_VIDEO_URI,\n",
    "    \"num_frames\": 16,\n",
    "    \"chunk_duration\": 10.0,\n",
    "}\n",
    "\n",
    "print(f\"Analyzing video: {SAMPLE_VIDEO_URI}\")\n",
    "print(f\"Stream ID: {payload['stream_id']}\\n\")\n",
    "\n",
    "start = time.perf_counter()\n",
    "with httpx.Client(timeout=300.0) as client:\n",
    "    response = client.post(\"http://localhost:8000/analyze\", json=payload)\n",
    "latency_ms = (time.perf_counter() - start) * 1000\n",
    "\n",
    "result = response.json()\n",
    "\n",
    "# Print results\n",
    "print(\"=\" * 60)\n",
    "print(f\"Video duration: {result['video_duration']:.1f}s\")\n",
    "print(f\"Chunks processed: {result['num_chunks']}\")\n",
    "print()\n",
    "\n",
    "print(\"Top Tags:\")\n",
    "for tag in result[\"tags\"]:\n",
    "    print(f\"  {tag['score']:.3f}  {tag['text']}\")\n",
    "print()\n",
    "\n",
    "print(\"Best Caption:\")\n",
    "print(f\"  {result['retrieval_caption']['score']:.3f}  {result['retrieval_caption']['text']}\")\n",
    "print()\n",
    "\n",
    "print(f\"Scene Changes: {result['num_scene_changes']}\")\n",
    "for sc in result[\"scene_changes\"][:5]:  # Show first 5\n",
    "    print(f\"  {sc['timestamp']:6.2f}s  score={sc['score']:.3f}\")\n",
    "print()\n",
    "\n",
    "print(\"Timing:\")\n",
    "timing = result[\"timing_ms\"]\n",
    "print(f\"  S3 download:  {timing['s3_download_ms']:7.1f} ms\")\n",
    "print(f\"  Video decode: {timing['decode_video_ms']:7.1f} ms\")\n",
    "print(f\"  Encode (GPU): {timing['encode_ms']:7.1f} ms\")\n",
    "print(f\"  Decode (CPU): {timing['decode_ms']:7.1f} ms\")\n",
    "print(f\"  Total server: {timing['total_ms']:7.1f} ms\")\n",
    "print(f\"  Round-trip:   {latency_ms:7.1f} ms\")\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Load Test Results\n",
    "\n",
    "To evaluate the pipeline's performance under realistic conditions, we ran load tests using the `client/load_test.py` script with **concurrency levels ranging from 2 to 64 concurrent requests**. The Ray Serve application was configured with autoscaling enabled, allowing replicas to scale dynamically based on demand.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methodology\n",
    "\n",
    "The load test was executed using:\n",
    "\n",
    "```bash\n",
    "python -m client.load_test --video s3://bucket/video.mp4 --concurrency <N>\n",
    "```\n",
    "\n",
    "**Test parameters:**\n",
    "- Concurrency levels: 2, 4, 8, 16, 32, 64\n",
    "- Chunk duration: 10 seconds\n",
    "- Frames per chunk: 16\n",
    "- Autoscaling: Enabled (replicas scale based on `target_num_ongoing_requests`)\n",
    "\n",
    "The charts below show autoscaling in action during the concurrency=32 and 64 load test:\n",
    "\n",
    "![Ongoing Requests](assets/ongoing_requests.png)\n",
    "\n",
    "![Replica Count](assets/replica_count.png)\n",
    "\n",
    "![Queue Length](assets/queue_len.png)\n",
    "\n",
    "As load increases, replicas scale up to maintain target queue depth. The system reaches steady state once enough replicas are provisioned to handle the request rate.\n",
    "\n",
    "To ensure fair comparison, we discarded the first half of each test run to exclude the autoscaling warm-up period where latencies are elevated due to replica initialization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results Summary\n",
    "\n",
    "| Concurrency | Requests | P50 (ms) | P95 (ms) | P99 (ms) | Throughput (req/s) |\n",
    "|-------------|----------|----------|----------|----------|-------------------|\n",
    "| 2           | 122      | 1,027    | 1,069    | 1,162    | 1.92              |\n",
    "| 4           | 236      | 1,008    | 1,346    | 1,480    | 3.75              |\n",
    "| 8           | 932      | 1,076    | 1,423    | 1,495    | 6.91              |\n",
    "| 16          | 2,612    | 1,121    | 1,424    | 1,501    | 13.69             |\n",
    "| 32          | 4,421    | 1,114    | 1,406    | 1,479    | 27.64             |\n",
    "| 64          | 16,971   | 1,129    | 1,426    | 1,493    | 54.69             |\n",
    "\n",
    "**Key findings:**\n",
    "\n",
    "- **100% success rate** across all 25,294 requests analyzed\n",
    "- **Near-linear throughput scaling**: Throughput increased from 1.92 req/s at concurrency 2 to **54.69 req/s at concurrency 64** (~28x improvement)\n",
    "- **Stable latencies under load**: P95 latency remained between 1,069ms and 1,426ms across all concurrency levels, demonstrating effective autoscaling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing Time Breakdown\n",
    "\n",
    "The chart below shows how processing time is distributed across pipeline stages at each concurrency level:\n",
    "\n",
    "![Processing Time Breakdown](assets/processing_time_breakdown.png)\n",
    "\n",
    "**At concurrency 64 (best throughput):**\n",
    "- **S3 Download**: 359ms (31%)\n",
    "- **Video Decode (FFmpeg)**: 136ms (12%)\n",
    "- **Encode (GPU)**: 651ms (56%)\n",
    "- **Decode (CPU)**: 6ms (1%)\n",
    "\n",
    "The GPU encoding stage dominates processing time, as expected for neural network inference. S3 download latency is the second largest component.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Throughput Analysis\n",
    "\n",
    "The charts below show throughput scaling and the latency-throughput tradeoff:\n",
    "\n",
    "![Throughput Analysis](assets/throughput_analysis.png)\n",
    "\n",
    "**Observations:**\n",
    "\n",
    "1. **Linear scaling**: Throughput scales almost linearly with concurrency, indicating that autoscaling successfully provisions enough replicas to handle increased load.\n",
    "\n",
    "2. **Latency-throughput tradeoff**: The right chart shows that P95 latency increases slightly as throughput grows (from ~1,069ms to ~1,426ms), but remains within acceptable bounds. This ~33% latency increase enables a ~28x throughput improvement.\n",
    "\n",
    "3. **No saturation point**: Even at concurrency 64, throughput continues to scale. The system could likely handle higher concurrency with additional GPU resources.\n",
    "\n",
    "The near-linear scaling demonstrates that the pipeline architecture—with separate GPU encoder, CPU decoder, and CPU-bound ingress—allows each component to scale independently based on its resource requirements.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
