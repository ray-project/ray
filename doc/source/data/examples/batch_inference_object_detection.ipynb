{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Image Object Detection using Ray Data\n",
    "\n",
    "In this example, we will demostrate how to use Ray Data to do batch object detection with a PyTorch model on a large set of images.\n",
    "\n",
    "## Vanilla Pyorch exmaple\n",
    "\n",
    "Let's take a look at the [vanilla PyTorch example](https://pytorch.org/vision/stable/models.html#object-detection) first. \n",
    "\n",
    "In this example, we use a pre-trained PyTorch model to detect the objects on a single image. For simplicity, we skip the result visualization in this example.\n",
    "\n",
    "```python\n",
    "from torchvision.io.image import read_image\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn_v2, FasterRCNN_ResNet50_FPN_V2_Weights\n",
    "\n",
    "img = read_image(\"/path/to/your/image.jpg\")\n",
    "\n",
    "# Step 1: Initialize model with the best available weights\n",
    "weights = FasterRCNN_ResNet50_FPN_V2_Weights.DEFAULT\n",
    "model = fasterrcnn_resnet50_fpn_v2(weights=weights, box_score_thresh=0.9)\n",
    "model.eval()\n",
    "\n",
    "# Step 2: Initialize the inference transforms\n",
    "preprocess = weights.transforms()\n",
    "\n",
    "# Step 3: Apply inference preprocessing transforms\n",
    "batch = [preprocess(img)]\n",
    "\n",
    "# Step 4: Use the model and visualize the prediction\n",
    "prediction = model(batch)[0]\n",
    "labels = [weights.meta[\"categories\"][i] for i in prediction[\"labels\"]]\n",
    "print(labels)\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Parallelizing with Ray Data\n",
    "\n",
    "Then we want to apply this object detection model to a large set of images. We can use Ray Data to scale it.\n",
    "\n",
    "### Reading Image Data \n",
    "\n",
    "First, we use `ray.data.read_images` API to read image data from S3.  The directory structure of the dataset is`<s3_url/{label_id}/{*.JPEG}>`. So we use Partitioning utility to load in all the images for all labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "from ray.data.datasource.partitioning import Partitioning\n",
    "\n",
    "s3_uri = \"s3://anonymous@air-example-data-2/imagenette2/val/\"\n",
    "\n",
    "# The S3 directory structure is {s3_uri}/{class_id}/{*.JPEG}\n",
    "partitioning = Partitioning(\"dir\", field_names=[\"class\"], base_dir=s3_uri)\n",
    "\n",
    "ds = ray.data.read_images(s3_uri, partitioning=partitioning, mode=\"RGB\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Bathces\n",
    "\n",
    "To boost performance with hardware vectorization, we usually do inference in batches. In Ray Data, a batch is by default defined as a `Dict[str, np.ndarray]`. \n",
    "\n",
    "In this case, the dict will have only one key named \"image\", and the value is an array of images represented in the `np.ndarray` format.\n",
    "\n",
    "In the following code snippet, we use the `take_batch` API to get a single batch and inspect its internal data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-09 14:23:20,380\tINFO streaming_executor.py:91 -- Executing DAG InputDataBuffer[Input] -> TaskPoolMapOperator[ReadImage]\n",
      "2023-05-09 14:23:20,381\tINFO streaming_executor.py:92 -- Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=None), locality_with_output=False, preserve_order=False, actor_locality_enabled=True, verbose_progress=False)\n",
      "2023-05-09 14:23:20,382\tINFO streaming_executor.py:94 -- Tip: For detailed progress reporting, run `ray.data.DataContext.get_current().execution_options.verbose_progress = True`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "659ce5e1ef0f4fc79c533c28cc7b9a17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running 0:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-09 14:23:29,210\tINFO streaming_executor.py:149 -- Shutting down <StreamingExecutor(Thread-6771, started daemon 13735325696)>.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4,) (480, 590, 3)\n"
     ]
    }
   ],
   "source": [
    "single_batch = ds.take_batch(batch_size=4)\n",
    "print(single_batch[\"image\"].shape, single_batch[\"image\"][0].shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch inference with Ray Data\n",
    "\n",
    "Then we will demostrate how to use Ray Data to do batch inference in a distributed cluster.\n",
    "\n",
    "The first step is to package the model in a Python class. The class is mainly consist of two parts. In the `__init__` constructor, we package the code that loads and initializes the model. And in the `__call__` method, we package the code that applies model on each batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from torchvision.models.detection import (FasterRCNN_ResNet50_FPN_V2_Weights,\n",
    "                                          fasterrcnn_resnet50_fpn_v2)\n",
    "\n",
    "\n",
    "class ObjecytDetectionModel:\n",
    "    def __init__(self):\n",
    "        # Define the model loading and initialization code in `__init__`\n",
    "        self.weights = FasterRCNN_ResNet50_FPN_V2_Weights.DEFAULT\n",
    "        self.model = fasterrcnn_resnet50_fpn_v2(\n",
    "            weights=self.weights,\n",
    "            box_score_thresh=0.9,\n",
    "        )\n",
    "        self.model.eval()\n",
    "        # Note, since the data in the batch input is `np.ndarray`s, \n",
    "        # we need `transforms.ToTensor` to convert the data to torch tensors.\n",
    "        self.prepross = transforms.Compose(\n",
    "            [transforms.ToTensor(), self.weights.transforms()]\n",
    "        )\n",
    "\n",
    "    def __call__(self, batch: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n",
    "        # Define the per-batch inference code in `__call__`\n",
    "        # Preprocess the images.\n",
    "        batch = [self.prepross(image) for image in batch[\"image\"]]\n",
    "        # Do inference on the images.\n",
    "        predictions = self.model(batch)\n",
    "        # Get the inferred labels and convert it to a np.ndarray\n",
    "        labels = np.array(\n",
    "            [\n",
    "                \",\".join(\n",
    "                    [self.weights.meta[\"categories\"][i] for i in prediction[\"labels\"]]\n",
    "                )\n",
    "                for prediction in predictions\n",
    "            ],\n",
    "            dtype=\"S\",\n",
    "        )\n",
    "        # `__call__` also returns a `Dict[str, np.ndarray]`.\n",
    "        return {\"labels\": labels}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we use the `map_batches` API to apply the model to the whole image data set. Here, the `compute` argument indicates the number of concurrent Ray actors that run the model, and the `batch_size` argument indicates the number of images in each batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.map_batches(\n",
    "    ObjecytDetectionModel,\n",
    "    compute=ray.data.ActorPoolStrategy(size=4),\n",
    "    batch_size=4,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we can use `ds.show` to inspect samples in the result, or the the `ds.write_xxx` APIS to write results to external storages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-09 14:23:31,331\tINFO streaming_executor.py:91 -- Executing DAG InputDataBuffer[Input] -> ActorPoolMapOperator[ReadImage->MapBatches(ObjecytDetectionModel)]\n",
      "2023-05-09 14:23:31,332\tINFO streaming_executor.py:92 -- Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=None), locality_with_output=False, preserve_order=False, actor_locality_enabled=True, verbose_progress=False)\n",
      "2023-05-09 14:23:31,332\tINFO streaming_executor.py:94 -- Tip: For detailed progress reporting, run `ray.data.DataContext.get_current().execution_options.verbose_progress = True`\n",
      "2023-05-09 14:23:31,346\tINFO actor_pool_map_operator.py:114 -- ReadImage->MapBatches(ObjecytDetectionModel): Waiting for 4 pool actors to start...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38b198e766394b6ca0e2101cc0393e7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running 0:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-09 14:27:58,835\tINFO streaming_executor.py:149 -- Shutting down <StreamingExecutor(Thread-6787, started daemon 13735325696)>.\n",
      "2023-05-09 14:27:58,997\tWARNING actor_pool_map_operator.py:264 -- To ensure full parallelization across an actor pool of size 4, the specified batch size should be at most 0. Your configured batch size for this operator was 16.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m ds\u001b[39m.\u001b[39;49mshow(\u001b[39m4\u001b[39;49m)\n",
      "File \u001b[0;32m~/code/ray/python/ray/data/datastream.py:2132\u001b[0m, in \u001b[0;36mDatastream.show\u001b[0;34m(self, limit)\u001b[0m\n\u001b[1;32m   2123\u001b[0m \u001b[39m@ConsumptionAPI\u001b[39m(pattern\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTime complexity:\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   2124\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mshow\u001b[39m(\u001b[39mself\u001b[39m, limit: \u001b[39mint\u001b[39m \u001b[39m=\u001b[39m \u001b[39m20\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   2125\u001b[0m     \u001b[39m\"\"\"Print up to the given number of records from the datastream.\u001b[39;00m\n\u001b[1;32m   2126\u001b[0m \n\u001b[1;32m   2127\u001b[0m \u001b[39m    Time complexity: O(limit specified)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2130\u001b[0m \u001b[39m        limit: The max number of records to print.\u001b[39;00m\n\u001b[1;32m   2131\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2132\u001b[0m     \u001b[39mfor\u001b[39;00m row \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtake(limit):\n\u001b[1;32m   2133\u001b[0m         \u001b[39mprint\u001b[39m(row)\n",
      "File \u001b[0;32m~/code/ray/python/ray/data/datastream.py:2090\u001b[0m, in \u001b[0;36mDatastream.take\u001b[0;34m(self, limit)\u001b[0m\n\u001b[1;32m   2085\u001b[0m     logger\u001b[39m.\u001b[39minfo(\n\u001b[1;32m   2086\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mTip: Use `take_batch()` instead of `take() / show()` to return \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2087\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mrecords in pandas or numpy batch format.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2088\u001b[0m     )\n\u001b[1;32m   2089\u001b[0m output \u001b[39m=\u001b[39m []\n\u001b[0;32m-> 2090\u001b[0m \u001b[39mfor\u001b[39;00m row \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39miter_rows():\n\u001b[1;32m   2091\u001b[0m     output\u001b[39m.\u001b[39mappend(row)\n\u001b[1;32m   2092\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(output) \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m limit:\n",
      "File \u001b[0;32m~/code/ray/python/ray/data/iterator.py:225\u001b[0m, in \u001b[0;36mDataIterator.iter_rows\u001b[0;34m(self, prefetch_blocks)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    222\u001b[0m     \u001b[39m# Since batch_size is None, 1 block is exactly 1 batch.\u001b[39;00m\n\u001b[1;32m    223\u001b[0m     iter_batch_args[\u001b[39m\"\u001b[39m\u001b[39mprefetch_batches\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m prefetch_blocks\n\u001b[0;32m--> 225\u001b[0m \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39miter_batches(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39miter_batch_args):\n\u001b[1;32m    226\u001b[0m     batch \u001b[39m=\u001b[39m BlockAccessor\u001b[39m.\u001b[39mfor_block(BlockAccessor\u001b[39m.\u001b[39mbatch_to_block(batch))\n\u001b[1;32m    227\u001b[0m     \u001b[39mfor\u001b[39;00m row \u001b[39min\u001b[39;00m batch\u001b[39m.\u001b[39miter_rows(public_row_format\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n",
      "File \u001b[0;32m~/code/ray/python/ray/data/iterator.py:158\u001b[0m, in \u001b[0;36mDataIterator.iter_batches\u001b[0;34m(self, prefetch_batches, batch_size, batch_format, drop_last, local_shuffle_buffer_size, local_shuffle_seed, _collate_fn, prefetch_blocks)\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mDeprecationWarning\u001b[39;00m(\n\u001b[1;32m    148\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`prefetch_blocks` arg is deprecated in Ray 2.4. Use \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    149\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mthe `prefetch_batches` arg instead to specify the amount of \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mto True in the DataContext.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    154\u001b[0m     )\n\u001b[1;32m    156\u001b[0m time_start \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mperf_counter()\n\u001b[0;32m--> 158\u001b[0m block_iterator, stats, blocks_owned_by_consumer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_to_block_iterator()\n\u001b[1;32m    159\u001b[0m \u001b[39mif\u001b[39;00m use_legacy:\n\u001b[1;32m    160\u001b[0m     \u001b[39m# Legacy iter_batches does not use metadata.\u001b[39;00m\n\u001b[1;32m    161\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mdrop_metadata\u001b[39m(block_iterator):\n",
      "File \u001b[0;32m~/code/ray/python/ray/data/_internal/iterator/iterator_impl.py:33\u001b[0m, in \u001b[0;36mDataIteratorImpl._to_block_iterator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_to_block_iterator\u001b[39m(\n\u001b[1;32m     26\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m     27\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[39mbool\u001b[39m,\n\u001b[1;32m     31\u001b[0m ]:\n\u001b[1;32m     32\u001b[0m     ds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_base_datastream\n\u001b[0;32m---> 33\u001b[0m     block_iterator, stats, executor \u001b[39m=\u001b[39m ds\u001b[39m.\u001b[39;49m_plan\u001b[39m.\u001b[39;49mexecute_to_iterator()\n\u001b[1;32m     34\u001b[0m     ds\u001b[39m.\u001b[39m_current_executor \u001b[39m=\u001b[39m executor\n\u001b[1;32m     35\u001b[0m     \u001b[39mreturn\u001b[39;00m block_iterator, stats, \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/code/ray/python/ray/data/_internal/plan.py:512\u001b[0m, in \u001b[0;36mExecutionPlan.execute_to_iterator\u001b[0;34m(self, allow_clear_input_blocks, force_read)\u001b[0m\n\u001b[1;32m    510\u001b[0m gen \u001b[39m=\u001b[39m \u001b[39miter\u001b[39m(block_iter)\n\u001b[1;32m    511\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 512\u001b[0m     block_iter \u001b[39m=\u001b[39m itertools\u001b[39m.\u001b[39mchain([\u001b[39mnext\u001b[39;49m(gen)], gen)\n\u001b[1;32m    513\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[1;32m    514\u001b[0m     \u001b[39mpass\u001b[39;00m\n",
      "File \u001b[0;32m~/code/ray/python/ray/data/_internal/execution/legacy_compat.py:57\u001b[0m, in \u001b[0;36mexecute_to_legacy_block_iterator\u001b[0;34m(executor, plan, allow_clear_input_blocks, datastream_uuid)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[39m\"\"\"Same as execute_to_legacy_bundle_iterator but returning blocks and metadata.\"\"\"\u001b[39;00m\n\u001b[1;32m     54\u001b[0m bundle_iter \u001b[39m=\u001b[39m execute_to_legacy_bundle_iterator(\n\u001b[1;32m     55\u001b[0m     executor, plan, allow_clear_input_blocks, datastream_uuid\n\u001b[1;32m     56\u001b[0m )\n\u001b[0;32m---> 57\u001b[0m \u001b[39mfor\u001b[39;00m bundle \u001b[39min\u001b[39;00m bundle_iter:\n\u001b[1;32m     58\u001b[0m     \u001b[39mfor\u001b[39;00m block, metadata \u001b[39min\u001b[39;00m bundle\u001b[39m.\u001b[39mblocks:\n\u001b[1;32m     59\u001b[0m         \u001b[39myield\u001b[39;00m block, metadata\n",
      "File \u001b[0;32m~/code/ray/python/ray/data/_internal/execution/interfaces.py:520\u001b[0m, in \u001b[0;36mOutputIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__next__\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m RefBundle:\n\u001b[0;32m--> 520\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_next()\n",
      "File \u001b[0;32m~/code/ray/python/ray/data/_internal/execution/streaming_executor.py:117\u001b[0m, in \u001b[0;36mStreamingExecutor.execute.<locals>.StreamIterator.get_next\u001b[0;34m(self, output_split_idx)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_next\u001b[39m(\u001b[39mself\u001b[39m, output_split_idx: Optional[\u001b[39mint\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m RefBundle:\n\u001b[1;32m    116\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m         item \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_outer\u001b[39m.\u001b[39;49m_output_node\u001b[39m.\u001b[39;49mget_output_blocking(\n\u001b[1;32m    118\u001b[0m             output_split_idx\n\u001b[1;32m    119\u001b[0m         )\n\u001b[1;32m    120\u001b[0m         \u001b[39m# Translate the special sentinel values for MaybeRefBundle into\u001b[39;00m\n\u001b[1;32m    121\u001b[0m         \u001b[39m# exceptions.\u001b[39;00m\n\u001b[1;32m    122\u001b[0m         \u001b[39mif\u001b[39;00m item \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/code/ray/python/ray/data/_internal/execution/streaming_executor_state.py:226\u001b[0m, in \u001b[0;36mOpState.get_output_blocking\u001b[0;34m(self, output_split_idx)\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mIndexError\u001b[39;00m:\n\u001b[1;32m    225\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[0;32m--> 226\u001b[0m time\u001b[39m.\u001b[39;49msleep(\u001b[39m0.01\u001b[39;49m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ds.show(4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using GPU\n",
    "\n",
    "To use GPU in the previous example, we first need to change to model to use GPU.\n",
    "\n",
    "```diff\n",
    "         self.model = fasterrcnn_resnet50_fpn_v2(\n",
    "             weights=self.weights,\n",
    "             box_score_thresh=0.9,\n",
    "-        )\n",
    "+        ).cuda()\n",
    "         self.model.eval()\n",
    "         # Note, since the data in the batch input is `np.ndarray`s,\n",
    "         # we need `transforms.ToTensor` to convert the data to torch tensors.\n",
    "```\n",
    "\n",
    "```diff\n",
    "     def __call__(self, batch: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n",
    "         # Define the per-batch inference code in `__call__`\n",
    "         # Preprocess the images.\n",
    "-        batch = [self.prepross(image) for image in batch[\"image\"]]\n",
    "+        batch = [self.prepross(image).cuda() for image in batch[\"image\"]]\n",
    "         # Do inference on the images.\n",
    "         predictions = self.model(batch)\n",
    "         # Get the inferred labels and convert it to a np.ndarray\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we add the `num_gpus` argument to the `map_batches` API. This will tell Ray to schedule the actors to nodes with GPU resources and assign a GPU to each actor.\n",
    "\n",
    "```diff\n",
    " ds = ds.map_batches(\n",
    "     ObjecytDetectionModel,\n",
    "     compute=ray.data.ActorPoolStrategy(size=4),\n",
    "     batch_size=16,\n",
    "+    num_gpus=1,\n",
    " )\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
