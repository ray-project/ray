{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Image Object Detection using Ray Data\n",
    "\n",
    "In this example, we will demostrate how to use Ray Data to do batch object detection with a PyTorch model on a large set of images.\n",
    "\n",
    "# Walkthrough\n",
    "\n",
    "## Vanilla Pyorch exmaple\n",
    "\n",
    "Let's take a look at the [vanilla PyTorch example](https://pytorch.org/vision/stable/models.html#object-detection) first. \n",
    "\n",
    "In this example, we use a pre-trained PyTorch model to detect the objects on a single image. For simplicity, we skip the result visualization in this example.\n",
    "\n",
    "```python\n",
    "from torchvision.io.image import read_image\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn_v2, FasterRCNN_ResNet50_FPN_V2_Weights\n",
    "\n",
    "img = read_image(\"/path/to/your/image.jpg\")\n",
    "\n",
    "# Step 1: Initialize model with the best available weights\n",
    "weights = FasterRCNN_ResNet50_FPN_V2_Weights.DEFAULT\n",
    "model = fasterrcnn_resnet50_fpn_v2(weights=weights, box_score_thresh=0.9)\n",
    "model.eval()\n",
    "\n",
    "# Step 2: Initialize the inference transforms\n",
    "preprocess = weights.transforms()\n",
    "\n",
    "# Step 3: Apply inference preprocessing transforms\n",
    "batch = [preprocess(img)]\n",
    "\n",
    "# Step 4: Use the model and visualize the prediction\n",
    "prediction = model(batch)[0]\n",
    "labels = [weights.meta[\"categories\"][i] for i in prediction[\"labels\"]]\n",
    "print(labels)\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Parallelizing with Ray Data\n",
    "\n",
    "Then we want to apply this object detection model to a large set of images. We can use Ray Data to scale the model.\n",
    "\n",
    "### Reading Image Data \n",
    "\n",
    "First, we use `ray.data.read_images` API to read image data from S3.  The directory structure of the dataset is`<s3_url/{label_id}/{*.JPEG}>`. So we use Partitioning utility to load in all the images for all labels. And we use the `parallelism` argument to specify the number of distributed tasks that are used for reading data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-08 14:01:11,335\tINFO worker.py:1607 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2023-05-08 14:01:13,294\tWARNING datastream.py:251 -- \u001b[33m[IMPORTANT]: Ray Data strict mode is on by default in Ray 2.5. When in strict mode, data schemas are required, standalone Python objects are no longer supported, and the default batch format changes to `numpy` from `pandas`. To disable strict mode temporarily, set the environment variable RAY_DATA_STRICT_MODE=0 on all cluster processes. Strict mode will not be possible to disable in future releases.\n",
      "\n",
      "Learn more here: https://docs.ray.io/en/master/data/faq.html#what-is-strict-mode\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import ray\n",
    "from ray.data.datasource.partitioning import Partitioning\n",
    "\n",
    "s3_uri = \"s3://anonymous@air-example-data-2/imagenette2/val/\"\n",
    "\n",
    "# The S3 directory structure is {s3_uri}/{class_id}/{*.JPEG}\n",
    "partitioning = Partitioning(\"dir\", field_names=[\"class\"], base_dir=s3_uri)\n",
    "\n",
    "ds = ray.data.read_images(s3_uri, parallelism=4, partitioning=partitioning, mode=\"RGB\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understand Bathces\n",
    "\n",
    "To boost performance with hardware vectorization, we usually do inference in batches. In Ray Data, a batch is by default defined as a `Dict[str, np.ndarray]`. \n",
    "\n",
    "In this case, the dict will have only one key named \"image\", and the value is an array of images represented in the `np.ndarray` format.\n",
    "\n",
    "In the following code snippet, we use the `take_batch` API to get a single batch and inspect its internal data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-08 14:01:13,315\tINFO streaming_executor.py:91 -- Executing DAG InputDataBuffer[Input] -> TaskPoolMapOperator[ReadImage]\n",
      "2023-05-08 14:01:13,315\tINFO streaming_executor.py:92 -- Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=None), locality_with_output=False, preserve_order=False, actor_locality_enabled=True, verbose_progress=False)\n",
      "2023-05-08 14:01:13,316\tINFO streaming_executor.py:94 -- Tip: For detailed progress reporting, run `ray.data.DataContext.get_current().execution_options.verbose_progress = True`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56cc2f257c7b46d8a7b0d876f953adf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running 0:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-08 14:03:31,694\tINFO streaming_executor.py:149 -- Shutting down <StreamingExecutor(Thread-7, started daemon 13338013696)>.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4,) (375, 500, 3)\n"
     ]
    }
   ],
   "source": [
    "single_batch = ds.take_batch(batch_size=4)\n",
    "print(single_batch[\"image\"].shape, single_batch[\"image\"][0].shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do batch inference with Ray Data\n",
    "\n",
    "Then we will demostrate how to use Ray Data to do batch inference in a distributed cluster.\n",
    "\n",
    "The first thing is to package the model in a Python class. The class is mainly consist of two parts. In the `__init__` constructor, we package the code that loads and initializes the model. And in the `__call__` method, we package the code that do inference for each batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from torchvision.models.detection import (FasterRCNN_ResNet50_FPN_V2_Weights,\n",
    "                                          fasterrcnn_resnet50_fpn_v2)\n",
    "\n",
    "\n",
    "class ObjecytDetectionModel:\n",
    "    def __init__(self):\n",
    "        # Define the model loading and initialization code in `__init__`\n",
    "        self.weights = FasterRCNN_ResNet50_FPN_V2_Weights.DEFAULT\n",
    "        self.model = fasterrcnn_resnet50_fpn_v2(\n",
    "            weights=self.weights,\n",
    "            box_score_thresh=0.9,\n",
    "        )\n",
    "        self.model.eval()\n",
    "        # Note, since the data in the batch input is `np.ndarray`s, \n",
    "        # we need `transforms.ToTensor` to convert the data to torch tensors.\n",
    "        self.prepross = transforms.Compose(\n",
    "            [transforms.ToTensor(), self.weights.transforms()]\n",
    "        )\n",
    "\n",
    "    def __call__(self, batch: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n",
    "        # Define the per-batch inference code in `__call__`\n",
    "        # Preprocess the images.\n",
    "        batch = [self.prepross(image) for image in batch[\"image\"]]\n",
    "        # Do inference on the images.\n",
    "        predictions = self.model(batch)\n",
    "        # Get the inferred labels and convert it to a np.ndarray\n",
    "        labels = np.array(\n",
    "            [\n",
    "                \",\".join(\n",
    "                    [self.weights.meta[\"categories\"][i] for i in prediction[\"labels\"]]\n",
    "                )\n",
    "                for prediction in predictions\n",
    "            ],\n",
    "            dtype=\"S\",\n",
    "        )\n",
    "        # `__call__` also returns a `Dict[str, np.ndarray]`.\n",
    "        return {\"labels\": labels}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we use the `map_batches` API to apply the model to the image data set. Here, `compute` argument indicates the number of concurrent models, and the `batch_size` argument indicates the number of images in each batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.map_batches(\n",
    "    ObjecytDetectionModel,\n",
    "    compute=ray.data.ActorPoolStrategy(size=4),\n",
    "    batch_size=16,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we can use `ds.show(4)` to inspect samples in the result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-08 14:03:32,642\tINFO datastream.py:2085 -- Tip: Use `take_batch()` instead of `take() / show()` to return records in pandas or numpy batch format.\n",
      "2023-05-08 14:03:32,646\tINFO streaming_executor.py:91 -- Executing DAG InputDataBuffer[Input] -> ActorPoolMapOperator[ReadImage->MapBatches(ObjecytDetectionModel)]\n",
      "2023-05-08 14:03:32,647\tINFO streaming_executor.py:92 -- Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=None), locality_with_output=False, preserve_order=False, actor_locality_enabled=True, verbose_progress=False)\n",
      "2023-05-08 14:03:32,647\tINFO streaming_executor.py:94 -- Tip: For detailed progress reporting, run `ray.data.DataContext.get_current().execution_options.verbose_progress = True`\n",
      "2023-05-08 14:03:32,661\tINFO actor_pool_map_operator.py:114 -- ReadImage->MapBatches(ObjecytDetectionModel): Waiting for 4 pool actors to start...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36c98233bc784785afbbc4ab12d68ba9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running 0:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds.show(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
