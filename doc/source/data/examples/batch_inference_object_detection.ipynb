{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flake8: noqa\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "# Suppress noisy requests warnings.\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ[\"PYTHONWARNINGS\"] = \"ignore\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Inference with a PyTorch Object Detection Model\n",
    "\n",
    "In this example, we will demostrate how to use Ray Data to do batch object detection with a PyTorch model on a large set of images in parallel.\n",
    "\n",
    "# Walkthrough\n",
    "\n",
    "## Vanilla Pyorch exmaple\n",
    "\n",
    "Let's take a look at the [vanilla PyTorch example](https://pytorch.org/vision/stable/models.html#object-detection) first. \n",
    "\n",
    "In this example, we use a pre-trained PyTorch model to detect the objects on a single image. For simplicity, we skip the result visualization in this example.\n",
    "\n",
    "```python\n",
    "from torchvision.io.image import read_image\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn_v2, FasterRCNN_ResNet50_FPN_V2_Weights\n",
    "\n",
    "img = read_image(\"/path/to/your/image.jpg\")\n",
    "\n",
    "# Step 1: Initialize model with the best available weights\n",
    "weights = FasterRCNN_ResNet50_FPN_V2_Weights.DEFAULT\n",
    "model = fasterrcnn_resnet50_fpn_v2(weights=weights, box_score_thresh=0.9)\n",
    "model.eval()\n",
    "\n",
    "# Step 2: Initialize the inference transforms\n",
    "preprocess = weights.transforms()\n",
    "\n",
    "# Step 3: Apply inference preprocessing transforms\n",
    "batch = [preprocess(img)]\n",
    "\n",
    "# Step 4: Use the model and visualize the prediction\n",
    "prediction = model(batch)[0]\n",
    "labels = [weights.meta[\"categories\"][i] for i in prediction[\"labels\"]]\n",
    "print(labels)\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Parallelizing with Ray Data\n",
    "\n",
    "Then we want to apply this object detection model to a large set of images. We can use Ray Data to scale the model.\n",
    "\n",
    "### Reading Image Data \n",
    "\n",
    "First, we use `ray.data.read_images` API to read image data from S3.  The directory structure of the dataset is`<s3_url/{label_id}/{*.JPEG}>`. So we use Partitioning utility to load in all the images for all labels. And we use the `parallelism` argument to specify the number of distributed tasks that are used for reading data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "from ray.data.datasource.partitioning import Partitioning\n",
    "\n",
    "s3_uri = \"s3://anonymous@air-example-data-2/imagenette2/val/\"\n",
    "\n",
    "# The S3 directory structure is {s3_uri}/{class_id}/{*.JPEG}\n",
    "partitioning = Partitioning(\"dir\", field_names=[\"class\"], base_dir=s3_uri)\n",
    "\n",
    "ds = ray.data.read_images(s3_uri, parallelism=4, partitioning=partitioning, mode=\"RGB\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understand Bathces\n",
    "\n",
    "To boost performance with hardware vectorization, we usually do inference in batches. In Ray Data, a batch is by default defined as a `Dict[str, np.ndarray]`. \n",
    "\n",
    "In this case, the dict will have only one key named \"image\", and the value is an array of images represented in the `np.ndarray` format.\n",
    "\n",
    "In the following code snippet, we use the `take_batch` API to get a single batch and inspect its internal data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_batch = ds.take_batch(batch_size=4)\n",
    "print(single_batch[\"image\"].shape, single_batch[\"image\"][0].shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do batch inference with Ray Data\n",
    "\n",
    "Then we will demostrate how to use Ray Data to do batch inference in a distributed cluster.\n",
    "\n",
    "The first thing is to package the model in a Python class. The class is mainly consist of two parts. In the `__init__` constructor, we package the code that loads and initializes the model. And in the `__call__` method, we package the code that do inference for each batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from torchvision.models.detection import (FasterRCNN_ResNet50_FPN_V2_Weights,\n",
    "                                          fasterrcnn_resnet50_fpn_v2)\n",
    "\n",
    "\n",
    "class ObjecytDetectionModel:\n",
    "    def __init__(self):\n",
    "        # Define the model loading and initialization code in `__init__`\n",
    "        self.weights = FasterRCNN_ResNet50_FPN_V2_Weights.DEFAULT\n",
    "        self.model = fasterrcnn_resnet50_fpn_v2(\n",
    "            weights=self.weights,\n",
    "            box_score_thresh=0.9,\n",
    "        )\n",
    "        self.model.eval()\n",
    "        # Note, since the data in the batch input is `np.ndarray`s, \n",
    "        # we need `transforms.ToTensor` to convert the data to torch tensors.\n",
    "        self.prepross = transforms.Compose(\n",
    "            [transforms.ToTensor(), self.weights.transforms()]\n",
    "        )\n",
    "\n",
    "    def __call__(self, batch: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n",
    "        # Define the per-batch inference code in `__call__`\n",
    "        # Preprocess the images.\n",
    "        batch = [self.prepross(image) for image in batch[\"image\"]]\n",
    "        # Do inference on the images.\n",
    "        predictions = self.model(batch)\n",
    "        # Get the inferred labels and convert it to a np.ndarray\n",
    "        labels = np.array(\n",
    "            [\n",
    "                \",\".join(\n",
    "                    [self.weights.meta[\"categories\"][i] for i in prediction[\"labels\"]]\n",
    "                )\n",
    "                for prediction in predictions\n",
    "            ],\n",
    "            dtype=\"S\",\n",
    "        )\n",
    "        # `__call__` also returns a `Dict[str, np.ndarray]`.\n",
    "        return {\"labels\": labels}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we use the `map_batches` API to apply the model to the image data set. Here, `compute` argument indicates the number of concurrent models, and the `batch_size` argument indicates the number of images in each batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.map_batches(\n",
    "    ObjecytDetectionModel,\n",
    "    compute=ray.data.ActorPoolStrategy(size=4),\n",
    "    batch_size=16,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we can use `ds.show(4)` to inspect samples in the result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.show(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
