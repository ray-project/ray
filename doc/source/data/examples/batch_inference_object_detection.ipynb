{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Image Object Detection using Ray Data\n",
    "\n",
    "In this example, we will demostrate how to use Ray Data to do batch object detection with a PyTorch model on a large set of images.\n",
    "\n",
    "## Vanilla Pyorch exmaple\n",
    "\n",
    "Let's take a look at the [vanilla PyTorch example](https://pytorch.org/vision/stable/models.html#object-detection) first. \n",
    "\n",
    "In this example, we use a pre-trained PyTorch model to detect the objects on a single image. For simplicity, we skip the result visualization in this example.\n",
    "\n",
    "```python\n",
    "from torchvision.io.image import read_image\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn_v2, FasterRCNN_ResNet50_FPN_V2_Weights\n",
    "\n",
    "img = read_image(\"/path/to/your/image.jpg\")\n",
    "\n",
    "# Step 1: Initialize model with the best available weights\n",
    "weights = FasterRCNN_ResNet50_FPN_V2_Weights.DEFAULT\n",
    "model = fasterrcnn_resnet50_fpn_v2(weights=weights, box_score_thresh=0.9)\n",
    "model.eval()\n",
    "\n",
    "# Step 2: Initialize the inference transforms\n",
    "preprocess = weights.transforms()\n",
    "\n",
    "# Step 3: Apply inference preprocessing transforms\n",
    "batch = [preprocess(img)]\n",
    "\n",
    "# Step 4: Use the model and visualize the prediction\n",
    "prediction = model(batch)[0]\n",
    "labels = [weights.meta[\"categories\"][i] for i in prediction[\"labels\"]]\n",
    "print(labels)\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Parallelizing with Ray Data\n",
    "\n",
    "Then we want to apply this object detection model to a large set of images. We can use Ray Data to scale it.\n",
    "\n",
    "### Reading Image Data \n",
    "\n",
    "First, we use `ray.data.read_images` API to read image data from S3.  The directory structure of the dataset is`<s3_url/{label_id}/{*.JPEG}>`. So we use Partitioning utility to load in all the images for all labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "from ray.data.datasource.partitioning import Partitioning\n",
    "\n",
    "s3_uri = \"s3://anonymous@air-example-data-2/imagenette2/val/\"\n",
    "\n",
    "# The S3 directory structure is {s3_uri}/{class_id}/{*.JPEG}\n",
    "partitioning = Partitioning(\"dir\", field_names=[\"class\"], base_dir=s3_uri)\n",
    "\n",
    "ds = ray.data.read_images(s3_uri, partitioning=partitioning, mode=\"RGB\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Batching\n",
    "\n",
    "To boost performance with hardware vectorization, we usually do inference in batches. In Ray Data, a batch is by default defined as a `Dict[str, np.ndarray]`. \n",
    "\n",
    "In this case, the dict will have only one key named \"image\", and the value is an array of images represented in the `np.ndarray` format.\n",
    "\n",
    "In the following code snippet, we use the `take_batch` API to get a single batch and inspect its internal data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-09 16:47:15,553\tINFO streaming_executor.py:91 -- Executing DAG InputDataBuffer[Input] -> TaskPoolMapOperator[ReadImage]\n",
      "2023-05-09 16:47:15,554\tINFO streaming_executor.py:92 -- Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=None), locality_with_output=False, preserve_order=False, actor_locality_enabled=True, verbose_progress=False)\n",
      "2023-05-09 16:47:15,554\tINFO streaming_executor.py:94 -- Tip: For detailed progress reporting, run `ray.data.DataContext.get_current().execution_options.verbose_progress = True`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08e680739a224c7d8a173e3adaa9061c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running 0:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-09 16:47:18,180\tINFO streaming_executor.py:149 -- Shutting down <StreamingExecutor(Thread-1089, started daemon 13592719360)>.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4,) (478, 612, 3)\n"
     ]
    }
   ],
   "source": [
    "single_batch = ds.take_batch(batch_size=4)\n",
    "print(single_batch[\"image\"].shape, single_batch[\"image\"][0].shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch inference with Ray Data\n",
    "\n",
    "Then we will demostrate how to use Ray Data to do batch inference in a distributed cluster.\n",
    "\n",
    "The first step is to package the model in a Python class. The class is mainly consist of two parts. In the `__init__` constructor, we package the code that loads and initializes the model. And in the `__call__` method, we package the code that applies model on each batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from torchvision.models.detection import (FasterRCNN_ResNet50_FPN_V2_Weights,\n",
    "                                          fasterrcnn_resnet50_fpn_v2)\n",
    "\n",
    "\n",
    "class ObjectDetectionModel:\n",
    "    def __init__(self):\n",
    "        # Define the model loading and initialization code in `__init__`\n",
    "        self.weights = FasterRCNN_ResNet50_FPN_V2_Weights.DEFAULT\n",
    "        self.model = fasterrcnn_resnet50_fpn_v2(\n",
    "            weights=self.weights,\n",
    "            box_score_thresh=0.9,\n",
    "        )\n",
    "        self.model.eval()\n",
    "        # Note, since the data in the batch input is `np.ndarray`s, \n",
    "        # we need `transforms.ToTensor` to convert the data to torch tensors.\n",
    "        self.preprossor = transforms.Compose(\n",
    "            [transforms.ToTensor(), self.weights.transforms()]\n",
    "        )\n",
    "\n",
    "    def __call__(self, batch: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n",
    "        # Define the per-batch inference code in `__call__`\n",
    "        # Preprocess the images.\n",
    "        batch = [self.preprossor(image) for image in batch[\"image\"]]\n",
    "        # Do inference on the images.\n",
    "        predictions = self.model(batch)\n",
    "        # Get the inferred labels and convert it to a np.ndarray\n",
    "        labels = np.array(\n",
    "            [\n",
    "                \",\".join(\n",
    "                    [self.weights.meta[\"categories\"][i] for i in prediction[\"labels\"]]\n",
    "                )\n",
    "                for prediction in predictions\n",
    "            ],\n",
    "            dtype=\"S\",\n",
    "        )\n",
    "        # `__call__` also returns a `Dict[str, np.ndarray]`.\n",
    "        return {\"labels\": labels}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we use the `map_batches` API to apply the model to the whole image data set. Here, the `compute` argument indicates the number of concurrent Ray actors that run the model, and the `batch_size` argument indicates the number of images in each batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.map_batches(\n",
    "    ObjectDetectionModel,\n",
    "    compute=ray.data.ActorPoolStrategy(size=4),\n",
    "    batch_size=4,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we can use `ds.show` to inspect samples in the result, or the the `ds.write_xxx` APIS to write results to external storages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-09 16:47:18,554\tINFO streaming_executor.py:91 -- Executing DAG InputDataBuffer[Input] -> ActorPoolMapOperator[ReadImage->MapBatches(ObjectDetectionModel)]\n",
      "2023-05-09 16:47:18,555\tINFO streaming_executor.py:92 -- Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=None), locality_with_output=False, preserve_order=False, actor_locality_enabled=True, verbose_progress=False)\n",
      "2023-05-09 16:47:18,555\tINFO streaming_executor.py:94 -- Tip: For detailed progress reporting, run `ray.data.DataContext.get_current().execution_options.verbose_progress = True`\n",
      "2023-05-09 16:47:18,569\tINFO actor_pool_map_operator.py:114 -- ReadImage->MapBatches(ObjectDetectionModel): Waiting for 4 pool actors to start...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2022ffcc96549b793e3b7b0a1de9d23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running 0:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-09 16:49:00,509\tINFO streaming_executor.py:149 -- Shutting down <StreamingExecutor(Thread-1104, started daemon 13592719360)>.\n",
      "2023-05-09 16:49:00,610\tWARNING actor_pool_map_operator.py:272 -- To ensure full parallelization across an actor pool of size 4, the Datastream should consist of at least 4 distinct blocks. Consider increasing the parallelism when creating the Datastream.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'labels': b'person,bird'}\n",
      "{'labels': b'person'}\n",
      "{'labels': b'bed'}\n",
      "{'labels': b'person'}\n"
     ]
    }
   ],
   "source": [
    "ds.show(4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using GPUs\n",
    "\n",
    "To use GPUs in the previous example, we first need to change to model to use GPU.\n",
    "\n",
    "```diff\n",
    "         self.model = fasterrcnn_resnet50_fpn_v2(\n",
    "             weights=self.weights,\n",
    "             box_score_thresh=0.9,\n",
    "-        )\n",
    "+        ).cuda()\n",
    "         self.model.eval()\n",
    "         # Note, since the data in the batch input is `np.ndarray`s,\n",
    "         # we need `transforms.ToTensor` to convert the data to torch tensors.\n",
    "```\n",
    "\n",
    "```diff\n",
    "     def __call__(self, batch: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n",
    "         # Define the per-batch inference code in `__call__`\n",
    "         # Preprocess the images.\n",
    "-        batch = [self.prepross(image) for image in batch[\"image\"]]\n",
    "+        batch = [self.prepross(image).cuda() for image in batch[\"image\"]]\n",
    "         # Do inference on the images.\n",
    "         predictions = self.model(batch)\n",
    "         # Get the inferred labels and convert it to a np.ndarray\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we add the `num_gpus` argument to the `map_batches` API. This will tell Ray to schedule the actors to nodes with GPU resources and assign a GPU to each actor.\n",
    "\n",
    "```diff\n",
    " ds = ds.map_batches(\n",
    "     ObjecytDetectionModel,\n",
    "     compute=ray.data.ActorPoolStrategy(size=4),\n",
    "     batch_size=16,\n",
    "+    num_gpus=1,\n",
    " )\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
