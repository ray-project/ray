{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Vision-language model batch inference with Ray Data\n",
        "\n",
        "<div align=\"left\">\n",
        "<a target=\"_blank\" href=\"https://console.anyscale.com/template-preview/llm-batch-inference-vision\"><img src=\"https://img.shields.io/badge/ðŸš€ Run_on-Anyscale-9hf\"></a>&nbsp;\n",
        "<a href=\"https://github.com/ray-project/ray/tree/master/doc/source/data/examples/llm-batch-inference-vision\" role=\"button\"><img src=\"https://img.shields.io/static/v1?label=&amp;message=View%20On%20GitHub&amp;color=586069&amp;logo=github&amp;labelColor=2f363d\"></a>&nbsp;\n",
        "</div>\n",
        "\n",
        "**â±ï¸ Time to complete**: 20 minutes\n",
        "\n",
        "This example shows you how to run batch inference for vision-language models (VLMs) using [Ray Data LLM APIs](https://docs.ray.io/en/latest/data/api/llm.html). In this use case, the batch inference job generates captions for a large-scale image dataset.\n",
        "\n",
        "## When to use LLM batch inference\n",
        "\n",
        "Offline (batch) inference optimizes for throughput over latency. Unlike online inference, which processes requests one at a time in real-time, batch inference processes thousands or millions of inputs together, maximizing GPU utilization and reducing per-inference costs.\n",
        "\n",
        "Choose batch inference when:\n",
        "- You have a fixed dataset to process (such as daily reports or data migrations)\n",
        "- Throughput matters more than immediate results\n",
        "- You want to take advantage of fault tolerance and checkpointing for long-running jobs\n",
        "\n",
        "On contrary, if you are more interested in optimizing for latency, consider [deploying your LLM with Ray Serve LLM for online inference](https://docs.ray.io/en/latest/serve/llm/index.html).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prepare a Ray Data dataset with images\n",
        "\n",
        "Ray Data LLM runs batch inference for VLMs on Ray Data datasets containing images. In this tutorial, you perform batch inference with a vision-language model to generate image captions from the `BLIP3o/BLIP3o-Pretrain-Short-Caption` dataset, which contains approximately 5 million images.\n",
        "\n",
        "First, load the data from a remote URL then repartition the dataset to ensure the workload can be distributed across multiple GPUs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import ray\n",
        "import datasets\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "\n",
        "# Load the BLIP3o/BLIP3o-Pretrain-Short-Caption dataset from Hugging Face with ~5M images.\n",
        "print(\"Loading BLIP3o/BLIP3o-Pretrain-Short-Caption dataset from Hugging Face...\")\n",
        "hf_dataset = datasets.load_dataset(\"BLIP3o/BLIP3o-Pretrain-Short-Caption\", split=\"train\", streaming=True)\n",
        "hf_dataset = hf_dataset.select_columns([\"jpg\"])\n",
        "\n",
        "ds = ray.data.from_huggingface(hf_dataset)\n",
        "print(\"Dataset loaded successfully.\")\n",
        "\n",
        "sample = ds.take(2)\n",
        "print(\"Sample data:\")\n",
        "IMAGE_COLUMN = 'jpg'\n",
        "for i, item in enumerate(sample):\n",
        "    print(f\"\\nSample {i+1}:\")\n",
        "    image = Image.open(BytesIO(item[IMAGE_COLUMN]['bytes']))\n",
        "    image.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For this initial example, limit the dataset to 10,000 rows for faster processing and testing. Later, you can scale up to process the full dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Limit the dataset to 100,000 images for this example.\n",
        "print(\"Limiting dataset to 10,000 images for initial processing.\")\n",
        "ds_small = ds.limit(10_000)\n",
        "\n",
        "# Repartition the dataset to enable parallelism across multiple workers (GPUs).\n",
        "# By default, streaming datasets might not be optimally partitioned. Repartitioning\n",
        "# splits the data into a specified number of blocks, allowing Ray to process them\n",
        "# in parallel.\n",
        "num_partitions = 64\n",
        "print(f\"Repartitioning dataset into {num_partitions} blocks for parallelism...\")\n",
        "ds_small = ds_small.repartition(num_blocks=num_partitions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configure Ray Data LLM\n",
        "\n",
        "Ray Data LLM provides a unified interface to run batch inference with different VLM engines. Configure the vLLM engine with a vision-language model, define preprocessing and postprocessing functions, and build the processor.\n",
        "\n",
        "### Configure the processor engine\n",
        "\n",
        "Configure the model and compute resources needed for inference using `vLLMEngineProcessorConfig` with vision support enabled.\n",
        "\n",
        "This example uses the `Qwen/Qwen2.5-VL-3B-Instruct` model, a vision-language model. The configuration specifies:\n",
        "- `model_source`: The Hugging Face model identifier.\n",
        "- `engine_kwargs`: vLLM engine parameters such as memory settings and batching.\n",
        "- `runtime_env`: Environment variables needed for the vLLM vision API (V1).\n",
        "- `batch_size`: Number of requests to batch together (set to 16 for vision models).\n",
        "- `accelerator_type`: GPU type to use (L4 in this case).\n",
        "- `concurrency`: Number of parallel workers (4 replicas).\n",
        "- `has_image`: Enable image input support.\n",
        "\n",
        "**Note:** Vision models typically require smaller batch sizes than text-only models due to the additional memory needed for image processing. Adjust batch size based on your image resolution and GPU memory.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from ray.data.llm import vLLMEngineProcessorConfig\n",
        "\n",
        "MAX_MODEL_LEN = 8192\n",
        "\n",
        "processor_config = vLLMEngineProcessorConfig(\n",
        "    model_source=\"Qwen/Qwen2.5-VL-3B-Instruct\",\n",
        "    engine_kwargs=dict(\n",
        "        max_model_len=MAX_MODEL_LEN,\n",
        "        max_num_batched_tokens=2048,\n",
        "    ),\n",
        "    batch_size=16,\n",
        "    accelerator_type=\"L4\",\n",
        "    concurrency=4,\n",
        "    has_image=True,  # Enable image input.\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For more details on the configuration options you can pass to the vLLM engine, see the [vLLM Engine Arguments documentation](https://docs.vllm.ai/en/stable/configuration/engine_args.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define the preprocess and postprocess functions\n",
        "\n",
        "The task is to generate descriptive captions for images using a vision-language model.\n",
        "\n",
        "Define a preprocess function to prepare `messages` with image content and `sampling_params` for the vLLM engine, and a postprocess function to extract the `generated_text`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import Any\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "\n",
        "# Preprocess function prepares messages with image content for the VLM.\n",
        "def preprocess(row: dict[str, Any]) -> dict[str, Any]:\n",
        "    # Convert bytes image to PIL \n",
        "    image = row[IMAGE_COLUMN]['bytes']\n",
        "    image = Image.open(BytesIO(image))\n",
        "    # Resize for consistency + predictable vision-token budget\n",
        "    image = image.resize((225, 225), Image.Resampling.BICUBIC)\n",
        "    \n",
        "    return dict(\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": \"You are a helpful assistant that generates accurate and descriptive captions for images.\"\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": [\n",
        "                    {\n",
        "                        \"type\": \"text\",\n",
        "                        \"text\": \"Describe this image in detail. Focus on the main subjects, actions, and setting.\"\n",
        "                    },\n",
        "                    {\n",
        "                        \"type\": \"image\",\n",
        "                        \"image\": image  # Ray Data accepts PIL Image or image URL.\n",
        "                    }\n",
        "                ]\n",
        "            },\n",
        "        ],\n",
        "        sampling_params=dict(\n",
        "            temperature=0.3,\n",
        "            max_tokens=256,\n",
        "            detokenize=False,\n",
        "        ),\n",
        "    )\n",
        "\n",
        "# Postprocess function extracts the generated caption.\n",
        "def postprocess(row: dict[str, Any]) -> dict[str, Any]:\n",
        "    return {\n",
        "        \"generated_caption\": row[\"generated_text\"],\n",
        "        # Note: Don't include **row here to avoid returning the large image data.\n",
        "        # Include only the fields you need in the output.\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Build the processor\n",
        "\n",
        "With the configuration and functions defined, build the processor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from ray.data.llm import build_llm_processor\n",
        "\n",
        "# Build the LLM processor with the configuration and functions.\n",
        "processor = build_llm_processor(\n",
        "    processor_config,\n",
        "    preprocess=preprocess,\n",
        "    postprocess=postprocess,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Process the dataset\n",
        "\n",
        "Run the processor on your small dataset to perform batch inference. Ray Data automatically distributes the workload across available GPUs and handles batching, retries, and resource management.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pprint import pprint\n",
        "\n",
        "# Run the processor on the small dataset.\n",
        "processed_small = processor(ds_small)\n",
        "\n",
        "# Materialize the dataset to memory.\n",
        "# You can also use writing APIs such as write_parquet() or write_json() to persist the dataset.\n",
        "processed_small = processed_small.materialize()\n",
        "\n",
        "# Display the first 3 entries to verify the output.\n",
        "sampled = processed_small.take(3)\n",
        "print(\"\\n==================GENERATED CAPTIONS===============\\n\")\n",
        "pprint(sampled)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Launch to production with Anyscale Jobs\n",
        "\n",
        "For production workloads, deploy your batch inference processor as an [Anyscale Job](https://docs.anyscale.com/platform/jobs). Anyscale takes care of the infrastructure layer and runs your jobs on your dedicated clusters with automatic retries, monitoring, and scheduling.\n",
        "\n",
        "### Configure an Anyscale Job\n",
        "\n",
        "Save your batch inference code as `batch_vision_inference.py`, then create a job configuration file:\n",
        "\n",
        "```yaml\n",
        "# job.yaml\n",
        "name: llm-batch-inference-vision\n",
        "entrypoint: python batch_inference_vision.py\n",
        "image_uri: anyscale/ray-llm:2.51.1-py311-cu128\n",
        "compute_config:\n",
        "  head_node:\n",
        "    instance_type: m5.2xlarge\n",
        "  worker_nodes:\n",
        "    - instance_type: g6.2xlarge\n",
        "      min_nodes: 1\n",
        "      max_nodes: 10\n",
        "requirements: # Python dependencies - can be list or path to requirements.txt\n",
        "  - datasets==4.4.1\n",
        "working_dir: .\n",
        "max_retries: 2\n",
        "\n",
        "\n",
        "```\n",
        "\n",
        "### Submit\n",
        "\n",
        "Submit your job using the Anyscale CLI:\n",
        "\n",
        "```bash\n",
        "anyscale job submit --config-file job.yaml\n",
        "```\n",
        "\n",
        "### Monitoring\n",
        "\n",
        "Track your job's progress in the Anyscale Console or through the CLI:\n",
        "\n",
        "```bash\n",
        "# Check job status.\n",
        "anyscale job status --name vlm-batch-inference-vision\n",
        "\n",
        "# View logs.\n",
        "anyscale job logs --name vlm-batch-inference-vision\n",
        "```\n",
        "\n",
        "The Ray Dashboard remains available for detailed monitoring. To access it, go over your Anyscale Job in your console.  \n",
        "For cluster-level information, click the **Metrics** tab then **Data** tab, and for task-level information, click the **Ray Workloads** tab then **Data** tab.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Monitor the execution\n",
        "\n",
        "Use the Ray Dashboard to monitor the execution. See [Monitoring your Workload](https://docs.ray.io/en/latest/data/monitoring-your-workload.html) for more information on visualizing your Ray Data jobs.\n",
        "\n",
        "The dashboard shows:\n",
        "- Operator-level metrics (throughput, task execution times).\n",
        "- Resource utilization (CPU, GPU, memory).\n",
        "- Progress and remaining time estimates.\n",
        "- Task status breakdown.\n",
        "\n",
        "**Tip**: If you encounter CUDA out of memory errors, reduce your batch size, use a smaller model, or switch to a larger GPU. For more troubleshooting tips, see [GPU Memory Management](https://docs.ray.io/en/latest/data/working-with-llms.html#gpu-memory-management-and-cuda-oom-prevention).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Scale up to larger datasets\n",
        "\n",
        "Your Ray Data processing pipeline can easily scale up to process more images. The Leopard-Instruct dataset contains approximately 1 million images, making it ideal for demonstrating large-scale batch inference.\n",
        "\n",
        "Redefine your Processor to include GPUs with more memory, increase the batch size and the concurrency."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "processor_config_large_concurrency = vLLMEngineProcessorConfig(\n",
        "    model_source=\"Qwen/Qwen2.5-VL-3B-Instruct\",\n",
        "    engine_kwargs=dict(\n",
        "        max_model_len=MAX_MODEL_LEN,\n",
        "        max_num_batched_tokens=2048\n",
        "    ),\n",
        "    batch_size=64,\n",
        "    accelerator_type=\"L40S\",\n",
        "    concurrency=10,\n",
        "    has_image=True,  # Enable image input.\n",
        ")\n",
        "processor_large_concurrency = build_llm_processor(\n",
        "    processor_config_large_concurrency,\n",
        "    preprocess=preprocess,\n",
        "    postprocess=postprocess,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The following example processes a configurable number of images from the dataset, controlled by the `LARGE_DATASET_LIMIT` environment variable (default: 100k images). You can increase this to process the full 1M images or any subset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# The BLIP3o/BLIP3o-Pretrain-Short-Caption dataset has ~5M images\n",
        "# Configure how many images to process (default: 1M for demonstration).\n",
        "dataset_limit = int(os.environ.get(\"LARGE_DATASET_LIMIT\", 1_000_000))\n",
        "print(f\"Processing {dataset_limit:,} images... (or the whole dataset if you picked >5M)\")\n",
        "ds_large = ds.limit(dataset_limit)\n",
        "\n",
        "# Repartition for better parallelism.\n",
        "num_partitions_large = 128\n",
        "print(f\"Repartitioning dataset into {num_partitions_large} blocks for parallelism...\")\n",
        "ds_large = ds_large.repartition(num_blocks=num_partitions_large)\n",
        "\n",
        "# Run the compute-scaled processor on the larger dataset.\n",
        "processed_large = processor_large_concurrency(ds_large)\n",
        "processed_large = processed_large.materialize()\n",
        "\n",
        "print(f\"\\nProcessed {processed_large.count()} images successfully.\")\n",
        "print(\"\\nSample outputs:\")\n",
        "pprint(processed_large.take(3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Performance optimization tips\n",
        "\n",
        "When scaling to larger datasets, consider these optimizations:\n",
        "\n",
        "**Adjust concurrency**  \n",
        "Increase the `concurrency` parameter to add more parallel workers and GPUs.\n",
        "\n",
        "**Tune batch size**  \n",
        "For vision models, smaller batch sizes (8-32) often work better due to memory constraints from image processing.\n",
        "\n",
        "**Optimize image loading**  \n",
        "Pre-resize images to a consistent size to reduce memory usage and improve throughput.\n",
        "\n",
        "**Repartition strategically**  \n",
        "Use more partitions (blocks) than the number of workers to enable better load balancing.\n",
        "\n",
        "**Enable checkpointing**  \n",
        "For very large datasets, configure checkpointing to recover from failures:\n",
        "\n",
        "```python\n",
        "processed = processor(ds_large).materialize(\n",
        "    checkpoint_path=\"s3://my-bucket/checkpoints/\"\n",
        ")\n",
        "```\n",
        "\n",
        "**Monitor GPU utilization**  \n",
        "Use the Ray Dashboard to identify bottlenecks and adjust parameters.\n",
        "\n",
        "For performance tuning, see the [Ray Data performance guide](https://docs.ray.io/en/latest/data/performance-tips.html). To configure your inference engine, see the [vLLM configuration options](https://docs.vllm.ai/en/latest/serving/engine_args.html).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save results\n",
        "\n",
        "After processing, save the results to a persistent storage location such as S3 or local disk.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save the processed dataset to JSON format (better for text outputs).\n",
        "# Replace this path with your desired output location.\n",
        "output_path_small = \"local:///tmp/processed_captions_small\"\n",
        "output_path_large = \"local:///tmp/processed_captions_large\"\n",
        "\n",
        "print(f\"Saving small processed dataset to {output_path_small}...\")\n",
        "processed_small.write_json(output_path_small)\n",
        "print(\"Saved successfully.\")\n",
        "\n",
        "print(f\"Saving large processed dataset to {output_path_large}...\")\n",
        "processed_large.write_json(output_path_large)\n",
        "print(\"Saved successfully.\")\n",
        "\n",
        "# Alternatively, save as Parquet for better compression:\n",
        "# processed_small.write_parquet(output_path_small)\n",
        "# processed_large.write_parquet(output_path_large)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For more information, see [Saving Data](https://docs.ray.io/en/latest/data/saving-data.html).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "In this notebook, you built an end-to-end vision batch inference pipeline: loading an HuggingFace image dataset into Ray Dataset, configuring a vLLM processor for the Qwen2.5-VL vision-language model, and adding pre/post-processing to generate image captions. You validated the flow on 100,000 images, scaled to 100k images, monitored progress in the Ray Dashboard, and saved the results to persistent storage.\n",
        "\n",
        "See [Anyscale batch inference optimization](https://docs.anyscale.com/llm/batch-inference) for more information on using Ray Data with Anyscale and for more advanced use cases, see [Working with LLMs](https://docs.ray.io/en/latest/data/working-with-llms.html).\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    },
    "orphan": true
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
