{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "3f47ada0",
      "metadata": {},
      "source": [
        "# TPC-H ETL benchmark with Ray Data\n",
        "\n",
        "**Time to complete**: 25 min | **Difficulty**: Intermediate | **Prerequisites**: Python, basic SQL knowledge\n",
        "\n",
        "This template provides a comprehensive introduction to Ray Data for Extract, Transform, Load (ETL) workflows using the industry-standard TPC-H benchmark dataset. We'll cover both practical ETL pipeline construction and the underlying architecture that makes Ray Data powerful for distributed data processing.\n",
        "\n",
        "## Learning Objectives\n",
        "\n",
        "By completing this template, you will master:\n",
        "\n",
        "**Ray Data ETL fundamentals**: Distributed data processing patterns using real enterprise benchmark data  \n",
        "**TPC-H benchmark operations**: Industry-standard queries and transformations used for database performance testing  \n",
        "**Advanced filtering techniques**: Complex data filtering patterns for business intelligence applications  \n",
        "**Production ETL patterns**: Scalable data transformation pipelines from single-node to distributed clusters  \n",
        "\n",
        "## Overview\n",
        "\n",
        "**The TPC-H Challenge**: The TPC-H benchmark simulates a complex business environment with customers, orders, suppliers, and products. Traditional ETL tools struggle with TPC-H's complex relationships and high data volumes that mirror real enterprise scenarios.\n",
        "\n",
        "**Ray Data Solution**: Ray Data provides distributed processing capabilities that handle TPC-H benchmark queries efficiently while demonstrating patterns applicable to real business data processing.\n",
        "\n",
        "**Business Impact**: TPC-H represents realistic business scenarios - the techniques you learn here apply directly to customer analytics, order processing, and supply chain optimization in enterprise environments.\n",
        "\n",
        "## Setup and Imports\n",
        "\n",
        "Let's start by importing the necessary libraries and setting up our environment for TPC-H processing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06dbf08a",
      "metadata": {},
      "outputs": [],
      "source": [
        "import ray\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pyarrow as pa\n",
        "from typing import Dict, Any\n",
        "import time\n",
        "\n",
        "from ray.data import DataContext\n",
        "from ray.data.aggregate import Count, Mean, Sum, Max\n",
        "\n",
        "# Configure Ray Data for cleaner outputs\n",
        "DataContext.get_current().enable_progress_bars = False\n",
        "\n",
        "# Initialize Ray\n",
        "ray.init(ignore_reinit_error=True)\n",
        "\n",
        "print(f\"Ray version: {ray.__version__}\")\n",
        "print(f\"Ray cluster resources: {ray.cluster_resources()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "48043519",
      "metadata": {},
      "source": [
        "## Part 1: Understanding TPC-H and Ray Data\n",
        "\n",
        "### What is TPC-H?\n",
        "\n",
        "The **TPC-H benchmark** is the industry standard for testing database and data processing performance. It simulates a complex business environment with realistic data relationships that mirror enterprise scenarios.\n",
        "\n",
        "**TPC-H Business Context**: The benchmark models a wholesale supplier managing customer orders, inventory, and supplier relationships - representing real-world complexity found in enterprise data systems.\n",
        "\n",
        "### TPC-H Schema Overview\n",
        "\n",
        "The TPC-H benchmark consists of 8 interconnected tables representing a complete business ecosystem:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f83298a4",
      "metadata": {},
      "outputs": [],
      "source": [
        "# TPC-H Schema Overview for ETL Processing\n",
        "tpch_tables = {\n",
        "    \"customer\": \"Customer master data with demographics and market segments\",\n",
        "    \"orders\": \"Order header information with dates, priorities, and status\",\n",
        "    \"lineitem\": \"Detailed line items for each order (largest table)\",\n",
        "    \"part\": \"Parts catalog with specifications and retail prices\", \n",
        "    \"supplier\": \"Supplier information including contact details\",\n",
        "    \"partsupp\": \"Part-supplier relationships with costs\",\n",
        "    \"nation\": \"Nation reference data with geographic regions\",\n",
        "    \"region\": \"Regional groupings for geographic analysis\"\n",
        "}\n",
        "\n",
        "print(\"TPC-H Schema (8 Tables):\")\n",
        "for table, description in tpch_tables.items():\n",
        "    print(f\"  {table.upper()}: {description}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ea9ac2e",
      "metadata": {},
      "source": [
        "## Part 2: Extract - Reading TPC-H Data\n",
        "\n",
        "The **Extract** phase involves reading TPC-H benchmark data from S3. Ray Data's distributed reading capabilities handle the complex table relationships efficiently.\n",
        "\n",
        "### Loading TPC-H Benchmark Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f6fab022",
      "metadata": {},
      "outputs": [],
      "source": [
        "# TPC-H benchmark data location\n",
        "TPCH_S3_PATH = \"s3://ray-benchmark-data/tpch/parquet/sf10\"\n",
        "\n",
        "print(\"Loading TPC-H benchmark data for distributed processing...\")\n",
        "start_time = time.time()\n",
        "\n",
        "# Read TPC-H Customer Master Data\n",
        "customers_ds = ray.data.read_parquet(f\"{TPCH_S3_PATH}/customer\")\n",
        "\n",
        "customers_ds = customers_ds.drop_columns([\"column8\"])\n",
        "customers_ds = customers_ds.rename_columns([\n",
        "    \"c_custkey\",\n",
        "    \"c_name\", \n",
        "    \"c_address\",\n",
        "    \"c_nationkey\",\n",
        "    \"c_phone\",\n",
        "    \"c_acctbal\",\n",
        "    \"c_mktsegment\",\n",
        "    \"c_comment\",\n",
        "])\n",
        "\n",
        "load_time = time.time() - start_time\n",
        "\n",
        "print(f\"TPC-H customer data loaded in {load_time:.2f} seconds\")\n",
        "print(\"Sample customer records:\")\n",
        "customers_ds.limit(3).to_pandas()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "baf3fa01",
      "metadata": {},
      "source": [
        "### Advanced Filtering Techniques\n",
        "\n",
        "Ray Data provides powerful filtering capabilities for complex business logic:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7582e1c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Business-focused filtering for customer segmentation\n",
        "print(\"Advanced Customer Filtering Techniques:\")\n",
        "\n",
        "# Filter high-value customers using business criteria\n",
        "high_value_customers = customers_ds.filter(\n",
        "    lambda x: x[\"c_acctbal\"] > 5000 and x[\"c_mktsegment\"] == \"AUTOMOBILE\"\n",
        ")\n",
        "\n",
        "print(\"High-value automotive customers:\")\n",
        "high_value_customers.limit(5).to_pandas()\n",
        "\n",
        "# Multi-criteria filtering for market analysis\n",
        "premium_segments = customers_ds.filter(\n",
        "    lambda x: x[\"c_mktsegment\"] in [\"AUTOMOBILE\", \"MACHINERY\"] and x[\"c_acctbal\"] > 0\n",
        ")\n",
        "\n",
        "print(\"Premium market segments:\")\n",
        "premium_segments.limit(5).to_pandas()\n",
        "\n",
        "# Geographic filtering for regional analysis\n",
        "def filter_by_region(batch):\n",
        "    \"\"\"Filter customers by geographic criteria.\"\"\"\n",
        "    df = pd.DataFrame(batch)\n",
        "    \n",
        "    # Complex business logic filtering\n",
        "    regional_customers = df[\n",
        "        (df[\"c_nationkey\"].isin([1, 2, 3, 4, 5])) &  # Specific nations\n",
        "        (df[\"c_acctbal\"] > 1000) &  # Minimum balance\n",
        "        (df[\"c_mktsegment\"] != \"HOUSEHOLD\")  # Exclude household segment\n",
        "    ]\n",
        "    \n",
        "    return regional_customers.to_dict('records')\n",
        "\n",
        "# Apply geographic filtering\n",
        "regional_analysis = customers_ds.map_batches(\n",
        "    filter_by_region,\n",
        "    batch_format=\"pandas\"\n",
        ")\n",
        "\n",
        "print(\"Regional customer analysis:\")\n",
        "regional_analysis.limit(5).to_pandas()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6170f959",
      "metadata": {},
      "source": [
        "## Part 3: Transform - Processing TPC-H Data\n",
        "\n",
        "The **Transform** phase applies business logic transformations to TPC-H data, demonstrating enterprise-grade data processing patterns.\n",
        "\n",
        "### Customer Market Segment Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03f59401",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze customer distribution across market segments\n",
        "from ray.data.aggregate import Count, Mean\n",
        "\n",
        "segment_analysis = customers_ds.groupby(\"c_mktsegment\").aggregate(\n",
        "    Count(),\n",
        "    Mean(\"c_acctbal\"),\n",
        ").rename_columns([\"c_mktsegment\", \"customer_count\", \"avg_account_balance\"])\n",
        "\n",
        "print(\"Customer Market Segment Distribution:\")\n",
        "segment_analysis.limit(10).to_pandas()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d71cbdef",
      "metadata": {},
      "source": [
        "### Geographic Reference Data Integration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b77ecaed",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load and process geographic reference data\n",
        "print(\"Loading TPC-H Nations Reference Data:\")\n",
        "\n",
        "nation_ds = ray.data.read_parquet(f\"{TPCH_S3_PATH}/nation\")\n",
        "\n",
        "nation_ds = (\n",
        "    nation_ds\n",
        "    .select_columns([\"column0\", \"column1\", \"column2\", \"column3\"])\n",
        "    .rename_columns([\"n_nationkey\", \"n_name\", \"n_regionkey\", \"n_comment\"])\n",
        ")\n",
        "\n",
        "print(\"Sample nation records:\")\n",
        "nation_ds.limit(5).to_pandas()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c89d6cbe",
      "metadata": {},
      "source": [
        "### Customer Demographics by Nation - Advanced Joins"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85b1c1e5",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Join customer and nation data for geographic analysis\n",
        "print(\"Customer Demographics by Nation:\")\n",
        "print(\"Joining customer and nation data for geographic analysis...\")\n",
        "\n",
        "customer_nation_analysis = (customers_ds\n",
        "    .join(nation_ds, left_on=\"c_nationkey\", right_on=\"n_nationkey\")\n",
        "    .groupby(\"n_name\")\n",
        "    .aggregate(\n",
        "        Count(),\n",
        "        Mean(\"c_acctbal\"),\n",
        "        Sum(\"c_acctbal\"),\n",
        "    )\n",
        "    .rename_columns([\"n_name\", \"customer_count\", \"avg_balance\", \"total_balance\"])\n",
        ")\n",
        "\n",
        "print(\"Customer distribution by nation:\")\n",
        "customer_nation_analysis.sort(\"customer_count\", descending=True).limit(10).to_pandas()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ece22dca",
      "metadata": {},
      "source": [
        "## Part 4: High-Volume Transaction Processing\n",
        "\n",
        "### Orders Data Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9b26974",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load TPC-H Orders table (transaction headers)\n",
        "orders_ds = ray.data.read_parquet(f\"{TPCH_S3_PATH}/orders\")\n",
        "\n",
        "orders_ds = (orders_ds\n",
        "    .select_columns([f\"column{i}\" for i in range(9)])\n",
        "    .rename_columns([\n",
        "        \"o_orderkey\",\n",
        "        \"o_custkey\", \n",
        "        \"o_orderstatus\",\n",
        "        \"o_totalprice\",\n",
        "        \"o_orderdate\",\n",
        "        \"o_orderpriority\",\n",
        "        \"o_clerk\",\n",
        "        \"o_shippriority\", \n",
        "        \"o_comment\",\n",
        "    ])\n",
        ")\n",
        "\n",
        "print(\"TPC-H Orders Data:\")\n",
        "print(\"Sample order records:\")\n",
        "orders_ds.limit(5).to_pandas()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5d07340f",
      "metadata": {},
      "source": [
        "### Business Logic Transformations\n",
        "\n",
        ":::tip GPU Acceleration Option\n",
        "For even faster performance on large datasets, you can replace `pandas` with **NVIDIA RAPIDS cuDF** in the `map_batches` function below. cuDF provides GPU-accelerated DataFrame operations that can significantly speed up complex transformations, especially beneficial for high-volume TPC-H processing.\n",
        "\n",
        "Simply replace `import pandas as pd` with `import cudf as pd` to leverage GPU acceleration for the pandas operations.\n",
        ":::"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "17fe6846",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Apply business transformations to order data\n",
        "def enrich_order_data(batch):\n",
        "    \"\"\"Apply business logic transformations to TPC-H orders.\"\"\"\n",
        "    df = pd.DataFrame(batch)\n",
        "    \n",
        "    # Parse order date and create time dimensions\n",
        "    df['o_orderdate'] = pd.to_datetime(df['o_orderdate'])\n",
        "    df['order_year'] = df['o_orderdate'].dt.year\n",
        "    df['order_quarter'] = df['o_orderdate'].dt.quarter\n",
        "    df['order_month'] = df['o_orderdate'].dt.month\n",
        "    \n",
        "    # Business classifications\n",
        "    df['is_large_order'] = df['o_totalprice'] > 200000\n",
        "    df['is_urgent'] = df['o_orderpriority'].isin(['1-URGENT', '2-HIGH'])\n",
        "    df['revenue_tier'] = pd.cut(\n",
        "        df['o_totalprice'],\n",
        "        bins=[0, 50000, 150000, 300000, float('inf')],\n",
        "        labels=['Small', 'Medium', 'Large', 'Enterprise']\n",
        "    )\n",
        "    \n",
        "    return df.to_dict('records')\n",
        "\n",
        "# Apply transformations to orders\n",
        "enriched_orders = orders_ds.map_batches(\n",
        "    enrich_order_data,\n",
        "    batch_format=\"pandas\"\n",
        ")\n",
        "\n",
        "print(\"Enriched order data with business logic:\")\n",
        "enriched_orders.limit(5).to_pandas()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "97566c94",
      "metadata": {},
      "source": [
        "### Advanced Filtering for Business Intelligence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a219f7f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Advanced filtering techniques for business analysis\n",
        "print(\"Business Intelligence Filtering:\")\n",
        "\n",
        "# Filter urgent large orders requiring special handling\n",
        "urgent_large_orders = enriched_orders.filter(\n",
        "    lambda x: x[\"is_urgent\"] and x[\"is_large_order\"]\n",
        ")\n",
        "\n",
        "print(\"Urgent large orders requiring expedited processing:\")\n",
        "urgent_large_orders.limit(10).to_pandas()\n",
        "\n",
        "# Filter orders by time period for trend analysis\n",
        "recent_orders = enriched_orders.filter(\n",
        "    lambda x: x[\"order_year\"] >= 1995\n",
        ")\n",
        "\n",
        "print(\"Recent orders for trend analysis:\")\n",
        "recent_orders.limit(5).to_pandas()\n",
        "\n",
        "# Complex business filtering for operational insights\n",
        "def complex_business_filter(batch):\n",
        "    \"\"\"Apply complex business filtering logic.\n",
        "    \n",
        "    Note: For GPU acceleration, replace 'import pandas as pd' with 'import cudf as pd'\n",
        "    to leverage NVIDIA RAPIDS for faster DataFrame operations on large datasets.\n",
        "    \"\"\"\n",
        "    df = pd.DataFrame(batch)\n",
        "    \n",
        "    # Multi-criteria business filtering\n",
        "    filtered_df = df[\n",
        "        (df[\"revenue_tier\"].isin([\"Large\", \"Enterprise\"])) &\n",
        "        (df[\"order_quarter\"].isin([3, 4])) &  # Peak seasons\n",
        "        (df[\"o_orderstatus\"] == \"F\")  # Completed orders only\n",
        "    ]\n",
        "    \n",
        "    return filtered_df.to_dict('records')\n",
        "\n",
        "# Apply complex filtering\n",
        "peak_performance_orders = enriched_orders.map_batches(\n",
        "    complex_business_filter,\n",
        "    batch_format=\"pandas\"\n",
        ")\n",
        "\n",
        "print(\"Peak season high-value completed orders:\")\n",
        "peak_performance_orders.limit(10).to_pandas()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58776538",
      "metadata": {},
      "source": [
        "## Part 5: Aggregations and Analytics\n",
        "\n",
        "### Executive Dashboard Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08e7e0dd",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create executive summary dashboard using TPC-H data\n",
        "print(\"Executive Dashboard (Business Intelligence Metrics):\")\n",
        "\n",
        "executive_summary = (enriched_orders\n",
        "    .groupby(\"order_quarter\")\n",
        "    .aggregate(\n",
        "        Count(),\n",
        "        Sum(\"o_totalprice\"),\n",
        "        Mean(\"o_totalprice\"),\n",
        "    )\n",
        "    .rename_columns([\n",
        "        \"order_quarter\",\n",
        "        \"total_orders\",\n",
        "        \"total_revenue\", \n",
        "        \"avg_order_value\",\n",
        "    ])\n",
        ")\n",
        "\n",
        "print(\"Quarterly Business Performance:\")\n",
        "executive_summary.limit(10).to_pandas()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fc2f66dd",
      "metadata": {},
      "source": [
        "### Operational Analytics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb0b2221",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Operational metrics for business process optimization\n",
        "print(\"Operational Analytics:\")\n",
        "\n",
        "operational_metrics = (enriched_orders\n",
        "    .groupby(\"revenue_tier\")\n",
        "    .aggregate(\n",
        "        Count(),\n",
        "        Sum(\"o_totalprice\"),\n",
        "        Mean(\"is_urgent\"),\n",
        "    )\n",
        "    .rename_columns([\n",
        "        \"revenue_tier\",\n",
        "        \"order_volume\",\n",
        "        \"total_revenue\",\n",
        "        \"urgent_order_percentage\",\n",
        "    ])\n",
        ")\n",
        "\n",
        "print(\"Performance by Revenue Tier:\")\n",
        "operational_metrics.limit(10).to_pandas()\n",
        "\n",
        "# Priority-based analysis for order management\n",
        "priority_performance = (enriched_orders\n",
        "    .groupby(\"o_orderpriority\")\n",
        "    .aggregate(\n",
        "        Count(),\n",
        "        Sum(\"o_totalprice\"),\n",
        "        Mean(\"o_totalprice\"),\n",
        "    )\n",
        "    .rename_columns([\n",
        "        \"o_orderpriority\",\n",
        "        \"priority_orders\",\n",
        "        \"priority_revenue\", \n",
        "        \"avg_order_value\",\n",
        "    ])\n",
        ")\n",
        "\n",
        "print(\"Performance by Order Priority:\")\n",
        "priority_performance.sort(\"priority_revenue\", descending=True).limit(10).to_pandas()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "49306005",
      "metadata": {},
      "source": [
        "## Part 6: Advanced ETL Patterns\n",
        "\n",
        "### Line Items Processing (High-Volume Data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "55d30a55",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Process TPC-H line items (largest table in benchmark)\n",
        "print(\"Processing TPC-H Line Items (High-Volume Transaction Data):\")\n",
        "\n",
        "lineitem_ds = ray.data.read_parquet(f\"{TPCH_S3_PATH}/lineitem\")\n",
        "\n",
        "lineitem_cols = [f\"column{str(i).zfill(2)}\" for i in range(16)]\n",
        "lineitem_ds = (lineitem_ds\n",
        "    .select_columns(lineitem_cols)\n",
        "    .rename_columns([\n",
        "        \"l_orderkey\",\n",
        "        \"l_partkey\", \n",
        "        \"l_suppkey\",\n",
        "        \"l_linenumber\",\n",
        "        \"l_quantity\",\n",
        "        \"l_extendedprice\",\n",
        "        \"l_discount\",\n",
        "        \"l_tax\",\n",
        "        \"l_returnflag\",\n",
        "        \"l_linestatus\",\n",
        "        \"l_shipdate\",\n",
        "        \"l_commitdate\",\n",
        "        \"l_receiptdate\", \n",
        "        \"l_shipinstruct\",\n",
        "        \"l_shipmode\",\n",
        "        \"l_comment\",\n",
        "    ])\n",
        ")\n",
        "\n",
        "print(\"Sample line item records:\")\n",
        "lineitem_ds.limit(3).to_pandas()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "35bbecd1",
      "metadata": {},
      "source": [
        "### Revenue Analysis with Complex Filtering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42714046",
      "metadata": {},
      "outputs": [],
      "source": [
        "# TPC-H Query 1: Revenue Analysis with advanced filtering\n",
        "print(\"TPC-H Query 1 - Revenue Analysis:\")\n",
        "\n",
        "# Filter line items by shipping date (business date range filtering)\n",
        "revenue_eligible_items = lineitem_ds.filter(\n",
        "    lambda x: x[\"l_shipdate\"] <= \"1998-09-01\"\n",
        ")\n",
        "\n",
        "# Revenue analysis by return flag and line status\n",
        "revenue_analysis = revenue_eligible_items.groupby([\"l_returnflag\", \"l_linestatus\"]).aggregate(\n",
        "    Count(),\n",
        "    Sum(\"l_quantity\"),\n",
        "    Sum(\"l_extendedprice\"),\n",
        "    Mean(\"l_quantity\"),\n",
        "    Mean(\"l_extendedprice\"),\n",
        ").rename_columns([\n",
        "    \"l_returnflag\",\n",
        "    \"l_linestatus\", \n",
        "    \"order_count\",\n",
        "    \"sum_quantity\",\n",
        "    \"sum_base_price\",\n",
        "    \"avg_quantity\",\n",
        "    \"avg_price\"\n",
        "])\n",
        "\n",
        "print(\"Revenue analysis results:\")\n",
        "revenue_analysis.limit(10).to_pandas()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dff69eb7",
      "metadata": {},
      "source": [
        "### Customer Order Analysis with Joins"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b51ebe8",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Advanced customer analysis combining multiple tables\n",
        "print(\"Customer Order Analysis with Multi-Table Joins:\")\n",
        "\n",
        "# Join customers with their orders for comprehensive analysis\n",
        "customer_orders = customers_ds.join(\n",
        "    orders_ds,\n",
        "    left_on=\"c_custkey\",\n",
        "    right_on=\"o_custkey\"\n",
        ")\n",
        "\n",
        "# Filter for specific business scenarios\n",
        "building_segment_orders = customer_orders.filter(\n",
        "    lambda x: (\n",
        "        x[\"c_mktsegment\"] == \"BUILDING\" and\n",
        "        x[\"o_orderdate\"] >= \"1995-01-01\" and\n",
        "        x[\"o_totalprice\"] > 100000\n",
        "    )\n",
        ")\n",
        "\n",
        "print(\"Building segment high-value orders:\")\n",
        "building_segment_orders.limit(10).to_pandas()\n",
        "\n",
        "# Customer spending analysis with aggregations\n",
        "customer_spending = customer_orders.groupby(\"c_custkey\").aggregate(\n",
        "    Count(),\n",
        "    Sum(\"o_totalprice\"),\n",
        "    Mean(\"o_totalprice\"),\n",
        "    Max(\"o_totalprice\"),\n",
        ").rename_columns([\n",
        "    \"c_custkey\",\n",
        "    \"order_count\", \n",
        "    \"total_spent\",\n",
        "    \"avg_order_value\",\n",
        "    \"largest_order\"\n",
        "])\n",
        "\n",
        "# Find top spending customers\n",
        "top_customers = customer_spending.sort(\"total_spent\", descending=True)\n",
        "print(\"Top customers by spending:\")\n",
        "top_customers.limit(10).to_pandas()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ee9371e",
      "metadata": {},
      "source": [
        "## Part 7: Load - Writing Processed Data\n",
        "\n",
        "The **Load** phase writes the processed TPC-H data to various destinations for business intelligence and analytics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af748bb5",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create output directory for processed TPC-H data\n",
        "import os\n",
        "OUTPUT_PATH = \"/tmp/tpch_etl_output\"\n",
        "os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
        "\n",
        "print(\"Writing TPC-H processed data to various formats...\")\n",
        "\n",
        "# Write business intelligence datasets\n",
        "print(\"Writing customer segment analysis...\")\n",
        "segment_analysis.write_parquet(f\"{OUTPUT_PATH}/customer_segments\")\n",
        "\n",
        "print(\"Writing executive summary metrics...\")\n",
        "executive_summary.write_parquet(f\"{OUTPUT_PATH}/executive_summary\")\n",
        "\n",
        "print(\"Writing customer geographic analysis...\")\n",
        "customer_nation_analysis.write_parquet(f\"{OUTPUT_PATH}/customer_demographics\")\n",
        "\n",
        "print(\"TPC-H ETL pipeline completed successfully!\")\n",
        "print(f\"Processed data available in: {OUTPUT_PATH}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a4997417",
      "metadata": {},
      "source": [
        "## Key Takeaways\n",
        "\n",
        "**TPC-H Benchmark Mastery**: Industry-standard data processing patterns applicable to real enterprise scenarios  \n",
        "**Advanced Filtering**: Complex business logic filtering for sophisticated data analysis requirements  \n",
        "**Distributed ETL**: Scalable patterns that work from single-node development to distributed production clusters  \n",
        "**Ray Data Fundamentals**: Core concepts for building production-ready data processing pipelines  \n",
        "\n",
        "## Troubleshooting Common Issues\n",
        "\n",
        "### **Problem: \"Memory pressure during large joins\"**\n",
        "**Solution**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89dc6a4d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use smaller batch sizes for memory-intensive operations\n",
        "large_join = customers_ds.join(orders_ds, \n",
        "    left_on=\"c_custkey\", right_on=\"o_custkey\"\n",
        ").map_batches(process_function, batch_size=1000, concurrency=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b43a72af",
      "metadata": {},
      "source": [
        "### **Problem: \"Slow aggregation performance\"**\n",
        "**Solution**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01fd9f52",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use Ray Data native aggregations instead of pandas\n",
        "from ray.data.aggregate import Count, Sum, Mean\n",
        "result = dataset.groupby(\"category\").aggregate(\n",
        "    Count(), Sum(\"amount\"), Mean(\"price\")\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dc60a9bd",
      "metadata": {},
      "source": [
        "### **Problem: \"Column name errors\"**\n",
        "**Solution**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c85ff7c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Always check schema before processing\n",
        "print(f\"Available columns: {dataset.schema().names}\")\n",
        "# Use consistent column naming\n",
        "dataset = dataset.rename_columns([\"col1\", \"col2\", \"col3\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "75a6eb59",
      "metadata": {},
      "source": [
        "## Action Items\n",
        "\n",
        "### **Immediate Implementation (This Week)**\n",
        "- [ ] Run TPC-H queries on your cluster to understand performance characteristics\n",
        "- [ ] Implement basic joins and aggregations using Ray Data native operations\n",
        "- [ ] Monitor resource utilization with Ray Dashboard\n",
        "- [ ] Test with different batch sizes to find optimal configuration\n",
        "\n",
        "### **Production Scaling (Next Month)**\n",
        "- [ ] Implement additional TPC-H benchmark queries using Ray Data patterns\n",
        "- [ ] Adapt these patterns to your organization's data processing needs\n",
        "- [ ] Scale to production clusters for processing larger TPC-H scale factors\n",
        "- [ ] Set up automated performance monitoring and optimization\n",
        "\n",
        "## Next Steps\n",
        "\n",
        "**Advanced Ray Data features**: Explore Ray Data's ML integration and GPU acceleration capabilities  \n",
        "**Production deployment**: Scale this template to enterprise TPC-H implementations  \n",
        "**Performance optimization**: Use Ray Dashboard for cluster tuning and resource optimization  \n",
        "\n",
        "---\n",
        "\n",
        "*The TPC-H benchmark provides realistic business scenarios - the techniques demonstrated here apply directly to enterprise customer analytics, order processing, and business intelligence applications.*"
      ]
    }
  ],
  "metadata": {},
  "nbformat": 4,
  "nbformat_minor": 5
}