{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Training with Ray Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Batch training** and tuning are common tasks in simple machine learning use-cases such as time series forecasting. They require fitting of simple models on data batches corresponding to different locations, products, etc.  Batch training can take less time to process all the data at once, but only if those batches can run in parallel!\n",
    "\n",
    "This notebook showcases how to conduct batch training regression algorithms from [XGBoost](https://docs.ray.io/en/latest/tune/examples/tune-xgboost.html) and [Scikit-learn](https://docs.ray.io/en/latest/ray-more-libs/joblib.html) with **[Ray Datasets](https://docs.ray.io/en/latest/data/dataset.html)**.  **XGBoost** is a popular open-source library used for regression and classification.  **Scikit-learn** is a popular open-source library with a vast assortment of well-known ML algorithms.\n",
    "\n",
    "![Batch training diagram](../../data/examples/images/batch-training.svg)\n",
    "\n",
    "For the data, we will use the [NYC Taxi dataset](https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page). This popular tabular dataset contains historical taxi pickups by timestamp and location in NYC.\n",
    "\n",
    "For the training, we will train separate regression models to predict `trip_duration`, with a different model for each dropoff location in NYC.  Specifically, we will conduct an experiment for each `dropoff_location_id`, to find the best either XGBoost or Scikit-learn model, per location."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Contents\n",
    "\n",
    "In this this tutorial, you will learn about:\n",
    " 1. [Creating a Ray Dataset](#create_ds)\n",
    " 2. [Filtering a Ray Dataset on Read](#filter_ds)\n",
    " 3. [Inspecting a Ray Dataset](#inspect_ds)\n",
    " 4. [Transforming a Ray Dataset in parallel](#transform_ds)\n",
    " 5. [Batch training with Ray Datasets in parallel](#batch_train_ds)\n",
    "\n",
    "# Walkthrough\n",
    "\n",
    "Let us start by importing a few required libraries, including open-source [Ray](https://github.com/ray-project/ray) itself!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: pip: command not found\n"
     ]
    }
   ],
   "source": [
    "# Ray datasets current requires pyarrow < 7.0.0\n",
    "!pip install -U \"pyarrow<7.0.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of CPUs in this system: 8\n",
      "numpy: 1.23.3\n",
      "pyarrow: 6.0.1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(f'Number of CPUs in this system: {os.cpu_count()}')\n",
    "from typing import Tuple, List, Union, Optional, Callable\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(f\"numpy: {np.__version__}\")\n",
    "import pyarrow\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow.dataset as pds\n",
    "\n",
    "print(f\"pyarrow: {pyarrow.__version__}\")\n",
    "from ray.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-06 14:28:21,327\tINFO worker.py:1223 -- Using address localhost:9031 set in the environment variable RAY_ADDRESS\n",
      "2022-12-06 14:28:21,328\tINFO worker.py:1333 -- Connecting to existing Ray cluster at address: 172.31.223.118:9031...\n",
      "2022-12-06 14:28:21,335\tINFO worker.py:1509 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32mhttps://console.anyscale-staging.com/api/v2/sessions/ses_gyl6mbksa8xt7b149ib6abld/services?redirect_to=dashboard \u001b[39m\u001b[22m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "    <div style=\"margin-left: 50px;display: flex;flex-direction: row;align-items: center\">\n",
       "        <h3 style=\"color: var(--jp-ui-font-color0)\">Ray</h3>\n",
       "        <svg version=\"1.1\" id=\"ray\" width=\"3em\" viewBox=\"0 0 144.5 144.6\" style=\"margin-left: 3em;margin-right: 3em\">\n",
       "            <g id=\"layer-1\">\n",
       "                <path fill=\"#00a2e9\" class=\"st0\" d=\"M97.3,77.2c-3.8-1.1-6.2,0.9-8.3,5.1c-3.5,6.8-9.9,9.9-17.4,9.6S58,88.1,54.8,81.2c-1.4-3-3-4-6.3-4.1\n",
       "                    c-5.6-0.1-9.9,0.1-13.1,6.4c-3.8,7.6-13.6,10.2-21.8,7.6C5.2,88.4-0.4,80.5,0,71.7c0.1-8.4,5.7-15.8,13.8-18.2\n",
       "                    c8.4-2.6,17.5,0.7,22.3,8c1.3,1.9,1.3,5.2,3.6,5.6c3.9,0.6,8,0.2,12,0.2c1.8,0,1.9-1.6,2.4-2.8c3.5-7.8,9.7-11.8,18-11.9\n",
       "                    c8.2-0.1,14.4,3.9,17.8,11.4c1.3,2.8,2.9,3.6,5.7,3.3c1-0.1,2,0.1,3,0c2.8-0.5,6.4,1.7,8.1-2.7s-2.3-5.5-4.1-7.5\n",
       "                    c-5.1-5.7-10.9-10.8-16.1-16.3C84,38,81.9,37.1,78,38.3C66.7,42,56.2,35.7,53,24.1C50.3,14,57.3,2.8,67.7,0.5\n",
       "                    C78.4-2,89,4.7,91.5,15.3c0.1,0.3,0.1,0.5,0.2,0.8c0.7,3.4,0.7,6.9-0.8,9.8c-1.7,3.2-0.8,5,1.5,7.2c6.7,6.5,13.3,13,19.8,19.7\n",
       "                    c1.8,1.8,3,2.1,5.5,1.2c9.1-3.4,17.9-0.6,23.4,7c4.8,6.9,4.6,16.1-0.4,22.9c-5.4,7.2-14.2,9.9-23.1,6.5c-2.3-0.9-3.5-0.6-5.1,1.1\n",
       "                    c-6.7,6.9-13.6,13.7-20.5,20.4c-1.8,1.8-2.5,3.2-1.4,5.9c3.5,8.7,0.3,18.6-7.7,23.6c-7.9,5-18.2,3.8-24.8-2.9\n",
       "                    c-6.4-6.4-7.4-16.2-2.5-24.3c4.9-7.8,14.5-11,23.1-7.8c3,1.1,4.7,0.5,6.9-1.7C91.7,98.4,98,92.3,104.2,86c1.6-1.6,4.1-2.7,2.6-6.2\n",
       "                    c-1.4-3.3-3.8-2.5-6.2-2.6C99.8,77.2,98.9,77.2,97.3,77.2z M72.1,29.7c5.5,0.1,9.9-4.3,10-9.8c0-0.1,0-0.2,0-0.3\n",
       "                    C81.8,14,77,9.8,71.5,10.2c-5,0.3-9,4.2-9.3,9.2c-0.2,5.5,4,10.1,9.5,10.3C71.8,29.7,72,29.7,72.1,29.7z M72.3,62.3\n",
       "                    c-5.4-0.1-9.9,4.2-10.1,9.7c0,0.2,0,0.3,0,0.5c0.2,5.4,4.5,9.7,9.9,10c5.1,0.1,9.9-4.7,10.1-9.8c0.2-5.5-4-10-9.5-10.3\n",
       "                    C72.6,62.3,72.4,62.3,72.3,62.3z M115,72.5c0.1,5.4,4.5,9.7,9.8,9.9c5.6-0.2,10-4.8,10-10.4c-0.2-5.4-4.6-9.7-10-9.7\n",
       "                    c-5.3-0.1-9.8,4.2-9.9,9.5C115,72.1,115,72.3,115,72.5z M19.5,62.3c-5.4,0.1-9.8,4.4-10,9.8c-0.1,5.1,5.2,10.4,10.2,10.3\n",
       "                    c5.6-0.2,10-4.9,9.8-10.5c-0.1-5.4-4.5-9.7-9.9-9.6C19.6,62.3,19.5,62.3,19.5,62.3z M71.8,134.6c5.9,0.2,10.3-3.9,10.4-9.6\n",
       "                    c0.5-5.5-3.6-10.4-9.1-10.8c-5.5-0.5-10.4,3.6-10.8,9.1c0,0.5,0,0.9,0,1.4c-0.2,5.3,4,9.8,9.3,10\n",
       "                    C71.6,134.6,71.7,134.6,71.8,134.6z\"/>\n",
       "            </g>\n",
       "        </svg>\n",
       "        <table>\n",
       "            <tr>\n",
       "                <td style=\"text-align: left\"><b>Python version:</b></td>\n",
       "                <td style=\"text-align: left\"><b>3.8.5</b></td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                <td style=\"text-align: left\"><b>Ray version:</b></td>\n",
       "                <td style=\"text-align: left\"><b> 2.0.0</b></td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "    <td style=\"text-align: left\"><b>Dashboard:</b></td>\n",
       "    <td style=\"text-align: left\"><b><a href=\"http://console.anyscale-staging.com/api/v2/sessions/ses_gyl6mbksa8xt7b149ib6abld/services?redirect_to=dashboard\" target=\"_blank\">http://console.anyscale-staging.com/api/v2/sessions/ses_gyl6mbksa8xt7b149ib6abld/services?redirect_to=dashboard</a></b></td>\n",
       "</tr>\n",
       "\n",
       "        </table>\n",
       "    </div>\n",
       "</div>\n"
      ],
      "text/plain": [
       "RayContext(dashboard_url='console.anyscale-staging.com/api/v2/sessions/ses_gyl6mbksa8xt7b149ib6abld/services?redirect_to=dashboard', python_version='3.8.5', ray_version='2.0.0', ray_commit='cba26cc83f6b5b8a2ff166594a65cb74c0ec8740', address_info={'node_ip_address': '172.31.223.118', 'raylet_ip_address': '172.31.223.118', 'redis_address': None, 'object_store_address': '/tmp/ray/session_2022-12-06_14-23-38_504627_162/sockets/plasma_store', 'raylet_socket_name': '/tmp/ray/session_2022-12-06_14-23-38_504627_162/sockets/raylet', 'webui_url': 'console.anyscale-staging.com/api/v2/sessions/ses_gyl6mbksa8xt7b149ib6abld/services?redirect_to=dashboard', 'session_dir': '/tmp/ray/session_2022-12-06_14-23-38_504627_162', 'metrics_export_port': 61063, 'gcs_address': '172.31.223.118:9031', 'address': '172.31.223.118:9031', 'dashboard_agent_listen_port': 52365, 'node_id': '43e92b348c3def5d59a37cd84a246e215f7be7e0565af0fda88fcfb3'})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ray\n",
    "\n",
    "if ray.is_initialized():\n",
    "    ray.shutdown()\n",
    "ray.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'CPU': 8.0, 'object_store_memory': 9093825331.0, 'memory': 18187650663.0, 'node:172.31.223.118': 1.0}\n"
     ]
    }
   ],
   "source": [
    "print(ray.cluster_resources())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For benchmarking purposes, we can print the times of various operations.\n",
    "# In order to reduce clutter in the output, this is set to False by default.\n",
    "PRINT_TIMES = False\n",
    "\n",
    "\n",
    "def print_time(msg: str):\n",
    "    if PRINT_TIMES:\n",
    "        print(msg)\n",
    "\n",
    "\n",
    "# To speed things up, we’ll only use a small subset of the full dataset consisting of two last months of 2019.\n",
    "# You can choose to use the full dataset for 2018-2019 by setting the SMOKE_TEST variable to False.\n",
    "SMOKE_TEST = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Ray Dataset <a class=\"anchor\" id=\"create_ds\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{tip}\n",
    "Ray Datasets uses PyArrow dataset and table for reading or writing large parquet files.  Its native multithreaded C++ adpater is faster than pandas read_parquet, even using engine=pyarrow.  For more details see [Ray Datasets User Guide](https://docs.ray.io/en/latest/data/user-guide.html).\n",
    "```\n",
    "\n",
    "[Ray Datasets](datasets) are the standard way to load and exchange data in Ray libraries and applications. We will use the [Ray Dataset APIs](dataset-api) to read the data and quickly inspect it.\n",
    "\n",
    "First, we will define some global variables we will use throughout the notebook, such as the list of S3 links to the files making up the dataset and the possible location IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NYC Taxi using 1 file(s)!\n",
      "s3_files: ['s3://anonymous@air-example-data/ursa-labs-taxi-data/by_year/2019/06/data.parquet/ab5b9d2b8cc94be19346e260b543ec35_000000.parquet']\n",
      "Locations: [141, 229, 173]\n"
     ]
    }
   ],
   "source": [
    "# Define some global variables.\n",
    "TARGET = \"trip_duration\"\n",
    "s3_partitions = pds.dataset(\n",
    "    \"s3://anonymous@air-example-data/ursa-labs-taxi-data/by_year/\",\n",
    "    partitioning=[\"year\", \"month\"],\n",
    ")\n",
    "s3_files = [f\"s3://anonymous@{file}\" for file in s3_partitions.files]\n",
    "\n",
    "# Obtain all location IDs\n",
    "location_ids = (\n",
    "    pq.read_table(s3_files[0], columns=[\"dropoff_location_id\"])[\"dropoff_location_id\"]\n",
    "    .unique()\n",
    "    .to_pylist()\n",
    ")\n",
    "\n",
    "# Use smoke testing or not.\n",
    "starting_idx = -1 if SMOKE_TEST else 0\n",
    "# TODO: drop location 199 to test error-handling before final git checkin\n",
    "sample_locations = [141, 229, 173] if SMOKE_TEST else all_location_ids\n",
    "\n",
    "# Display what data will be used.\n",
    "s3_files = s3_files[starting_idx:]\n",
    "print(f\"NYC Taxi using {len(s3_files)} file(s)!\")\n",
    "print(f\"s3_files: {s3_files}\")\n",
    "print(f\"Locations: {sample_locations}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The easiest way to create a ray dataset is to use `ray.data.read_parquet` to read parquet files in parallel onto the Ray cluster.\n",
    "\n",
    "Uncomment the cell below if you want to try it out.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # This cell commented out because it can take a long time!\n",
    "# # In the next section \"Filtering Read\" we make it faster.\n",
    "\n",
    "# # Read everything in the files list into a ray dataset.\n",
    "# start = time.time()\n",
    "# ds = ray.data.read_parquet(s3_files)\n",
    "# print(f\"Data loading time: {data_loading_time:.2f} seconds\")\n",
    "# ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering a Ray Dataset on Read <a class=\"anchor\" id=\"filter_ds\"></a>\n",
    "\n",
    "Normally there is some last-mile data processing required before training. Let's just assume we know the data processing steps are:\n",
    "- Drop negative trip distances, 0 fares, 0 passengers.\n",
    "- Drop 2 unknown zones: `['264', '265']`.\n",
    "- Calculate trip duration and add it as a new column.\n",
    "- Drop trip durations smaller than 1 minute and greater than 24 hours.\n",
    "\n",
    "Instead of blindly reading all the data, it would be better if we only read the data we needed.  This is similar concept to SQL `SELECT only rows, columns you need` vs `SELECT *`.\n",
    "\n",
    "```{tip}\n",
    "Best practice is to filter as much as you can directly in the Ray Dataset `read_parquet()`.\n",
    "```\n",
    "\n",
    "Note that Ray Datasets' Parquet reader supports projection (column selection) and row filter pushdown, where we can push the above column selection and the row-based filter to the Parquet read. If we specify column selection at Parquet read time, the unselected columns won't even be read from disk. This can save a lot of memory, especially with big datasets, and allow us to avoid OOM issues.\n",
    "\n",
    "The row-based filter is specified via [Arrow's dataset field expressions](https://arrow.apache.org/docs/6.0/python/generated/pyarrow.dataset.Expression.html#pyarrow.dataset.Expression). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pushdown_read_data(files_list: list, sample_ids: list) -> Dataset:\n",
    "    start = time.time()\n",
    "\n",
    "    filter_expr = (\n",
    "        (pds.field(\"passenger_count\") > 0)\n",
    "        & (pds.field(\"trip_distance\") > 0)\n",
    "        & (pds.field(\"fare_amount\") > 0)\n",
    "        & (~pds.field(\"pickup_location_id\").isin([264, 265]))\n",
    "        & (~pds.field(\"dropoff_location_id\").isin([264, 265]))\n",
    "        & (pds.field(\"dropoff_location_id\").isin(sample_ids))\n",
    "    )\n",
    "\n",
    "    dataset = ray.data.read_parquet(\n",
    "        files_list,\n",
    "        columns=[\n",
    "            \"pickup_at\",\n",
    "            \"dropoff_at\",\n",
    "            \"pickup_location_id\",\n",
    "            \"dropoff_location_id\",\n",
    "            \"passenger_count\",\n",
    "            \"trip_distance\",\n",
    "            \"fare_amount\",\n",
    "        ],\n",
    "        filter=filter_expr,\n",
    "    )\n",
    "\n",
    "    data_loading_time = time.time() - start\n",
    "    print_time(f\"Data loading time: {data_loading_time:.2f} seconds\")\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-06 14:28:23,404\tWARNING read_api.py:291 -- ⚠️  The number of blocks in this dataset (1) limits its parallelism to 1 concurrent tasks. This is much less than the number of available CPU slots in the cluster. Use `.repartition(n)` to increase the number of dataset blocks.\n"
     ]
    }
   ],
   "source": [
    "# Test the pushdown_read_data function\n",
    "ds_raw = pushdown_read_data(s3_files, sample_locations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting a Ray Dataset <a class=\"anchor\" id=\"inspect_ds\"></a>\n",
    "\n",
    "Let's get some basic statistics about our newly created Ray Dataset.\n",
    "\n",
    "As our Ray Dataset is backed by Parquet, we can obtain the number of rows from the metadata without triggering a full data read.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 6941024\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of rows: {ds_raw.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we can obtain the Dataset size (in bytes) from the metadata.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size bytes (from parquet metadata): 925892280\n"
     ]
    }
   ],
   "source": [
    "print(f\"Size bytes (from parquet metadata): {ds_raw.size_bytes()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Let's fetch and inspect the schema of the underlying Parquet files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Schema data types:\n",
      "pickup_at: timestamp[us]\n",
      "dropoff_at: timestamp[us]\n",
      "pickup_location_id: int32\n",
      "dropoff_location_id: int32\n",
      "passenger_count: int8\n",
      "trip_distance: float\n",
      "fare_amount: float\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nSchema data types:\")\n",
    "data_types = list(zip(ds_raw.schema().names, ds_raw.schema().types))\n",
    "for s in data_types:\n",
    "    print(f\"{s[0]}: {s[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming a Ray Dataset in parallel using custom functions <a class=\"anchor\" id=\"transform_ds\"></a>\n",
    "\n",
    "Ray Datasets allows you to specify custom data transform functions. These [user defined functions (UDFs)](transforming_datasets) can be called using `Dataset.map_batches(my_udf_function)`. The transformation will be conducted in parallel for each data batch.\n",
    "\n",
    "```{tip}\n",
    "You may need to call `Dataset.repartition(n)` first to split the Dataset into more blocks internally. By default, each block corresponds to one file. The upper bound of parallelism is the number of blocks.\n",
    "```\n",
    "\n",
    "You can specify the data format you are using in the `batch_format` parameter. The dataset will be divided into batches and those batches converted into the specified format. Available data formats you can specify in the `batch_format` paramater include `\"pandas\", \"pyarrow\", \"numpy\"`. Tabular data will be passed into your UDF by default as a pandas DataFrame. Tensor data will be passed into your UDF as a numpy array.\n",
    "\n",
    "Here, we will use `batch_format=\"pandas\"` explicitly for clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A pandas DataFrame UDF for transforming the Dataset in parallel.\n",
    "def transform_df(input_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = input_df.copy()\n",
    "    \n",
    "    # calculate trip_duration\n",
    "    df[\"trip_duration\"] = (df[\"dropoff_at\"] - df[\"pickup_at\"]).dt.seconds\n",
    "    # filter trip_durations > 1 minute and less than 24 hours\n",
    "    df = df[df[\"trip_duration\"] > 60]\n",
    "    df = df[df[\"trip_duration\"] < 24 * 60 * 60]\n",
    "    # keep only necessary columns\n",
    "    df.drop(\n",
    "        [\"dropoff_at\", \n",
    "         \"pickup_at\", \n",
    "         \"pickup_location_id\", \n",
    "         \"fare_amount\"],\n",
    "        axis=1,\n",
    "        inplace=True,\n",
    "    )\n",
    "    df[\"dropoff_location_id\"] = df[\"dropoff_location_id\"].fillna(-1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows before transformation: 6941024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read: 100%|██████████| 1/1 [00:01<00:00,  1.80s/it]\n",
      "Repartition: 100%|██████████| 14/14 [00:01<00:00,  8.78it/s]\n",
      "Map_Batches: 100%|██████████| 14/14 [00:01<00:00,  9.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows after transformation: 285323\n",
      "CPU times: user 367 ms, sys: 139 ms, total: 506 ms\n",
      "Wall time: 4.89 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "# Test the transform UDF.\n",
    "print(f\"Number of rows before transformation: {ds_raw.count()}\")\n",
    "\n",
    "# Repartition the dataset to allow for higher parallelism.\n",
    "ds = ds_raw.repartition(14) \n",
    "\n",
    "# .map_batches applies a UDF to each partition of the data in parallel.\n",
    "ds = ds.map_batches(transform_df, batch_format=\"pandas\")\n",
    "\n",
    "# Verify row count.\n",
    "print(f\"Number of rows after transformation: {ds.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch training with Ray Datasets <a class=\"anchor\" id=\"batch_train_ds\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have learned more about our data and written a pandas UDF to transform our data, we are ready to train a model on batches of this data in parallel.\n",
    "\n",
    "1. We will use the `dropoff_location_id` column in the dataset to group the dataset into data batches. \n",
    "2. Then we will fit a separate model for each batch to predict `trip_duration`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn: 1.1.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ray/anaconda3/lib/python3.8/site-packages/xgboost/compat.py:31: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xgboost: 1.3.3\n"
     ]
    }
   ],
   "source": [
    "# import standard sklearn libraries\n",
    "import sklearn\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "print(f\"sklearn: {sklearn.__version__}\")\n",
    "import xgboost as xgb\n",
    "\n",
    "print(f\"xgboost: {xgb.__version__}\")\n",
    "# set global random seed for sklearn models\n",
    "np.random.seed(415)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define search space for training\n",
    "\n",
    "In this notebook, we will run parallel training jobs per data batch, drop-off location. The training jobs will be defined using a search space and simple grid search. Depending on your need, fancier search spaces and search algorithms are possible with [Ray Tune](https://docs.ray.io/en/latest/tune/examples/batch_tuning.html).\n",
    "\n",
    "**Below, we define our search space consists of:**\n",
    "\n",
    "- Different algorithms, either:\n",
    "  - Linear Regression or XGBoost Tree Regression.\n",
    "  \n",
    "We want to train using every algorithm in the search space. What this means is every algorithm will be applied to every NYC Taxi drop-off location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALGORITHMS = [LinearRegression(fit_intercept=True), xgb.XGBRegressor(max_depth=4)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Define training functions\n",
    "\n",
    "We want to fit a linear regression model to the trip duration for each drop-off location. For scoring, we will calculate mean absolute error on the validation set, and report that as model error per drop-off location.\n",
    "\n",
    "The `fit_and_score_sklearn` function contains the logic necessary to fit a scikit-learn model and evaluate it using mean absolute error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_and_score_sklearn(\n",
    "    train_df: pd.DataFrame, \n",
    "    test_df: pd.DataFrame, \n",
    "    model: BaseEstimator) -> pd.DataFrame:\n",
    "    \n",
    "    # Assemble train/test pandas dfs\n",
    "    train_X = train_df[[\"passenger_count\", \"trip_distance\"]]\n",
    "    train_y = train_df[TARGET]\n",
    "    test_X = test_df[[\"passenger_count\", \"trip_distance\"]]\n",
    "    test_y = test_df[TARGET]\n",
    "\n",
    "    # Start training.\n",
    "    model = model.fit(train_X, train_y)\n",
    "    pred_y = model.predict(test_X)\n",
    "    \n",
    "    # Evaluate.\n",
    "    error = sklearn.metrics.mean_absolute_error(test_y, pred_y)\n",
    "    if error is None:\n",
    "        error = 10000.0\n",
    "    \n",
    "    # Assemble return as a pandas dataframe.\n",
    "    return_df = pd.DataFrame({'model': [model], 'error': [error]})\n",
    "\n",
    "    # return model, error\n",
    "    return return_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `train_and_evaluate` function contains the logic for train-test splitting and fitting of a model using the `fit_and_score_sklearn` function.\n",
    "\n",
    "As an input, this function takes in a pandas DataFrame. When we call `Dataset.map_batches` or `Dataset.groupby().map_groups()`, the Dataset will be batched into multiple pandas DataFrames and this function will run for each batch in parallel. We will return the model and its error. Those results will be collected back into a Ray Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(\n",
    "    df: pd.DataFrame, \n",
    "    models: List[BaseEstimator], \n",
    "    location_id: int\n",
    ") -> pd.DataFrame:\n",
    "    \n",
    "    # We need at least 4 rows to create a train / test split.\n",
    "    if len(df) < 4:\n",
    "        print_time(f\"Data batch for LocID {location_id} is empty or smaller than 4 rows\")\n",
    "        return None\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    # Train / test split\n",
    "    # Randomly split the data into 80/20 train/test.\n",
    "    train_df, test_df = train_test_split(df, test_size=0.2, shuffle=True)\n",
    "\n",
    "    # Launch a fit and score task for each model.\n",
    "    # results is a list of pandas dataframes, one per model\n",
    "    results = [fit_and_score_sklearn(train_df, test_df, model) for model in models]\n",
    "\n",
    "    # Assemble location_id, name of model, and metrics in a pandas DataFrame\n",
    "    # results_df = pd.concat(results) \n",
    "    results_df = pd.concat(results, axis=0, join='inner', ignore_index=True)\n",
    "    results_df.insert(0, column='location_id', value=location_id)\n",
    "\n",
    "    training_time = time.time() - start\n",
    "    print_time(f\"Training time for LocID {location_id}: {training_time:.2f} seconds\")\n",
    "\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall how we wrote a data transform `transform_batch` UDF? It was called with pattern:\n",
    "- `Dataset.map_batches(transform_batch, batch_format=\"pandas\")`\n",
    "\n",
    "Similarly, we can write a custom groupy-aggregate function `agg_func` which will run for each [Ray Dataset *group-by*](datasets-groupbys) group in parallel. The usage pattern is:\n",
    "- `Dataset.groupby(column).map_groups(agg_func, batch_format=\"pandas\")`.\n",
    "\n",
    "In the cell below, we define our custom `agg_func`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A Pandas DataFrame aggregation function for processing \n",
    "# grouped batches of Ray Dataset data.\n",
    "def agg_func(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    location_id = df[\"dropoff_location_id\"][0]\n",
    "\n",
    "    # Handle errors in data groups\n",
    "    try:\n",
    "        # Transform the input pandas AND fit_and_evaluate the transformed pandas\n",
    "        results_df = train_and_evaluate(df, ALGORITHMS, location_id)\n",
    "        assert results_df is not None\n",
    "    except Exception:\n",
    "        # assemble a null entry\n",
    "        print(f\"Failed on LocID {location_id}!\")\n",
    "        results_df = pd.DataFrame([[location_id, None, 10000.0]], \n",
    "                         columns=[\"location_id\", \"model\", \"error\"],\n",
    "                         dtypes=[\"int32\", BaseEstimator, \"float64\"])\n",
    "\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run batch training using `map_groups`\n",
    "\n",
    "Finally, the main \"driver code\" reads each Parquet file (where each file corresponds to one month of NYC taxi data) into a Ray Dataset `ds`. Then we use Ray Dataset *group-by* to map each group into a batch of data and run `agg_func` on each of them in parallel by calling `ds.groupby(\"dropoff_location_id\").map_groups(agg_func, batch_format=\"pandas\")`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sort Sample: 100%|██████████| 14/14 [00:01<00:00, 13.66it/s]\n",
      "Shuffle Map: 100%|██████████| 14/14 [00:00<00:00, 42.54it/s]\n",
      "Shuffle Reduce: 100%|██████████| 14/14 [00:00<00:00, 262.62it/s]\n",
      "Map_Batches:  64%|██████▍   | 9/14 [00:00<00:00, 13.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=1401)\u001b[0m Failed on LocID 229!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=1401)\u001b[0m 2022-12-06 14:28:47,419\tINFO worker.py:756 -- Task failed with retryable exception: TaskID(575dd43a3ded8278ffffffffffffffffffffffff01000000).\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=1401)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=1401)\u001b[0m   File \"/tmp/ipykernel_1185/2951074600.py\", line 9, in agg_func\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=1401)\u001b[0m NameError: name 'MODELS' is not defined\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=1401)\u001b[0m \n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=1401)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=1401)\u001b[0m \n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=1401)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=1401)\u001b[0m   File \"python/ray/_raylet.pyx\", line 662, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=1401)\u001b[0m   File \"python/ray/_raylet.pyx\", line 666, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=1401)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/data/_internal/compute.py\", line 449, in _map_block_nosplit\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=1401)\u001b[0m     for new_block in block_fn(block, *fn_args, **fn_kwargs):\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=1401)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/data/dataset.py\", line 457, in transform\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=1401)\u001b[0m     applied = batch_fn(view, *fn_args, **fn_kwargs)\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=1401)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/data/grouped_dataset.py\", line 281, in group_fn\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=1401)\u001b[0m     applied = fn(group)\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=1401)\u001b[0m   File \"/tmp/ipykernel_1185/2951074600.py\", line 14, in agg_func\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=1401)\u001b[0m TypeError: __init__() got an unexpected keyword argument 'dtypes'\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=1596)\u001b[0m 2022-12-06 14:28:51,507\tINFO worker.py:756 -- Task failed with retryable exception: TaskID(4481ec012324614bffffffffffffffffffffffff01000000).\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=1596)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=1596)\u001b[0m   File \"/tmp/ipykernel_1185/2951074600.py\", line 9, in agg_func\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=1596)\u001b[0m NameError: name 'MODELS' is not defined\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=1596)\u001b[0m \n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=1596)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=1596)\u001b[0m \n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=1596)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=1596)\u001b[0m   File \"python/ray/_raylet.pyx\", line 662, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=1596)\u001b[0m   File \"python/ray/_raylet.pyx\", line 666, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=1596)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/data/_internal/compute.py\", line 449, in _map_block_nosplit\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=1596)\u001b[0m     for new_block in block_fn(block, *fn_args, **fn_kwargs):\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=1596)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/data/dataset.py\", line 457, in transform\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=1596)\u001b[0m     applied = batch_fn(view, *fn_args, **fn_kwargs)\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=1596)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/data/grouped_dataset.py\", line 281, in group_fn\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=1596)\u001b[0m     applied = fn(group)\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=1596)\u001b[0m   File \"/tmp/ipykernel_1185/2951074600.py\", line 14, in agg_func\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=1596)\u001b[0m TypeError: __init__() got an unexpected keyword argument 'dtypes'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=1596)\u001b[0m Failed on LocID 141!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map_Batches:  86%|████████▌ | 12/14 [00:20<00:00, 13.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=1401)\u001b[0m Failed on LocID 229!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=1401)\u001b[0m 2022-12-06 14:29:01,403\tINFO worker.py:756 -- Task failed with retryable exception: TaskID(575dd43a3ded8278ffffffffffffffffffffffff01000000).\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=1401)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=1401)\u001b[0m   File \"/tmp/ipykernel_1185/2951074600.py\", line 9, in agg_func\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=1401)\u001b[0m NameError: name 'MODELS' is not defined\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=1401)\u001b[0m \n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=1401)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=1401)\u001b[0m \n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=1401)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=1401)\u001b[0m   File \"python/ray/_raylet.pyx\", line 662, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=1401)\u001b[0m   File \"python/ray/_raylet.pyx\", line 666, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=1401)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/data/_internal/compute.py\", line 449, in _map_block_nosplit\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=1401)\u001b[0m     for new_block in block_fn(block, *fn_args, **fn_kwargs):\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=1401)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/data/dataset.py\", line 457, in transform\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=1401)\u001b[0m     applied = batch_fn(view, *fn_args, **fn_kwargs)\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=1401)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/data/grouped_dataset.py\", line 281, in group_fn\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=1401)\u001b[0m     applied = fn(group)\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=1401)\u001b[0m   File \"/tmp/ipykernel_1185/2951074600.py\", line 14, in agg_func\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=1401)\u001b[0m TypeError: __init__() got an unexpected keyword argument 'dtypes'\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=1596)\u001b[0m 2022-12-06 14:29:09,515\tINFO worker.py:756 -- Task failed with retryable exception: TaskID(4481ec012324614bffffffffffffffffffffffff01000000).\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=1596)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=1596)\u001b[0m   File \"/tmp/ipykernel_1185/2951074600.py\", line 9, in agg_func\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=1596)\u001b[0m NameError: name 'MODELS' is not defined\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=1596)\u001b[0m \n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=1596)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=1596)\u001b[0m \n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=1596)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=1596)\u001b[0m   File \"python/ray/_raylet.pyx\", line 662, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=1596)\u001b[0m   File \"python/ray/_raylet.pyx\", line 666, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=1596)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/data/_internal/compute.py\", line 449, in _map_block_nosplit\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=1596)\u001b[0m     for new_block in block_fn(block, *fn_args, **fn_kwargs):\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=1596)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/data/dataset.py\", line 457, in transform\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=1596)\u001b[0m     applied = batch_fn(view, *fn_args, **fn_kwargs)\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=1596)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/data/grouped_dataset.py\", line 281, in group_fn\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=1596)\u001b[0m     applied = fn(group)\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=1596)\u001b[0m   File \"/tmp/ipykernel_1185/2951074600.py\", line 14, in agg_func\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=1596)\u001b[0m TypeError: __init__() got an unexpected keyword argument 'dtypes'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=1596)\u001b[0m Failed on LocID 141!\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=1401)\u001b[0m Failed on LocID 229!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=1401)\u001b[0m 2022-12-06 14:29:15,481\tINFO worker.py:756 -- Task failed with retryable exception: TaskID(575dd43a3ded8278ffffffffffffffffffffffff01000000).\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=1401)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=1401)\u001b[0m   File \"/tmp/ipykernel_1185/2951074600.py\", line 9, in agg_func\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=1401)\u001b[0m NameError: name 'MODELS' is not defined\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=1401)\u001b[0m \n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=1401)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=1401)\u001b[0m \n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=1401)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=1401)\u001b[0m   File \"python/ray/_raylet.pyx\", line 662, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=1401)\u001b[0m   File \"python/ray/_raylet.pyx\", line 666, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=1401)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/data/_internal/compute.py\", line 449, in _map_block_nosplit\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=1401)\u001b[0m     for new_block in block_fn(block, *fn_args, **fn_kwargs):\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=1401)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/data/dataset.py\", line 457, in transform\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=1401)\u001b[0m     applied = batch_fn(view, *fn_args, **fn_kwargs)\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=1401)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/data/grouped_dataset.py\", line 281, in group_fn\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=1401)\u001b[0m     applied = fn(group)\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=1401)\u001b[0m   File \"/tmp/ipykernel_1185/2951074600.py\", line 14, in agg_func\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=1401)\u001b[0m TypeError: __init__() got an unexpected keyword argument 'dtypes'\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=1596)\u001b[0m 2022-12-06 14:29:27,698\tINFO worker.py:756 -- Task failed with retryable exception: TaskID(4481ec012324614bffffffffffffffffffffffff01000000).\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=1596)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=1596)\u001b[0m   File \"/tmp/ipykernel_1185/2951074600.py\", line 9, in agg_func\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=1596)\u001b[0m NameError: name 'MODELS' is not defined\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=1596)\u001b[0m \n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=1596)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=1596)\u001b[0m \n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=1596)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=1596)\u001b[0m   File \"python/ray/_raylet.pyx\", line 662, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=1596)\u001b[0m   File \"python/ray/_raylet.pyx\", line 666, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=1596)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/data/_internal/compute.py\", line 449, in _map_block_nosplit\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=1596)\u001b[0m     for new_block in block_fn(block, *fn_args, **fn_kwargs):\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=1596)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/data/dataset.py\", line 457, in transform\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=1596)\u001b[0m     applied = batch_fn(view, *fn_args, **fn_kwargs)\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=1596)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/data/grouped_dataset.py\", line 281, in group_fn\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=1596)\u001b[0m     applied = fn(group)\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=1596)\u001b[0m   File \"/tmp/ipykernel_1185/2951074600.py\", line 14, in agg_func\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=1596)\u001b[0m TypeError: __init__() got an unexpected keyword argument 'dtypes'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=1596)\u001b[0m Failed on LocID 141!\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=1401)\u001b[0m Failed on LocID 229!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=1401)\u001b[0m 2022-12-06 14:29:29,437\tINFO worker.py:756 -- Task failed with retryable exception: TaskID(575dd43a3ded8278ffffffffffffffffffffffff01000000).\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=1401)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=1401)\u001b[0m   File \"/tmp/ipykernel_1185/2951074600.py\", line 9, in agg_func\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=1401)\u001b[0m NameError: name 'MODELS' is not defined\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=1401)\u001b[0m \n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=1401)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=1401)\u001b[0m \n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=1401)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=1401)\u001b[0m   File \"python/ray/_raylet.pyx\", line 662, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=1401)\u001b[0m   File \"python/ray/_raylet.pyx\", line 666, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=1401)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/data/_internal/compute.py\", line 449, in _map_block_nosplit\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=1401)\u001b[0m     for new_block in block_fn(block, *fn_args, **fn_kwargs):\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=1401)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/data/dataset.py\", line 457, in transform\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=1401)\u001b[0m     applied = batch_fn(view, *fn_args, **fn_kwargs)\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=1401)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/data/grouped_dataset.py\", line 281, in group_fn\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=1401)\u001b[0m     applied = fn(group)\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=1401)\u001b[0m   File \"/tmp/ipykernel_1185/2951074600.py\", line 14, in agg_func\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=1401)\u001b[0m TypeError: __init__() got an unexpected keyword argument 'dtypes'\n"
     ]
    },
    {
     "ename": "RayTaskError(TypeError)",
     "evalue": "\u001b[36mray::_map_block_nosplit()\u001b[39m (pid=1401, ip=172.31.223.118)\n\nDuring handling of the above exception, another exception occurred:\n\n\u001b[36mray::_map_block_nosplit()\u001b[39m (pid=1401, ip=172.31.223.118)\n  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/data/_internal/compute.py\", line 449, in _map_block_nosplit\n    for new_block in block_fn(block, *fn_args, **fn_kwargs):\n  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/data/dataset.py\", line 457, in transform\n    applied = batch_fn(view, *fn_args, **fn_kwargs)\n  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/data/grouped_dataset.py\", line 281, in group_fn\n    applied = fn(group)\n  File \"/tmp/ipykernel_1185/2951074600.py\", line 14, in agg_func\nTypeError: __init__() got an unexpected keyword argument 'dtypes'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRayTaskError(TypeError)\u001b[0m                   Traceback (most recent call last)",
      "Input \u001b[0;32mIn [20]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Read data into Ray Dataset\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# ds = pushdown_read_data(s3_files, sample_locations).repartition(14)\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Use Ray Dataset groupby.map_groups() to process each group in parallel and return a Ray Dataset.\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdropoff_location_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_groups\u001b[49m\u001b[43m(\u001b[49m\u001b[43magg_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mbatch_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpandas\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m total_time_taken \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal number of models: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresults\u001b[38;5;241m.\u001b[39mcount()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/site-packages/ray/data/grouped_dataset.py:290\u001b[0m, in \u001b[0;36mGroupedDataset.map_groups\u001b[0;34m(self, fn, compute, batch_format, **ray_remote_args)\u001b[0m\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m rs\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# Note we set batch_size=None here, so it will use the entire block as a batch,\u001b[39;00m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;66;03m# which ensures that each group will be contained within a batch in entirety.\u001b[39;00m\n\u001b[0;32m--> 290\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msorted_ds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_batches\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroup_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompute\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mray_remote_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/site-packages/ray/data/dataset.py:495\u001b[0m, in \u001b[0;36mDataset.map_batches\u001b[0;34m(self, fn, batch_size, compute, batch_format, fn_args, fn_kwargs, fn_constructor_args, fn_constructor_kwargs, **ray_remote_args)\u001b[0m\n\u001b[1;32m    480\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m output_buffer\u001b[38;5;241m.\u001b[39mnext()\n\u001b[1;32m    482\u001b[0m plan \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_plan\u001b[38;5;241m.\u001b[39mwith_stage(\n\u001b[1;32m    483\u001b[0m     OneToOneStage(\n\u001b[1;32m    484\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmap_batches\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    493\u001b[0m     )\n\u001b[1;32m    494\u001b[0m )\n\u001b[0;32m--> 495\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mplan\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_epoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_lazy\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/site-packages/ray/data/dataset.py:201\u001b[0m, in \u001b[0;36mDataset.__init__\u001b[0;34m(self, plan, epoch, lazy, defer_execution)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lazy \u001b[38;5;241m=\u001b[39m lazy\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m lazy \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m defer_execution:\n\u001b[0;32m--> 201\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_plan\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mallow_clear_input_blocks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/site-packages/ray/data/_internal/plan.py:308\u001b[0m, in \u001b[0;36mExecutionPlan.execute\u001b[0;34m(self, allow_clear_input_blocks, force_read)\u001b[0m\n\u001b[1;32m    306\u001b[0m     clear_input_blocks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    307\u001b[0m stats_builder \u001b[38;5;241m=\u001b[39m stats\u001b[38;5;241m.\u001b[39mchild_builder(stage\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m--> 308\u001b[0m blocks, stage_info \u001b[38;5;241m=\u001b[39m \u001b[43mstage\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m    \u001b[49m\u001b[43mblocks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclear_input_blocks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_by_consumer\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stage_info:\n\u001b[1;32m    312\u001b[0m     stats \u001b[38;5;241m=\u001b[39m stats_builder\u001b[38;5;241m.\u001b[39mbuild_multistage(stage_info)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/site-packages/ray/data/_internal/plan.py:662\u001b[0m, in \u001b[0;36mOneToOneStage.__call__\u001b[0;34m(self, blocks, clear_input_blocks, run_by_consumer)\u001b[0m\n\u001b[1;32m    657\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m blocks\u001b[38;5;241m.\u001b[39m_owned_by_consumer:\n\u001b[1;32m    658\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[1;32m    659\u001b[0m         run_by_consumer\n\u001b[1;32m    660\u001b[0m     ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBlocks owned by consumer can only be consumed by consumer\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 662\u001b[0m blocks \u001b[38;5;241m=\u001b[39m \u001b[43mcompute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    663\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblock_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    664\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mray_remote_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    665\u001b[0m \u001b[43m    \u001b[49m\u001b[43mblocks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    666\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclear_input_blocks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    667\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfn_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfn_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfn_constructor_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn_constructor_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfn_constructor_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn_constructor_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(blocks, BlockList), blocks\n\u001b[1;32m    675\u001b[0m blocks\u001b[38;5;241m.\u001b[39m_owned_by_consumer \u001b[38;5;241m=\u001b[39m run_by_consumer\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/site-packages/ray/data/_internal/compute.py:126\u001b[0m, in \u001b[0;36mTaskPoolStrategy._apply\u001b[0;34m(self, block_fn, remote_args, block_list, clear_input_blocks, name, fn, fn_args, fn_kwargs, fn_constructor_args, fn_constructor_kwargs)\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;66;03m# Reraise the original task failure exception.\u001b[39;00m\n\u001b[0;32m--> 126\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    128\u001b[0m new_blocks, new_metadata \u001b[38;5;241m=\u001b[39m [], []\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m context\u001b[38;5;241m.\u001b[39mblock_splitting_enabled:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/site-packages/ray/data/_internal/compute.py:113\u001b[0m, in \u001b[0;36mTaskPoolStrategy._apply\u001b[0;34m(self, block_fn, remote_args, block_list, clear_input_blocks, name, fn, fn_args, fn_kwargs, fn_constructor_args, fn_constructor_kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;66;03m# Common wait for non-data refs.\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 113\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mmap_bar\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrefs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ray\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mRayTaskError, \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;66;03m# One or more mapper tasks failed, or we received a SIGINT signal\u001b[39;00m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# while waiting; either way, we cancel all map tasks.\u001b[39;00m\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m ref \u001b[38;5;129;01min\u001b[39;00m refs:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/site-packages/ray/data/_internal/progress_bar.py:75\u001b[0m, in \u001b[0;36mProgressBar.fetch_until_complete\u001b[0;34m(self, refs)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m remaining:\n\u001b[1;32m     74\u001b[0m     done, remaining \u001b[38;5;241m=\u001b[39m ray\u001b[38;5;241m.\u001b[39mwait(remaining, fetch_local\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\n\u001b[0;32m---> 75\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m ref, result \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(done, \u001b[43mray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdone\u001b[49m\u001b[43m)\u001b[49m):\n\u001b[1;32m     76\u001b[0m         ref_to_result[ref] \u001b[38;5;241m=\u001b[39m result\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mlen\u001b[39m(done))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/site-packages/ray/_private/client_mode_hook.py:105\u001b[0m, in \u001b[0;36mclient_mode_hook.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minit\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m is_client_mode_enabled_by_default:\n\u001b[1;32m    104\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(ray, func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 105\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/site-packages/ray/_private/worker.py:2275\u001b[0m, in \u001b[0;36mget\u001b[0;34m(object_refs, timeout)\u001b[0m\n\u001b[1;32m   2273\u001b[0m     worker\u001b[38;5;241m.\u001b[39mcore_worker\u001b[38;5;241m.\u001b[39mdump_object_store_memory_usage()\n\u001b[1;32m   2274\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, RayTaskError):\n\u001b[0;32m-> 2275\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m value\u001b[38;5;241m.\u001b[39mas_instanceof_cause()\n\u001b[1;32m   2276\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2277\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m value\n",
      "\u001b[0;31mRayTaskError(TypeError)\u001b[0m: \u001b[36mray::_map_block_nosplit()\u001b[39m (pid=1401, ip=172.31.223.118)\n\nDuring handling of the above exception, another exception occurred:\n\n\u001b[36mray::_map_block_nosplit()\u001b[39m (pid=1401, ip=172.31.223.118)\n  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/data/_internal/compute.py\", line 449, in _map_block_nosplit\n    for new_block in block_fn(block, *fn_args, **fn_kwargs):\n  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/data/dataset.py\", line 457, in transform\n    applied = batch_fn(view, *fn_args, **fn_kwargs)\n  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/data/grouped_dataset.py\", line 281, in group_fn\n    applied = fn(group)\n  File \"/tmp/ipykernel_1185/2951074600.py\", line 14, in agg_func\nTypeError: __init__() got an unexpected keyword argument 'dtypes'"
     ]
    }
   ],
   "source": [
    "# Driver code to run this.\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# Read data into Ray Dataset\n",
    "# ds = pushdown_read_data(s3_files, sample_locations).repartition(14)\n",
    "\n",
    "# Use Ray Dataset groupby.map_groups() to process each group in parallel and return a Ray Dataset.\n",
    "results = ds.groupby(\"dropoff_location_id\")\\\n",
    "            .map_groups(agg_func, \n",
    "                        batch_format=\"pandas\")\n",
    "\n",
    "total_time_taken = time.time() - start\n",
    "print(f\"Total number of models: {results.count()}\")\n",
    "print(f\"TOTAL TIME TAKEN: {total_time_taken:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can inspect the models we have trained and their errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort values by location id\n",
    "results_df = results.to_pandas()\n",
    "results_df.sort_values(by=[\"location_id\"], ascending=True, inplace=True)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Keep only 1 model per location_id with minimum error\n",
    "# final_df = results_df.loc[results_df.groupby('location_id')['error'].idxmin()].copy()\n",
    "# final_df.reset_index(inplace=True, drop=True)\n",
    "# final_df.sort_values(by=\"error\", ascending=True)\n",
    "# final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-06 14:29:35,376\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::_map_block_nosplit()\u001b[39m (pid=1596, ip=172.31.223.118)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/data/_internal/compute.py\", line 449, in _map_block_nosplit\n",
      "    for new_block in block_fn(block, *fn_args, **fn_kwargs):\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/data/dataset.py\", line 457, in transform\n",
      "    applied = batch_fn(view, *fn_args, **fn_kwargs)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/data/grouped_dataset.py\", line 276, in group_fn\n",
      "    boundaries = get_boundaries(block_accessor)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/data/grouped_dataset.py\", line 264, in get_boundaries\n",
      "    for i, item in enumerate(block.iter_rows()):\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/data/_internal/table_block.py\", line 157, in __next__\n",
      "    return outer._get_row(self._cur)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/data/_internal/table_block.py\", line 115, in _get_row\n",
      "    row = self.slice(index, index + 1, copy=copy)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/data/_internal/pandas_block.py\", line 145, in slice\n",
      "    view = self._table[start:end]\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/pandas/core/frame.py\", line 3788, in __getitem__\n",
      "    return self._slice(indexer, axis=0)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/pandas/core/generic.py\", line 4106, in _slice\n",
      "    result = self._constructor(self._mgr.get_slice(slobj, axis=axis))\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "\u001b[36mray::_map_block_nosplit()\u001b[39m (pid=1596, ip=172.31.223.118)\n",
      "ray.exceptions.TaskCancelledError: Task: TaskID(4481ec012324614bffffffffffffffffffffffff01000000) was cancelled\n",
      "2022-12-06 14:29:35,378\tERROR worker.py:399 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::_map_block_nosplit()\u001b[39m (pid=1401, ip=172.31.223.118)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "\u001b[36mray::_map_block_nosplit()\u001b[39m (pid=1401, ip=172.31.223.118)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/data/_internal/compute.py\", line 449, in _map_block_nosplit\n",
      "    for new_block in block_fn(block, *fn_args, **fn_kwargs):\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/data/dataset.py\", line 457, in transform\n",
      "    applied = batch_fn(view, *fn_args, **fn_kwargs)\n",
      "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/data/grouped_dataset.py\", line 281, in group_fn\n",
      "    applied = fn(group)\n",
      "  File \"/tmp/ipykernel_1185/2951074600.py\", line 14, in agg_func\n",
      "TypeError: __init__() got an unexpected keyword argument 'dtypes'\n"
     ]
    }
   ],
   "source": [
    "# Keep only 1 model per location_id with minimum error\n",
    "final_df = results_df.copy()\n",
    "final_df = final_df.loc[(final_df.error > 0), :]\n",
    "final_df = final_df.loc[final_df.groupby(\"location_id\")[\"error\"].idxmin()]\n",
    "final_df.sort_values(by=[\"error\"], inplace=True)\n",
    "final_df.set_index(\"location_id\", inplace=True, drop=True)\n",
    "print(final_df.dtypes)\n",
    "final_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "3c0d54d489a08ae47a06eae2fd00ff032d6cddb527c382959b7b2575f6a8167f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
