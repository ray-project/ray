{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Training with Ray Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Batch training and tuning are common tasks in simple machine learning use-cases such as time series forecasting. They require fitting of simple models on multiple data batches corresponding to locations, products, etc.\n",
    "\n",
    "In the context of this notebook, batch training is understood as creating the same model(s) for different and separate datasets or subsets of a dataset. This notebook showcases how to conduct batch training using [Ray Dataset](https://docs.ray.io/en/latest/data/dataset.html).\n",
    "\n",
    "![Batch training diagram](./images/batch-training.svg)\n",
    "\n",
    "For the data, we will use the [NYC Taxi dataset](https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page). This popular tabular dataset contains historical taxi pickups by timestamp and location in NYC. To demonstrate batch training, we will simplify the data to a regression problem to predict `trip_duration` and use scikit-learn.\n",
    "\n",
    "To demonstrate how batch training can be parallelized, we will train a separate model for each dropoff location. This means we can use the `dropoff_location_id` column in the dataset to group the dataset into data batches. Then we will fit a separate model for each batch and evaluate it.\n",
    "\n",
    "# Contents\n",
    "\n",
    "In this this tutorial, you will learn about:\n",
    " 1. Creating a Ray Dataset,\n",
    " 2. Inspecting a Ray Dataset,\n",
    " 3. Running data transformations on a Ray Dataset in parallel,\n",
    " 4. How to perform batch training with Ray Datasets using *group-by*.\n",
    "\n",
    "# Walkthrough\n",
    "\n",
    "Let’s start by importing a few required libraries, including open-source [Ray](https://github.com/ray-project/ray) itself!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, List, Union, Optional, Callable\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyarrow.dataset as pds\n",
    "from pyarrow import fs\n",
    "from pyarrow import parquet as pq\n",
    "from ray.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "    <div style=\"margin-left: 50px;display: flex;flex-direction: row;align-items: center\">\n",
       "        <h3 style=\"color: var(--jp-ui-font-color0)\">Ray</h3>\n",
       "        <svg version=\"1.1\" id=\"ray\" width=\"3em\" viewBox=\"0 0 144.5 144.6\" style=\"margin-left: 3em;margin-right: 3em\">\n",
       "            <g id=\"layer-1\">\n",
       "                <path fill=\"#00a2e9\" class=\"st0\" d=\"M97.3,77.2c-3.8-1.1-6.2,0.9-8.3,5.1c-3.5,6.8-9.9,9.9-17.4,9.6S58,88.1,54.8,81.2c-1.4-3-3-4-6.3-4.1\n",
       "                    c-5.6-0.1-9.9,0.1-13.1,6.4c-3.8,7.6-13.6,10.2-21.8,7.6C5.2,88.4-0.4,80.5,0,71.7c0.1-8.4,5.7-15.8,13.8-18.2\n",
       "                    c8.4-2.6,17.5,0.7,22.3,8c1.3,1.9,1.3,5.2,3.6,5.6c3.9,0.6,8,0.2,12,0.2c1.8,0,1.9-1.6,2.4-2.8c3.5-7.8,9.7-11.8,18-11.9\n",
       "                    c8.2-0.1,14.4,3.9,17.8,11.4c1.3,2.8,2.9,3.6,5.7,3.3c1-0.1,2,0.1,3,0c2.8-0.5,6.4,1.7,8.1-2.7s-2.3-5.5-4.1-7.5\n",
       "                    c-5.1-5.7-10.9-10.8-16.1-16.3C84,38,81.9,37.1,78,38.3C66.7,42,56.2,35.7,53,24.1C50.3,14,57.3,2.8,67.7,0.5\n",
       "                    C78.4-2,89,4.7,91.5,15.3c0.1,0.3,0.1,0.5,0.2,0.8c0.7,3.4,0.7,6.9-0.8,9.8c-1.7,3.2-0.8,5,1.5,7.2c6.7,6.5,13.3,13,19.8,19.7\n",
       "                    c1.8,1.8,3,2.1,5.5,1.2c9.1-3.4,17.9-0.6,23.4,7c4.8,6.9,4.6,16.1-0.4,22.9c-5.4,7.2-14.2,9.9-23.1,6.5c-2.3-0.9-3.5-0.6-5.1,1.1\n",
       "                    c-6.7,6.9-13.6,13.7-20.5,20.4c-1.8,1.8-2.5,3.2-1.4,5.9c3.5,8.7,0.3,18.6-7.7,23.6c-7.9,5-18.2,3.8-24.8-2.9\n",
       "                    c-6.4-6.4-7.4-16.2-2.5-24.3c4.9-7.8,14.5-11,23.1-7.8c3,1.1,4.7,0.5,6.9-1.7C91.7,98.4,98,92.3,104.2,86c1.6-1.6,4.1-2.7,2.6-6.2\n",
       "                    c-1.4-3.3-3.8-2.5-6.2-2.6C99.8,77.2,98.9,77.2,97.3,77.2z M72.1,29.7c5.5,0.1,9.9-4.3,10-9.8c0-0.1,0-0.2,0-0.3\n",
       "                    C81.8,14,77,9.8,71.5,10.2c-5,0.3-9,4.2-9.3,9.2c-0.2,5.5,4,10.1,9.5,10.3C71.8,29.7,72,29.7,72.1,29.7z M72.3,62.3\n",
       "                    c-5.4-0.1-9.9,4.2-10.1,9.7c0,0.2,0,0.3,0,0.5c0.2,5.4,4.5,9.7,9.9,10c5.1,0.1,9.9-4.7,10.1-9.8c0.2-5.5-4-10-9.5-10.3\n",
       "                    C72.6,62.3,72.4,62.3,72.3,62.3z M115,72.5c0.1,5.4,4.5,9.7,9.8,9.9c5.6-0.2,10-4.8,10-10.4c-0.2-5.4-4.6-9.7-10-9.7\n",
       "                    c-5.3-0.1-9.8,4.2-9.9,9.5C115,72.1,115,72.3,115,72.5z M19.5,62.3c-5.4,0.1-9.8,4.4-10,9.8c-0.1,5.1,5.2,10.4,10.2,10.3\n",
       "                    c5.6-0.2,10-4.9,9.8-10.5c-0.1-5.4-4.5-9.7-9.9-9.6C19.6,62.3,19.5,62.3,19.5,62.3z M71.8,134.6c5.9,0.2,10.3-3.9,10.4-9.6\n",
       "                    c0.5-5.5-3.6-10.4-9.1-10.8c-5.5-0.5-10.4,3.6-10.8,9.1c0,0.5,0,0.9,0,1.4c-0.2,5.3,4,9.8,9.3,10\n",
       "                    C71.6,134.6,71.7,134.6,71.8,134.6z\"/>\n",
       "            </g>\n",
       "        </svg>\n",
       "        <table>\n",
       "            <tr>\n",
       "                <td style=\"text-align: left\"><b>Python version:</b></td>\n",
       "                <td style=\"text-align: left\"><b>3.8.5</b></td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                <td style=\"text-align: left\"><b>Ray version:</b></td>\n",
       "                <td style=\"text-align: left\"><b> 2.0.0</b></td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "    <td style=\"text-align: left\"><b>Dashboard:</b></td>\n",
       "    <td style=\"text-align: left\"><b><a href=\"http://console.anyscale-staging.com/api/v2/sessions/ses_ZmHebxHaZpYkw9x9efJ5wBVX/services?redirect_to=dashboard\" target=\"_blank\">http://console.anyscale-staging.com/api/v2/sessions/ses_ZmHebxHaZpYkw9x9efJ5wBVX/services?redirect_to=dashboard</a></b></td>\n",
       "</tr>\n",
       "\n",
       "        </table>\n",
       "    </div>\n",
       "</div>\n"
      ],
      "text/plain": [
       "RayContext(dashboard_url='console.anyscale-staging.com/api/v2/sessions/ses_ZmHebxHaZpYkw9x9efJ5wBVX/services?redirect_to=dashboard', python_version='3.8.5', ray_version='2.0.0', ray_commit='cba26cc83f6b5b8a2ff166594a65cb74c0ec8740', address_info={'node_ip_address': '172.31.126.77', 'raylet_ip_address': '172.31.126.77', 'redis_address': None, 'object_store_address': '/tmp/ray/session_2022-10-13_14-16-40_624725_151/sockets/plasma_store', 'raylet_socket_name': '/tmp/ray/session_2022-10-13_14-16-40_624725_151/sockets/raylet', 'webui_url': 'console.anyscale-staging.com/api/v2/sessions/ses_ZmHebxHaZpYkw9x9efJ5wBVX/services?redirect_to=dashboard', 'session_dir': '/tmp/ray/session_2022-10-13_14-16-40_624725_151', 'metrics_export_port': 59933, 'gcs_address': '172.31.126.77:9031', 'address': '172.31.126.77:9031', 'dashboard_agent_listen_port': 52365, 'node_id': '616124ce14c8bf5164a99baf8d82f79b37b4d9ec58c8b32678e936ae'})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ray\n",
    "\n",
    "ray.init(ignore_reinit_error=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For benchmarking purposes, we can print the times of various operations.\n",
    "# In order to reduce clutter in the output, this is set to False by default.\n",
    "PRINT_TIMES = False\n",
    "\n",
    "\n",
    "def print_time(msg: str):\n",
    "    if PRINT_TIMES:\n",
    "        print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To speed things up, we’ll only use a small subset of the full dataset consisting of two last months of 2019.\n",
    "# You can choose to use the full dataset for 2018-2019 by setting the SMOKE_TEST variable to False.\n",
    "\n",
    "SMOKE_TEST = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Ray Datasets <a class=\"anchor\" id=\"dataset\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Ray Datasets](datasets) are the standard way to load and exchange data in Ray libraries and applications. We will use the [Ray Dataset APIs](dataset-api) to read the data and quickly inspect it.\n",
    "\n",
    "First, we will define some global variables we will use throughout the notebook, such as the list of S3 links to the files making up the dataset and the possible location IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NYC Taxi using 2 file(s)!\n",
      "s3_files: ['s3://air-example-data/ursa-labs-taxi-data/by_year/2019/05/data.parquet/359c21b3e28f40328e68cf66f7ba40e2_000000.parquet', 's3://air-example-data/ursa-labs-taxi-data/by_year/2019/06/data.parquet/ab5b9d2b8cc94be19346e260b543ec35_000000.parquet']\n"
     ]
    }
   ],
   "source": [
    "# Define some global variables.\n",
    "target = \"trip_duration\"\n",
    "s3_partitions = pds.dataset(\n",
    "    \"s3://anonymous@air-example-data/ursa-labs-taxi-data/by_year/\",\n",
    "    partitioning=[\"year\", \"month\"],\n",
    ")\n",
    "\n",
    "s3_files = [f\"s3://{file}\" for file in s3_partitions.files]\n",
    "\n",
    "# Obtain all location IDs\n",
    "location_ids = (\n",
    "    pq.read_table(s3_files[0], columns=[\"pickup_location_id\"])[\"pickup_location_id\"]\n",
    "    .unique()\n",
    "    .to_pylist()\n",
    ")\n",
    "\n",
    "starting_idx = -2 if SMOKE_TEST else 0\n",
    "\n",
    "s3_files = s3_files[starting_idx:]\n",
    "print(f\"NYC Taxi using {len(s3_files)} file(s)!\")\n",
    "print(f\"s3_files: {s3_files}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will call `ray.data.read_parquet` to create a Ray dataset from a list of S3 URIs. This will read the files in parallel onto the Ray cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-13 14:40:13,586\tWARNING read_api.py:280 -- ⚠️  The blocks of this dataset are estimated to be 1.5x larger than the target block size of 512 MiB. This may lead to out-of-memory errors during processing. Consider reducing the size of input files or using `.repartition(n)` to increase the number of dataset blocks.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset(num_blocks=2, num_rows=14506285, schema={vendor_id: string, pickup_at: timestamp[us], dropoff_at: timestamp[us], passenger_count: int8, trip_distance: float, rate_code_id: string, store_and_fwd_flag: string, pickup_location_id: int32, dropoff_location_id: int32, payment_type: string, fare_amount: float, extra: float, mta_tax: float, tip_amount: float, tolls_amount: float, improvement_surcharge: float, total_amount: float, congestion_surcharge: float})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = ray.data.read_parquet(s3_files)\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ray Dataset statistics\n",
    "\n",
    "Let's get some basic statistics about our newly created Ray Dataset.\n",
    "\n",
    "As our Ray Dataset is backed by Parquet, we can obtain the number of rows from the metadata without triggering a full data read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 14506285\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of rows: {ds.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we can obtain the Dataset size (in bytes) from the metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size bytes (from parquet metadata): 1928872430\n"
     ]
    }
   ],
   "source": [
    "print(f\"Size bytes (from parquet metadata): {ds.size_bytes()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's fetch and inspect the schema of the underlying Parquet files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Schema data types:\n",
      "vendor_id: string\n",
      "pickup_at: timestamp[us]\n",
      "dropoff_at: timestamp[us]\n",
      "passenger_count: int8\n",
      "trip_distance: float\n",
      "rate_code_id: string\n",
      "store_and_fwd_flag: string\n",
      "pickup_location_id: int32\n",
      "dropoff_location_id: int32\n",
      "payment_type: string\n",
      "fare_amount: float\n",
      "extra: float\n",
      "mta_tax: float\n",
      "tip_amount: float\n",
      "tolls_amount: float\n",
      "improvement_surcharge: float\n",
      "total_amount: float\n",
      "congestion_surcharge: float\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nSchema data types:\")\n",
    "data_types = list(zip(ds.schema().names, ds.schema().types))\n",
    "for s in data_types:\n",
    "    print(f\"{s[0]}: {s[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter on Read - Projection and Filter Pushdown\n",
    "\n",
    "Note that Ray Datasets' Parquet reader supports projection (column selection) and row filter pushdown, where we can push the above column selection and the row-based filter to the Parquet read. If we specify column selection at Parquet read time, the unselected columns won't even be read from disk. This can save a lot of memory, especially with big datasets, and allow us to avoid OOM issues.\n",
    "\n",
    "The row-based filter is specified via [Arrow's dataset field expressions](https://arrow.apache.org/docs/6.0/python/generated/pyarrow.dataset.Expression.html#pyarrow.dataset.Expression). \n",
    "\n",
    "```{tip}\n",
    "Best practice is to filter as much as you can directly in the Ray Dataset `read_parquet()`.\n",
    "```\n",
    "\n",
    "Normally, there is some data exploration to determine the cleaning steps. Let's just assume we know the data cleaning steps are:\n",
    "- Drop negative trip distances, 0 fares, 0 passengers and trip durations smaller than 1 minute.\n",
    "- Drop 2 unknown zones: `['264', '265']`.\n",
    "- Calculate trip duration and add it as a new column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pushdown_read_data(files_list: list, sample_ids: list) -> Dataset:\n",
    "    filter_expr = (\n",
    "        (pds.field(\"passenger_count\") > 0)\n",
    "        & (pds.field(\"trip_distance\") > 0)\n",
    "        & (pds.field(\"fare_amount\") > 0)\n",
    "        & (~pds.field(\"pickup_location_id\").isin([264, 265]))\n",
    "        & (~pds.field(\"dropoff_location_id\").isin([264, 265]))\n",
    "        & (pds.field(\"dropoff_location_id\").isin(sample_ids))\n",
    "    )\n",
    "\n",
    "    dataset = ray.data.read_parquet(\n",
    "        files_list,\n",
    "        columns=[\n",
    "            \"pickup_at\",\n",
    "            \"dropoff_at\",\n",
    "            \"pickup_location_id\",\n",
    "            \"dropoff_location_id\",\n",
    "            \"passenger_count\",\n",
    "            \"trip_distance\",\n",
    "            \"fare_amount\",\n",
    "        ],\n",
    "        filter=filter_expr,\n",
    "    )\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-13 14:40:16,302\tWARNING read_api.py:280 -- ⚠️  The blocks of this dataset are estimated to be 1.5x larger than the target block size of 512 MiB. This may lead to out-of-memory errors during processing. Consider reducing the size of input files or using `.repartition(n)` to increase the number of dataset blocks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number rows: 14506285\n",
      "\n",
      "Metadata: \n",
      "Dataset(num_blocks=2, num_rows=14506285, schema={pickup_at: timestamp[us], dropoff_at: timestamp[us], pickup_location_id: int32, dropoff_location_id: int32, passenger_count: int8, trip_distance: float, fare_amount: float})\n",
      "\n",
      "Schema:\n",
      "pickup_at: timestamp[us]\n",
      "dropoff_at: timestamp[us]\n",
      "pickup_location_id: int32\n",
      "dropoff_location_id: int32\n",
      "passenger_count: int8\n",
      "trip_distance: float\n",
      "fare_amount: float\n",
      "-- schema metadata --\n",
      "pandas: '{\"index_columns\": [{\"kind\": \"range\", \"name\": null, \"start\": 0, \"' + 2548\n",
      "\n",
      "Look at a sample row:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[ArrowRow({'pickup_at': datetime.datetime(2019, 5, 1, 0, 35, 54),\n",
       "           'dropoff_at': datetime.datetime(2019, 5, 1, 0, 37, 27),\n",
       "           'pickup_location_id': 145,\n",
       "           'dropoff_location_id': 145,\n",
       "           'passenger_count': 1,\n",
       "           'trip_distance': 1.5,\n",
       "           'fare_amount': 3.0})]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the pushdown_read_data function\n",
    "pushdown_ds = pushdown_read_data(s3_files, location_ids)\n",
    "\n",
    "print(f\"Number rows: {pushdown_ds.count()}\")\n",
    "# Display some metadata about the dataset.\n",
    "print(\"\\nMetadata: \")\n",
    "print(pushdown_ds)\n",
    "# Fetch the schema from the underlying Parquet metadata.\n",
    "print(\"\\nSchema:\")\n",
    "print(pushdown_ds.schema())\n",
    "# Take a peek at a single row\n",
    "print(\"\\nLook at a sample row:\")\n",
    "pushdown_ds.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use `to_pandas` to convert a Ray Dataset into a pandas DataFrame and inspect that.\n",
    "\n",
    "```{note}\n",
    "Converting a Ray Dataset to pandas is not recommended with large data sizes, as it will load all the data into the memory of a single node. To help avoid OOM errors, `to_pandas` will by default raise an exception if there are more than 10000 rows in the Dataset. As we only want to see how the data looks like, we grab the first 100 rows and convert that.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read progress: 100%|██████████| 1/1 [00:00<00:00, 781.79it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pickup_at</th>\n",
       "      <th>dropoff_at</th>\n",
       "      <th>pickup_location_id</th>\n",
       "      <th>dropoff_location_id</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>fare_amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-05-01 00:35:54</td>\n",
       "      <td>2019-05-01 00:37:27</td>\n",
       "      <td>145</td>\n",
       "      <td>145</td>\n",
       "      <td>1</td>\n",
       "      <td>1.50</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-05-01 00:37:45</td>\n",
       "      <td>2019-05-01 00:37:49</td>\n",
       "      <td>145</td>\n",
       "      <td>145</td>\n",
       "      <td>1</td>\n",
       "      <td>1.50</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-05-01 00:44:57</td>\n",
       "      <td>2019-05-01 00:50:11</td>\n",
       "      <td>161</td>\n",
       "      <td>161</td>\n",
       "      <td>1</td>\n",
       "      <td>0.70</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-05-01 00:59:48</td>\n",
       "      <td>2019-05-01 01:10:22</td>\n",
       "      <td>163</td>\n",
       "      <td>141</td>\n",
       "      <td>1</td>\n",
       "      <td>2.00</td>\n",
       "      <td>9.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-05-01 00:23:20</td>\n",
       "      <td>2019-05-01 00:32:57</td>\n",
       "      <td>260</td>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>2.50</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>2019-05-01 00:27:58</td>\n",
       "      <td>2019-05-01 00:30:26</td>\n",
       "      <td>106</td>\n",
       "      <td>106</td>\n",
       "      <td>1</td>\n",
       "      <td>10.60</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>2019-05-01 00:51:47</td>\n",
       "      <td>2019-05-01 01:15:42</td>\n",
       "      <td>45</td>\n",
       "      <td>241</td>\n",
       "      <td>1</td>\n",
       "      <td>15.50</td>\n",
       "      <td>42.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>2019-05-01 00:20:43</td>\n",
       "      <td>2019-05-01 00:26:22</td>\n",
       "      <td>231</td>\n",
       "      <td>87</td>\n",
       "      <td>1</td>\n",
       "      <td>1.16</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>2019-05-01 00:47:54</td>\n",
       "      <td>2019-05-01 00:53:01</td>\n",
       "      <td>162</td>\n",
       "      <td>162</td>\n",
       "      <td>2</td>\n",
       "      <td>0.88</td>\n",
       "      <td>5.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>2019-05-01 00:08:37</td>\n",
       "      <td>2019-05-01 00:13:51</td>\n",
       "      <td>239</td>\n",
       "      <td>142</td>\n",
       "      <td>1</td>\n",
       "      <td>0.90</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             pickup_at          dropoff_at  pickup_location_id  \\\n",
       "0  2019-05-01 00:35:54 2019-05-01 00:37:27                 145   \n",
       "1  2019-05-01 00:37:45 2019-05-01 00:37:49                 145   \n",
       "2  2019-05-01 00:44:57 2019-05-01 00:50:11                 161   \n",
       "3  2019-05-01 00:59:48 2019-05-01 01:10:22                 163   \n",
       "4  2019-05-01 00:23:20 2019-05-01 00:32:57                 260   \n",
       "..                 ...                 ...                 ...   \n",
       "95 2019-05-01 00:27:58 2019-05-01 00:30:26                 106   \n",
       "96 2019-05-01 00:51:47 2019-05-01 01:15:42                  45   \n",
       "97 2019-05-01 00:20:43 2019-05-01 00:26:22                 231   \n",
       "98 2019-05-01 00:47:54 2019-05-01 00:53:01                 162   \n",
       "99 2019-05-01 00:08:37 2019-05-01 00:13:51                 239   \n",
       "\n",
       "    dropoff_location_id  passenger_count  trip_distance  fare_amount  \n",
       "0                   145                1           1.50          3.0  \n",
       "1                   145                1           1.50          2.5  \n",
       "2                   161                1           0.70          5.0  \n",
       "3                   141                1           2.00          9.5  \n",
       "4                    56                1           2.50         10.0  \n",
       "..                  ...              ...            ...          ...  \n",
       "95                  106                1          10.60          2.5  \n",
       "96                  241                1          15.50         42.5  \n",
       "97                   87                1           1.16          6.0  \n",
       "98                  162                2           0.88          5.5  \n",
       "99                  142                1           0.90          6.0  \n",
       "\n",
       "[100 rows x 7 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pushdown_ds.limit(100).to_pandas()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom data transform functions\n",
    "\n",
    "Ray Datasets allows you to specify custom data transform functions. These [user defined functions (UDFs)](transforming_datasets) can be called using `Dataset.map_batches(my_UDF)`. The transformation will be conducted in parallel for each data batch.\n",
    "\n",
    "```{tip}\n",
    "You may need to call `Dataset.repartition(n)` first to split the Dataset into more blocks internally. By default, each block corresponds to one file. The upper bound of parallelism is the number of blocks.\n",
    "```\n",
    "\n",
    "You can specify the data format you are using in the `batch_format` parameter. The dataset will be divided into batches and those batches converted into the specified format. Available data formats you can specify in the `batch_format` paramater include `\"pandas\", \"pyarrow\", \"numpy\"`. Tabular data will be passed into your UDF by default as a pandas DataFrame. Tensor data will be passed into your UDF as a numpy array.\n",
    "\n",
    "Here, we will use `batch_format=\"pandas\"` explicitly for clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A pandas DataFrame UDF for transforming the Dataset in parallel.\n",
    "def transform_batch(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df[\"trip_duration\"] = (df[\"dropoff_at\"] - df[\"pickup_at\"]).dt.seconds\n",
    "    df = df[df[\"trip_duration\"] > 60]\n",
    "    df.drop([\"dropoff_at\", \"pickup_at\", \"pickup_location_id\"], axis=1, inplace=True)\n",
    "    df[\"dropoff_location_id\"] = df[\"dropoff_location_id\"].fillna(-1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows before transformation: 14506285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read: 100%|██████████| 2/2 [00:03<00:00,  1.88s/it]\n",
      "Repartition: 100%|██████████| 16/16 [00:02<00:00,  7.60it/s]\n",
      "Map_Batches: 100%|██████████| 16/16 [00:02<00:00,  5.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows after transformation: 13893562\n"
     ]
    }
   ],
   "source": [
    "# Test the transform UDF.\n",
    "print(f\"Number of rows before transformation: {pushdown_ds.count()}\")\n",
    "\n",
    "# Repartition the dataset to allow for higher parallelism.\n",
    "pushdown_ds = pushdown_ds.repartition(16)\n",
    "\n",
    "# batch_format=\"pandas\" tells Datasets to provide the UDF with batches\n",
    "# represented as pandas DataFrames.\n",
    "pushdown_ds = pushdown_ds.map_batches(transform_batch, batch_format=\"pandas\")\n",
    "\n",
    "# Verify row count.\n",
    "print(f\"Number of rows after transformation: {pushdown_ds.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tidying up\n",
    "\n",
    "We'll delete the datasets we have been using in order to free up memory in our Ray cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "del ds\n",
    "del pushdown_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make our code easier to read, let's summarize the data processing functions again here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter parquet data using Ray Datasets read_parquet()\n",
    "def pushdown_read_data(files_list: list, sample_ids: list) -> Dataset:\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    filter_expr = (\n",
    "        (pds.field(\"passenger_count\") > 0)\n",
    "        & (pds.field(\"trip_distance\") > 0)\n",
    "        & (pds.field(\"fare_amount\") > 0)\n",
    "        & (~pds.field(\"pickup_location_id\").isin([264, 265]))\n",
    "        & (~pds.field(\"dropoff_location_id\").isin([264, 265]))\n",
    "        & (pds.field(\"dropoff_location_id\").isin(sample_ids))\n",
    "    )\n",
    "\n",
    "    dataset = ray.data.read_parquet(\n",
    "        files_list,\n",
    "        columns=[\n",
    "            \"pickup_at\",\n",
    "            \"dropoff_at\",\n",
    "            \"pickup_location_id\",\n",
    "            \"dropoff_location_id\",\n",
    "            \"passenger_count\",\n",
    "            \"trip_distance\",\n",
    "            \"fare_amount\",\n",
    "        ],\n",
    "        filter=filter_expr,\n",
    "    )\n",
    "\n",
    "    data_loading_time = time.time() - start\n",
    "    print_time(f\"Data loading time: {data_loading_time:.2f} seconds\")\n",
    "    return dataset\n",
    "\n",
    "\n",
    "# A Pandas DataFrame UDF for transforming the underlying blocks of a Dataset in parallel.\n",
    "def transform_batch(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df[\"trip_duration\"] = (df[\"dropoff_at\"] - df[\"pickup_at\"]).dt.seconds\n",
    "    df = df[df[\"trip_duration\"] > 60]\n",
    "    df.drop([\"dropoff_at\", \"pickup_at\", \"pickup_location_id\"], axis=1, inplace=True)\n",
    "    df[\"dropoff_location_id\"] = df[\"dropoff_location_id\"].fillna(-1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch training with Ray Datasets <a class=\"anchor\" id=\"train_func\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have learned more about our data and written a pandas UDF to transform our data, we are ready to train a model on batches of this data in parallel.\n",
    "\n",
    "1. We will use the `dropoff_location_id` column in the dataset to group the dataset into data batches. \n",
    "2. Then we will fit a separate model for each batch to predict `trip_duration`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from ray.train.sklearn import SklearnTrainer, SklearnPredictor\n",
    "from ray.train.batch_predictor import BatchPredictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define training functions\n",
    "\n",
    "We want to fit a linear regression model to the trip duration for each drop-off location. For scoring, we will calculate mean absolute error on the validation set, and report that as model error per drop-off location.\n",
    "\n",
    "The `fit_and_score_sklearn` function contains the logic necessary to fit a scikit-learn model and evaluate it using mean absolute error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_and_score_sklearn(\n",
    "    train_df: pd.DataFrame, test_df: pd.DataFrame, model: BaseEstimator\n",
    ") -> Tuple[BaseEstimator, float]:\n",
    "    # Assemble train/test pandas dfs\n",
    "    train_X = train_df[[\"passenger_count\", \"trip_distance\", \"fare_amount\"]]\n",
    "    train_y = train_df.trip_duration\n",
    "    test_X = test_df[[\"passenger_count\", \"trip_distance\", \"fare_amount\"]]\n",
    "    test_y = test_df.trip_duration\n",
    "\n",
    "    # Start training.\n",
    "    model = model.fit(train_X, train_y)\n",
    "    pred_y = model.predict(test_X)\n",
    "    error = sklearn.metrics.mean_absolute_error(test_y, pred_y)\n",
    "\n",
    "    return model, error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `train_and_evaluate` function contains the logic for train-test splitting and fitting of a model using the `fit_and_score_sklearn` function.\n",
    "\n",
    "As an input, this function takes in a pandas DataFrame. When we call `Dataset.map_batches` or `Dataset.groupby().map_groups()`, the Dataset will be batched into multiple pandas DataFrames and this function will be ran for each one in parallel. We will return the model and its error. Those results will be collected back into a Ray Dataset automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(\n",
    "    df: pd.DataFrame, model: BaseEstimator, location_id: int\n",
    ") -> pd.DataFrame:\n",
    "    # check if input df is big enough for training\n",
    "    if len(df) < 4:\n",
    "        print_time(f\"Data batch for LocID {location_id} is empty or smaller than 4 rows\")\n",
    "        return None\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    # Train / test split\n",
    "    # Randomly split the data into 80/20 train/test.\n",
    "    train_df, test_df = train_test_split(df, test_size=0.2, shuffle=True)\n",
    "\n",
    "    results = fit_and_score_sklearn(train_df, test_df, model)\n",
    "\n",
    "    # Assemble location_id, name of model, and metrics in a pandas DataFrame\n",
    "    results = [location_id] + list(results)\n",
    "    results_df = pd.DataFrame([results], columns=[\"location_id\", \"model\", \"error\"])\n",
    "\n",
    "    training_time = time.time() - start\n",
    "    print_time(f\"Training time for LocID {location_id}: {training_time:.2f} seconds\")\n",
    "\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the model itself, we will use scikit-learn's Linear Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = LinearRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall how we wrote a data transform `transform_batch` UDF? It was called with pattern:\n",
    "- `Dataset.map_batches(transform_batch, batch_format=\"pandas\")`\n",
    "\n",
    "Similarly, a groupby-aggregation function can be used later when we perform a [Ray Dataset *group-by*](datasets-groupbys). We will define our aggregation function `agg_func` which will be ran for each group in parallel. The usage pattern is:\n",
    "- `Dataset.groupby(column).map_groups(agg_func, batch_format=\"pandas\")`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A Pandas DataFrame aggregation function for processing grouped batches of Ray Dataset data.\n",
    "def agg_func(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    location_id = df[\"dropoff_location_id\"][0]\n",
    "\n",
    "    # Handle errors in data groups\n",
    "    try:\n",
    "        # Transform the input pandas AND fit_and_evaluate the transformed pandas\n",
    "        transformed_df = transform_batch(df)\n",
    "        results_df = train_and_evaluate(transformed_df, MODEL, location_id)\n",
    "        assert results_df is not None\n",
    "    except Exception:\n",
    "        # assemble a null entry\n",
    "        print(f\"Failed on LocID {location_id}!\")\n",
    "        results_df = pd.DataFrame([[location_id, None, None]], columns=[\"location_id\", \"model\", \"error\"])\n",
    "\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run batch training using `map_groups`\n",
    "\n",
    "Finally, the main \"driver code\" reads each Parquet file (each file corresponds to one month of NYC taxi data) into a Ray Dataset `ds`. Then we use Ray Dataset *group-by* to map each group into a batch of data and run `agg_func` on each of them in parallel by calling `ds.groupby(\"dropoff_location_id\").map_groups(agg_func, batch_format=\"pandas\")`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-13 14:40:31,234\tWARNING read_api.py:280 -- ⚠️  The blocks of this dataset are estimated to be 1.5x larger than the target block size of 512 MiB. This may lead to out-of-memory errors during processing. Consider reducing the size of input files or using `.repartition(n)` to increase the number of dataset blocks.\n",
      "Read: 100%|██████████| 2/2 [00:01<00:00,  1.08it/s]\n",
      "Repartition: 100%|██████████| 16/16 [00:01<00:00, 13.22it/s]\n",
      "Sort Sample: 100%|██████████| 16/16 [00:01<00:00, 13.27it/s]\n",
      "Shuffle Map: 100%|██████████| 16/16 [00:01<00:00, 11.53it/s]\n",
      "Shuffle Reduce: 100%|██████████| 16/16 [00:01<00:00, 12.34it/s]\n",
      "Map_Batches:  100%|██████████| 16/16 [00:01<00:00,  6.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(_map_block_nosplit pid=66620) Failed on LocID 199!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map_Batches: 100%|██████████| 16/16 [02:53<00:00, 10.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "groupby.map_groups() finished!\n",
      "Total number of models: 257\n",
      "TOTAL TIME TAKEN: 180.74 seconds\n"
     ]
    }
   ],
   "source": [
    "# Driver code to run this.\n",
    "start = time.time()\n",
    "\n",
    "# Read data into Ray Dataset\n",
    "ds = pushdown_read_data(s3_files, location_ids).repartition(16)\n",
    "\n",
    "# Use Ray Dataset groupby.map_groups() to process each group in parallel and return a Ray Dataset.\n",
    "results = ds.groupby(\"dropoff_location_id\").map_groups(agg_func, batch_format=\"pandas\")\n",
    "print(f\"groupby.map_groups() finished!\")\n",
    "\n",
    "total_time_taken = time.time() - start\n",
    "print(f\"Total number of models: {results.count()}\")\n",
    "print(f\"TOTAL TIME TAKEN: {total_time_taken:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can inspect the models we have trained and their errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset(num_blocks=16, num_rows=257, schema={location_id: int32, model: object, error: float64})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>location_id</th>\n",
       "      <th>model</th>\n",
       "      <th>error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>LinearRegression()</td>\n",
       "      <td>1186.550249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>LinearRegression()</td>\n",
       "      <td>207.324870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>LinearRegression()</td>\n",
       "      <td>1412.273083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>LinearRegression()</td>\n",
       "      <td>382.897671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>LinearRegression()</td>\n",
       "      <td>2515.787663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>259</td>\n",
       "      <td>LinearRegression()</td>\n",
       "      <td>495.479018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>260</td>\n",
       "      <td>LinearRegression()</td>\n",
       "      <td>506.609215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>261</td>\n",
       "      <td>LinearRegression()</td>\n",
       "      <td>684.562030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>262</td>\n",
       "      <td>LinearRegression()</td>\n",
       "      <td>389.035014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>263</td>\n",
       "      <td>LinearRegression()</td>\n",
       "      <td>359.866881</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>257 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     location_id               model        error\n",
       "0              1  LinearRegression()  1186.550249\n",
       "1              2  LinearRegression()   207.324870\n",
       "2              3  LinearRegression()  1412.273083\n",
       "3              4  LinearRegression()   382.897671\n",
       "4              5  LinearRegression()  2515.787663\n",
       "..           ...                 ...          ...\n",
       "252          259  LinearRegression()   495.479018\n",
       "253          260  LinearRegression()   506.609215\n",
       "254          261  LinearRegression()   684.562030\n",
       "255          262  LinearRegression()   389.035014\n",
       "256          263  LinearRegression()   359.866881\n",
       "\n",
       "[257 rows x 3 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sort values by location id\n",
    "results_df = results.to_pandas(limit=float(\"inf\"))\n",
    "results_df.sort_values(by=[\"location_id\"], ascending=True, inplace=True)\n",
    "results_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "3c0d54d489a08ae47a06eae2fd00ff032d6cddb527c382959b7b2575f6a8167f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
