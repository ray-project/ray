{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500957a1",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# flake8: noqa\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "# Suppress noisy requests warnings.\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ[\"PYTHONWARNINGS\"] = \"ignore\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377a7f17",
   "metadata": {},
   "source": [
    "# Extracting Information from Documents using Ray Datasets\n",
    "\n",
    "In this example, we will show you how to run optical character recognition (OCR) on a set of documents and analyze the resulting text with the natural language processing library spaCy. To make it more interesting, we will run the analysis on the [LightShot](https://www.kaggle.com/datasets/datasnaek/lightshot) dataset. It is a large publicly available OCR dataset with a wide variety of different documents, all of them screen shots of various forms. It is easy to replace that dataset with your own data and adapt the example to your own use cases!\n",
    "\n",
    "## Overview\n",
    "\n",
    "This tutorial will cover:\n",
    " - Creating a Ray Dataset that represents the images in the dataset\n",
    " - Running the computationally expensive OCR process on each image in the dataset in parallel\n",
    " - Filtering out images that actually contain text\n",
    " - Performing various NLP operations on the text\n",
    "\n",
    "## Walkthrough\n",
    "\n",
    "Let's start by preparing the dependencies and downloading the dataset. You can download the dataset at [LightShot](https://www.kaggle.com/datasets/datasnaek/lightshot). Install the OCR software `tesseract` and extract the `archive.zip` file with the following commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff29fef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo apt-get install tesseract-ocr\n",
    "%pip install pytesseract\n",
    "!sudo apt-get install -y unzip unrar\n",
    "!unzip archive.zip\n",
    "!unrar x LightShot13k.rar ~/LightShot13k/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29d37c9",
   "metadata": {},
   "source": [
    "Let's now import Ray and initialize a local Ray cluster. If you want to run OCR at a very large scale, you should run this workload on a multi-node cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db7232c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import ray and initialize a local Ray cluster.\n",
    "import ray\n",
    "ray.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a42ba7",
   "metadata": {},
   "source": [
    "### Running the OCR software on the data\n",
    "\n",
    "We first create a list `files` of absolute paths of the file names and then convert it into a Ray Dataset with the `ray.data.from_items` function. We can now run the `.map` function on this dataset of file names to run the actual OCR process on each file and convert the screen shots into text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e600fcf9",
   "metadata": {
    "scrolled": false,
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "files = glob(os.path.expanduser(\"~/LightShot13k/LightShot13k/*\"))\n",
    "ds = ray.data.from_items(files)\n",
    "results = ds.map(lambda path: pytesseract.image_to_string(path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a870048",
   "metadata": {},
   "source": [
    "Let us have a look at some of the data points with the `take` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eed2501",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.take()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c4127d",
   "metadata": {},
   "source": [
    "### Process the extracted text data with spaCy\n",
    "\n",
    "We can now process the extracted text with spaCy. Let's first make sure the libraries are installed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b934ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install spacy\n",
    "!python -m spacy download en_core_web_sm\n",
    "!pip install spacy_langdetect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5c8c33",
   "metadata": {},
   "source": [
    "This is some code to determine the language of a piece of text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1acdef1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "@Language.factory(\"language_detector\")\n",
    "def get_lang_detector(nlp, name):\n",
    "    return LanguageDetector()\n",
    "\n",
    "nlp.add_pipe('language_detector', last=True)\n",
    "nlp(\"This is an English sentence\")._.language"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119f6571",
   "metadata": {},
   "source": [
    "In order to run the code on the dataset, we have to use an actor since the `nlp` object is not serializable and we want to avoid having to recreate it for each individual sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1b3cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.language import Language\n",
    "from spacy_langdetect import LanguageDetector\n",
    "\n",
    "class SpacyBatchInference:\n",
    "    def __init__(self):\n",
    "        self.nlp = spacy.load('en_core_web_sm')\n",
    "        @Language.factory(\"language_detector\")\n",
    "\n",
    "        def get_lang_detector(nlp, name):\n",
    "           return LanguageDetector()\n",
    "\n",
    "        self.nlp.add_pipe('language_detector', last=True)\n",
    "\n",
    "    def __call__(self, row):\n",
    "        doc = self.nlp(row[\"value\"])\n",
    "        return doc._.language\n",
    "\n",
    "results.limit(10).map(SpacyBatchInference, compute=\"actors\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419f8f8e",
   "metadata": {},
   "source": [
    "We can now get language statistics over the whole dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7fefa0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results.map(SpacyBatchInference, compute=\"actors\").groupby(\"language\").count().show()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
