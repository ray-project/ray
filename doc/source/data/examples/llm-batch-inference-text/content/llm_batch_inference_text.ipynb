{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM batch inference with Ray Data\n",
    "\n",
    "<div align=\"left\">\n",
    "<a target=\"_blank\" href=\"https://console.anyscale.com/template-preview/llm-batch-inference-text\"><img src=\"https://img.shields.io/badge/ðŸš€ Run_on-Anyscale-9hf\"></a>&nbsp;\n",
    "<a href=\"https://github.com/ray-project/ray/tree/master/doc/source/data/examples/llm-batch-inference-text\" role=\"button\"><img src=\"https://img.shields.io/static/v1?label=&amp;message=View%20On%20GitHub&amp;color=586069&amp;logo=github&amp;labelColor=2f363d\"></a>&nbsp;\n",
    "</div>\n",
    "\n",
    "**â±ï¸ Time to complete**: 15 minutes\n",
    "\n",
    "This example shows you how to run batch inference for large language models (LLMs) using [Ray Data LLM APIs](https://docs.ray.io/en/latest/data/api/llm.html). In this use case, the batch inference job reformats dates across a large customer dataset.\n",
    "\n",
    "\n",
    "## When to use LLM batch inference\n",
    "\n",
    "Offline (batch) inference optimizes for throughput over latency. Unlike online inference, which processes requests one at a time in real-time, batch inference processes thousands or millions of inputs together, maximizing GPU utilization and reducing per-inference costs.\n",
    "\n",
    "Choose batch inference when:\n",
    "- You have a fixed dataset to process (such as daily reports or data migrations)\n",
    "- Throughput matters more than immediate results\n",
    "- You want to take advantage of fault tolerance and checkpointing for long-running jobs\n",
    "\n",
    "On contrary, if you are more interested in optimizing for latency, consider [deploying your LLM with Ray Serve LLM for online inference](https://docs.ray.io/en/latest/serve/llm/index.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare a Ray Data dataset\n",
    "\n",
    "Ray Data LLM runs batch inference for LLMs on Ray Data datasets. In this tutorial, you perform batch inference with an LLM to reformat dates and the source is a 2-million-row CSV file containing sample customer data.\n",
    "\n",
    "First, load the data from a remote URL then repartition the dataset to ensure the workload can be distributed across multiple GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "\n",
    "# Define the path to the sample CSV file hosted on S3.\n",
    "# This dataset contains 2 million rows of synthetic customer data.\n",
    "path = \"https://llm-guide.s3.us-west-2.amazonaws.com/data/ray-data-llm/customers-2000000.csv\"\n",
    "\n",
    "# Load the CSV file into a Ray Dataset.\n",
    "print(\"Loading dataset from remote URL...\")\n",
    "ds = ray.data.read_csv(path)\n",
    "\n",
    "# Inspect the dataset schema and a few rows to verify it loaded correctly.\n",
    "print(\"\\nDataset schema:\")\n",
    "print(ds.schema())\n",
    "print(\"\\nSample rows:\")\n",
    "ds.show(limit=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this initial example, limit the dataset to 10,000 rows for faster processing and testing. Later, you can scale up to process the full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limit the dataset to 10,000 rows for this example.\n",
    "print(\"Limiting dataset to 10,000 rows for initial processing.\")\n",
    "ds_small = ds.limit(10_000)\n",
    "\n",
    "# Repartition the dataset to enable parallelism across multiple workers (GPUs).\n",
    "# By default, a large remote file might be read into a single block. Repartitioning\n",
    "# splits the data into a specified number of blocks, allowing Ray to process them\n",
    "# in parallel.\n",
    "num_partitions = 64\n",
    "print(f\"Repartitioning dataset into {num_partitions} blocks for parallelism...\")\n",
    "ds_small = ds_small.repartition(num_blocks=num_partitions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Ray Data LLM\n",
    "\n",
    "Ray Data LLM provides a unified interface to run batch inference with different LLM engines. Configure the vLLM engine, define preprocessing and postprocessing functions, and build the processor.\n",
    "\n",
    "### Configure the processor engine\n",
    "\n",
    "Configure the model and compute resources needed for inference using `vLLMEngineProcessorConfig`.\n",
    "\n",
    "This example uses the `unsloth/Llama-3.1-8B-Instruct` model. The configuration specifies:\n",
    "- `model_source`: The Hugging Face model identifier.\n",
    "- `engine_kwargs`: vLLM engine parameters such as tensor parallelism and memory settings.\n",
    "- `batch_size`: Number of requests to batch together (set to 256 for small prompts and outputs).\n",
    "- `accelerator_type`: GPU type to use (L4 in this case).\n",
    "- `concurrency`: Number of parallel workers (4 replicas).\n",
    "\n",
    "**Note:** Because the input prompts and expected output token lengths are small, `batch_size=256` is appropriate. However, depending on your workload, a large batch size can lead to increased idle GPU time when decoding long sequences. Adjust this value to find the optimal trade-off between throughput and latency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.data.llm import vLLMEngineProcessorConfig\n",
    "\n",
    "processor_config = vLLMEngineProcessorConfig(\n",
    "    model_source=\"unsloth/Llama-3.1-8B-Instruct\",\n",
    "    engine_kwargs=dict(\n",
    "        max_model_len= 256, # estimate system prompt + user prompt + output tokens (+ reasoning tokens if any)\n",
    "    ),\n",
    "    batch_size=256,\n",
    "    accelerator_type=\"L4\",\n",
    "    concurrency=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more details on the configuration options you can pass to the vLLM engine, see the [vLLM Engine Arguments documentation](https://docs.vllm.ai/en/stable/configuration/engine_args.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the preprocess and postprocess functions\n",
    "\n",
    "The task is to format the `Subscription Date` field as `MM-DD-YYYY` using an LLM.\n",
    "\n",
    "Define a preprocess function to prepare `messages` and `sampling_params` for the vLLM engine, and a postprocess function to extract the `generated_text`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "# Preprocess function prepares `messages` and `sampling_params` for vLLM engine.\n",
    "# All other fields are ignored by the engine.\n",
    "def preprocess(row: dict[str, Any]) -> dict[str, Any]:\n",
    "    return dict(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a helpful assistant that reformats dates to MM-DD-YYYY.\"\n",
    "                            \"Be concise and output only the formatted date and nothing else.\"\n",
    "                            \"For example, if we ask to reformat 'Subscription Date': datetime.date(2020, 11, 29)' then your answer should only be '11-29-2020'\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Convert this date:\\n{row['Subscription Date']}.\"\n",
    "            },\n",
    "        ],\n",
    "        sampling_params=dict(\n",
    "            temperature=0.3,\n",
    "            max_tokens=32, # low max tokens because we are simply formatting a date\n",
    "            detokenize=False,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "# Postprocess function extracts the generated text from the engine output.\n",
    "# The **row syntax returns all original columns in the input dataset.\n",
    "def postprocess(row: dict[str, Any]) -> dict[str, Any]:\n",
    "    return {\n",
    "        \"formatted_date\": row[\"generated_text\"],\n",
    "        **row,  # Include all original columns.\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the processor\n",
    "\n",
    "With the configuration and functions defined, build the processor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.data.llm import build_llm_processor\n",
    "\n",
    "# Build the LLM processor with the configuration and functions.\n",
    "processor = build_llm_processor(\n",
    "    processor_config,\n",
    "    preprocess=preprocess,\n",
    "    postprocess=postprocess,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process the dataset\n",
    "\n",
    "Run the processor on your small dataset to perform batch inference. Ray Data automatically distributes the workload across available GPUs and handles batching, retries, and resource management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "# Run the processor on the small dataset.\n",
    "processed_small = processor(ds_small)\n",
    "\n",
    "# Materialize the dataset to memory.\n",
    "# You can also use writing APIs such as write_parquet() or write_csv() to persist the dataset.\n",
    "processed_small = processed_small.materialize()\n",
    "\n",
    "# Display the first 3 entries to verify the output.\n",
    "sampled = processed_small.take(3)\n",
    "print(\"\\n==================GENERATED OUTPUT===============\\n\")\n",
    "pprint(sampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Launch to production with Anyscale Jobs\n",
    "\n",
    "For production workloads, deploy your batch inference processor as an [Anyscale Job](https://docs.anyscale.com/platform/jobs). Anyscale takes care of the infrastructure layer and runs your jobs on your dedicated clusters with automatic retries, monitoring, and scheduling.\n",
    "\n",
    "### Configure an Anyscale Job\n",
    "\n",
    "Save your batch inference code as `batch_inference.py`, then create a job configuration file:\n",
    "\n",
    "```yaml\n",
    "# job.yaml\n",
    "name: my-llm-batch-inference-text\n",
    "entrypoint: python batch_inference_text.py\n",
    "image_uri: anyscale/ray-llm:2.51.1-py311-cu128\n",
    "compute_config:\n",
    "  head_node:\n",
    "    instance_type: m5.2xlarge\n",
    "  worker_nodes:\n",
    "    - instance_type: g6.2xlarge\n",
    "      min_nodes: 0\n",
    "      max_nodes: 10\n",
    "working_dir: .\n",
    "max_retries: 2\n",
    "```\n",
    "\n",
    "### Submit\n",
    "\n",
    "Submit your job using the Anyscale CLI:\n",
    "\n",
    "```bash\n",
    "anyscale job submit --config-file job.yaml\n",
    "```\n",
    "\n",
    "### Monitoring\n",
    "\n",
    "Track your job's progress in the Anyscale Console or through the CLI:\n",
    "\n",
    "```bash\n",
    "# Check job status\n",
    "anyscale job status --name my-llm-batch-inference-text\n",
    "\n",
    "# View logs\n",
    "anyscale job logs --name my-llm-batch-inference-text\n",
    "```\n",
    "\n",
    "The Ray Dashboard remains available for detailed monitoring. To access it, go over your Anyscale Job in your console.  \n",
    "For cluster-level information, click the **Metrics** tab then **Data** tab, and for task-level information, click the **Ray Workloads** tab then **Data** tab.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitor the execution\n",
    "\n",
    "Use the Ray Dashboard to monitor the execution. See [Monitoring your Workload](https://docs.ray.io/en/latest/data/monitoring-your-workload.html) for more information on visualizing your Ray Data jobs.\n",
    "\n",
    "The dashboard shows:\n",
    "- Operator-level metrics (throughput, task execution times).\n",
    "- Resource utilization (CPU, GPU, memory).\n",
    "- Progress and remaining time estimates.\n",
    "- Task status breakdown.\n",
    "\n",
    "**Tip**: If you encounter CUDA out of memory errors, reduce your batch size, use a smaller model, or switch to a larger GPU. For more troubleshooting tips, see [GPU Memory Management](https://docs.ray.io/en/latest/data/working-with-llms.html#gpu-memory-management-and-cuda-oom-prevention)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale up to larger datasets\n",
    "\n",
    "Your Ray Data processing pipeline can easily scale up to process more data. By default, this section processes 1M rows.  \n",
    "You can control the dataset size through the `LARGE_DATASET_LIMIT` environment variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Configure how many images to process (default: 1M for demonstration).\n",
    "dataset_limit = int(os.environ.get(\"LARGE_DATASET_LIMIT\", 1_000_000))\n",
    "print(f\"Scaling dataset to: {dataset_limit:,} rows...\")\n",
    "\n",
    "# Apply the limit to the dataset.\n",
    "ds_large = ds.limit(dataset_limit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can scale the number of concurrent replicas based on the compute available in your cluster. In this case, each replica is a copy of your Llama model and fits in a single L4 GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor_config = vLLMEngineProcessorConfig(\n",
    "    model_source=\"unsloth/Llama-3.1-8B-Instruct\",\n",
    "    engine_kwargs=dict(\n",
    "        max_model_len= 256, # estimate system prompt + user prompt + output tokens (+ reasoning tokens if any)\n",
    "    ),\n",
    "    batch_size=256,\n",
    "    accelerator_type=\"L4\",\n",
    "    concurrency=10, # Deploy 10 replicas across 10 GPUs to maximize throughput\n",
    ")\n",
    "\n",
    "# Build the LLM processor with the configuration and functions.\n",
    "processor = build_llm_processor(\n",
    "    processor_config,\n",
    "    preprocess=preprocess,\n",
    "    postprocess=postprocess,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With additional replicas, repartition your dataset into more blocks for better parallelism. Ray data can efficiently schedule those smaller blocks accross all your additional replicas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repartition for better parallelism.\n",
    "num_partitions_large = 128\n",
    "print(f\"Repartitioning dataset into {num_partitions_large} blocks...\")\n",
    "ds_large = ds_large.repartition(num_blocks=num_partitions_large)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the same processor on the larger dataset.\n",
    "processed_large = processor(ds_large)\n",
    "processed_large = processed_large.materialize()\n",
    "\n",
    "print(f\"\\nProcessed {processed_large.count()} rows successfully.\")\n",
    "print(\"\\nSample outputs:\")\n",
    "pprint(processed_large.take(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance optimization tips\n",
    "\n",
    "When scaling to larger datasets, consider these optimizations tips:\n",
    "\n",
    "**Analyze your pipeline**\n",
    "Use *stats()* to analyze each steps in your pipeline and identify any bottlenecks.\n",
    "```python\n",
    "processed = processor(ds).materialize()\n",
    "print(processed.stats())\n",
    "```\n",
    "The outputs contains detailed description of each step in your pipeline.\n",
    "```text\n",
    "Operator 0 ...\n",
    "\n",
    "...\n",
    "\n",
    "Operator 8 MapBatches(vLLMEngineStageUDF): 3908 tasks executed, 3908 blocks produced in 340.21s\n",
    "    * Remote wall time: ...\n",
    "    ...\n",
    "\n",
    "...\n",
    "\n",
    "Dataset throughput:\n",
    "\t* Ray Data throughput: ...\n",
    "\t* Estimated single node throughput: ...\n",
    "```\n",
    "\n",
    "**Adjust concurrency**  \n",
    "Increase the `concurrency` parameter to add more parallel workers.\n",
    "\n",
    "**Tune batch size**  \n",
    "Larger batch sizes improve throughput but increase memory usage.\n",
    "\n",
    "**Repartition strategically**  \n",
    "Use more partitions (blocks) than the number of workers to enable better load balancing.\n",
    "\n",
    "**Enable checkpointing**  \n",
    "For very large datasets, configure checkpointing to recover from failures.\n",
    "\n",
    "```python\n",
    "processed = processor(ds_large).materialize(\n",
    "    checkpoint_path=\"s3://my-bucket/checkpoints/\"\n",
    ")\n",
    "```\n",
    "\n",
    "**Monitor GPU utilization**  \n",
    "Use the Ray Dashboard to identify bottlenecks and adjust parameters.\n",
    "\n",
    "For performance tuning, see the [Ray Data performance guide](https://docs.ray.io/en/latest/data/performance-tips.html). To configure your inference engine, see the [vLLM configuration options](https://docs.vllm.ai/en/latest/serving/engine_args.html).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save results\n",
    "\n",
    "After processing, save the results to a persistent storage location such as S3 or local disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the processed dataset to Parquet format.\n",
    "# Replace this path with your desired output location.\n",
    "output_path_small = \"local:///tmp/processed_customers_small\"\n",
    "output_path_large = \"local:///tmp/processed_customers_large\"\n",
    "\n",
    "print(f\"Saving small processed dataset to {output_path_small}...\")\n",
    "processed_small.write_parquet(output_path_small)\n",
    "print(\"Saved successfully.\")\n",
    "\n",
    "print(f\"Saving large processed dataset to {output_path_large}...\")\n",
    "processed_large.write_parquet(output_path_large)\n",
    "print(\"Saved successfully.\")\n",
    "\n",
    "# Alternatively, save as CSV:\n",
    "# processed_small.write_csv(output_path_small)\n",
    "# processed_large.write_csv(output_path_large)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more information, see [Saving Data](https://docs.ray.io/en/latest/data/saving-data.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, you built an end-to-end batch pipeline: loading a customer dataset from S3 into a Ray Dataset, configuring a vLLM processor for Llama 3.1 8 B, and adding simple pre/post-processing to normalize dates. You validated the flow on 10,000 rows, scaled to 1M+ records, monitored progress in the Ray Dashboard, and saved the results to persistent storage.\n",
    "\n",
    "See [Anyscale batch inference optimization](https://docs.anyscale.com/llm/batch-inference) for more information on using Ray Data with Anyscale and for more advanced use cases, see [Working with LLMs](https://docs.ray.io/en/latest/data/working-with-llms.html)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orphan": true
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
