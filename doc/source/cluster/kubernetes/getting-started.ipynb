{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f34f1b75",
   "metadata": {},
   "source": [
    "(kuberay-quickstart)=\n",
    "\n",
    "[//]: <> (TODO: migrate this content away from ipynb)\n",
    "\n",
    "# Getting Started\n",
    "\n",
    "In this guide, we show you how to manage and interact with Ray clusters on Kubernetes.\n",
    "\n",
    "You can download this guide as an executable Jupyter notebook by clicking the download button on the top right of the page.\n",
    "\n",
    "\n",
    "## Preparation\n",
    "\n",
    "### Install the latest Ray release\n",
    "This step is needed to interact with remote Ray clusters using {ref}`Ray Job Submission <kuberay-job>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dcd7d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -U \"ray[default]\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656a0707",
   "metadata": {},
   "source": [
    "See {ref}`installation` for more details. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0933e2f",
   "metadata": {},
   "source": [
    "### Install kubectl\n",
    "\n",
    "We will use kubectl to interact with Kubernetes. Find installation instructions at the [Kubernetes documentation](https://kubernetes.io/docs/tasks/tools/#kubectl).\n",
    "\n",
    "### Access a Kubernetes cluster\n",
    "\n",
    "We will need access to a Kubernetes cluster. There are two options:\n",
    "1. Configure access to a remote Kubernetes cluster\n",
    "**OR**\n",
    "\n",
    "2. Run the examples locally by [installing kind](https://kind.sigs.k8s.io/docs/user/quick-start/#installation). Start your [kind](https://kind.sigs.k8s.io/) cluster by running the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c764b3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "! kind create cluster"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "278726e0",
   "metadata": {},
   "source": [
    "To run the example in this guide, make sure your Kubernetes cluster (or local Kind cluster) can accomodate\n",
    "additional resource requests of 3 CPU and 3Gi memory (for example, by setting your Docker resource limits high enough). Also, make sure your Kubernetes cluster and Kubectl are both at version at least 1.19.\n",
    "\n",
    "## Deploying Ray cluster to Kubernetes\n",
    "\n",
    "There are two ways to deploy the Ray cluster to Kubernetes: \n",
    "1. Deploying Ray cluster using KubeRay\n",
    "\n",
    "2. Deploying a static Ray cluster without KubeRay\n",
    "\n",
    "Installing CustomResourceDefinitions (CRDs) is a prerequisite to using KubeRay. Even though the KubeRay operator supports running at single namespace scope, the fact that the CRDs are cluster-scoped cannot be avoided. If the admin permission is not granted to perform the deployment, you can use the second option to deploy the Ray cluster to Kubernetes.\n",
    "\n",
    "### Deploying Ray cluster using KubeRay\n",
    "\n",
    "(kuberay-operator-deploy)=\n",
    "#### Deploying the KubeRay operator\n",
    "\n",
    "Deploy the KubeRay Operator by applying the relevant configuration files from the KubeRay GitHub repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c66922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This creates the KubeRay operator and all of the resources it needs.\n",
    "! kubectl create -k \"github.com/ray-project/kuberay/ray-operator/config/default?ref=v0.4.0&timeout=90s\"\n",
    "\n",
    "# Note that we must use \"kubectl create\" in the above command. \"kubectl apply\" will not work due to https://github.com/ray-project/kuberay/issues/271\n",
    "\n",
    "# You may alternatively clone the KubeRay GitHub repo and deploy the operator's configuration from your local file system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522f9a97",
   "metadata": {},
   "source": [
    "Confirm that the operator is running in the namespace `ray-system`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfec8bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "! kubectl -n ray-system get pod --selector=app.kubernetes.io/component=kuberay-operator\n",
    "\n",
    "# NAME                                READY   STATUS    RESTARTS   AGE\n",
    "# kuberay-operator-557c6c8bcd-t9zkz   1/1     Running   0          XXs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3f0d3d17",
   "metadata": {},
   "source": [
    "##### Namespace-scoped operator\n",
    "\n",
    "Note that the above command deploys the operator at _Kubernetes cluster scope_; the operator will manage resources in all Kubernetes namespaces.\n",
    "**If your use-case requires running the operator at single namespace scope**, refer to [the instructions at the KubeRay docs](https://ray-project.github.io/kuberay/deploy/installation/#single-namespace-version)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e1fdf3f5",
   "metadata": {},
   "source": [
    "#### Deploying a Ray Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac860db",
   "metadata": {},
   "source": [
    "Once the KubeRay operator is running, we are ready to deploy a Ray cluster. To do so, we create a RayCluster Custom Resource (CR).\n",
    "\n",
    "In the rest of this guide, we will deploy resources into the default namespace. To use a non-default namespace, specify the namespace in your kubectl commands:\n",
    "\n",
    "`kubectl -n <your-namespace> ...`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30645643",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy a sample Ray Cluster CR from the KubeRay repo:\n",
    "! kubectl apply -f https://raw.githubusercontent.com/ray-project/kuberay/master/ray-operator/config/samples/ray-cluster.autoscaler.yaml\n",
    "\n",
    "# This Ray cluster is named `raycluster-autoscaler` because it has optional Ray Autoscaler support enabled."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6abf52",
   "metadata": {},
   "source": [
    "Once the RayCluster CR has been created, you can view it by running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2363bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "! kubectl get raycluster\n",
    "\n",
    "# NAME                    AGE\n",
    "# raycluster-autoscaler   XXs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4bd4e47",
   "metadata": {},
   "source": [
    "The KubeRay operator will detect the RayCluster object. The operator will then start your Ray cluster by creating head and worker pods. To view Ray cluster's pods, run the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d938b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the pods in the Ray cluster named \"raycluster-autoscaler\"\n",
    "! kubectl get pods --selector=ray.io/cluster=raycluster-autoscaler\n",
    "\n",
    "# NAME                                             READY   STATUS    RESTARTS   AGE\n",
    "# raycluster-autoscaler-head-xxxxx                 2/2     Running   0          XXs\n",
    "# raycluster-autoscaler-worker-small-group-yyyyy   1/1     Running   0          XXs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb1c582",
   "metadata": {},
   "source": [
    "We see a Ray head pod with two containers -- the Ray container and autoscaler sidecar. We also have a Ray worker with its single Ray container.\n",
    "\n",
    "Wait for the pods to reach Running state. This may take a few minutes -- most of this time is spent downloading the Ray images. In a separate shell, you may wish to observe the pods' status in real-time with the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68dab885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you're on MacOS, first `brew install watch`.\n",
    "# Run in a separate shell:\n",
    "! watch -n 1 kubectl get pod"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a01c9a9",
   "metadata": {},
   "source": [
    "If your pods are stuck in the Pending state, you can check for errors via `kubectl describe pod raycluster-autoscaler-xxxx-xxxxx` and ensure that your Docker resource limits are set high enough."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fab157b",
   "metadata": {},
   "source": [
    "Note that in production scenarios, you will want to use larger Ray pods. In fact, it is advantageous to size each Ray pod to take up an entire Kubernetes node. See the [configuration guide](kuberay-config) for more details."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ef55f61f",
   "metadata": {},
   "source": [
    "### Deploying a static Ray cluster\n",
    "\n",
    "\n",
    "#### Deploying a static Ray cluster without KubeRay\n",
    "\n",
    "In this section, we will deploy a static Ray cluster into the `default` namespace without using KubeRay. To use another namespace, specify the namespace in your kubectl commands:\n",
    "\n",
    "`kubectl -n <your-namespace> ...`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8267f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy a sample Ray Cluster from the Ray repo:\n",
    "! kubectl apply -f https://raw.githubusercontent.com/ray-project/ray/master/doc/source/cluster/kubernetes/configs/static-ray-cluster.with-fault-tolerance.yaml\n",
    "\n",
    "# Note that the Ray cluster has fault tolerance enabled by default using the external Redis. Please set the Redis IP address in the config."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "134f6e63",
   "metadata": {},
   "source": [
    "Once the Ray cluster has been deployed, you can view the pods for the head node and worker nodes by running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60e518b",
   "metadata": {},
   "outputs": [],
   "source": [
    "! kubectl get pods\n",
    "\n",
    "# NAME                                             READY   STATUS    RESTARTS   AGE\n",
    "# deployment-ray-head-xxxxx                        1/1     Running   0          XXs\n",
    "# deployment-ray-worker-xxxxx                      1/1     Running   0          XXs\n",
    "# deployment-ray-worker-xxxxx                      1/1     Running   0          XXs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cd8e59c6",
   "metadata": {},
   "source": [
    "Wait for the pods to reach the `Running` state. This may take a few minutes -- most of this time is spent downloading the Ray images. In a separate shell, you may wish to observe the pods' status in real-time with the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1afd109c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you're on MacOS, first `brew install watch`.\n",
    "# Run in a separate shell:\n",
    "! watch -n 1 kubectl get pod"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "258c4b5f",
   "metadata": {},
   "source": [
    "If your pods are stuck in the `Pending` state, you can check for errors via `kubectl describe pod deployment-ray-head-xxxx-xxxxx` and ensure that your Docker resource limits are set high enough."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4dd57a6a",
   "metadata": {},
   "source": [
    "Note that in production scenarios, you will want to use larger Ray pods. In fact, it is advantageous to size each Ray pod to take up an entire Kubernetes node. See the [configuration guide](kuberay-config) for more details."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8f61dbc3",
   "metadata": {},
   "source": [
    "#### Deploying a network policy for the static Ray cluster\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c2db0639",
   "metadata": {},
   "source": [
    "If your Kubernetes has a default deny network policy for pods, you need to manually create a network policy to allow the bidirectional communication among the head and worker nodes in the Ray cluster as mentioned in [the Ports configurations doc](https://docs.ray.io/en/latest/ray-core/configure.html#ports-configurations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf57a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample network policy for the static Ray cluster from the Ray repo:\n",
    "! kubectl apply -f https://raw.githubusercontent.com/ray-project/ray/master/doc/source/cluster/kubernetes/configs/static-ray-cluster-networkpolicy.yaml"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "87686b72",
   "metadata": {},
   "source": [
    "Once the network policy has been deployed, you can view the network policy for the static Ray cluster by running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b8adcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "! kubectl get networkpolicies\n",
    "\n",
    "# NAME                               POD-SELECTOR                           AGE\n",
    "# ray-cluster-egress                 app=ray-cluster-head                   XXs\n",
    "# ray-cluster-ingress                app=ray-cluster-head                   XXs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3178b34a",
   "metadata": {},
   "source": [
    "#### External Redis Integration for fault tolerance\n",
    "Ray by default uses an internal key-value store, called the Global Control Store (GCS). The GCS runs on the head node and stores cluster metadata. One drawback of this approach is that the head node loses the metadata if it crashes. \n",
    "Ray can also write this metadata to an external Redis for reliability and high availability. With this setup, the static Ray cluster can recover from head node crasehs and tolerate GCS failures without losing connections to worker nodes.\n",
    "\n",
    "To use this feature, we need to pass in the RAY_REDIS_ADDRESS env var and --redis-password in the Ray head node section of [the Kubernetes deployment config file](https://raw.githubusercontent.com/ray-project/ray/master/doc/source/cluster/kubernetes/configs/static-ray-cluster.with-fault-tolerance.yaml)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63e1ab9",
   "metadata": {},
   "source": [
    "## Running Applications on a Ray Cluster\n",
    "\n",
    "Now, let's interact with the Ray cluster we've deployed.\n",
    "\n",
    "### Accessing the cluster with kubectl exec\n",
    "\n",
    "The most straightforward way to experiment with your Ray cluster is to\n",
    "exec directly into the head pod. First, identify your Ray cluster's head pod:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5acb95e6",
   "metadata": {},
   "source": [
    "If the Ray cluster was deployed using KubeRay, run the command below to get the head pod:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2538c4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "! kubectl get pods --selector=ray.io/cluster=raycluster-autoscaler --selector=ray.io/node-type=head -o custom-columns=POD:metadata.name --no-headers\n",
    "    \n",
    "# raycluster-autoscaler-head-xxxxx"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "691b4535",
   "metadata": {},
   "source": [
    "If the Ray cluster was deployed using the static Ray cluster config, run the command below to get the head pod:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081c3491",
   "metadata": {},
   "outputs": [],
   "source": [
    "! kubectl get pods --selector=app=ray-cluster-head"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190b2163",
   "metadata": {},
   "source": [
    "Now, we can run a Ray program on the head pod. The Ray program in the next cell simply connects to the Ray Cluster, then exits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35b2454",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Substitute your output from the last cell in place of \"raycluster-autoscaler-head-xxxxx\"\n",
    "\n",
    "! kubectl exec raycluster-autoscaler-head-xxxxx -it -c ray-head -- python -c \"import ray; ray.init()\"\n",
    "# 2022-08-10 11:23:17,093 INFO worker.py:1312 -- Connecting to existing Ray cluster at address: <IP address>:6379...\n",
    "# 2022-08-10 11:23:17,097 INFO worker.py:1490 -- Connected to Ray cluster."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fa9c6e9d",
   "metadata": {},
   "source": [
    "While this can be useful for ad-hoc execution on the Ray Cluster, the recommended way to execute an application on a Ray Cluster is to use [Ray Jobs](jobs-quickstart).\n",
    "\n",
    "(kuberay-job)=\n",
    "### Ray Job submission\n",
    "\n",
    "To set up your Ray Cluster for Ray Jobs submission, we just need to make sure that the Ray Jobs port is visible to the client.\n",
    "Ray listens for Job requests through the head pod's Dashboard server.\n",
    "\n",
    "First, we need to find the location of the Ray head node. Both the KubeRay operator and the static Ray cluster configuration file configure a [Kubernetes service](https://kubernetes.io/docs/concepts/services-networking/service/) targeting the Ray head pod. This service lets us interact with Ray clusters without directly executing commands in the Ray container. To identify the Ray head service for our example cluster, run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3dae5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the Ray cluster is deployed using KubeRay\n",
    "! kubectl get service raycluster-autoscaler-head-svc\n",
    "\n",
    "# NAME                             TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)                       AGE\n",
    "# raycluster-autoscaler-head-svc   ClusterIP   10.96.114.20   <none>        6379/TCP,8265/TCP,10001/TCP   XXs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1b9f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the Ray cluster is deployed using the static Ray cluster configuration file\n",
    "! kubectl get service service-ray-cluster\n",
    "\n",
    "# NAME                             TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)                            AGE\n",
    "# service-ray-cluster              ClusterIP   10.96.114.20   <none>        6379/TCP,8265/TCP,10001/TCP...     XXs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b3c759",
   "metadata": {},
   "source": [
    "Now that we have the name of the service, we can use port-forwarding to access the Ray Dashboard port (8265 by default).\n",
    "\n",
    "Note: The following port-forwarding command is blocking. If you are following along from a Jupyter notebook, the command must be executed in a separate shell outside of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c113b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute this in a separate shell.\n",
    "# Substitute the service name in place of \"raycluster-autoscaler-head-svc\"\n",
    "! kubectl port-forward service/raycluster-autoscaler-head-svc 8265:8265"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec66649c",
   "metadata": {},
   "source": [
    "Note: We use port-forwarding in this guide as a simple way to experiment with a Ray cluster's services. For production use-cases, you would typically either \n",
    "- Access the service from within the Kubernetes cluster or\n",
    "- Use an ingress controller to expose the service outside the cluster.\n",
    "\n",
    "See the {ref}`networking notes <kuberay-networking>` for details.\n",
    "\n",
    "Now that we have access to the Dashboard port, we can submit jobs to the Ray Cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a6bca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following job's logs will show the Ray cluster's total resource capacity, including 3 CPUs.\n",
    "\n",
    "! ray job submit --address http://localhost:8265 -- python -c \"import ray; ray.init(); print(ray.cluster_resources())\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8453b2a",
   "metadata": {},
   "source": [
    "For a more detailed guide on using Ray Jobs to run applications on a Ray Cluster, check out the [quickstart guide](jobs-quickstart)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b87a4f2e",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "\n",
    "### Deleting a Ray Cluster\n",
    "\n",
    "To delete the KubeRay Ray Cluster we deployed in this example, run the following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a34ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete by reference to the RayCluster custom resource\n",
    "! kubectl delete raycluster raycluster-autoscaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de87d9d",
   "metadata": {},
   "source": [
    "Confirm that the Ray Cluster's pods are gone by running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b557ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "! kubectl get pods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc5dfff",
   "metadata": {},
   "source": [
    "Note that it may take several seconds for the Ray pods to be fully terminated.\n",
    "\n",
    "### Deleting the KubeRay operator\n",
    "\n",
    "In typical operation, the KubeRay operator should be left as a long-running process that manages many Ray clusters.\n",
    "If you would like to delete the operator and associated resources, run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c371a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "! kubectl delete -k \"github.com/ray-project/kuberay/ray-operator/config/default?ref=v0.4.0&timeout=90s\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f740f6f8",
   "metadata": {},
   "source": [
    "### Deleting the static Ray cluster\n",
    "If you'd like to delete the static Ray cluster, run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b647011d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the static Ray cluster service and deployments\n",
    "! kubectl delete -f https://raw.githubusercontent.com/ray-project/ray/master/doc/source/cluster/kubernetes/configs/static-ray-cluster.with-fault-tolerance.yaml\n",
    "\n",
    "# Delete the static Ray cluster network policy\n",
    "! kubectl delete -f https://raw.githubusercontent.com/ray-project/ray/master/doc/source/cluster/kubernetes/configs/static-ray-cluster-networkpolicy.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a76c96",
   "metadata": {},
   "source": [
    "### Deleting a local kind cluster\n",
    "\n",
    "Finally, if you'd like to delete your local kind cluster, run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d4d6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "! kind delete cluster"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.11 ('ray-py38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "250a0c8ad49f9e0ab80d6ffa587b8bd67c2b62f7c5238d34c3fd259cc7d4f5bf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
