{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Inference with Structural Outputs (Guided Decoding)\n",
    "\n",
    "Structural output (or named guided decoding, JSON mode) is a useful feature that ensures the LLM responses following the given output schema in either JSON or the context free grammar.\n",
    "\n",
    "In this example, we show how to perform batch inference using Ray Data LLM with structural outputs in JSON format.\n",
    "\n",
    "To run this example, we need to install the following dependencies:\n",
    "\n",
    "```bash\n",
    "pip install -qU \"ray[data]\" \"vllm==0.7.2\" \"xgrammar==0.1.11\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "import ray\n",
    "from ray.data.llm import build_llm_processor, vLLMEngineProcessorConfig\n",
    "\n",
    "# 1. Construct a guided decoding schema. It can be:\n",
    "# choice: List[str]\n",
    "# json: str\n",
    "# grammar: str\n",
    "# See https://docs.vllm.ai/en/latest/getting_started/examples/structured_outputs.html\n",
    "# for more details about how to construct the schema. Here we use JSON as an example.\n",
    "class AnswerWithExplain(BaseModel):\n",
    "    answer: int\n",
    "    explain: str\n",
    "\n",
    "json_schema = AnswerWithExplain.model_json_schema()\n",
    "\n",
    "# 2. construct a vLLM processor config.\n",
    "processor_config = vLLMEngineProcessorConfig(\n",
    "    # The base model.\n",
    "    model=\"unsloth/Llama-3.2-1B-Instruct\",\n",
    "    # vLLM engine config.\n",
    "    engine_kwargs=dict(\n",
    "        # Specify the guided decoding library to use. The default is \"xgrammar\".\n",
    "        # See https://docs.vllm.ai/en/latest/serving/engine_args.html\n",
    "        # for other available libraries.\n",
    "        guided_decoding_backend=\"xgrammar\",\n",
    "    ),\n",
    "    # The batch size used in Ray Data.\n",
    "    batch_size=16,\n",
    "    # Use one GPU in this example.\n",
    "    concurrency=1,\n",
    "    # Older GPUs (e.g. T4) don't support bfloat16. You should remove\n",
    "    # this line if you're using later GPUs.\n",
    "    dtype=\"half\",\n",
    ")\n",
    "\n",
    "# 3. construct a processor using the processor config.\n",
    "processor = build_llm_processor(\n",
    "    processor_config,\n",
    "    # Convert the input data to the OpenAI chat form.\n",
    "    preprocess=lambda row: dict(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a math teacher. Give the answer to \"\n",
    "                \"the equation and explain it. Output in JSON\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"3 ** {row['id']} + 5 = ?\",\n",
    "            },\n",
    "        ],\n",
    "        sampling_params=dict(\n",
    "            temperature=0.3,\n",
    "            max_tokens=150,\n",
    "            detokenize=False,\n",
    "            # Specify the guided decoding schema.\n",
    "            guided_decoding=dict(json=json_schema),\n",
    "        ),\n",
    "    ),\n",
    "    # Only keep the generated text in the output dataset.\n",
    "    postprocess=lambda row: {\n",
    "        \"resp\": row[\"generated_text\"],\n",
    "    },\n",
    ")\n",
    "\n",
    "# Synthesize a dataset with 30 rows.\n",
    "ds = ray.data.range(30)\n",
    "ds = ds.map(lambda x: {\"id\": x[\"id\"]})\n",
    "\n",
    "# Apply the processor to the dataset. Note that this line won't kick off\n",
    "# anything because processor is execution lazily.\n",
    "ds = processor(ds)\n",
    "# Materialization kicks off the pipeline execution.\n",
    "ds = ds.materialize()\n",
    "\n",
    "# Print all outputs.\n",
    "for out in ds.take_all():\n",
    "    print(out)\n",
    "    print(\"==========\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "orphan": true
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
