{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Inference with LoRA Adapters\n",
    "\n",
    "In this example, we show how to perform batch inference using Ray Data LLM with LLM and a LoRA adapter. \n",
    "\n",
    "To run this example, we need to install the following dependencies:\n",
    "\n",
    "```bash\n",
    "pip install -qU \"ray[data,llm]\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-21 15:58:15 __init__.py:190] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-21 15:58:17,900\tINFO worker.py:1654 -- Connecting to existing Ray cluster at address: 10.0.126.121:6379...\n",
      "2025-02-21 15:58:17,910\tINFO worker.py:1832 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32mhttps://session-6aga4yn8zhbb2tsdr587kwe46n.i.anyscaleuserdata-staging.com \u001b[39m\u001b[22m\n",
      "2025-02-21 15:58:17,911\tINFO packaging.py:367 -- Pushing file package 'gcs://_ray_pkg_cb9f39504b588559e56f00112d1f2e92ee33dbce.zip' (0.00MiB) to Ray cluster...\n",
      "2025-02-21 15:58:17,912\tINFO packaging.py:380 -- Successfully pushed file package 'gcs://_ray_pkg_cb9f39504b588559e56f00112d1f2e92ee33dbce.zip'.\n",
      "2025-02-21 15:58:17,942\tINFO streaming_executor.py:108 -- Starting execution of Dataset. Full logs are in /tmp/ray/session_2025-02-21_09-38-35_001765_2789/logs/ray-data\n",
      "2025-02-21 15:58:17,942\tINFO streaming_executor.py:109 -- Execution plan of Dataset: InputDataBuffer[Input] -> ActorPoolMapOperator[ReadRange->Map(<lambda>)->Map(_preprocess)->MapBatches(ChatTemplateUDF)] -> ActorPoolMapOperator[MapBatches(TokenizeUDF)] -> ActorPoolMapOperator[MapBatches(vLLMEngineStageUDF)] -> ActorPoolMapOperator[MapBatches(DetokenizeUDF)] -> TaskPoolMapOperator[Map(_postprocess)]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aebbc18d153c43798809050546a15137",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running 0: 0.00 row [00:00, ? row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_MapWorker pid=154950)\u001b[0m INFO 02-21 15:58:25 __init__.py:190] Automatically detected platform cuda.\n",
      "\u001b[36m(_MapWorker pid=155088)\u001b[0m INFO 02-21 15:58:35 __init__.py:190] Automatically detected platform cuda.\n",
      "\u001b[36m(_MapWorker pid=155187)\u001b[0m INFO 02-21 15:58:44 __init__.py:190] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_MapWorker pid=155187)\u001b[0m Max pending requests is set to 141\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_MapWorker pid=155187)\u001b[0m WARNING 02-21 15:58:55 arg_utils.py:1145] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.\n",
      "\u001b[36m(_MapWorker pid=155187)\u001b[0m INFO 02-21 15:58:55 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.2) with config: model='unsloth/Llama-3.2-1B-Instruct', speculative_config=None, tokenizer='unsloth/Llama-3.2-1B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=unsloth/Llama-3.2-1B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "\u001b[36m(_MapWorker pid=155187)\u001b[0m INFO 02-21 15:58:57 cuda.py:230] Using Flash Attention backend.\n",
      "\u001b[36m(_MapWorker pid=155187)\u001b[0m INFO 02-21 15:58:57 model_runner.py:1110] Starting to load model unsloth/Llama-3.2-1B-Instruct...\n",
      "\u001b[36m(_MapWorker pid=155187)\u001b[0m INFO 02-21 15:58:58 weight_utils.py:252] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_MapWorker pid=155187)\u001b[0m INFO 02-21 15:58:58 weight_utils.py:297] No model.safetensors.index.json found in remote.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.39it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.39it/s]\n",
      "\u001b[36m(_MapWorker pid=155187)\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_MapWorker pid=155187)\u001b[0m INFO 02-21 15:58:58 model_runner.py:1115] Loading model weights took 2.3185 GB\n",
      "\u001b[36m(_MapWorker pid=155187)\u001b[0m INFO 02-21 15:58:58 punica_selector.py:18] Using PunicaWrapperGPU.\n",
      "\u001b[36m(_MapWorker pid=155187)\u001b[0m INFO 02-21 15:59:02 worker.py:267] Memory profiling takes 3.16 seconds\n",
      "\u001b[36m(_MapWorker pid=155187)\u001b[0m INFO 02-21 15:59:02 worker.py:267] the current vLLM instance can use total_gpu_memory (44.53GiB) x gpu_memory_utilization (0.90) = 40.07GiB\n",
      "\u001b[36m(_MapWorker pid=155187)\u001b[0m INFO 02-21 15:59:02 worker.py:267] model weights take 2.32GiB; non_torch_memory takes 0.08GiB; PyTorch activation peak memory takes 7.51GiB; the rest of the memory reserved for KV Cache is 30.16GiB.\n",
      "\u001b[36m(_MapWorker pid=155187)\u001b[0m INFO 02-21 15:59:02 executor_base.py:110] # CUDA blocks: 61774, # CPU blocks: 8192\n",
      "\u001b[36m(_MapWorker pid=155187)\u001b[0m INFO 02-21 15:59:02 executor_base.py:115] Maximum concurrency for 131072 tokens per request: 7.54x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_MapWorker pid=155187)\u001b[0m INFO 02-21 15:59:05 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:28,  1.21it/s]\n",
      "Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:26,  1.27it/s]\n",
      "Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:02<00:22,  1.43it/s]\n",
      "Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:20,  1.55it/s]\n",
      "Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:03<00:17,  1.70it/s]\n",
      "Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:16,  1.80it/s]\n",
      "Capturing CUDA graph shapes:  20%|██        | 7/35 [00:04<00:15,  1.83it/s]\n",
      "Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:04<00:14,  1.89it/s]\n",
      "Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:13,  1.95it/s]\n",
      "Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:05<00:12,  1.96it/s]\n",
      "Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:06<00:12,  1.99it/s]\n",
      "Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:06<00:11,  2.01it/s]\n",
      "Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:07<00:10,  2.03it/s]\n",
      "Capturing CUDA graph shapes:  40%|████      | 14/35 [00:07<00:10,  2.05it/s]\n",
      "Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:08<00:09,  2.05it/s]\n",
      "Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:08<00:09,  2.05it/s]\n",
      "Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:09<00:08,  2.05it/s]\n",
      "Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:09<00:08,  1.98it/s]\n",
      "Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:10<00:07,  2.00it/s]\n",
      "Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:10<00:07,  2.02it/s]\n",
      "Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:11<00:06,  2.03it/s]\n",
      "Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:11<00:06,  2.03it/s]\n",
      "Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:12<00:05,  2.02it/s]\n",
      "Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:12<00:05,  2.03it/s]\n",
      "Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:13<00:04,  2.03it/s]\n",
      "Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:13<00:04,  2.03it/s]\n",
      "Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:14<00:03,  2.04it/s]\n",
      "Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:14<00:03,  2.03it/s]\n",
      "Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:15<00:02,  2.04it/s]\n",
      "Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:15<00:02,  2.05it/s]\n",
      "Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:16<00:01,  2.05it/s]\n",
      "Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:16<00:01,  2.06it/s]\n",
      "Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:16<00:00,  2.05it/s]\n",
      "Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:17<00:00,  2.06it/s]\n",
      "Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:17<00:00,  1.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_MapWorker pid=155187)\u001b[0m INFO 02-21 15:59:23 model_runner.py:1562] Graph capturing finished in 18 secs, took 1.12 GiB\n",
      "\u001b[36m(_MapWorker pid=155187)\u001b[0m INFO 02-21 15:59:23 llm_engine.py:431] init engine (profile, create kv cache, warmup model) took 24.21 seconds\n",
      "\u001b[36m(_MapWorker pid=155539)\u001b[0m INFO 02-21 15:59:32 __init__.py:190] Automatically detected platform cuda.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d2e0360241e4e60afcce0d0984d8add",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "- ReadRange->Map(<lambda>)->Map(_preprocess)->MapBatches(ChatTemplateUDF) 1: 0.00 row [00:00, ? row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9f819690df0423586065d4d9a6b3d35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "- MapBatches(TokenizeUDF) 2: 0.00 row [00:00, ? row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a1e1a803b3b4b3d92b4eacbb50cad6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "- MapBatches(vLLMEngineStageUDF) 3: 0.00 row [00:00, ? row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e47bcc5e1b1945b3b8d0e7973b4a60f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "- MapBatches(DetokenizeUDF) 4: 0.00 row [00:00, ? row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39044febc5c6445eba1e66fa2add401e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "- Map(_postprocess) 5: 0.00 row [00:00, ? row/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-21 15:59:34,080\tWARNING progress_bar.py:120 -- Truncating long operator name to 100 characters. To disable this behavior, set `ray.data.DataContext.get_current().DEFAULT_ENABLE_PROGRESS_BAR_NAME_TRUNCATION = False`.\n",
      "Fetching 15 files: 100%|██████████| 15/15 [00:00<00:00, 19070.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=155187)\u001b[0m INFO 02-21 15:59:35 metrics.py:455] Avg prompt throughput: 150.2 tokens/s, Avg generation throughput: 2.6 tokens/s, Running: 30 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=155187)\u001b[0m [vLLM] Elapsed time for batch 2eba76b6bd0d4125a75466be157196e6 with size 14: 1.1405171180012985\n",
      "\u001b[36m(MapWorker(MapBatches(vLLMEngineStageUDF)) pid=155187)\u001b[0m [vLLM] Elapsed time for batch 5171ea7cc1d247d089c86ff5455f4bfe with size 16: 1.1826866699993843\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'resp': '21.'}\n",
      "==========\n",
      "{'resp': 'The answer is 27.'}\n",
      "==========\n",
      "{'resp': 'The answer is: 30.'}\n",
      "==========\n",
      "{'resp': 'The answer is: 36.'}\n",
      "==========\n",
      "{'resp': 'The answer is  39.'}\n",
      "==========\n",
      "{'resp': 'The answer is  24.'}\n",
      "==========\n",
      "{'resp': '0 ** 3 = 0.'}\n",
      "==========\n",
      "{'resp': '11 × 3 = 33.'}\n",
      "==========\n",
      "{'resp': '15 × 3 = 45.'}\n",
      "==========\n",
      "{'resp': '5 × 3 = 15.'}\n",
      "==========\n",
      "{'resp': '1  * 3 = 3.'}\n",
      "==========\n",
      "{'resp': 'The answer to 3 × 3 is 9.'}\n",
      "==========\n",
      "{'resp': 'The answer is  6 **  3 = 18.'}\n",
      "==========\n",
      "{'resp': 'The answer is: 42. For in this number there is hidden a mystery that none, except'}\n",
      "==========\n",
      "{'resp': 'God bless Thee, O Thou the Most Merciful! The answer is 6. For if'}\n",
      "==========\n",
      "{'resp': 'I am unable to provide the answer to the question 4 × 3 = 12. If'}\n",
      "==========\n",
      "{'resp': '961.'}\n",
      "==========\n",
      "{'resp': 'The answer is 6.'}\n",
      "==========\n",
      "{'resp': 'The answer is 60.'}\n",
      "==========\n",
      "{'resp': 'The answer is  51.'}\n",
      "==========\n",
      "{'resp': 'The answer is  69.'}\n",
      "==========\n",
      "{'resp': 'The answer is  66.'}\n",
      "==========\n",
      "{'resp': 'The answer is  69.'}\n",
      "==========\n",
      "{'resp': 'The answer is:  8'}\n",
      "==========\n",
      "{'resp': 'The answer is  75.'}\n",
      "==========\n",
      "{'resp': 'The answer is  9.'}\n",
      "==========\n",
      "{'resp': '16 × 3 = 48.'}\n",
      "==========\n",
      "{'resp': 'The answer to 2 × 8 is 16.'}\n",
      "==========\n",
      "{'resp': '  26  multiplied by 3 is  78.'}\n",
      "==========\n",
      "{'resp': 'The answer is  29  x 3 = 87.'}\n",
      "==========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(autoscaler +30m24s)\u001b[0m Tip: use `ray status` to view detailed cluster status. To disable these messages, set RAY_SCHEDULER_EVENTS=0.\n"
     ]
    }
   ],
   "source": [
    "import ray\n",
    "from ray.data.llm import build_llm_processor, vLLMEngineProcessorConfig\n",
    "\n",
    "# First construct a vLLM processor config.\n",
    "processor_config = vLLMEngineProcessorConfig(\n",
    "    # The base model.\n",
    "    model=\"unsloth/Llama-3.2-1B-Instruct\",\n",
    "    # vLLM engine config.\n",
    "    engine_kwargs=dict(\n",
    "        # Enable LoRA in the vLLM engine; otherwise you won't be able to\n",
    "        # process requests with LoRA adapters.\n",
    "        enable_lora=True,\n",
    "        # You need to set the LoRA rank for the adapter.\n",
    "        # The LoRA rank is the value of \"r\" in the LoRA config.\n",
    "        # If you want to use multiple LoRA adapters in this pipeline,\n",
    "        # please specify the maximum LoRA rank amount all of them.\n",
    "        max_lora_rank=32,\n",
    "        # The maximum number of LoRA adapters vLLM cached. \"1\" means\n",
    "        # vLLM only caches one LoRA adapter at a time, so if your dataset\n",
    "        # needs more than one LoRA adapters, then there would be context\n",
    "        # switching. On the other hand, while increasing max_loras reduces\n",
    "        # the context switching, it increases the memory footprint.\n",
    "        max_loras=1,\n",
    "    ),\n",
    "    # The batch size used in Ray Data.\n",
    "    batch_size=16,\n",
    "    # Use one GPU in this example.\n",
    "    concurrency=1,\n",
    ")\n",
    "\n",
    "# Then construct a processor using the processor config.\n",
    "processor = build_llm_processor(\n",
    "    processor_config,\n",
    "    # Convert the input data to the OpenAI chat form.\n",
    "    preprocess=lambda row: dict(\n",
    "        # If you specify \"model\" in a request, and the model is different\n",
    "        # from the model you specify in the processor config, then this\n",
    "        # is the LoRA adapter. The \"model\" here can be a LoRA adapter\n",
    "        # available in the HuggingFace Hub or a local path.\n",
    "        model=\"EdBergJr/Llama32_Baha_3\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\",\n",
    "             \"content\": \"You are a calculator. Please only output the answer \"\n",
    "                \"of the given equation.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"{row['id']} ** 3 = ?\"},\n",
    "        ],\n",
    "        sampling_params=dict(\n",
    "            temperature=0.3,\n",
    "            max_tokens=20,\n",
    "            detokenize=False,\n",
    "        ),\n",
    "    ),\n",
    "    # Only keep the generated text in the output dataset.\n",
    "    postprocess=lambda row: {\n",
    "        \"resp\": row[\"generated_text\"],\n",
    "    },\n",
    ")\n",
    "\n",
    "# Synthesize a dataset with 30 rows.\n",
    "ds = ray.data.range(30)\n",
    "ds = ds.map(lambda x: {\"id\": x[\"id\"]})\n",
    "\n",
    "# Apply the processor to the dataset. Note that this line won't kick off\n",
    "# anything because processor is execution lazily.\n",
    "ds = processor(ds)\n",
    "# Materialization kicks off the pipeline execution.\n",
    "ds = ds.materialize()\n",
    "\n",
    "# Print all outputs.\n",
    "for out in ds.take_all():\n",
    "    print(out)\n",
    "    print(\"==========\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
