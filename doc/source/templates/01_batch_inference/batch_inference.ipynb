{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6fbc3e3c",
   "metadata": {},
   "source": [
    "# Scaling Batch Inference with Ray Data\n",
    "\n",
    "This template is a quickstart to using [Ray Data](https://docs.ray.io/en/latest/data/data.html) for batch inference. Ray Data is one of many libraries under the [Ray AI Runtime](https://docs.ray.io/en/latest/ray-air/getting-started.html). See [this blog post](https://www.anyscale.com/blog/model-batch-inference-in-ray-actors-actorpool-and-datasets) for more information on why and how you should perform batch inference with Ray!\n",
    "\n",
    "This template walks through GPU batch prediction on an image dataset using a PyTorch model, but the framework and data format are there just to help you build your own application!\n",
    "\n",
    "At a high level, this template will:\n",
    "1. [Load your dataset using Ray Data.](https://docs.ray.io/en/latest/data/creating-datastreams.html)\n",
    "2. [Preprocess your dataset before feeding it to your model.](https://docs.ray.io/en/latest/data/transforming-datastreams.html)\n",
    "3. [Initialize your model and perform inference on a shard of your dataset with a remote actor.](https://docs.ray.io/en/latest/data/transforming-datastreams.html#callable-class-udfs)\n",
    "4. [Save your prediction results.](https://docs.ray.io/en/latest/data/api/input_output.html)\n",
    "\n",
    "> Slot in your code below wherever you see the ✂️ icon to build a many model training Ray application off of this template!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065e7765",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import tempfile\n",
    "from typing import Dict\n",
    "\n",
    "import ray\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c99f142a",
   "metadata": {},
   "source": [
    ">✂️ Play around with these values!\n",
    ">\n",
    ">For example, for a cluster with 4 GPU nodes, you may want 4 workers, each using 1 GPU.\n",
    ">Be sure to stay within the resource constraints of your Ray Cluster if autoscaling is not enabled.\n",
    ">You can check the available resources in your Ray Cluster with: `ray status`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770bbdc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ray status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d49681f-baf0-4ed8-9740-5c4e38744311",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "NUM_WORKERS: int = 4\n",
    "NUM_GPUS_PER_WORKER: float = 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23321ba8",
   "metadata": {},
   "source": [
    "```{tip}\n",
    "Try setting `NUM_GPUS_PER_WORKER` to a fractional amount! This will leverage Ray's fractional resource allocation, which means you can schedule multiple batch inference workers to happen on the same GPU.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6f2352",
   "metadata": {},
   "source": [
    "> ✂️ Replace this function with logic to load your own data with Ray Data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615f4a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ray_dataset() -> ray.data.Datastream:\n",
    "    from ray.data.datasource.partitioning import Partitioning\n",
    "\n",
    "    s3_uri = \"s3://anonymous@air-example-data-2/imagenette2/val/\"\n",
    "    partitioning = Partitioning(\"dir\", field_names=[\"class\"], base_dir=s3_uri)\n",
    "    ds = ray.data.read_images(\n",
    "        s3_uri, size=(256, 256), partitioning=partitioning, mode=\"RGB\"\n",
    "    )\n",
    "    return ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966bcfdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_ray_dataset()\n",
    "ds.schema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d01e3c",
   "metadata": {},
   "source": [
    "> ✂️ Replace this function with your own data preprocessing logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652121bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(batch: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n",
    "    from torchvision import transforms\n",
    "\n",
    "    def to_tensor(batch: np.ndarray) -> torch.Tensor:\n",
    "        tensor = torch.as_tensor(batch, dtype=torch.float)\n",
    "        # (B, H, W, C) -> (B, C, H, W)\n",
    "        tensor = tensor.permute(0, 3, 1, 2).contiguous()\n",
    "        # [0., 255.] -> [0., 1.]\n",
    "        tensor = tensor.div(255)\n",
    "        return tensor\n",
    "\n",
    "    transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.Lambda(to_tensor),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ]\n",
    "    )\n",
    "    return {\"image\": transform(batch[\"image\"]).numpy()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35f5a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.map_batches(preprocess, batch_format=\"numpy\")\n",
    "ds.schema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad059e54",
   "metadata": {},
   "source": [
    "> ✂️ Replace parts of this Callable class with your own model initialization and inference logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42cac828",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredictCallable:\n",
    "    def __init__(self):\n",
    "        # <Replace this with your own model initialization>\n",
    "        from torchvision import models\n",
    "\n",
    "        self.model = models.resnet152(pretrained=True)\n",
    "        self.model.eval()\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(self.device)\n",
    "\n",
    "    def __call__(self, batch: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n",
    "        # <Replace this with your own model inference logic>\n",
    "        input_data = torch.as_tensor(batch[\"image\"], device=self.device)\n",
    "        with torch.no_grad():\n",
    "            result = self.model(input_data)\n",
    "        return {\"predictions\": result.cpu().numpy()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda0c298",
   "metadata": {},
   "source": [
    "Now, perform batch prediction using Ray Data! Ray Data will perform model inference using `NUM_WORKERS` copies of the `PredictCallable` class you defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331e21e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = ds.map_batches(\n",
    "    PredictCallable,\n",
    "    batch_size=128,\n",
    "    compute=ray.data.ActorPoolStrategy(\n",
    "        # Fix the number of batch inference workers to a specified value.\n",
    "        size=NUM_WORKERS,\n",
    "    ),\n",
    "    num_gpus=NUM_GPUS_PER_WORKER,\n",
    "    batch_format=\"numpy\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e77ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = predictions.materialize()\n",
    "preds.schema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d606556",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds.take(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceddd984",
   "metadata": {},
   "source": [
    "```{tip}\n",
    "Play around with the `min_size` and `max_size` parameters to enable autoscaling!\n",
    "For example, try commenting out `max_size`: this will autoscale up to an infinite number of workers, if you have free resources in the cluster.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ec67e8",
   "metadata": {},
   "source": [
    "Shard the predictions into a few partitions, and save each partition to a file!\n",
    "\n",
    "```{note}\n",
    "This currently saves to the local filesystem under `/tmp/predictions`, but you could also save to a cloud bucket (e.g., `s3://predictions-bucket`).\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1887e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_shards = 3\n",
    "\n",
    "with tempfile.TemporaryDirectory() as temp_dir:\n",
    "    predictions.repartition(num_shards).write_parquet(f\"local://{temp_dir}\")\n",
    "    print(f\"Predictions saved to `{temp_dir}`!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "265d195fda5292fe8f69c6e37c435a5634a1ed3b6799724e66a975f68fa21517"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
