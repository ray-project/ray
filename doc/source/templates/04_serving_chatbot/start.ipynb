{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "597c13c0",
   "metadata": {},
   "source": [
    "# Serving a Chatbot with Ray Serve\n",
    "\n",
    "| Template Specification | Description |\n",
    "| ---------------------- | ----------- |\n",
    "| Summary | This template demonstrates how to serve a chatbot with [Ray Serve](https://docs.ray.io/en/latest/serve/index.html).|\n",
    "| Time to Run | |\n",
    "| Minimum Compute Requirements | At least 1 GPU node. The default is 1 node with 4 CPUs, and 1 node with 1 NVIDIA T4 GPU. |\n",
    "| Cluster Environment | This template uses a docker image built on top of the latest Anyscale-provided Ray image using Python 3.9: [`anyscale/ray:latest-py39-cu118`](https://docs.anyscale.com/reference/base-images/overview). See the appendix below for more details. |\n",
    "\n",
    "## Get Started\n",
    "\n",
    "**When the workspace is up and running, start coding by clicking on the Jupyter or VSCode icon above. Open the `start.ipynb` file and follow the instructions there.**\n",
    "\n",
    "By the end, we'll have a chatbot that can have a conversation using GPT-J.\n",
    "\n",
    "The application will look something like this:\n",
    "\n",
    "```text\n",
    ">>> Hi there!\n",
    "\n",
    "...\n",
    "\n",
    ">>> What's your name?\n",
    "\n",
    "...\n",
    "```\n",
    "\n",
    "![Example output]()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5a615a0a",
   "metadata": {},
   "source": [
    "> Slot in your code below wherever you see the ✂️ icon to build off of this template!\n",
    ">\n",
    "> The framework and data format used in this template can be easily replaced to suit your own application!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2ef26fb0",
   "metadata": {},
   "source": [
    "First, add the imports and the Serve logger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59842da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import logging\n",
    "from queue import Empty\n",
    "\n",
    "from fastapi import FastAPI, WebSocket, WebSocketDisconnect\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TextIteratorStreamer\n",
    "\n",
    "from ray import serve\n",
    "\n",
    "logger = logging.getLogger(\"ray.serve\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "520ef4d7",
   "metadata": {},
   "source": [
    "## Build and run the Ray Serve chatbot application\n",
    "\n",
    "Create a FastAPI deployment that initializes the model in the constructor and exposes a WebSocket endpoint, so users can query it:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f203efd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fastapi_app = FastAPI()\n",
    "\n",
    "\n",
    "@serve.deployment\n",
    "@serve.ingress(fastapi_app)\n",
    "class Chatbot:\n",
    "    def __init__(self, model_id: str):\n",
    "        self.loop = asyncio.get_running_loop()\n",
    "\n",
    "        self.model_id = model_id\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(self.model_id)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_id)\n",
    "\n",
    "    @fastapi_app.websocket(\"/\")\n",
    "    async def handle_request(self, ws: WebSocket) -> None:\n",
    "        await ws.accept()\n",
    "\n",
    "        conversation = \"\"\n",
    "        try:\n",
    "            while True:\n",
    "                prompt = await ws.receive_text()\n",
    "                logger.info(f'Got prompt: \"{prompt}\"')\n",
    "                conversation += prompt\n",
    "                streamer = TextIteratorStreamer(\n",
    "                    self.tokenizer,\n",
    "                    timeout=0,\n",
    "                    skip_prompt=True,\n",
    "                    skip_special_tokens=True,\n",
    "                )\n",
    "                self.loop.run_in_executor(\n",
    "                    None, self.generate_text, conversation, streamer\n",
    "                )\n",
    "                response = \"\"\n",
    "                async for text in self.consume_streamer(streamer):\n",
    "                    await ws.send_text(text)\n",
    "                    response += text\n",
    "                await ws.send_text(\"<<Response Finished>>\")\n",
    "                conversation += response\n",
    "        except WebSocketDisconnect:\n",
    "            print(\"Client disconnected.\")\n",
    "        \n",
    "    def generate_text(self, prompt: str, streamer: TextIteratorStreamer):\n",
    "        input_ids = self.tokenizer([prompt], return_tensors=\"pt\").input_ids\n",
    "        self.model.generate(input_ids, streamer=streamer, max_length=10000)\n",
    "\n",
    "    async def consume_streamer(self, streamer: TextIteratorStreamer):\n",
    "        while True:\n",
    "            try:\n",
    "                for token in streamer:\n",
    "                    logger.info(f'Yielding token: \"{token}\"')\n",
    "                    yield token\n",
    "                break\n",
    "            except Empty:\n",
    "                await asyncio.sleep(0.001)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0134aa54",
   "metadata": {},
   "source": [
    "`Chatbot` uses three methods to handle requests:\n",
    "\n",
    "* `handle_request`: the entrypoint for WebSocket requests. The `handle_request` method is decorated with a `@fastapi_app.websocket` decorator, which lets it accept WebSocket requests. First it `awaits` to accept the client's WebSocket request. Then, until the client disconnects, it does the following:\n",
    "    * gets the prompt from the client with `ws.receive_text`\n",
    "    * starts a new `TextIteratorStreamer` to access generated tokens\n",
    "    * runs the model in a background thread on the conversation so far\n",
    "    * streams the model's output back using `ws.send_text`\n",
    "    * stores the prompt and the response in the `conversation` string\n",
    "* `generate_text`: the method that runs the model. This method runs in a background thread kicked off by `handle_request`. It pushes generated tokens into the streamer constructed by `handle_request`.\n",
    "* `consume_streamer`: a generator method that consumes the streamer constructed by `handle_request`. This method keeps yielding tokens from the streamer until the model in `generate_text` closes the streamer. This method avoids blocking the event loop by calling `asyncio.sleep` with a brief timeout whenever the streamer is empty and waiting for a new token."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "81b02967",
   "metadata": {},
   "source": [
    "**Handling Disconnects:** Each time `handle_request` gets a new prompt from a client, it runs the whole conversation– with the new prompt appended– through the model. When the model is finished generating tokens, `handle_request` sends the `\"<<Response Finished>>\"` string to inform the client that all tokens have been generated. `handle_request` continues to run until the client explicitly disconnects. This disconnect raises a `WebSocketDisconnect` exception, which ends the call."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8fd4f182",
   "metadata": {},
   "source": [
    "Bind the `Chatbot` to a language model. For this tutorial, use the `\"EleutherAI/gpt-j-6B\"` model.\n",
    "\n",
    "✂️ You can replace the model ID to use a different language model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f80fee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "app = Chatbot.bind(\"EleutherAI/gpt-j-6B\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "61b8916d",
   "metadata": {},
   "source": [
    "Now, deploy the Ray Serve application locally at `ws://localhost:8000`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc2e244",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Shutdown any existing Serve replicas, if they're still around.\n",
    "serve.shutdown()\n",
    "serve.run(app, port=8000, name=\"Chat\")\n",
    "print(\"Done setting up replicas! Now accepting requests...\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "757678cc",
   "metadata": {},
   "source": [
    "## Make requests to the endpoint\n",
    "\n",
    "Next, we'll build a simple WebSocket client to chat with the chatbot."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "008976b5",
   "metadata": {},
   "source": [
    "Run the client script in the next cell to start chatting!\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c8949cc7",
   "metadata": {},
   "source": [
    "Once the stable diffusion model finishes generating your image(s), it will be included in the HTTP response body.\n",
    "The client saves all the images in a local directory for you to view, and they'll also show up in the notebook cell!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336cc2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from websockets.sync.client import connect\n",
    "\n",
    "with connect(\"ws://localhost:8000\") as websocket:\n",
    "    user_prompt = input(\">>> \")\n",
    "    websocket.send(user_prompt)\n",
    "    print(\"Chatbot: \", end=\"\", flush=True)\n",
    "    while True:\n",
    "        received = websocket.recv()\n",
    "        if received == \"<<Response Finished>>\":\n",
    "            break\n",
    "        print(received, end=\"\", flush=True)\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e360cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shut down Serve once you're done!\n",
    "serve.shutdown()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "880c2d6f",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This template used [Ray Serve](https://docs.ray.io/en/latest/serve/index.html) to serve a chatbot. Ray Serve is one of many libraries under the [Ray AI Runtime](https://docs.ray.io/en/latest/ray-air/getting-started.html).\n",
    "\n",
    "At a high level, this template showed how to:\n",
    "1. Load a HuggingFace model in a Ray Serve deployment and perform inference.\n",
    "2. Provide bidirectional communication in Ray Serve deployments with WebSockets\n",
    "3. Stream inputs and ouputs between a Ray Serve deployment and an end user\n",
    "\n",
    "See this [getting started guide](https://docs.ray.io/en/latest/serve/getting_started.html) for a more detailed walkthrough of Ray Serve."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ray_dev_py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "265d195fda5292fe8f69c6e37c435a5634a1ed3b6799724e66a975f68fa21517"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
