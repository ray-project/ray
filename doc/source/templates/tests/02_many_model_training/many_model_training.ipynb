{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46369bd2",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# ==== Code for testing purposes to exclude in user-facing template. ====\n",
    "\n",
    "import os\n",
    "\n",
    "SMOKE_TEST = True if os.environ.get(\"SMOKE_TEST\", \"0\") == \"1\" else False\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "98e0d4f3",
   "metadata": {},
   "source": [
    "# Scaling Many Model Training with Ray Tune\n",
    "\n",
    "This template is a quickstart to using [Ray Tune](https://docs.ray.io/en/latest/tune/index.html) for training many models in parallel. Ray Tune is one of many libraries in the [Ray AI Runtime](https://docs.ray.io/en/latest/ray-air/getting-started.html). See [this blog post](https://www.anyscale.com/blog/training-one-million-machine-learning-models-in-record-time-with-ray) for more information on the benefits of performing many model training with Ray!\n",
    "\n",
    "This template walks through time-series forecasting using `statsforecast`, but the framework and data format can be swapped out easily -- they are there just to help you build your own application!\n",
    "\n",
    "At a high level, this template will:\n",
    "\n",
    "1. [Define the training function for a single partition of data.](https://docs.ray.io/en/latest/tune/tutorials/tune-run.html)\n",
    "2. [Define a Tune search space to run training over many partitions of data.](https://docs.ray.io/en/latest/tune/tutorials/tune-search-spaces.html)\n",
    "3. [Extract the best model per dataset partition from the Tune experiment output.](https://docs.ray.io/en/latest/tune/examples/tune_analyze_results.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e65f8d",
   "metadata": {},
   "source": [
    "> Slot in your code below wherever you see the ✂️ icon to build a many model training Ray application off of this template!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182f65ea",
   "metadata": {},
   "source": [
    "## Handling Dependencies\n",
    "\n",
    "This template requires certain Python packages to be available to every node in the cluster.\n",
    "\n",
    "> ✂️ Add your own package dependencies in the `requirements.txt` file!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511f1722",
   "metadata": {},
   "outputs": [],
   "source": [
    "requirements_path = \"./requirements.txt\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a44498",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(requirements_path):\n",
    "    # CWD is at the ray root in CI\n",
    "    requirements_path = \"doc/source/templates/tests/02_many_model_training/requirements.txt\"\n",
    "    assert os.path.exists(requirements_path), (requirements_path, os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd9da7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(requirements_path, \"r\") as f:\n",
    "    requirements = f.read().strip().splitlines()\n",
    "\n",
    "print(\"Requirements:\")\n",
    "print(\"\\n\".join(requirements))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a96c5b",
   "metadata": {},
   "source": [
    "First, we may want to use these modules right here in our script, which is running on the head node.\n",
    "Install the Python packages on the head node using `pip install`.\n",
    "\n",
    "```{note}\n",
    "You may need to restart this notebook kernel to access the installed packages.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18069827",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r {requirements_path} --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e17a4da",
   "metadata": {},
   "source": [
    "Next, we need to make sure all worker nodes also have access to the dependencies.\n",
    "For this, use a [Ray Runtime Environment](https://docs.ray.io/en/latest/ray-core/handling-dependencies.html#runtime-environments)\n",
    "to dynamically set up dependencies throughout the cluster.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e268225d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "\n",
    "ray.init(runtime_env={\"pip\": requirements})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389adc20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyarrow import parquet as pq\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from ray import tune\n",
    "from ray.air import session\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8fc83d0",
   "metadata": {},
   "source": [
    "> ✂️ Replace this value to change the number of data partitions you will use. This will be total the number of Tune trials you will run!\n",
    ">\n",
    "> Note that this template fits two models per data partition and reports the best performing one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5390c232",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "NUM_DATA_PARTITIONS: int = 500\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6fb7a2d",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "if SMOKE_TEST:\n",
    "    NUM_DATA_PARTITIONS: int = 10\n",
    "\n",
    "import time\n",
    "\n",
    "start_time = time.monotonic()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2f3d16",
   "metadata": {},
   "source": [
    "> ✂️ Replace the following with your own data-loading and evaluation helper functions. (Or, just delete these!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b14061",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_m5_partition(unique_id: str) -> pd.DataFrame:\n",
    "    df = (\n",
    "        pq.read_table(\n",
    "            \"s3://anonymous@m5-benchmarks/data/train/target.parquet\",\n",
    "            columns=[\"item_id\", \"timestamp\", \"demand\"],\n",
    "            filters=[(\"item_id\", \"=\", unique_id)],\n",
    "        )\n",
    "        .to_pandas()\n",
    "        .rename(columns={\"item_id\": \"unique_id\", \"timestamp\": \"ds\", \"demand\": \"y\"})\n",
    "    )\n",
    "    df[\"unique_id\"] = df[\"unique_id\"].astype(str)\n",
    "    df[\"ds\"] = pd.to_datetime(df[\"ds\"])\n",
    "    return df.dropna()\n",
    "\n",
    "\n",
    "def evaluate_cross_validation(df: pd.DataFrame, metric) -> pd.DataFrame:\n",
    "    models = df.drop(columns=[\"ds\", \"cutoff\", \"y\"]).columns.tolist()\n",
    "    evals = []\n",
    "    for model in models:\n",
    "        eval_ = (\n",
    "            df.groupby([\"unique_id\", \"cutoff\"])\n",
    "            .apply(lambda x: metric(x[\"y\"].values, x[model].values))\n",
    "            .to_frame()\n",
    "        )\n",
    "        eval_.columns = [model]\n",
    "        evals.append(eval_)\n",
    "    evals = pd.concat(evals, axis=1)\n",
    "    evals = evals.groupby([\"unique_id\"]).mean(numeric_only=True)\n",
    "    evals[\"best_model\"] = evals.idxmin(axis=1)\n",
    "    return evals\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060ee3ce",
   "metadata": {},
   "source": [
    "> ✂️ Replace this with your own training logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faaa0dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(config: dict):\n",
    "    try:\n",
    "        from statsforecast import StatsForecast\n",
    "        from statsforecast.models import AutoARIMA, AutoETS\n",
    "    except ImportError as e:\n",
    "        raise RuntimeError(\"Did you set a runtime env to install dependencies?\") from e\n",
    "\n",
    "    data_partition_id = config[\"data_partition_id\"]\n",
    "    train_df = get_m5_partition(data_partition_id)\n",
    "\n",
    "    models = [AutoARIMA(), AutoETS()]\n",
    "    n_windows = 1\n",
    "    forecast_horizon = 4\n",
    "\n",
    "    sf = StatsForecast(\n",
    "        df=train_df,\n",
    "        models=models,\n",
    "        freq=\"D\",\n",
    "        n_jobs=n_windows * len(models),\n",
    "    )\n",
    "    cv_df = sf.cross_validation(\n",
    "        h=forecast_horizon,\n",
    "        step_size=forecast_horizon,\n",
    "        n_windows=n_windows,\n",
    "    )\n",
    "\n",
    "    eval_df = evaluate_cross_validation(df=cv_df, metric=mean_squared_error)\n",
    "    best_model = eval_df[\"best_model\"][data_partition_id]\n",
    "    forecast_mse = eval_df[best_model][data_partition_id]\n",
    "\n",
    "    # Report the best-performing model and its corresponding eval metric.\n",
    "    session.report({\"forecast_mse\": forecast_mse, \"best_model\": best_model})\n",
    "\n",
    "\n",
    "trainable = train_fn\n",
    "trainable = tune.with_resources(trainable, resources={\"CPU\": 2 * 1})\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "301c7c58",
   "metadata": {},
   "source": [
    "```{note}\n",
    "`tune.with_resources` is used at the end to specify the number of resources to assign *each trial*.\n",
    "Feel free to change this to the resources required by your application! You can also comment out the `tune.with_resources` block to assign `1 CPU` (the default) to each trial.\n",
    "\n",
    "Note that this is purely for Tune to know how many trials to schedule concurrently -- setting the number of CPUs does not actually enforce any kind of resource isolation!\n",
    "In this template, `statsforecast` runs cross validation in parallel with M models * N temporal cross-validation windows (e.g. 2 * 1).\n",
    "```\n",
    "\n",
    "See [Ray Tune's guide on assigning resources](https://docs.ray.io/en/latest/tune/tutorials/tune-resources.html) for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89741e7a",
   "metadata": {},
   "source": [
    "> ✂️ Replace this with your desired hyperparameter search space!\n",
    ">\n",
    "> For example, this template searches over the data partition ID to train a model on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9f2825",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the list of item ids used to partition the dataset.\n",
    "data_partitions = list(\n",
    "    pd.read_csv(\n",
    "        \"https://air-example-data.s3.us-west-2.amazonaws.com/m5_benchmarks_item_ids.csv\"\n",
    "    )[\"item_id\"]\n",
    ")\n",
    "if NUM_DATA_PARTITIONS > len(data_partitions):\n",
    "    print(f\"There are only {len(data_partitions)} partitions!\")\n",
    "\n",
    "param_space = {\n",
    "    \"data_partition_id\": tune.grid_search(data_partitions[:NUM_DATA_PARTITIONS]),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b4dd3e",
   "metadata": {},
   "source": [
    "Run many model training using Ray Tune!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ef8245",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = tune.Tuner(trainable, param_space=param_space)\n",
    "result_grid = tuner.fit()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ba1a07d0",
   "metadata": {},
   "source": [
    "View the reported results of all trials as a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7baa29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = result_grid.get_dataframe()\n",
    "results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a66e5cc",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "release_test_out = os.environ.get(\"TEST_OUTPUT_JSON\", \"/tmp/release_test_out.json\")\n",
    "\n",
    "elapsed = time.monotonic() - start_time\n",
    "with open(release_test_out, \"wt\") as f:\n",
    "    json.dump({\"total_time\": elapsed}, f)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "265d195fda5292fe8f69c6e37c435a5634a1ed3b6799724e66a975f68fa21517"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
