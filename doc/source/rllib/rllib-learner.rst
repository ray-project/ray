.. include:: /_includes/rllib/we_are_hiring.rst

.. include:: /_includes/rllib/new_api_stack.rst

.. include:: /_includes/rllib/new_api_stack_component.rst

.. |tensorflow| image:: images/tensorflow.png
    :class: inline-figure
    :width: 16

.. |pytorch| image:: images/pytorch.png
    :class: inline-figure
    :width: 16


Learner (Alpha)
===============

:py:class:`~ray.rllib.core.learner.learner.Learner` allows you to abstract the training 
logic of RLModules. It supports both gradient-based and non-gradient-based updates (e.g.
polyak averaging, etc.) The API enables you to distribute the Learner using data-
distributed parallel (DDP). The Learner achieves the following:


(1) Facilitates gradient-based updates on :ref:`RLModule <rlmodule-guide>`.
(2) Provides abstractions for non-gradient based updates such as polyak averaging, etc.
(3) Reporting training statistics.
(4) Checkpoints the modules and optimizer states for durable training.

The :py:class:`~ray.rllib.core.learner.learner.Learner` class supports data-distributed-
parallel style training using the 
:py:class:`~ray.rllib.core.learner.learner_group.LearnerGroup` API. Under this paradigm, 
the :py:class:`~ray.rllib.core.learner.learner_group.LearnerGroup` maintains multiple 
copies of the same :py:class:`~ray.rllib.core.learner.learner.Learner` with identical 
parameters and hyperparameters. Each of these
:py:class:`~ray.rllib.core.learner.learner.Learner` instances computes the loss and gradients on a
shard of a sample batch and then accumulates the gradients across the 
:py:class:`~ray.rllib.core.learner.learner.Learner` instances. Learn more about data-distributed 
parallel learning in 
`this article. <https://pytorch.org/tutorials/intermediate/ddp_tutorial.html>`_

:py:class:`~ray.rllib.core.learner.learner_group.LearnerGroup` also allows for 
asynchronous training and (distributed) checkpointing for durability during training.

Enabling Learner API in RLlib experiments
=========================================

Adjust the amount of resources for training using the 
`num_gpus_per_learner`, `num_cpus_per_learner`, and `num_learners`
arguments in the :py:class:`~ray.rllib.algorithms.algorithm_config.AlgorithmConfig`.

.. testcode::
	:hide:

    from ray.rllib.algorithms.ppo.ppo import PPOConfig

.. testcode::

    config = (
        PPOConfig()
        .api_stack(enable_rl_module_and_learner=True)
        .learners(
            num_learners=0,  # Set this to greater than 1 to allow for DDP style updates.
            num_gpus_per_learner=0,  # Set this to 1 to enable GPU training.
            num_cpus_per_learner=1,
        )
    )

.. testcode::
	:hide:

	config = config.environment("CartPole-v1")
	config.build()  # test that the algorithm can be built with the given resources


.. note::
    
    This features is in alpha. If you migrate to this algorithm, enable the feature by 
    via `AlgorithmConfig.api_stack(enable_rl_module_and_learner=True)`.

    The following algorithms support :py:class:`~ray.rllib.core.learner.learner.Learner` out of the box. Implement
    an algorithm with a custom :py:class:`~ray.rllib.core.learner.learner.Learner` to leverage this API for other algorithms.

    .. list-table::
       :header-rows: 1
       :widths: 60 60

       * - Algorithm
         - Supported Framework
       * - **PPO**
         - |pytorch| |tensorflow|
       * - **Impala**
         - |pytorch| |tensorflow|
       * - **APPO**
         - |pytorch| |tensorflow|


Basic usage
===========

Use the :py:class:`~ray.rllib.core.learner.learner_group.LearnerGroup` utility to interact with multiple learners. 

Construction
------------

If you enable the :ref:`RLModule <rlmodule-guide>`
and :py:class:`~ray.rllib.core.learner.learner.Learner` APIs via the :py:class:`~ray.rllib.algorithms.algorithm_config.AlgorithmConfig`, then calling :py:meth:`~ray.rllib.algorithms.algorithm_config.AlgorithmConfig.build` constructs a :py:class:`~ray.rllib.core.learner.learner_group.LearnerGroup` for you, but if youâ€™re using these APIs standalone, you can construct the :py:class:`~ray.rllib.core.learner.learner_group.LearnerGroup` as follows.

.. testcode::
    :hide:

    # imports for the examples
    import gymnasium as gym
    import numpy as np

    import ray
    from ray.rllib.algorithms.ppo import PPOConfig
    from ray.rllib.core.rl_module.rl_module import SingleAgentRLModuleSpec
    from ray.rllib.core.learner.learner_group import LearnerGroup


.. tab-set::
    
    .. tab-item:: Contstructing a LearnerGroup


        .. testcode::

            env = gym.make("CartPole-v1")

            # Create an AlgorithmConfig object from which we can build the
            # LearnerGroup.
            config = (
                PPOConfig()
                # Number of Learner workers (Ray actors).
                # Use 0 for no actors, only create a local Learner.
                # Use >=1 to create n DDP-style Learner workers (Ray actors).
                .learners(num_learners=1)
                # Specify the learner's hyperparameters.
                .training(
                    use_kl_loss=True,
                    kl_coeff=0.01,
                    kl_target=0.05,
                    clip_param=0.2,
                    vf_clip_param=0.2,
                    entropy_coeff=0.05,
                    vf_loss_coeff=0.5
                )
            )

            # Construct a new LearnerGroup using our config object.
            learner_group = config.build_learner_group(env=env)

    .. tab-item:: Constructing a Learner

        .. testcode::

            env = gym.make("CartPole-v1")

            # Create an AlgorithmConfig object from which we can build the
            # Learner.
            config = (
                PPOConfig()
                # Specify the Learner's hyperparameters.
                .training(
                    use_kl_loss=True,
                    kl_coeff=0.01,
                    kl_target=0.05,
                    clip_param=0.2,
                    vf_clip_param=0.2,
                    entropy_coeff=0.05,
                    vf_loss_coeff=0.5
                )
            )
            # Construct a new Learner using our config object.
            learner = config.build_learner(env=env)


Updates
-------

.. testcode::
    :hide:

    import time

    from ray.rllib.core import DEFAULT_MODULE_ID
    from ray.rllib.evaluation.postprocessing import Postprocessing
    from ray.rllib.policy.sample_batch import SampleBatch, MultiAgentBatch

    DUMMY_BATCH = {
        SampleBatch.OBS: np.array(
            [[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8], [0.9, 1.0, 1.1, 1.2]],
            dtype=np.float32,
        ),
        SampleBatch.NEXT_OBS: np.array(
            [[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8], [0.9, 1.0, 1.1, 1.2]],
            dtype=np.float32,
        ),
        SampleBatch.ACTIONS: np.array([0, 1, 1]),
        SampleBatch.PREV_ACTIONS: np.array([0, 1, 1]),
        SampleBatch.REWARDS: np.array([1.0, -1.0, 0.5], dtype=np.float32),
        SampleBatch.PREV_REWARDS: np.array([1.0, -1.0, 0.5], dtype=np.float32),
        SampleBatch.TERMINATEDS: np.array([False, False, True]),
        SampleBatch.TRUNCATEDS: np.array([False, False, False]),
        SampleBatch.VF_PREDS: np.array([0.5, 0.6, 0.7], dtype=np.float32),
        SampleBatch.ACTION_DIST_INPUTS: np.array(
            [[-2.0, 0.5], [-3.0, -0.3], [-0.1, 2.5]], dtype=np.float32
        ),
        SampleBatch.ACTION_LOGP: np.array([-0.5, -0.1, -0.2], dtype=np.float32),
        SampleBatch.EPS_ID: np.array([0, 0, 0]),
        SampleBatch.AGENT_INDEX: np.array([0, 0, 0]),
        Postprocessing.ADVANTAGES: np.array([0.1, 0.2, 0.3], dtype=np.float32),
        Postprocessing.VALUE_TARGETS: np.array([0.5, 0.6, 0.7], dtype=np.float32),
    }
    default_batch = SampleBatch(DUMMY_BATCH)
    DUMMY_BATCH = default_batch.as_multi_agent()

    learner.build() # needs to be called on the learner before calling any functions


.. tab-set::
        
    .. tab-item:: Updating a LearnerGroup

        .. testcode::

            TIMESTEPS = {"num_env_steps_sampled_lifetime": 250}

            # This is a blocking update.
            results = learner_group.update_from_batch(batch=DUMMY_BATCH, timesteps=TIMESTEPS)

            # This is a non-blocking update. The results are returned in a future
            # call to `update_from_batch(..., async_update=True)`
            _ = learner_group.update_from_batch(batch=DUMMY_BATCH, async_update=True, timesteps=TIMESTEPS)

            # Artificially wait for async request to be done to get the results
            # in the next call to
            # `LearnerGroup.update_from_batch(..., async_update=True)`.
            time.sleep(5)
            results = learner_group.update_from_batch(
                batch=DUMMY_BATCH, async_update=True, timesteps=TIMESTEPS
            )
            # `results` is an already reduced dict, which is the result of
            # reducing over the individual async `update_from_batch(..., async_update=True)`
            # calls.
            assert isinstance(results, dict), results

        When updating a :py:class:`~ray.rllib.core.learner.learner_group.LearnerGroup` you can perform blocking or async updates on batches of data.
        Async updates are necessary for implementing async algorithms such as APPO/IMPALA.

    .. tab-item:: Updating a Learner

        .. testcode::

            # This is a blocking update (given a training batch).
            result = learner.update_from_batch(batch=DUMMY_BATCH, timesteps=TIMESTEPS)

        When updating a :py:class:`~ray.rllib.core.learner.learner.Learner` you can only perform blocking updates on batches of data.
        You can perform non-gradient based updates before or after the gradient-based ones by overriding
        :py:meth:`~ray.rllib.core.learner.learner.Learner.before_gradient_based_update` and
        :py:meth:`~ray.rllib.core.learner.learner.Learner.after_gradient_based_update`.


Getting and setting state
-------------------------

.. tab-set::
    
    .. tab-item:: Getting and Setting State for a LearnerGroup

        .. testcode::

            # Get the LearnerGroup's RLModule weights and optimizer states.
            state = learner_group.get_state()
            learner_group.set_state(state)

            # Only get the RLModule weights.
            weights = learner_group.get_weights()
            learner_group.set_weights(weights)

        Set/get the state dict of all learners through learner_group via 
        :py:meth:`~ray.rllib.core.learner.learner_group.LearnerGroup.set_state` or 
        :py:meth:`~ray.rllib.core.learner.learner_group.LearnerGroup.get_state`. 
        This includes all states including both neural network weights, 
        and optimizer states on each learner. You can set and get the weights of 
        the RLModule of all learners through learner_group via 
        :py:meth:`~ray.rllib.core.learner.learner_group.LearnerGroup.set_weights` or
        :py:meth:`~ray.rllib.core.learner.learner_group.LearnerGroup.get_weights`. 
        This does not include optimizer states.
    
    .. tab-item:: Getting and Setting State for a Learner

        .. testcode::

            # Get the Learner's RLModule weights and optimizer states.
            state = learner.get_state()
            learner.set_state(state)

            # Only get the RLModule weights (as numpy arrays).
            module_state = learner.get_module_state()
            learner.module.set_state(module_state)

        You can set and get the weights of a :py:class:`~ray.rllib.core.learner.learner.Learner` 
        using :py:meth:`~ray.rllib.core.learner.learner.Learner.set_state` 
        and :py:meth:`~ray.rllib.core.learner.learner.Learner.get_state` .
        For setting or getting only RLModule weights (without optimizer states), use 
        :py:meth:`~ray.rllib.core.learner.learner.Learner.set_module_state` 
        or :py:meth:`~ray.rllib.core.learner.learner.Learner.get_module_state` API.


.. testcode::
	:hide:

	import tempfile

	LEARNER_CKPT_DIR = str(tempfile.TemporaryDirectory())
	LEARNER_GROUP_CKPT_DIR = str(tempfile.TemporaryDirectory())


Checkpointing
-------------

.. tab-set::
    
    .. tab-item:: Checkpointing a LearnerGroup

        .. testcode::

            learner_group.save_state(LEARNER_GROUP_CKPT_DIR)
            learner_group.load_state(LEARNER_GROUP_CKPT_DIR)
        
        Checkpoint the state of all learners in the :py:class:`~ray.rllib.core.learner.learner_group.LearnerGroup` via :py:meth:`~ray.rllib.core.learner.learner_group.LearnerGroup.save_state` and
        :py:meth:`~ray.rllib.core.learner.learner_group.LearnerGroup.load_state`. This includes all states including neural network weights and any
        optimizer states. Note that since the state of all of the :py:class:`~ray.rllib.core.learner.learner.Learner` instances is identical,
        only the states from the first :py:class:`~ray.rllib.core.learner.learner.Learner` need to be saved.

    .. tab-item:: Checkpointing a Learner

        .. testcode::

            learner.save_state(LEARNER_CKPT_DIR)
            learner.load_state(LEARNER_CKPT_DIR)

        Checkpoint the state of a :py:class:`~ray.rllib.core.learner.learner.Learner` 
        via :py:meth:`~ray.rllib.core.learner.learner.Learner.save_state` and 
        :py:meth:`~ray.rllib.core.learner.learner.Learner.load_state`. This 
        includes all states including neural network weights and any optimizer states.


Implementation
==============
:py:class:`~ray.rllib.core.learner.learner.Learner` has many APIs for flexible implementation, however the core ones that you need to implement are:

.. list-table::
   :widths: 60 60
   :header-rows: 1

   * - Method
     - Description
   * - :py:meth:`~ray.rllib.core.learner.learner.Learner.configure_optimizers_for_module()`
     - set up any optimizers for a RLModule.
   * - :py:meth:`~ray.rllib.core.learner.learner.Learner.compute_loss_for_module()`
     - calculate the loss for gradient based update to a module.
   * - :py:meth:`~ray.rllib.core.learner.learner.Learner.before_gradient_based_update()`
     - do any non-gradient based updates to a RLModule before(!) the gradient based ones, e.g. add noise to your network.

Starter Example
---------------

A :py:class:`~ray.rllib.core.learner.learner.Learner` that implements behavior cloning could look like the following:

.. testcode::
    :hide:

    from typing import Any, Dict, DefaultDict

    from ray.rllib.algorithms.algorithm_config import AlgorithmConfig
    from ray.rllib.core.learner.learner import Learner
    from ray.rllib.core.learner.torch.torch_learner import TorchLearner
    from ray.rllib.policy.sample_batch import SampleBatch
    from ray.rllib.utils.annotations import override
    from ray.rllib.utils.nested_dict import NestedDict
    from ray.rllib.utils.numpy import convert_to_numpy
    from ray.rllib.utils.typing import ModuleID, TensorType

.. testcode::

    class BCTorchLearner(TorchLearner):

        @override(Learner)
        def compute_loss_for_module(
            self,
            *,
            module_id: ModuleID,
            config: AlgorithmConfig = None,
            batch: NestedDict,
            fwd_out: Dict[str, TensorType],
        ) -> TensorType:

            # standard behavior cloning loss 
            action_dist_inputs = fwd_out[SampleBatch.ACTION_DIST_INPUTS]
            action_dist_class = self._module[module_id].get_train_action_dist_cls()
            action_dist = action_dist_class.from_logits(action_dist_inputs)
            loss = -torch.mean(action_dist.logp(batch[SampleBatch.ACTIONS]))

            return loss


