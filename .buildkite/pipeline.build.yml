- label: ":java: Java"
  conditions: ["RAY_CI_JAVA_AFFECTED"]
  instance_size: medium
  commands:
    - ./java/test.sh

- label: ":java: Java (streaming and routing FFs off)"
  conditions: ["RAY_CI_JAVA_AFFECTED"]
  instance_size: medium
  commands:
    - export RAY_SERVE_ENABLE_EXPERIMENTAL_STREAMING=0 RAY_SERVE_ENABLE_NEW_ROUTING=0 && ./java/test.sh

- label: ":serverless: Dashboard Tests"
  conditions:
    [
        "RAY_CI_DASHBOARD_AFFECTED",
        "RAY_CI_PYTHON_AFFECTED",
    ]
  instance_size: medium
  commands:
    - cleanup() { if [ "${BUILDKITE_PULL_REQUEST}" = "false" ]; then ./ci/build/upload_build_info.sh; fi }; trap cleanup EXIT
    - ./ci/env/env_info.sh
    - ./dashboard/tests/run_ui_tests.sh
    - bazel test --config=ci $(./ci/run/bazel_export_options) python/ray/dashboard/...

- label: ":serverless: Serve Release Tests"
  conditions:
    [
        "RAY_CI_SERVE_AFFECTED",
        "RAY_CI_PYTHON_AFFECTED",
    ]
  instance_size: medium
  commands:
    - cleanup() { if [ "${BUILDKITE_PULL_REQUEST}" = "false" ]; then ./ci/build/upload_build_info.sh; fi }; trap cleanup EXIT
    - TORCH_VERSION=1.9.0 ./ci/env/install-dependencies.sh
    - 'git clone https://github.com/wg/wrk.git /tmp/wrk && pushd /tmp/wrk && make -j && sudo cp wrk /usr/local/bin && popd'
    - ./ci/env/env_info.sh
    - bazel test --config=ci $(./ci/run/bazel_export_options)
      --test_tag_filters=team:serve
      release/...

# the bulk of serve tests are now run on civ2 infra, and defined in
# pipeline.build_serve.yml
- label: ":serverless: Serve HA Tests"
  conditions:
    [
        "RAY_CI_SERVE_AFFECTED",
        "RAY_CI_PYTHON_AFFECTED",
        "RAY_CI_ML_AFFECTED",
    ]
  instance_size: medium
  commands:
    - cleanup() { if [ "${BUILDKITE_PULL_REQUEST}" = "false" ]; then ./ci/build/upload_build_info.sh; fi }; trap cleanup EXIT
    - TORCH_VERSION=1.9.0 ./ci/env/install-dependencies.sh
    - bash ./ci/ci.sh prepare_docker
    - 'git clone https://github.com/wg/wrk.git /tmp/wrk && pushd /tmp/wrk && make -j && sudo cp wrk /usr/local/bin && popd'
    - ./ci/env/env_info.sh
    - bazel test --config=ci $(./ci/run/bazel_export_options)
      --test_tag_filters=-post_wheel_build,-gpu
      --test_env=DOCKER_HOST=tcp://docker:2376
      --test_env=DOCKER_TLS_VERIFY=1
      --test_env=DOCKER_CERT_PATH=/certs/client
      --test_env=DOCKER_TLS_CERTDIR=/certs
      --test_tag_filters=-post_wheel_build,-gpu,xcommit
      python/ray/serve/...

- label: ":serverless: Serve Tests (streaming FF off)"
  parallelism: 3
  conditions:
    [
        "RAY_CI_SERVE_AFFECTED",
        "RAY_CI_PYTHON_AFFECTED",
        "RAY_CI_ML_AFFECTED",
    ]
  instance_size: large
  commands:
    - cleanup() { if [ "${BUILDKITE_PULL_REQUEST}" = "false" ]; then ./ci/build/upload_build_info.sh; fi }; trap cleanup EXIT
    - TORCH_VERSION=1.9.0 ./ci/env/install-dependencies.sh
    - bash ./ci/ci.sh prepare_docker
    - 'git clone https://github.com/wg/wrk.git /tmp/wrk && pushd /tmp/wrk && make -j && sudo cp wrk /usr/local/bin && popd'
    - ./ci/env/env_info.sh
    - >-
      set -x;
      python ./ci/ray_ci/bazel_sharding.py
      --exclude_manual
      --index "\${BUILDKITE_PARALLEL_JOB}" --count "\${BUILDKITE_PARALLEL_JOB_COUNT}"
      --tag_filters=-post_wheel_build,-gpu
      python/ray/serve/...
      > test_shard.txt
    - cat test_shard.txt
    - bazel test --config=ci $(./ci/run/bazel_export_options)
      --test_tag_filters=-post_wheel_build,-gpu
      --test_env=DOCKER_HOST=tcp://docker:2376
      --test_env=DOCKER_TLS_VERIFY=1
      --test_env=DOCKER_CERT_PATH=/certs/client
      --test_env=DOCKER_TLS_CERTDIR=/certs
      --test_env=RAY_SERVE_ENABLE_EXPERIMENTAL_STREAMING=0
      $(cat test_shard.txt)

- label: ":serverless: Serve Tests (streaming and routing FFs off)"
  parallelism: 3
  conditions:
    [
        "RAY_CI_SERVE_AFFECTED",
        "RAY_CI_PYTHON_AFFECTED",
        "RAY_CI_ML_AFFECTED",
    ]
  instance_size: large
  commands:
    - cleanup() { if [ "${BUILDKITE_PULL_REQUEST}" = "false" ]; then ./ci/build/upload_build_info.sh; fi }; trap cleanup EXIT
    - TORCH_VERSION=1.9.0 ./ci/env/install-dependencies.sh
    - bash ./ci/ci.sh prepare_docker
    - 'git clone https://github.com/wg/wrk.git /tmp/wrk && pushd /tmp/wrk && make -j && sudo cp wrk /usr/local/bin && popd'
    - ./ci/env/env_info.sh
    - >-
      set -x;
      python ./ci/ray_ci/bazel_sharding.py
      --exclude_manual
      --index "\${BUILDKITE_PARALLEL_JOB}" --count "\${BUILDKITE_PARALLEL_JOB_COUNT}"
      --tag_filters=-post_wheel_build,-gpu
      python/ray/serve/...
      > test_shard.txt
    - cat test_shard.txt
    - bazel test --config=ci $(./ci/run/bazel_export_options)
      --test_tag_filters=-post_wheel_build,-gpu
      --test_env=DOCKER_HOST=tcp://docker:2376
      --test_env=DOCKER_TLS_VERIFY=1
      --test_env=DOCKER_CERT_PATH=/certs/client
      --test_env=DOCKER_TLS_CERTDIR=/certs
      --test_env=RAY_SERVE_ENABLE_NEW_ROUTING=0
      --test_env=RAY_SERVE_ENABLE_EXPERIMENTAL_STREAMING=0
      $(cat test_shard.txt)

- label: ":python: Minimal install Python {{matrix}}"
  conditions: ["RAY_CI_PYTHON_AFFECTED"]
  instance_size: medium
  commands:
    - cleanup() { if [ "${BUILDKITE_PULL_REQUEST}" = "false" ]; then ./ci/build/upload_build_info.sh; fi }; trap cleanup EXIT
    - ./ci/ci.sh test_minimal {{matrix}}
    - ./ci/ci.sh test_latest_core_dependencies {{matrix}}
  matrix:
    - "3.7"
    - "3.8"
    - "3.9"
    - "3.10"
    - "3.11"

- label: ":python: Default install"
  conditions: ["RAY_CI_PYTHON_AFFECTED"]
  instance_size: small
  commands:
    - cleanup() { if [ "${BUILDKITE_PULL_REQUEST}" = "false" ]; then ./ci/build/upload_build_info.sh; fi }; trap cleanup EXIT
    - ./ci/env/install-default.sh
    - ./ci/env/env_info.sh
    - bazel test --test_output=streamed --config=ci --test_env=RAY_DEFAULT=1 $(./ci/run/bazel_export_options)
      python/ray/dashboard/test_dashboard

- label: ":python: Ray Serve default install"
  conditions: ["RAY_CI_PYTHON_AFFECTED"]
  instance_size: small
  commands:
    - cleanup() { if [ "${BUILDKITE_PULL_REQUEST}" = "false" ]; then ./ci/build/upload_build_info.sh; fi }; trap cleanup EXIT
    - ./ci/env/install-serve.sh
    - ./ci/env/env_info.sh
    - bazel test --test_output=streamed --config=ci --test_env=RAY_DEFAULT=1 $(./ci/run/bazel_export_options)
      python/ray/serve/test_deployment_graph
    - bazel test --test_output=streamed --config=ci --test_env=RAY_DEFAULT=1 $(./ci/run/bazel_export_options)
      python/ray/serve/test_api

- label: ":python: civ1 tests"
  conditions: ["RAY_CI_PYTHON_AFFECTED"]
  instance_size: medium
  commands:
    - cleanup() { if [ "${BUILDKITE_PULL_REQUEST}" = "false" ]; then ./ci/build/upload_build_info.sh; fi }; trap cleanup EXIT
    - bash ./ci/ci.sh prepare_docker
    - ./ci/env/env_info.sh
    - pip install ray[client]
    - bazel test --config=ci $(./ci/run/bazel_export_options)
      --test_tag_filters=xcommit
      --test_env=DOCKER_HOST=tcp://docker:2376
      --test_env=DOCKER_TLS_VERIFY=1
      --test_env=DOCKER_CERT_PATH=/certs/client
      --test_env=DOCKER_TLS_CERTDIR=/certs
      -- python/ray/tests/...

- label: ":python: Debug Test"
  conditions: ["RAY_CI_PYTHON_AFFECTED"]
  instance_size: medium
  commands:
    - cleanup() { if [ "${BUILDKITE_PULL_REQUEST}" = "false" ]; then ./ci/build/upload_build_info.sh; fi }; trap cleanup EXIT
    - pip uninstall -y ray
    - RAY_DEBUG_BUILD=debug ./ci/ci.sh build
    - ./ci/env/env_info.sh
    - bazel test --config=ci-debug $(./ci/run/bazel_export_options)
      --test_tag_filters=-kubernetes,debug_tests
      python/ray/tests/...

- label: ":python: (ASAN tests)"
  conditions: ["RAY_CI_PYTHON_AFFECTED"]
  instance_size: medium
  commands:
    - cleanup() { if [ "${BUILDKITE_PULL_REQUEST}" = "false" ]; then ./ci/build/upload_build_info.sh; fi }; trap cleanup EXIT
    - RLLIB_TESTING=1 ./ci/env/install-dependencies.sh
    - ./ci/env/env_info.sh
    - bazel build $(./ci/run/bazel_export_options) --no//:jemalloc_flag //:ray_pkg
    - bazel test --config=ci --config=asan $(./ci/run/bazel_export_options)
      --config=asan-buildkite
      --test_tag_filters=-kubernetes,asan_tests
      --test_env=CONDA_EXE
      --test_env=CONDA_PYTHON_EXE
      --test_env=CONDA_SHLVL
      --test_env=CONDA_PREFIX
      --test_env=CONDA_DEFAULT_ENV
      python/ray/tests/...

- label: ":book: Doctest (CPU)"
  instance_size: large
  commands:
    - cleanup() { if [ "${BUILDKITE_PULL_REQUEST}" = "false" ]; then ./ci/build/upload_build_info.sh; fi }; trap cleanup EXIT
    # Todo (krfricke): Move mosaicml to train-test-requirements.txt
    - pip install "mosaicml==0.12.1"
    - DOC_TESTING=1 ./ci/env/install-dependencies.sh
    # TODO(scottjlee): Move datasets to train/data-test-requirements.txt 
    # (see https://github.com/ray-project/ray/pull/38432/)
    - pip install "datasets==2.14.0"
    - ./ci/env/install-horovod.sh
    - ./ci/env/env_info.sh
    - bazel test --config=ci $(./scripts/bazel_export_options)
      --test_tag_filters=doctest,-gpu
      python/ray/... doc/...

- label: ":python: Ray on Spark Test"
  conditions: ["RAY_CI_PYTHON_AFFECTED"]
  instance_size: medium
  commands:
    - cleanup() { if [ "${BUILDKITE_PULL_REQUEST}" = "false" ]; then ./ci/build/upload_build_info.sh; fi }; trap cleanup EXIT
    - pip uninstall -y ray
    - RAY_DEBUG_BUILD=debug ./ci/ci.sh build
    # Install latest pyspark. We cannot move this to the requirements file as subdependencies conflict
    - pip install -U https://ml-team-public-read.s3.us-west-2.amazonaws.com/spark-pkgs/pyspark-3.4.0.dev0-0cb0fa313979e1b82ddd711a05d8c4e78cf6c9f5.tar.gz
    - ./ci/env/env_info.sh
    - bazel test --config=ci-debug $(./ci/run/bazel_export_options)
      --test_env=RAY_ON_SPARK_BACKGROUND_JOB_STARTUP_WAIT=1
      --test_env=RAY_ON_SPARK_RAY_WORKER_NODE_STARTUP_INTERVAL=5
      --test_tag_filters=-kubernetes,spark_plugin_tests
      python/ray/tests/...

- label: ":kubernetes: operator"
  conditions: ["RAY_CI_LINUX_WHEELS_AFFECTED"]
  instance_size: medium
  commands:
    - |
      cleanup() {
        if [ "${BUILDKITE_PULL_REQUEST}" = "false" ]; then ./ci/build/upload_build_info.sh; fi
        python python/ray/tests/kuberay/setup/teardown_kuberay.py || true
        kind delete cluster
      }
      trap cleanup EXIT
    - ./ci/env/install-minimal.sh 3.8
    - PYTHON=3.8 ./ci/env/install-dependencies.sh
    # Specifying above somehow messes up the Ray install.
    # Uninstall and re-install Ray so that we can use Ray Client.
    # (Remove thirdparty_files to sidestep an issue with psutil.)
    - pip uninstall -y ray && rm -rf /ray/python/ray/thirdparty_files
    - pip install -e /ray/python
    - echo "--- Setting up local kind cluster."
    - ./ci/k8s/prep-k8s-environment.sh
    - echo "--- Building py38-cpu Ray image for the test."
    - LINUX_WHEELS=1 ./ci/ci.sh build
    - pip install -q docker
    - python ci/build/build-docker-images.py --py-versions py38 --device-types cpu --build-type LOCAL --build-base
    # Tag the image built in the last step. We want to be sure to distinguish the image from the real Ray nightly.
    - docker tag rayproject/ray:nightly-py38-cpu ray-ci:kuberay-test
    # Load the image into the kind node
    - kind load docker-image ray-ci:kuberay-test
    - echo "--- Setting up KubeRay operator."
    - python python/ray/tests/kuberay/setup/setup_kuberay.py
    - ./ci/env/env_info.sh
    - echo "--- Running the test."
    - bazel test --config=ci $(./ci/run/bazel_export_options)
      --test_tag_filters=kuberay_operator
      --test_env=RAY_IMAGE=docker.io/library/ray-ci:kuberay-test
      --test_env=PULL_POLICY=IfNotPresent
      --test_env=KUBECONFIG=/root/.kube/config
      python/ray/tests/...

- label: ":kubernetes: :mending_heart: chaos network delay test"
  conditions: ["RAY_CI_LINUX_WHEELS_AFFECTED"]
  instance_size: medium
  commands:
    - |
      cleanup() {
        if [ "${BUILDKITE_PULL_REQUEST}" = "false" ]; then ./ci/build/upload_build_info.sh; fi
        kind delete cluster
      }
      trap cleanup EXIT
    - ./ci/env/install-minimal.sh 3.8
    - PYTHON=3.8 ./ci/env/install-dependencies.sh
    # Specifying above somehow messes up the Ray install.
    # Uninstall and re-install Ray so that we can use Ray Client.
    # (Remove thirdparty_files to sidestep an issue with psutil.)
    - pip uninstall -y ray && rm -rf /ray/python/ray/thirdparty_files
    - pip install -e /ray/python
    - echo "--- Setting up local kind cluster."
    - ./ci/k8s/prep-k8s-environment.sh
    - ./ci/k8s/prep-helm.sh
    - echo "--- Building py38-cpu Ray image for the test."
    - LINUX_WHEELS=1 ./ci/ci.sh build
    - pip install -q docker
    - python ci/build/build-docker-images.py --py-versions py38 --device-types cpu --build-type LOCAL --build-base
    # Tag the image built in the last step. We want to be sure to distinguish the image from the real Ray nightly.
    - docker tag rayproject/ray:nightly-py38-cpu ray-ci:kuberay-test
    # Load the image into the kind node
    - kind load docker-image ray-ci:kuberay-test
    # Helm install KubeRay
    - echo "--- Installing KubeRay operator and cluser."
    - helm repo add kuberay https://ray-project.github.io/kuberay-helm/
    - helm install kuberay-operator kuberay/kuberay-operator
    - kubectl wait pod  -l app.kubernetes.io/name=kuberay-operator --for=condition=Ready=True  --timeout=5m
    # We are in m4i.xlarge and have 4 cpus. Can't have too many nodes.
    - helm install raycluster kuberay/ray-cluster --set image.repository=ray-ci --set image.tag=kuberay-test --set worker.replicas=2 --set worker.resources.limits.cpu=500m --set worker.resources.requests.cpu=500m --set head.resources.limits.cpu=500m --set head.resources.requests.cpu=500m
    - kubectl wait pod -l ray.io/cluster=raycluster-kuberay --for=condition=Ready=True --timeout=5m
    - kubectl port-forward --address 0.0.0.0 service/raycluster-kuberay-head-svc 8265:8265 &
    # Helm install chaos-mesh
    - echo "--- Installing chaos-mesh operator and CR."
    - helm repo add chaos-mesh https://charts.chaos-mesh.org
    - kubectl create ns chaos-mesh
    - helm install chaos-mesh chaos-mesh/chaos-mesh -n=chaos-mesh --set chaosDaemon.runtime=containerd --set chaosDaemon.socketPath=/run/containerd/containerd.sock --version 2.6.1
    - kubectl wait pod  --namespace chaos-mesh  -l app.kubernetes.io/instance=chaos-mesh --for=condition=Ready=True
    - echo "--- Running the script without faults"
    - ray job submit --address http://localhost:8265 --runtime-env python/ray/tests/chaos/runtime_env.yaml --working-dir python/ray/tests/chaos -- python potato_passer.py --num-actors=3 --pass-times=1000 --sleep-secs=0.01
    # Now add the delay, rerun the job
    - kubectl apply -f python/ray/tests/chaos/chaos_network_delay.yaml
    - echo "--- Running the script with fault of networking delay"
    - ray job submit --address http://localhost:8265 --runtime-env python/ray/tests/chaos/runtime_env.yaml --working-dir python/ray/tests/chaos -- python potato_passer.py --num-actors=3 --pass-times=1000 --sleep-secs=0.01

# TODO: write a test that needs bandwidth heavy lifting
- label: ":kubernetes: :mending_heart: chaos bandwidth test"
  conditions: ["RAY_CI_LINUX_WHEELS_AFFECTED"]
  instance_size: medium
  commands:
    - |
      cleanup() {
        if [ "${BUILDKITE_PULL_REQUEST}" = "false" ]; then ./ci/build/upload_build_info.sh; fi
        kind delete cluster
      }
      trap cleanup EXIT
    - ./ci/env/install-minimal.sh 3.8
    - PYTHON=3.8 ./ci/env/install-dependencies.sh
    # Specifying above somehow messes up the Ray install.
    # Uninstall and re-install Ray so that we can use Ray Client.
    # (Remove thirdparty_files to sidestep an issue with psutil.)
    - pip uninstall -y ray && rm -rf /ray/python/ray/thirdparty_files
    - pip install -e /ray/python
    - echo "--- Setting up local kind cluster."
    - ./ci/k8s/prep-k8s-environment.sh
    - ./ci/k8s/prep-helm.sh
    - echo "--- Building py38-cpu Ray image for the test."
    - LINUX_WHEELS=1 ./ci/ci.sh build
    - pip install -q docker
    - python ci/build/build-docker-images.py --py-versions py38 --device-types cpu --build-type LOCAL --build-base
    # Tag the image built in the last step. We want to be sure to distinguish the image from the real Ray nightly.
    - docker tag rayproject/ray:nightly-py38-cpu ray-ci:kuberay-test
    # Load the image into the kind node
    - kind load docker-image ray-ci:kuberay-test
    # Helm install KubeRay
    - echo "--- Installing KubeRay operator and cluser."
    - helm repo add kuberay https://ray-project.github.io/kuberay-helm/
    - helm install kuberay-operator kuberay/kuberay-operator
    - kubectl wait pod  -l app.kubernetes.io/name=kuberay-operator --for=condition=Ready=True  --timeout=5m
    # We are in m4i.xlarge and have 4 cpus. Can't have too many nodes.
    - helm install raycluster kuberay/ray-cluster --set image.repository=ray-ci --set image.tag=kuberay-test --set worker.replicas=2 --set worker.resources.limits.cpu=500m --set worker.resources.requests.cpu=500m --set head.resources.limits.cpu=500m --set head.resources.requests.cpu=500m
    - kubectl wait pod -l ray.io/cluster=raycluster-kuberay --for=condition=Ready=True --timeout=5m
    - kubectl port-forward --address 0.0.0.0 service/raycluster-kuberay-head-svc 8265:8265 &
    # Helm install chaos-mesh
    - echo "--- Installing chaos-mesh operator and CR."
    - helm repo add chaos-mesh https://charts.chaos-mesh.org
    - kubectl create ns chaos-mesh
    - helm install chaos-mesh chaos-mesh/chaos-mesh -n=chaos-mesh --set chaosDaemon.runtime=containerd --set chaosDaemon.socketPath=/run/containerd/containerd.sock --version 2.6.1
    - kubectl wait pod  --namespace chaos-mesh  -l app.kubernetes.io/instance=chaos-mesh --for=condition=Ready=True
    - echo "--- Running the script without faults"
    - ray job submit --address http://localhost:8265 --runtime-env python/ray/tests/chaos/runtime_env.yaml --working-dir python/ray/tests/chaos -- python potato_passer.py --num-actors=3 --pass-times=1000 --sleep-secs=0.01
    # Now add the delay, rerun the job
    - kubectl apply -f python/ray/tests/chaos/chaos_network_bandwidth.yaml
    - echo "--- Running the script with fault of bandwidth limit"
    - ray job submit --address http://localhost:8265 --runtime-env python/ray/tests/chaos/runtime_env.yaml --working-dir python/ray/tests/chaos -- python potato_passer.py --num-actors=3 --pass-times=1000 --sleep-secs=0.01


- label: ":kubernetes: :mending_heart: :ray-serve: serve chaos network delay test"
  conditions: ["RAY_CI_LINUX_WHEELS_AFFECTED"]
  instance_size: medium
  commands:
    - |
      cleanup() {
        if [ "${BUILDKITE_PULL_REQUEST}" = "false" ]; then ./ci/build/upload_build_info.sh; fi
        kind delete cluster
      }
      trap cleanup EXIT
    - ./ci/env/install-minimal.sh 3.8
    - PYTHON=3.8 ./ci/env/install-dependencies.sh
    # Specifying above somehow messes up the Ray install.
    # Uninstall and re-install Ray so that we can use Ray Client.
    # (Remove thirdparty_files to sidestep an issue with psutil.)
    - pip uninstall -y ray && rm -rf /ray/python/ray/thirdparty_files
    - pip install -e /ray/python
    - echo "--- Setting up local kind cluster."
    - ./ci/k8s/prep-k8s-environment.sh
    - ./ci/k8s/prep-helm.sh
    - echo "--- Building py38-cpu Ray image for the test."
    - LINUX_WHEELS=1 ./ci/ci.sh build
    - pip install -q docker
    - python ci/build/build-docker-images.py --py-versions py38 --device-types cpu --build-type LOCAL --build-base
    # Tag the image built in the last step. We want to be sure to distinguish the image from the real Ray nightly.
    - docker tag rayproject/ray:nightly-py38-cpu ray-ci:kuberay-test
    # Load the image into the kind node
    - kind load docker-image ray-ci:kuberay-test
    # Helm install KubeRay
    - echo "--- Installing KubeRay operator and cluser."
    - helm repo add kuberay https://ray-project.github.io/kuberay-helm/
    - helm install kuberay-operator kuberay/kuberay-operator
    - kubectl wait pod  -l app.kubernetes.io/name=kuberay-operator --for=condition=Ready=True  --timeout=5m
    # We are in m4i.xlarge and have 4 cpus. Can't have too many nodes.
    - helm install raycluster kuberay/ray-cluster --set image.repository=ray-ci --set image.tag=kuberay-test --set worker.replicas=2 --set worker.resources.limits.cpu=500m --set worker.resources.requests.cpu=500m --set head.resources.limits.cpu=500m --set head.resources.requests.cpu=500m --set head.containerEnv[0].name=RAY_SERVE_ENABLE_EXPERIMENTAL_STREAMING --set head.containerEnv[0].value=\"1\"
    - kubectl wait pod -l ray.io/cluster=raycluster-kuberay --for=condition=Ready=True --timeout=5m
    - kubectl port-forward --address 0.0.0.0 service/raycluster-kuberay-head-svc 8265:8265 &
    # Helm install chaos-mesh
    - echo "--- Installing chaos-mesh operator and CR."
    - helm repo add chaos-mesh https://charts.chaos-mesh.org
    - kubectl create ns chaos-mesh
    - helm install chaos-mesh chaos-mesh/chaos-mesh -n=chaos-mesh --set chaosDaemon.runtime=containerd --set chaosDaemon.socketPath=/run/containerd/containerd.sock --version 2.6.1
    - kubectl wait pod  --namespace chaos-mesh  -l app.kubernetes.io/instance=chaos-mesh --for=condition=Ready=True
    - echo "--- Running the script without faults"
    - ray job submit --address http://localhost:8265 --runtime-env python/ray/tests/chaos/streaming_llm.yaml --working-dir python/ray/tests/chaos -- python streaming_llm.py --num_queries_per_task=100 --num_tasks=2 --num_words_per_query=100
    # Now add the delay, rerun the job
    - kubectl apply -f python/ray/tests/chaos/chaos_network_delay.yaml
    - echo "--- Running the script with fault of networking delay"
    - ray job submit --address http://localhost:8265 --runtime-env python/ray/tests/chaos/streaming_llm.yaml --working-dir python/ray/tests/chaos -- python streaming_llm.py --num_queries_per_task=100 --num_tasks=2 --num_words_per_query=100
    

- label: ":book: Documentation"
  commands:
    - export LINT=1
    - ./ci/env/install-dependencies.sh
    # Specifying above somehow messes up the Ray install.
    # Uninstall and re-install Ray so that we can use Ray Client
    # (remove thirdparty_files to sidestep an issue with psutil).
    - pip uninstall -y ray && rm -rf /ray/python/ray/thirdparty_files
    - pushd /ray && git clean -f -f -x -d -e .whl -e python/ray/dashboard/client && popd
    - bazel clean --expunge
    - export WANDB_MODE=offline
    # Horovod needs to be installed separately (needed for API ref imports)
    - ./ci/env/install-horovod.sh
    # See https://stackoverflow.com/questions/63383400/error-cannot-uninstall-ruamel-yaml-while-creating-docker-image-for-azure-ml-a
    # Pin urllib to avoid downstream ssl incompatibility issues. This matches requirements-doc.txt.
    - pip install "mosaicml==0.12.1" "urllib3<1.27" "typing-extensions==4.5.0" --ignore-installed
    - ./ci/env/env_info.sh
    - ./ci/ci.sh build_sphinx_docs

- label: ":octopus: Tune multinode tests"
  conditions: ["NO_WHEELS_REQUIRED", "RAY_CI_TUNE_AFFECTED"]
  instance_size: medium
  commands:
    - LINUX_WHEELS=1 ./ci/ci.sh build
    - mkdir -p ~/.docker/cli-plugins/ && curl -SL https://github.com/docker/compose/releases/download/v2.0.1/docker-compose-linux-x86_64 -o ~/.docker/cli-plugins/docker-compose && chmod +x ~/.docker/cli-plugins/docker-compose
    - pip install -U docker aws_requests_auth boto3
    - ./ci/env/env_info.sh
    - python ./ci/build/build-docker-images.py --py-versions py38 --device-types cpu --build-type LOCAL --build-base
    - python ./ci/build/build-multinode-image.py rayproject/ray:nightly-py38-cpu rayproject/ray:multinode-py38
    - bazel test --config=ci $(./ci/run/bazel_export_options) --build_tests_only
      --test_tag_filters=multinode,-example,-flaky,-soft_imports,-gpu_only,-rllib
      python/ray/tune/...
      --test_env=RAY_HAS_SSH="1"
      --test_env=RAY_DOCKER_IMAGE="rayproject/ray:multinode-py38"
      --test_env=RAY_TEMPDIR="/ray-mount"
      --test_env=RAY_HOSTDIR="/ray"
      --test_env=RAY_TESTHOST="dind-daemon"
      --test_env=DOCKER_HOST=tcp://docker:2376
      --test_env=DOCKER_TLS_VERIFY=1
      --test_env=DOCKER_CERT_PATH=/certs/client
      --test_env=DOCKER_TLS_CERTDIR=/certs

- label: ":hadoop: Ray AIR HDFS tests"
  conditions: ["RAY_CI_ML_AFFECTED"]
  instance_size: medium
  commands:
    - cleanup() { if [ "${BUILDKITE_PULL_REQUEST}" = "false" ]; then ./ci/build/upload_build_info.sh; fi }; trap cleanup EXIT
    - INSTALL_HDFS=1 ./ci/env/install-dependencies.sh
    - ./ci/env/env_info.sh
    - cat /tmp/hdfs_env
    - bazel test --config=ci $(./ci/run/bazel_export_options) --test_tag_filters=hdfs python/ray/air/...


# Test to see if Train can be used without torch, tf, etc. installed
- label: ":steam_locomotive: Train minimal install"
  conditions: ["RAY_CI_TRAIN_AFFECTED"]
  instance_size: small
  commands:
      - cleanup() { if [ "${BUILDKITE_PULL_REQUEST}" = "false" ]; then ./ci/build/upload_build_info.sh; fi }; trap cleanup EXIT
      - TRAIN_MINIMAL_INSTALL=1 ./ci/env/install-minimal.sh
      - ./ci/env/env_info.sh
      - python ./ci/env/check_minimal_install.py
      - bazel test --config=ci $(./ci/run/bazel_export_options)  --build_tests_only --test_tag_filters=minimal 
        --test_env=RAY_AIR_NEW_PERSISTENCE_MODE=1
        python/ray/train/...
