[{"sha":"7eeb73f7f362f506a9d6358d9ff14f82c2fa0d48","filename":"doc/source/serve/advanced-guides/external-scaling-webhook.md","status":"added","additions":153,"deletions":0,"changes":153,"blob_url":"https://github.com/ray-project/ray/blob/1c22056eb240d871bb816191419f4a569ed3ae69/doc%2Fsource%2Fserve%2Fadvanced-guides%2Fexternal-scaling-webhook.md","raw_url":"https://github.com/ray-project/ray/raw/1c22056eb240d871bb816191419f4a569ed3ae69/doc%2Fsource%2Fserve%2Fadvanced-guides%2Fexternal-scaling-webhook.md","contents_url":"https://api.github.com/repos/ray-project/ray/contents/doc%2Fsource%2Fserve%2Fadvanced-guides%2Fexternal-scaling-webhook.md?ref=1c22056eb240d871bb816191419f4a569ed3ae69","patch":"@@ -0,0 +1,153 @@\n+(serve-external-scale-webhook)=\n+\n+:::{warning}\n+This API is in alpha and may change before becoming stable.\n+:::\n+\n+# External Scaling Webhook\n+\n+Ray Serve exposes a REST API endpoint that you can use to dynamically scale your deployments from outside the Ray cluster. This endpoint gives you flexibility to implement custom scaling logic based on any metrics or signals you choose, such as external monitoring systems, business metrics, or predictive models.\n+\n+## Overview\n+\n+The external scaling webhook provides programmatic control over the number of replicas for any deployment in your Ray Serve application. Unlike Ray Serve's built-in autoscaling, which scales based on queue depth and ongoing requests, this webhook allows you to scale based on any external criteria you define.\n+\n+## Prerequisites\n+\n+Before you can use the external scaling webhook, you must enable it in your Ray Serve application configuration:\n+\n+### Enable external scaler\n+\n+Set `external_scaler_enabled: true` in your application configuration:\n+\n+```yaml\n+applications:\n+  - name: my-app\n+    import_path: my_module:app\n+    external_scaler_enabled: true\n+    deployments:\n+      - name: my-deployment\n+        num_replicas: 1\n+```\n+\n+:::{warning}\n+External scaling and built-in autoscaling are mutually exclusive. You can't use both for the same application.\n+\n+- If you set `external_scaler_enabled: true`, you **must not** configure `autoscaling_config` on any deployment in that application.\n+- If you configure `autoscaling_config` on any deployment, you **must not** set `external_scaler_enabled: true` for the application.\n+\n+Attempting to use both will result in an error.\n+:::\n+\n+### Get authentication token\n+\n+The external scaling webhook requires authentication using a bearer token. You can obtain this token from the Ray Dashboard UI:\n+\n+1. Open the Ray Dashboard in your browser (typically at `http://localhost:8265`).\n+2. Navigate to the Serve section.\n+3. Find and copy the authentication token for your application.\n+\n+## API endpoint\n+\n+The webhook is available at the following endpoint:\n+\n+```\n+POST /api/v1/applications/{application_name}/deployments/{deployment_name}/scale\n+```\n+\n+**Path Parameters:**\n+- `application_name`: The name of your Serve application.\n+- `deployment_name`: The name of the deployment you want to scale.\n+\n+**Headers:**\n+- `Authorization` (required): Bearer token for authentication. Format: `Bearer <token>`\n+- `Content-Type` (required): Must be `application/json`\n+\n+**Request Body:**\n+\n+The following example shows the request body structure:\n+\n+```json\n+{\n+    \"target_num_replicas\": 5\n+}\n+```\n+\n+The request body must conform to the [`ScaleDeploymentRequest`](https://docs.ray.io/en/latest/serve/api/doc/ray.serve.schema.ScaleDeploymentRequest.html) schema:\n+\n+- `target_num_replicas` (integer, required): The target number of replicas for the deployment. Must be a non-negative integer.\n+\n+\n+## Example - Predictive scaling\n+\n+Implement predictive scaling based on historical patterns or forecasts. For instance, you can preemptively scale up before anticipated traffic spikes:\n+\n+```python\n+import requests\n+from datetime import datetime\n+\n+def predictive_scale(\n+    application_name: str,\n+    deployment_name: str,\n+    auth_token: str,\n+    serve_endpoint: str = \"http://localhost:8000\"\n+) -> bool:\n+    \"\"\"Scale based on time of day and historical patterns.\"\"\"\n+    hour = datetime.now().hour\n+    \n+    # Define scaling profile based on historical traffic patterns\n+    if 9 <= hour < 17:  # Business hours\n+        target_replicas = 10\n+    elif 17 <= hour < 22:  # Evening peak\n+        target_replicas = 15\n+    else:  # Off-peak hours\n+        target_replicas = 3\n+    \n+    url = (\n+        f\"{serve_endpoint}/api/v1/applications/{application_name}\"\n+        f\"/deployments/{deployment_name}/scale\"\n+    )\n+    \n+    headers = {\n+        \"Authorization\": f\"Bearer {auth_token}\",\n+        \"Content-Type\": \"application/json\"\n+    }\n+    \n+    response = requests.post(\n+        url,\n+        headers=headers,\n+        json={\"target_num_replicas\": target_replicas}\n+    )\n+    \n+    return response.status_code == 200\n+\n+```\n+\n+## Use cases\n+\n+The external scaling webhook is useful for several scenarios where you need custom scaling logic beyond what Ray Serve's built-in autoscaling provides:\n+\n+### Custom metric-based scaling\n+\n+Scale your deployments based on business or application metrics that Ray Serve doesn't track automatically:\n+\n+- External monitoring systems such as Prometheus, Datadog, or CloudWatch metrics.\n+- Database query latencies or connection pool sizes.\n+- Cost metrics to optimize for budget constraints.\n+\n+### Predictive and scheduled scaling\n+\n+Implement predictive scaling based on historical patterns or business schedules:\n+\n+- Preemptive scaling before anticipated traffic spikes (such as daily or weekly patterns).\n+- Event-driven scaling for known traffic events (such as sales, launches, or scheduled batch jobs).\n+- Time-of-day based scaling profiles for predictable workloads.\n+\n+### Manual and operational control\n+\n+Direct control over replica counts for operational scenarios:\n+\n+- Manual scaling for load testing or performance testing.\n+- Cost optimization by scaling down during off-peak hours or weekends.\n+- Development and staging environment management.\n+"},{"sha":"08669127d2640545a840badb6f06a95b75461f18","filename":"doc/source/serve/advanced-guides/index.md","status":"modified","additions":2,"deletions":0,"changes":2,"blob_url":"https://github.com/ray-project/ray/blob/1c22056eb240d871bb816191419f4a569ed3ae69/doc%2Fsource%2Fserve%2Fadvanced-guides%2Findex.md","raw_url":"https://github.com/ray-project/ray/raw/1c22056eb240d871bb816191419f4a569ed3ae69/doc%2Fsource%2Fserve%2Fadvanced-guides%2Findex.md","contents_url":"https://api.github.com/repos/ray-project/ray/contents/doc%2Fsource%2Fserve%2Fadvanced-guides%2Findex.md?ref=1c22056eb240d871bb816191419f4a569ed3ae69","patch":"@@ -16,6 +16,7 @@ deploy-vm\n multi-app-container\n custom-request-router\n multi-node-gpu-troubleshooting\n+external-scaling-webhook\n ```\n \n If youâ€™re new to Ray Serve, start with the [Ray Serve Quickstart](serve-getting-started).\n@@ -33,3 +34,4 @@ Use these advanced guides for more options and configurations:\n - [Run Applications in Different Containers](serve-container-runtime-env-guide)\n - [Use Custom Algorithm for Request Routing](custom-request-router)\n - [Troubleshoot multi-node GPU setups for serving LLMs](multi-node-gpu-troubleshooting)\n+- [External Scaling Webhook API](external-scaling-webhook)"},{"sha":"60e5a2c046f8f071fb13f67dcba1f1d9c61d8c2f","filename":"doc/source/serve/production-guide/config.md","status":"modified","additions":3,"deletions":1,"changes":4,"blob_url":"https://github.com/ray-project/ray/blob/1c22056eb240d871bb816191419f4a569ed3ae69/doc%2Fsource%2Fserve%2Fproduction-guide%2Fconfig.md","raw_url":"https://github.com/ray-project/ray/raw/1c22056eb240d871bb816191419f4a569ed3ae69/doc%2Fsource%2Fserve%2Fproduction-guide%2Fconfig.md","contents_url":"https://api.github.com/repos/ray-project/ray/contents/doc%2Fsource%2Fserve%2Fproduction-guide%2Fconfig.md?ref=1c22056eb240d871bb816191419f4a569ed3ae69","patch":"@@ -40,7 +40,8 @@ applications:\n - name: ...\n   route_prefix: ...\n   import_path: ...\n-  runtime_env: ... \n+  runtime_env: ...\n+  external_scaler_enabled: ...\n   deployments:\n   - name: ...\n     num_replicas: ...\n@@ -99,6 +100,7 @@ These are the fields per `application`:\n - **`route_prefix`**: An application can be called via HTTP at the specified route prefix. It defaults to `/`. The route prefix for each application must be unique.\n - **`import_path`**: The path to your top-level Serve deployment (or the same path passed to `serve run`). The most minimal config file consists of only an `import_path`.\n - **`runtime_env`**: Defines the environment that the application runs in. Use this parameter to package application dependencies such as `pip` packages (see {ref}`Runtime Environments <runtime-environments>` for supported fields). The `import_path` must be available _within_ the `runtime_env` if it's specified. The Serve config's `runtime_env` can only use [remote URIs](remote-uris) in its `working_dir` and `py_modules`; it can't use local zip files or directories. [More details on runtime env](serve-runtime-env).\n+- **`external_scaler_enabled`**: Enables the external scaling webhook, which lets you scale deployments from outside the Ray cluster using a REST API. When enabled, you can't use built-in autoscaling (`autoscaling_config`) for any deployment in this application. Defaults to `False`. See [External Scaling Webhook](serve-external-scale-webhook) for details.\n - **`deployments (optional)`**: A list of deployment options that allows you to override the `@serve.deployment` settings specified in the deployment graph code. Each entry in this list must include the deployment `name`, which must match one in the code. If this section is omitted, Serve launches all deployments in the graph with the parameters specified in the code. See how to [configure serve deployment options](serve-configure-deployment).\n - **`args`**: Arguments that are passed to the [application builder](serve-app-builder-guide).\n "}]