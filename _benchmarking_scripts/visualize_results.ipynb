{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install matplotlib seaborn -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global imports & constants\n",
    "import json\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "from textwrap import wrap\n",
    "from typing import Optional\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from scipy.signal import savgol_filter\n",
    "\n",
    "\n",
    "# --- Global Settings & Constants ---\n",
    "\n",
    "# Plotting Style (applied globally when this module is imported)\n",
    "plt.style.use(\"seaborn-v0_8-whitegrid\")\n",
    "sns.set_context(\"notebook\")\n",
    "sns.set_palette(\"colorblind\") # Use a colorblind-friendly palette\n",
    "\n",
    "# Default Colors for plots with multiple lines/bars\n",
    "# Using seaborn's colorblind palette directly for consistency\n",
    "DEFAULT_COLORS = sns.color_palette(\"colorblind\")\n",
    "\n",
    "# Default Figure Size\n",
    "DEFAULT_FIGSIZE = (12, 4)\n",
    "\n",
    "# Map strategy prefixes to full names for titles/legends\n",
    "STRATEGY_PREFIXES = {\n",
    "    \"random\": \"Random\",\n",
    "    \"round\": \"Round Robin\",\n",
    "    \"pow\": \"Power of 2\",\n",
    "    \"prefix\": \"Prefix Aware\"\n",
    "}\n",
    "STRATEGY_NAMES = {\n",
    "    \"random\": \"Random\",\n",
    "    \"round_robin\": \"Round Robin\",\n",
    "    \"pow_of_2\": \"Power of 2\",\n",
    "    \"prefix_aware\": \"Prefix Aware\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Serve and vLLM constants\n",
    "\n",
    "AVAILABLE_SERVE_METRICS = {\n",
    "    \"duration\": \"Benchmark Duration (s)\",\n",
    "    # \"completed\": \"Completed Requests\",\n",
    "    # \"request_throughput\": \"Request Throughput (req/s)\",\n",
    "    # \"input_throughput\": \"Input Throughput (tokens/s)\",\n",
    "    # \"output_throughput\": \"Output Throughput (tokens/s)\",\n",
    "    \"mean_ttft_ms\": \"Mean TTFT (ms)\",\n",
    "    # \"median_ttft_ms\": \"Median TTFT (ms)\",\n",
    "    # \"std_ttft_ms\": \"Std Dev TTFT (ms)\",\n",
    "    # \"p99_ttft_ms\": \"P99 TTFT (ms)\",\n",
    "    \"mean_tpot_ms\": \"Mean TPOT (ms)\",\n",
    "    # \"median_tpot_ms\": \"Median TPOT (ms)\",\n",
    "    # \"std_tpot_ms\": \"Std Dev TPOT (ms)\",\n",
    "    # \"p99_tpot_ms\": \"P99 TPOT (ms)\",\n",
    "    # \"mean_itl_ms\": \"Mean ITL (ms)\",\n",
    "    # \"median_itl_ms\": \"Median ITL (ms)\",\n",
    "    # \"std_itl_ms\": \"Std Dev ITL (ms)\",\n",
    "    # \"p99_itl_ms\": \"P99 ITL (ms)\",\n",
    "    \"mean_e2e_latency_ms\": \"Mean E2E Latency (ms)\",\n",
    "    # \"median_e2e_latency_ms\": \"Median E2E Latency (ms)\",\n",
    "}\n",
    "\n",
    "VLLM_BENCHMARK_METRICS = {\n",
    "    # Saved by benchmark\n",
    "    # \"ray_vllm:prompt_tokens_total\": \"Total Prompt Tokens\",\n",
    "    # \"ray_vllm:generation_tokens_total\": \"Total Generation Tokens\",\n",
    "    # \"ray_vllm:request_success_total\": \"Total Successful Requests\",\n",
    "    # \"ray_vllm:time_to_first_token_seconds\": \"Mean TTFT (ms)\",\n",
    "    # \"ray_vllm:time_per_output_token_seconds\": \"Mean TPOT (ms)\",\n",
    "    # \"ray_vllm:e2e_request_latency_seconds\": \"Mean E2E Latency (ms)\",\n",
    "    \"ray_vllm:request_queue_time_seconds\": \"Mean Request Queue Time (ms)\",\n",
    "    # \"ray_vllm:request_inference_time_seconds\": \"Mean Request Inference Time (ms)\",\n",
    "    \"ray_vllm:request_prefill_time_seconds\": \"Mean Request Prefill Time (ms)\",\n",
    "    \"ray_vllm:request_decode_time_seconds\": \"Mean Request Decode Time (ms)\",\n",
    "}\n",
    "ALL_AVAILABLE_VLLM_METRICS = {\n",
    "    # Saved by benchmark\n",
    "    \"ray_vllm:prompt_tokens_total\": \"Total Prompt Tokens\",\n",
    "    \"ray_vllm:generation_tokens_total\": \"Total Generation Tokens\",\n",
    "    \"ray_vllm:request_success_total\": \"Total Successful Requests\",\n",
    "    \"ray_vllm:time_to_first_token_seconds\": \"Mean TTFT (ms)\",\n",
    "    \"ray_vllm:time_per_output_token_seconds\": \"Mean TPOT (ms)\",\n",
    "    \"ray_vllm:e2e_request_latency_seconds\": \"Mean E2E Latency (ms)\",\n",
    "    \"ray_vllm:request_queue_time_seconds\": \"Mean Request Queue Time (ms)\",\n",
    "    \"ray_vllm:request_inference_time_seconds\": \"Mean Request Inference Time (ms)\",\n",
    "    \"ray_vllm:request_prefill_time_seconds\": \"Mean Request Prefill Time (ms)\",\n",
    "    \"ray_vllm:request_decode_time_seconds\": \"Mean Request Decode Time (ms)\",\n",
    "    # Other metrics\n",
    "    \"ray_vllm:num_requests_running\": \"Number of Running Requests\",\n",
    "    \"ray_vllm:num_requests_swapped\": \"Number of Swapped Requests\",\n",
    "    \"ray_vllm:num_requests_waiting\": \"Number of Waiting Requests\",\n",
    "    \"ray_vllm:gpu_cache_usage_perc\": \"GPU Cache Usage\",\n",
    "    \"ray_vllm:cpu_cache_usage_perc\": \"CPU Cache Usage\",\n",
    "    \"ray_vllm:gpu_prefix_cache_hit_rate\": \"GPU Prefix Cache Hit Rate\",\n",
    "    \"ray_vllm:cpu_prefix_cache_hit_rate\": \"CPU Prefix Cache Hit Rate\",\n",
    "    \"ray_vllm:iteration_tokens_total\": \"Total Iteration Tokens\"\n",
    "}\n",
    "\n",
    "VLLM_METRIC_TYPES = {\n",
    "    # Saved by benchmark\n",
    "    \"ray_vllm:prompt_tokens_total\": \"counter\",\n",
    "    \"ray_vllm:generation_tokens_total\": \"counter\",\n",
    "    \"ray_vllm:request_success_total\": \"counter\",\n",
    "    \"ray_vllm:time_to_first_token_seconds\": \"histogram\",\n",
    "    \"ray_vllm:time_per_output_token_seconds\": \"histogram\",\n",
    "    \"ray_vllm:e2e_request_latency_seconds\": \"histogram\",\n",
    "    \"ray_vllm:request_queue_time_seconds\": \"histogram\",\n",
    "    \"ray_vllm:request_inference_time_seconds\": \"histogram\",\n",
    "    \"ray_vllm:request_prefill_time_seconds\": \"histogram\",\n",
    "    \"ray_vllm:request_decode_time_seconds\": \"histogram\",\n",
    "\n",
    "    # Other metrics\n",
    "    \"ray_vllm:num_requests_running\": \"gauge\",\n",
    "    \"ray_vllm:num_requests_swapped\": \"gauge\",\n",
    "    \"ray_vllm:num_requests_waiting\": \"gauge\",\n",
    "    \"ray_vllm:gpu_cache_usage_perc\": \"gauge\",\n",
    "    \"ray_vllm:cpu_cache_usage_perc\": \"gauge\",\n",
    "    \"ray_vllm:gpu_prefix_cache_hit_rate\": \"gauge\",\n",
    "    \"ray_vllm:cpu_prefix_cache_hit_rate\": \"gauge\",\n",
    "    \"ray_vllm:iteration_tokens_total\": \"histogram\",\n",
    "    # \"ray_vllm:lora_requests_info\": \"gauge\",\n",
    "    # \"ray_vllm:request_params_n\": \"histogram\",\n",
    "    # \"ray_vllm:request_params_max_tokens\": \"histogram\",\n",
    "    # \"ray_vllm:time_in_queue_requests\": \"histogram\", # Redundant with request_queue_time_seconds\n",
    "    # \"ray_vllm:request_prompt_tokens\": \"histogram\", # Redundant with prompt_tokens_total\n",
    "    # \"ray_vllm:request_generation_tokens\": \"histogram\", # Redundant with generation_tokens_total\n",
    "    # \"ray_vllm:request_max_num_generation_tokens\": \"histogram\", # Redundant with request_generation_tokens\n",
    "    # \"ray_vllm:num_preemptions_total\": \"counter\",\n",
    "    # \"ray_vllm:cache_config_info\": \"gauge\",\n",
    "    # \"ray_vllm:tokens_total\": \"counter\", # Redundant\n",
    "    # \"ray_vllm:model_forward_time_milliseconds\": \"histogram\",\n",
    "    # \"ray_vllm:model_execute_time_milliseconds\": \"histogram\",\n",
    "    # \"ray_vllm:spec_decode_draft_acceptance_rate\": \"gauge\",\n",
    "    # \"ray_vllm:spec_decode_efficiency\": \"gauge\",\n",
    "    # \"ray_vllm:spec_decode_num_accepted_tokens_total\": \"counter\",\n",
    "    # \"ray_vllm:spec_decode_num_draft_tokens_total\": \"counter\",\n",
    "    # \"ray_vllm:spec_decode_num_emitted_tokens_total\": \"counter\"\n",
    "}\n",
    "\n",
    "VLLM_METRIC_SPECIAL_Y_AXIS_LIMITS = {\n",
    "    \"ray_vllm:gpu_cache_usage_perc\": [-0.1, 1.1],\n",
    "    \"ray_vllm:cpu_cache_usage_perc\": [-0.1, 1.1],\n",
    "    \"ray_vllm:gpu_prefix_cache_hit_rate\": [-0.1, 1.1],\n",
    "    \"ray_vllm:cpu_prefix_cache_hit_rate\": [-0.1, 1.1],\n",
    "    \"ray_vllm:num_requests_running\": [-1, 21]\n",
    "}\n",
    "\n",
    "def matches_vllm_metric_name(key, metric_name, suffix=\"\"):\n",
    "    # Split the key at the '{' character and take the first part\n",
    "    key_prefix = key.split(\"{\")[0]\n",
    "    # Check if the key_prefix exactly matches metric_name + suffix\n",
    "    return key_prefix == metric_name + suffix\n",
    "\n",
    "def combine_log_files(output_file_name, strategy_prefix, log_directory):\n",
    "    # Create the output file path\n",
    "    output_path = f\"{log_directory}/{strategy_prefix}_{output_file_name}.csv\"\n",
    "\n",
    "    # Find all log files in the directory\n",
    "    pattern = os.path.join(log_directory, f\"{strategy_prefix}_*.csv\")\n",
    "    log_files = glob.glob(pattern)\n",
    "\n",
    "    if not log_files:\n",
    "        print(f\"No matching log files found in {log_directory}\")\n",
    "        return None\n",
    "    # Combine all files into one\n",
    "    with open(output_path, \"w\") as outfile:\n",
    "        for log_file in log_files:\n",
    "            with open(log_file, \"r\") as infile:\n",
    "                outfile.write(infile.read())\n",
    "                # Add a newline between files if needed\n",
    "                outfile.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_serve_metric_summary(file_name, metric):\n",
    "    df = pd.read_csv(file_name)\n",
    "    df = df[df[\"gpu_type\"] != \"gpu_type\"]\n",
    "    df[metric] = pd.to_numeric(df[metric], errors=\"raise\")\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    df[\"benchmark_label\"] = df[\"benchmark_label\"].str.replace(\"_\", \" \")\n",
    "    df[\"wrapped_label\"] = df[\"benchmark_label\"].apply(lambda x: \"\\n\".join(wrap(x, 15)))\n",
    "    original_label_order = df[\"wrapped_label\"].unique()\n",
    "\n",
    "    strategy_counts_by_label = df.groupby([\"wrapped_label\", \"scheduler_strategy\"]).size().reset_index(name=\"count\")\n",
    "    num_seeds = strategy_counts_by_label[\"count\"].min()\n",
    "    if len(strategy_counts_by_label) == 0:\n",
    "        num_seeds = -1\n",
    "\n",
    "    grouped = df.groupby([\"wrapped_label\", \"scheduler_strategy\"])\n",
    "    avg_df = grouped[metric].mean().reset_index()\n",
    "    min_df = grouped[metric].min().reset_index()\n",
    "    max_df = grouped[metric].max().reset_index()\n",
    "\n",
    "    avg_df = avg_df.merge(min_df, on=[\"wrapped_label\", \"scheduler_strategy\"], suffixes=(\"\", \"_min\"))\n",
    "    avg_df = avg_df.merge(max_df, on=[\"wrapped_label\", \"scheduler_strategy\"], suffixes=(\"\", \"_max\"))\n",
    "\n",
    "    # Get unique strategies present in the data\n",
    "    present_strategies = avg_df[\"scheduler_strategy\"].unique()\n",
    "    # Filter STRATEGY_NAMES to only include present strategies\n",
    "    filtered_strategy_names = {k: v for k, v in STRATEGY_NAMES.items() if k in present_strategies}\n",
    "\n",
    "    # Create a custom palette that maintains consistent colors for each strategy\n",
    "    all_strategies = list(STRATEGY_NAMES.keys())\n",
    "    # Using the colorblind-friendly palette already set above\n",
    "    palette = sns.color_palette(n_colors=len(all_strategies))\n",
    "    strategy_to_color = dict(zip(all_strategies, palette))\n",
    "\n",
    "    # Filter the palette to only include present strategies while maintaining color consistency\n",
    "    filtered_palette = [strategy_to_color[strategy] for strategy in all_strategies if strategy in present_strategies]\n",
    "\n",
    "    ax = sns.barplot(\n",
    "        x=\"wrapped_label\",\n",
    "        y=metric,\n",
    "        hue=\"scheduler_strategy\",\n",
    "        hue_order=[s for s in all_strategies if s in present_strategies],\n",
    "        data=avg_df,\n",
    "        order=original_label_order,\n",
    "        palette=filtered_palette\n",
    "    )\n",
    "\n",
    "    # Match whiskers to exact bars\n",
    "    bar_map = {}  # (wrapped_label, scheduler_strategy) -> (x_center, width)\n",
    "    for p in ax.patches:\n",
    "        x_center = p.get_x() + p.get_width() / 2.\n",
    "        width = p.get_width()\n",
    "        height = p.get_height()\n",
    "        for _, row in avg_df.iterrows():\n",
    "            avg_val = row[metric]\n",
    "            if abs(height - avg_val) < 1e-2:\n",
    "                key = (row[\"wrapped_label\"], row[\"scheduler_strategy\"])\n",
    "                if key not in bar_map:\n",
    "                    bar_map[key] = (x_center, width)\n",
    "                break\n",
    "\n",
    "    # Draw \"I\"-shaped whiskers\n",
    "    for _, row in avg_df.iterrows():\n",
    "        key = (row[\"wrapped_label\"], row[\"scheduler_strategy\"])\n",
    "        if key in bar_map:\n",
    "            x, width = bar_map[key]\n",
    "            y_min = row[f\"{metric}_min\"]\n",
    "            y_max = row[f\"{metric}_max\"]\n",
    "            cap_width = width * 0.4\n",
    "\n",
    "            # Vertical whisker\n",
    "            ax.plot([x, x], [y_min, y_max], color=\"black\", linewidth=1.2, zorder=5)\n",
    "            # Top and bottom caps\n",
    "            ax.plot([x - cap_width / 2, x + cap_width / 2], [y_max, y_max], color=\"black\", linewidth=1.2, zorder=5)\n",
    "            ax.plot([x - cap_width / 2, x + cap_width / 2], [y_min, y_min], color=\"black\", linewidth=1.2, zorder=5)\n",
    "\n",
    "    # Value labels on top of each bar\n",
    "    for p in ax.patches:\n",
    "        value = p.get_height()\n",
    "        text = f\"{value:.1f}\"\n",
    "        ax.annotate(text, (p.get_x() + p.get_width() / 2., value),\n",
    "                    ha=\"center\", va=\"bottom\")\n",
    "\n",
    "    plt.title(f\"Serve Metric: {AVAILABLE_SERVE_METRICS[metric]}\")\n",
    "    plt.xlabel(\"Benchmark Label\")\n",
    "    plt.ylabel(AVAILABLE_SERVE_METRICS[metric])\n",
    "\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    strategy_handles = handles\n",
    "    strategy_labels = [filtered_strategy_names[label] for label in labels]\n",
    "\n",
    "    extra_lines = []\n",
    "    if num_seeds != -1:\n",
    "        seed_info = f\"*Averaged over {num_seeds} seeds\"\n",
    "        extra_lines.append(Line2D([], [], color=\"none\", label=seed_info))\n",
    "\n",
    "    ax.legend(\n",
    "        handles=strategy_handles + extra_lines,\n",
    "        labels=strategy_labels + [line.get_label() for line in extra_lines],\n",
    "        title=\"Routing Strategy\",\n",
    "        bbox_to_anchor=(1.05, 1),\n",
    "        loc=\"upper left\"\n",
    "    )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_serve_load_distribution(json_file_path, title=None):\n",
    "    \"\"\"\n",
    "    Plot the load distribution over time for each replica on a single chart.\n",
    "    Args:\n",
    "        json_file_path: Path to the JSON file containing load distribution data\n",
    "    \"\"\"\n",
    "\n",
    "    # Load the data from the JSON file\n",
    "    with open(json_file_path, \"r\") as f:\n",
    "        load_data = json.load(f)\n",
    "\n",
    "    # Convert the data to a more usable format\n",
    "    times = sorted([float(t) for t in load_data.keys() if load_data[t]])\n",
    "    replicas = list(load_data[str(times[0])].keys())\n",
    "\n",
    "    # Create a single figure with smaller size\n",
    "    plt.figure(figsize=(12, 4))\n",
    "\n",
    "    # Define colors for each replica\n",
    "    colors = [\"blue\", \"red\", \"green\", \"purple\"]\n",
    "\n",
    "    # Store all replica loads to calculate average\n",
    "    all_replica_loads = []\n",
    "\n",
    "    # Plot each replica's load over time on the same chart\n",
    "    for i, replica_id in enumerate(replicas):\n",
    "        if i >= 4:  # Only plot the first 4 replicas\n",
    "            break\n",
    "\n",
    "        # Extract data for this replica\n",
    "        replica_loads = [load_data[str(t)].get(replica_id, 0) for t in times]\n",
    "        all_replica_loads.append(replica_loads)\n",
    "\n",
    "        # Plot the data with different color for each replica (more transparent)\n",
    "        # Using smaller marker size (markersize=4)\n",
    "        plt.plot(times, replica_loads, marker=\"o\", markersize=1, linestyle=\"-\", linewidth=0.5,\n",
    "                 color=colors[i], label=f\"Replica ID: {replica_id}\", alpha=0.5)\n",
    "\n",
    "    # Calculate and plot the average load\n",
    "    avg_loads = None\n",
    "    avg_load_value = 0\n",
    "    if all_replica_loads:\n",
    "        avg_loads = np.mean(all_replica_loads, axis=0)\n",
    "\n",
    "        # Calculate average load during high-load period (when avg > 5)\n",
    "        if any(load > 5 for load in avg_loads):\n",
    "            start_idx = next((i for i, load in enumerate(avg_loads) if load > 5), 0)\n",
    "            end_idx = len(avg_loads) - next((i for i, load in enumerate(reversed(avg_loads)) if load > 5), 0) - 1\n",
    "            if start_idx <= end_idx:\n",
    "                high_load_window = avg_loads[start_idx:end_idx+1]\n",
    "                avg_load_value = np.mean(high_load_window) if high_load_window.size > 0 else 0\n",
    "\n",
    "        plt.plot(times, avg_loads, marker=\"s\", markersize=2, linestyle=\"-\", linewidth=2,\n",
    "                 color=\"black\", label=f\"Average Load ({avg_load_value:.2f})\", alpha=0.5, zorder=10)\n",
    "    if title is not None:\n",
    "        plot_title = f\"Serve Metric: Load Distribution â€“ {title}\"\n",
    "    else:\n",
    "        plot_title = \"Serve Metric: Load Distribution\"\n",
    "\n",
    "    plt.title(plot_title)\n",
    "    plt.xlabel(\"Time (seconds)\")\n",
    "    plt.ylabel(\"Number of Requests\")\n",
    "    plt.grid(True)\n",
    "    # Place legend outside the plot to avoid overlapping with lines\n",
    "\n",
    "    # Real legend handles and labels\n",
    "    handles, labels = plt.gca().get_legend_handles_labels()\n",
    "\n",
    "    # Draw final legend\n",
    "    plt.legend(handles=handles, labels=labels, bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "\n",
    "    # Set y-axis to integers from 0 to 20 inclusive\n",
    "    ax = plt.gca()\n",
    "    ax.set_ylim(0, 20)\n",
    "    ax.yaxis.set_major_locator(MultipleLocator(1))\n",
    "\n",
    "    # Make sure the ticks are visible\n",
    "    ax.tick_params(axis=\"y\", which=\"major\", length=6)\n",
    "\n",
    "    # Adjust layout to make room for the legend\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(right=0.8)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_all_serve_load_distributions(load_dist_dir):\n",
    "    # Get all JSON files in the directory\n",
    "    json_files = glob.glob(os.path.join(load_dist_dir, \"*.json\"))\n",
    "\n",
    "    # Extract the last part of the directory for the title\n",
    "    dir_title = os.path.basename(os.path.normpath(load_dist_dir))\n",
    "\n",
    "    # Create a dictionary to group files by strategy\n",
    "    strategy_files = {}\n",
    "    for file_path in json_files:\n",
    "        filename = os.path.basename(file_path)\n",
    "        strategy_prefix = filename.split(\"_\")[0]  # Get the part before the first underscore\n",
    "\n",
    "        if strategy_prefix not in strategy_files:\n",
    "            strategy_files[strategy_prefix] = []\n",
    "\n",
    "        strategy_files[strategy_prefix].append(file_path)\n",
    "\n",
    "    # Sort each group of files\n",
    "    for strategy in strategy_files:\n",
    "        strategy_files[strategy] = sorted(strategy_files[strategy])\n",
    "\n",
    "    # Process files in order of STRATEGY_PREFIXES, then by filename\n",
    "    for strategy_prefix in STRATEGY_PREFIXES.keys():\n",
    "        if strategy_prefix in strategy_files:\n",
    "            for file_path in strategy_files[strategy_prefix]:\n",
    "                # Get the full strategy name or use the prefix if not found\n",
    "                strategy_name = STRATEGY_PREFIXES.get(strategy_prefix, strategy_prefix.capitalize())\n",
    "                # Plot the load distribution with directory title\n",
    "                plot_serve_load_distribution(file_path, f\"{strategy_name} Strategy - {dir_title} Directory\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_serve_prefix_match_rate(json_file_path, title=None):\n",
    "    \"\"\"\n",
    "    Plot the prefix match rates for each replica on a single chart.\n",
    "    Args:\n",
    "        json_file_path: Path to the JSON file containing prefix match rates data\n",
    "        title: Optional title for the plot\n",
    "    \"\"\"\n",
    "\n",
    "    # Load the data from the JSON file\n",
    "    with open(json_file_path, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Create a figure\n",
    "    plt.figure(figsize=(12, 4))\n",
    "\n",
    "    # Define colors for different replicas\n",
    "    colors = [\"blue\", \"red\", \"green\", \"purple\", \"orange\", \"brown\", \"pink\", \"gray\"]\n",
    "\n",
    "    # Store all match rates for calculating average\n",
    "    all_match_rates = []\n",
    "    max_length = 0\n",
    "\n",
    "    # Plot each replica's match rates\n",
    "    for i, (replica_id, match_rates) in enumerate(data.items()):\n",
    "        # Create indices for x-axis (0 to len(match_rates)-1)\n",
    "        indices = np.arange(len(match_rates))\n",
    "\n",
    "        # Track the maximum length for average calculation\n",
    "        max_length = max(max_length, len(match_rates))\n",
    "\n",
    "        # Store match rates for average calculation\n",
    "        all_match_rates.append(match_rates)\n",
    "\n",
    "        # Apply jitter for zero values based on replica index\n",
    "        jittered_match_rates = []\n",
    "        for rate in match_rates:\n",
    "            jittered_match_rates.append(rate + random.uniform(-0.02, 0.02))\n",
    "\n",
    "        # Calculate replica-specific statistics\n",
    "        replica_avg_rate = np.mean(match_rates) if match_rates else 0\n",
    "        # replica_matches = sum(1 for rate in match_rates if rate > 0.1)\n",
    "        # replica_total = len(match_rates)\n",
    "        # replica_match_percentage = (replica_matches / replica_total * 100) if replica_total > 0 else 0\n",
    "\n",
    "        # Plot the data points with enhanced legend\n",
    "        plt.scatter(indices, jittered_match_rates,\n",
    "                   color=colors[i % len(colors)],\n",
    "                   alpha=0.5,\n",
    "                   s=5,  # smaller point size\n",
    "                   label=f\"Replica ID: {replica_id}: Avg Hit Rate: {replica_avg_rate:.2f}\")\n",
    "                #    label=f'Replica ID: {replica_id}: Avg Hit Rate: {replica_avg_rate:.2f}, Matches: {replica_matches}/{replica_total} ({replica_match_percentage:.1f}%)')\n",
    "\n",
    "    # Calculate the overall average hit rate across all data points\n",
    "    all_rates = []\n",
    "    for rates in all_match_rates:\n",
    "        all_rates.extend(rates)\n",
    "    overall_avg_rate = np.mean(all_rates) if all_rates else 0\n",
    "\n",
    "    # Calculate the number of matches (match rate > 0.1)\n",
    "    # num_matches = sum(1 for rate in all_rates if rate > 0.1)\n",
    "    # total_requests = len(all_rates)\n",
    "    # match_percentage = (num_matches / total_requests * 100) if total_requests > 0 else 0\n",
    "\n",
    "    # Calculate and plot the average hit rate\n",
    "    if all_match_rates:\n",
    "        # Pad shorter arrays with NaN to make them all the same length\n",
    "        padded_rates = []\n",
    "        for rates in all_match_rates:\n",
    "            if len(rates) < max_length:\n",
    "                padded = rates + [np.nan] * (max_length - len(rates))\n",
    "                padded_rates.append(padded)\n",
    "            else:\n",
    "                padded_rates.append(rates)\n",
    "\n",
    "        # Convert to numpy array and calculate average, ignoring NaN values\n",
    "        avg_rates = np.nanmean(padded_rates, axis=0)\n",
    "\n",
    "        # Smooth the average line using Savitzky-Golay filter\n",
    "        # Only apply smoothing if we have enough data points\n",
    "        if max_length > 10:\n",
    "            window_length = min(max_length // 4 * 2 + 1, 51)  # Must be odd and less than data length\n",
    "            window_length = max(5, window_length)  # At least 5\n",
    "            polyorder = min(3, window_length - 1)  # Must be less than window_length\n",
    "\n",
    "            # Replace NaN values with interpolated values for smoothing\n",
    "            nan_indices = np.isnan(avg_rates)\n",
    "            if np.any(nan_indices):\n",
    "                x = np.arange(len(avg_rates))\n",
    "                avg_rates_no_nan = avg_rates.copy()\n",
    "                avg_rates_no_nan[nan_indices] = np.interp(\n",
    "                    x[nan_indices], x[~nan_indices], avg_rates[~nan_indices]\n",
    "                )\n",
    "                smoothed_avg = savgol_filter(avg_rates_no_nan, window_length, polyorder)\n",
    "            else:\n",
    "                smoothed_avg = savgol_filter(avg_rates, window_length, polyorder)\n",
    "\n",
    "            # Plot the smoothed average line with average hit rate in the legend\n",
    "            plt.plot(np.arange(max_length), smoothed_avg,\n",
    "                     color=\"black\",\n",
    "                     linewidth=2,\n",
    "                     label=f\"Average Hit Rate: {overall_avg_rate:.2f}\",\n",
    "                    #  label=f'Average Hit Rate: {overall_avg_rate:.2f}, Matches: {num_matches}/{total_requests} ({match_percentage:.1f}%)',\n",
    "                     zorder=10)\n",
    "        else:\n",
    "            # If not enough data points for smoothing, plot the original average\n",
    "            plt.plot(np.arange(max_length), avg_rates,\n",
    "                     color=\"black\",\n",
    "                     linewidth=2,\n",
    "                     label=f\"Average Hit Rate: {overall_avg_rate:.2f}\",\n",
    "                    #  label=f'Average Hit Rate: {overall_avg_rate:.2f}, Matches: {num_matches}/{total_requests} ({match_percentage:.1f}%)',\n",
    "                     zorder=10)\n",
    "\n",
    "    # Set plot title and labels\n",
    "    plt.title(f\"Serve Metric: Prefix Match Rate - {title}\" if title else \"Serve Metric: Prefix Match Rate\")\n",
    "    plt.xlabel(\"Index\")\n",
    "    plt.ylabel(\"Match Rate (0-1)\")\n",
    "\n",
    "    # Set y-axis limits\n",
    "    plt.ylim(-0.05, 1.05)\n",
    "\n",
    "    # Add grid\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    # Add legend\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "\n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(right=0.8)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_all_serve_prefix_match_rates(match_rates_dir):\n",
    "    \"\"\"\n",
    "    Plot prefix match rates for all JSON files in the specified directory.\n",
    "    Args:\n",
    "        match_rates_dir (str): Directory containing prefix match rates JSON files\n",
    "    \"\"\"\n",
    "\n",
    "    # Get all JSON files in the directory\n",
    "    json_files = glob.glob(os.path.join(match_rates_dir, \"*.json\"))\n",
    "\n",
    "    # Extract the last part of the directory for the title\n",
    "    dir_title = os.path.basename(os.path.normpath(match_rates_dir))\n",
    "\n",
    "    # Create a dictionary to group files by strategy\n",
    "    strategy_files = {}\n",
    "    for file_path in json_files:\n",
    "        filename = os.path.basename(file_path)\n",
    "        strategy_prefix = filename.split(\"_\")[0]  # Get the part before the first underscore\n",
    "\n",
    "        if strategy_prefix not in strategy_files:\n",
    "            strategy_files[strategy_prefix] = []\n",
    "\n",
    "        strategy_files[strategy_prefix].append(file_path)\n",
    "\n",
    "    # Sort each group of files\n",
    "    for strategy in strategy_files:\n",
    "        strategy_files[strategy] = sorted(strategy_files[strategy])\n",
    "\n",
    "    # Process files in order of STRATEGY_PREFIXES, then by filename\n",
    "    for strategy_prefix in STRATEGY_PREFIXES.keys():\n",
    "        if strategy_prefix in strategy_files:\n",
    "            for file_path in strategy_files[strategy_prefix]:\n",
    "                # Get the full strategy name or use the prefix if not found\n",
    "                strategy_name = STRATEGY_PREFIXES.get(strategy_prefix, strategy_prefix.capitalize())\n",
    "\n",
    "                # Plot the prefix match rates with directory title\n",
    "                plot_serve_prefix_match_rate(file_path, f\"{strategy_name} Strategy - {dir_title} Directory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_vllm_metric_summary(file_name, metric):\n",
    "    \"\"\"\n",
    "    Plot a vLLM metric from the CSV file for the given metric, with min-max whiskers.\n",
    "    Args:\n",
    "        file_name: Path to the CSV file containing vLLM sweep results.\n",
    "        metric: A string matching one of the vLLM metric column names\n",
    "                (e.g., \"ray_vllm:time_to_first_token_seconds\").\n",
    "    \"\"\"\n",
    "    # Read CSV data\n",
    "    df = pd.read_csv(file_name)\n",
    "    df = df[df[\"gpu_type\"] != \"gpu_type\"]\n",
    "    df[metric] = pd.to_numeric(df[metric], errors=\"raise\")\n",
    "    if metric.endswith(\"seconds\"):\n",
    "        df[metric] = df[metric] * 1000\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    df[\"benchmark_label\"] = df[\"benchmark_label\"].str.replace(\"_\", \" \")\n",
    "    df[\"wrapped_label\"] = df[\"benchmark_label\"].apply(lambda x: \"\\n\".join(wrap(x, 15)))\n",
    "    original_label_order = df[\"wrapped_label\"].unique()\n",
    "\n",
    "    strategy_counts_by_label = df.groupby([\"wrapped_label\", \"scheduler_strategy\"]).size().reset_index(name=\"count\")\n",
    "    num_seeds = strategy_counts_by_label[\"count\"].min()\n",
    "    if len(strategy_counts_by_label) == 0:\n",
    "        num_seeds = -1\n",
    "\n",
    "    grouped = df.groupby([\"wrapped_label\", \"scheduler_strategy\"])\n",
    "    avg_df = grouped[metric].mean().reset_index()\n",
    "    min_df = grouped[metric].min().reset_index()\n",
    "    max_df = grouped[metric].max().reset_index()\n",
    "\n",
    "    avg_df = avg_df.merge(min_df, on=[\"wrapped_label\", \"scheduler_strategy\"], suffixes=(\"\", \"_min\"))\n",
    "    avg_df = avg_df.merge(max_df, on=[\"wrapped_label\", \"scheduler_strategy\"], suffixes=(\"\", \"_max\"))\n",
    "\n",
    "    ax = sns.barplot(\n",
    "        x=\"wrapped_label\",\n",
    "        y=metric,\n",
    "        hue=\"scheduler_strategy\",\n",
    "        hue_order=STRATEGY_NAMES.keys(),\n",
    "        data=avg_df,\n",
    "        order=original_label_order\n",
    "    )\n",
    "\n",
    "    bar_width = ax.patches[0].get_width()\n",
    "    cap_width = bar_width * 0.5  # Cap spans 50% of the bar width\n",
    "\n",
    "    # Match whiskers to exact bar positions\n",
    "    bar_map = {}  # (wrapped_label, scheduler_strategy) -> bar center x\n",
    "    for p in ax.patches:\n",
    "        x_center = p.get_x() + bar_width / 2.\n",
    "        height = p.get_height()\n",
    "        # Try to match this patch to the correct group in avg_df\n",
    "        for _, row in avg_df.iterrows():\n",
    "            avg_val = row[metric]\n",
    "            if abs(height - avg_val) < 1e-2:\n",
    "                key = (row[\"wrapped_label\"], row[\"scheduler_strategy\"])\n",
    "                if key not in bar_map:\n",
    "                    bar_map[key] = x_center\n",
    "                break\n",
    "\n",
    "    # Draw whiskers\n",
    "    for _, row in avg_df.iterrows():\n",
    "        key = (row[\"wrapped_label\"], row[\"scheduler_strategy\"])\n",
    "        if key in bar_map:\n",
    "            x = bar_map[key]\n",
    "            y_min = row[f\"{metric}_min\"]\n",
    "            y_max = row[f\"{metric}_max\"]\n",
    "            ax.plot([x, x], [y_min, y_max], color=\"black\", linewidth=1.2, zorder=5)\n",
    "\n",
    "            # Top cap at max\n",
    "            ax.plot([x - cap_width / 2, x + cap_width / 2], [y_max, y_max], color=\"black\", linewidth=1.2, zorder=5)\n",
    "            # Bottom cap at min\n",
    "            ax.plot([x - cap_width / 2, x + cap_width / 2], [y_min, y_min], color=\"black\", linewidth=1.2, zorder=5)\n",
    "\n",
    "    # Add value labels\n",
    "    for p in ax.patches:\n",
    "        value = p.get_height()\n",
    "        label_text = f\"{value:.1f}\"\n",
    "        ax.annotate(\n",
    "            label_text,\n",
    "            (p.get_x() + p.get_width() / 2., value),\n",
    "            ha=\"center\",\n",
    "            va=\"bottom\"\n",
    "        )\n",
    "\n",
    "    plt.title(f\"vLLM Metric: {VLLM_BENCHMARK_METRICS[metric]}\")\n",
    "    plt.xlabel(\"Benchmark Label\")\n",
    "    plt.ylabel(VLLM_BENCHMARK_METRICS[metric])\n",
    "\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    strategy_handles = handles\n",
    "    strategy_labels = [STRATEGY_NAMES[label] for label in labels]\n",
    "\n",
    "    extra_lines = []\n",
    "    if num_seeds != -1:\n",
    "        seed_info = f\"*Averaged over {num_seeds} seeds\"\n",
    "        extra_lines.append(Line2D([], [], color=\"none\", label=seed_info))\n",
    "\n",
    "    ax.legend(\n",
    "        handles=strategy_handles + extra_lines,\n",
    "        labels=strategy_labels + [line.get_label() for line in extra_lines],\n",
    "        title=\"Routing Strategy\",\n",
    "        bbox_to_anchor=(1.05, 1),\n",
    "        loc=\"upper left\"\n",
    "    )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_vllm_request_lifecycle(file_name):\n",
    "    \"\"\"\n",
    "    Plot a vLLM request lifecycle chart from the CSV file, stacking three metrics:\n",
    "      - Queue time (bottom)\n",
    "      - Prefill time (middle)\n",
    "      - Decode time (top)\n",
    "\n",
    "    For each (benchmark_label, scheduler_strategy) group, a single stacked bar is drawn where:\n",
    "      - The bottom segment represents the average queue time,\n",
    "      - The middle segment represents the average prefill time,\n",
    "      - The top segment represents the average decode time.\n",
    "\n",
    "    All segments for a given group are the same color (determined by scheduler_strategy).\n",
    "    Each segment is annotated:\n",
    "      - \"Queue\" is labeled at y = 5,\n",
    "      - \"Prefill\" is labeled 5 ms above the top of the stacked queue+prefill segment,\n",
    "      - \"Decode\" is labeled 5 ms above the top of the entire stacked bar.\n",
    "\n",
    "    Assumes the CSV has columns:\n",
    "        \"gpu_type\",\n",
    "        \"benchmark_label\",\n",
    "        \"scheduler_strategy\",\n",
    "        \"ray_vllm:request_queue_time_seconds\",\n",
    "        \"ray_vllm:request_prefill_time_seconds\",\n",
    "        \"ray_vllm:request_decode_time_seconds\"\n",
    "\n",
    "    Time values are converted from seconds to milliseconds.\n",
    "    \"\"\"\n",
    "    # Read CSV data\n",
    "    df = pd.read_csv(file_name)\n",
    "    df = df[df[\"gpu_type\"] != \"gpu_type\"]\n",
    "\n",
    "    # Define lifecycle metrics\n",
    "    queue_metric = \"ray_vllm:request_queue_time_seconds\"\n",
    "    prefill_metric = \"ray_vllm:request_prefill_time_seconds\"\n",
    "    decode_metric = \"ray_vllm:request_decode_time_seconds\"\n",
    "\n",
    "    # Convert metrics to numeric and milliseconds\n",
    "    for metric in [queue_metric, prefill_metric, decode_metric]:\n",
    "        df[metric] = pd.to_numeric(df[metric], errors=\"raise\")\n",
    "        if metric.endswith(\"seconds\"):\n",
    "            df[metric] = df[metric] * 1000\n",
    "\n",
    "    plt.figure(figsize=(10, 20))\n",
    "\n",
    "    # Process benchmark labels: replace underscores and wrap long text\n",
    "    df[\"benchmark_label\"] = df[\"benchmark_label\"].str.replace(\"_\", \" \")\n",
    "    df[\"wrapped_label\"] = df[\"benchmark_label\"].apply(lambda x: \"\\n\".join(wrap(x, 15)))\n",
    "    original_label_order = df[\"wrapped_label\"].unique()\n",
    "\n",
    "    # Group by benchmark label and scheduler_strategy; compute average for each lifecycle stage\n",
    "    grouped = df.groupby([\"wrapped_label\", \"scheduler_strategy\"])\n",
    "    avg_df = grouped.agg({\n",
    "        queue_metric: \"mean\",\n",
    "        prefill_metric: \"mean\",\n",
    "        decode_metric: \"mean\"\n",
    "    }).reset_index()\n",
    "\n",
    "    # Build mapping for x positions: group bars by wrapped_label,\n",
    "    # then order by scheduler_strategy as in STRATEGY_NAMES.\n",
    "    mapping = {}         # (wrapped_label, scheduler_strategy) -> x position\n",
    "    tick_positions = {}  # wrapped_label -> average x position for the group\n",
    "    x = 0\n",
    "    strat_keys = list(STRATEGY_NAMES.keys())\n",
    "    for label in original_label_order:\n",
    "        positions = []\n",
    "        for strat in strat_keys:\n",
    "            subset = avg_df[(avg_df[\"wrapped_label\"] == label) & (avg_df[\"scheduler_strategy\"] == strat)]\n",
    "            if not subset.empty:\n",
    "                mapping[(label, strat)] = x\n",
    "                positions.append(x)\n",
    "                x += 0.8\n",
    "        if positions:\n",
    "            tick_positions[label] = np.mean(positions)\n",
    "        x += 0.5  # extra gap between different benchmark_label groups\n",
    "\n",
    "    # Draw stacked bars for each (wrapped_label, scheduler_strategy)\n",
    "    bar_width = 0.8\n",
    "    ax = plt.gca()\n",
    "    # Use default color cycle for scheduler strategies (same as original function)\n",
    "    color_cycle = plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\n",
    "\n",
    "    for _, row in avg_df.iterrows():\n",
    "        label = row[\"wrapped_label\"]\n",
    "        strat = row[\"scheduler_strategy\"]\n",
    "        if (label, strat) not in mapping:\n",
    "            continue\n",
    "        x_pos = mapping[(label, strat)]\n",
    "\n",
    "        # Get the average times for each stage (in ms)\n",
    "        queue_val = row[queue_metric]\n",
    "        prefill_val = row[prefill_metric]\n",
    "        decode_val = row[decode_metric]\n",
    "\n",
    "        # Choose the color based on scheduler_strategy\n",
    "        color = color_cycle[strat_keys.index(strat) % len(color_cycle)]\n",
    "\n",
    "        # Plot the stacked bar segments\n",
    "        ax.bar(x_pos, queue_val, bar_width, color=color, edgecolor=\"black\")\n",
    "        ax.bar(x_pos, prefill_val, bar_width, bottom=queue_val, color=color, edgecolor=\"black\")\n",
    "        ax.bar(x_pos, decode_val, bar_width, bottom=queue_val+prefill_val, color=color, edgecolor=\"black\")\n",
    "\n",
    "        # Annotate each segment:\n",
    "        # \"Queue\" label: fixed 5 ms above the bottom (y = 5)\n",
    "        ax.text(x_pos, queue_val - 40, \"Queue\", ha=\"center\", va=\"bottom\", fontsize=12)\n",
    "        # \"Prefill\": 5 ms above the top of the queue+prefill stack\n",
    "        ax.text(x_pos, queue_val + prefill_val - 40, \"Prefill\", ha=\"center\", va=\"bottom\", fontsize=12)\n",
    "        # \"Decode\": 5 ms above the top of the entire stacked bar\n",
    "        ax.text(x_pos, queue_val + prefill_val + decode_val - 40, \"Decode\", ha=\"center\", va=\"bottom\", fontsize=12)\n",
    "\n",
    "    # Set the x-axis ticks to display benchmark_label groups\n",
    "    xticks = list(tick_positions.values())\n",
    "    xtick_labels = list(tick_positions.keys())\n",
    "    ax.set_xticks(xticks)\n",
    "    ax.set_xticklabels(xtick_labels)\n",
    "\n",
    "    plt.xlabel(\"Benchmark Label\")\n",
    "    plt.ylabel(\"Time (ms)\")\n",
    "    plt.title(\"vLLM Request Lifecycle\")\n",
    "\n",
    "    # Build legend by scheduler_strategy (same style as your original function)\n",
    "    legend_elements = []\n",
    "    for strat in strat_keys:\n",
    "        color = color_cycle[strat_keys.index(strat) % len(color_cycle)]\n",
    "        legend_elements.append(plt.Rectangle((0,0), 1, 1, facecolor=color, edgecolor=\"black\", label=STRATEGY_NAMES[strat]))\n",
    "\n",
    "    # Optionally include a legend note for number of seeds\n",
    "    strategy_counts = df.groupby([\"wrapped_label\", \"scheduler_strategy\"]).size().reset_index(name=\"count\")\n",
    "    num_seeds = strategy_counts[\"count\"].min() if len(strategy_counts) > 0 else -1\n",
    "    extra_lines = []\n",
    "    if num_seeds != -1:\n",
    "        seed_info = f\"*Averaged over {num_seeds} seeds\"\n",
    "        extra_lines.append(plt.Rectangle((0,0), 1, 1, facecolor=\"none\", edgecolor=\"none\", label=seed_info))\n",
    "\n",
    "    ax.legend(handles=legend_elements + extra_lines, title=\"Routing Strategy\", bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_vllm_metric_timeseries(\n",
    "    json_file_path: str,\n",
    "    metric_name: str,\n",
    "    title: Optional[str] = None,\n",
    "    window_sec: Optional[float] = 5.0\n",
    "):\n",
    "    \"\"\"\n",
    "    Generalized vLLM metric plotter with improved styling.\n",
    "    Args:\n",
    "        json_file_path: Path to the JSON file with vLLM metrics.\n",
    "        metric_name: Metric name (e.g. 'ray_vllm:gpu_cache_usage_perc' or base name like 'time_to_first_token_seconds').\n",
    "        title: Optional plot title.\n",
    "        window_sec: Window size in seconds for moving average (only applies for histogram metrics).\n",
    "    \"\"\"\n",
    "    is_histogram = VLLM_METRIC_TYPES.get(metric_name, \"unknown\") == \"histogram\"\n",
    "\n",
    "    # Load metric data from JSON file\n",
    "    with open(json_file_path, \"r\") as f:\n",
    "        raw_data = json.load(f)\n",
    "\n",
    "    # Convert timestamp strings to float and sort them\n",
    "    times = sorted([float(t) for t in raw_data.keys()])\n",
    "\n",
    "    all_replica_metrics = []\n",
    "\n",
    "    # Count how many times each replica appears across all timestamps\n",
    "    worker_counts = {}\n",
    "    for t in raw_data:\n",
    "        for rid in raw_data[t]:\n",
    "            worker_counts[rid] = worker_counts.get(rid, 0) + 1\n",
    "\n",
    "    # Only keep replicas that appear in more than 75% of timestamps\n",
    "    min_appearance = len(times) * 0.75\n",
    "    valid_replicas = {rid for rid, count in worker_counts.items() if count > min_appearance}\n",
    "\n",
    "    # Filter replicas that contain the specified metric\n",
    "    replica_ids = set()\n",
    "    for t in raw_data:\n",
    "        for rid in raw_data[t]:\n",
    "            if rid not in valid_replicas:\n",
    "                continue\n",
    "            keys = raw_data[t][rid].keys()\n",
    "\n",
    "            if is_histogram:\n",
    "                match_prefix_sum = [k for k in keys if matches_vllm_metric_name(k, metric_name, \"_sum\")]\n",
    "                match_prefix_count = [k for k in keys if matches_vllm_metric_name(k, metric_name, \"_count\")]\n",
    "\n",
    "                if match_prefix_sum and match_prefix_count:\n",
    "                    replica_ids.add(rid)\n",
    "            else:\n",
    "                match_prefix = [k for k in keys if matches_vllm_metric_name(k, metric_name)]\n",
    "\n",
    "                if match_prefix:\n",
    "                    replica_ids.add(rid)\n",
    "\n",
    "    replica_ids = sorted(replica_ids)\n",
    "\n",
    "    if not replica_ids:\n",
    "        print(\"[WARN] No matching replicas found.\")\n",
    "        return\n",
    "\n",
    "    # Create figure with improved styling\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # Add grid with dotted lines for all x and y axis ticks\n",
    "    plt.grid(True, linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "    for i, replica_id in enumerate(replica_ids):\n",
    "        y_vals = []\n",
    "        for current_time in times:\n",
    "            current_snapshot = raw_data.get(str(current_time), {}).get(replica_id, {})\n",
    "\n",
    "            if is_histogram:\n",
    "                # Find current sum and count\n",
    "                current_sum = next((v for k, v in current_snapshot.items() if matches_vllm_metric_name(k, metric_name, \"_sum\")), 0)\n",
    "                current_count = next((v for k, v in current_snapshot.items() if matches_vllm_metric_name(k, metric_name, \"_count\")), 0)\n",
    "                if window_sec:\n",
    "                    # Compute moving average over a specified time window\n",
    "                    window_start = current_time - window_sec\n",
    "\n",
    "                    # Find the first timestamp that is >= window_start\n",
    "                    start_time = times[0]  # Default to first timestamp\n",
    "                    for time in times:\n",
    "                        if time >= window_start:\n",
    "                            start_time = time\n",
    "                            break\n",
    "\n",
    "                    # If there's not enough data for a window, skip\n",
    "                    if start_time == current_time:\n",
    "                        y_vals.append(0.0)\n",
    "                        continue\n",
    "\n",
    "                    start_snapshot = raw_data.get(str(start_time), {}).get(replica_id, {})\n",
    "\n",
    "                    # Calculate the sum and count differences over the window\n",
    "                    start_sum = next((v for k, v in start_snapshot.items() if matches_vllm_metric_name(k, metric_name, \"_sum\")), 0)\n",
    "                    start_count = next((v for k, v in start_snapshot.items() if matches_vllm_metric_name(k, metric_name, \"_count\")), 0)\n",
    "\n",
    "                    # Compute average value for the window\n",
    "                    sum_diff = current_sum - start_sum\n",
    "                    count_diff = current_count - start_count\n",
    "                    running_avg = (sum_diff / count_diff) if count_diff > 0 else 0.0\n",
    "                    y_vals.append(running_avg)\n",
    "                else:\n",
    "                    # Compute running average with window_sec = infinity\n",
    "                    current_avg = current_sum / current_count if current_count > 0 else 0.0\n",
    "                    y_vals.append(current_avg)\n",
    "            else:\n",
    "                # For raw metrics, just extract the value directly\n",
    "                current_val = next((v for k, v in current_snapshot.items() if matches_vllm_metric_name(k, metric_name)), 0)\n",
    "                y_vals.append(current_val)\n",
    "\n",
    "        # Warn if the series is all zero (likely invalid or missing data)\n",
    "        if all(v == 0 for v in y_vals):\n",
    "            print(f\"[WARN] All values are zero for replica {replica_id}.\")\n",
    "        if metric_name.endswith(\"seconds\"):\n",
    "            y_vals = [v * 1000 for v in y_vals]\n",
    "\n",
    "        # Plot with larger markers and improved styling\n",
    "        plt.plot(times, y_vals, marker=\"o\", markersize=5, linestyle=\"-\", linewidth=1.5,\n",
    "                 color=DEFAULT_COLORS[i % len(DEFAULT_COLORS)], label=f\"Worker ID: {replica_id}\")\n",
    "\n",
    "    # Plot the average across all replicas with improved styling\n",
    "    if all_replica_metrics:\n",
    "        avg_vals = np.mean(all_replica_metrics, axis=0)\n",
    "        plt.plot(times, avg_vals, marker=\"s\", markersize=6, linestyle=\"-\", linewidth=2,\n",
    "                 color=\"black\", label=\"Average\")\n",
    "\n",
    "    # Format clean title\n",
    "    formatted_name = ALL_AVAILABLE_VLLM_METRICS[metric_name]\n",
    "\n",
    "    # Set axis labels\n",
    "    plt.xlabel(\"Time (seconds)\")\n",
    "    plt.ylabel(formatted_name)\n",
    "\n",
    "    plot_title = f\"vLLM Metric: {formatted_name}\"\n",
    "    if title:\n",
    "        plot_title += f\" â€“ {title}\"\n",
    "    plt.title(plot_title, fontsize=14, pad=10)\n",
    "\n",
    "    # Parse metric type and axis settings\n",
    "    metric_type = VLLM_METRIC_TYPES.get(metric_name, \"unknown\")\n",
    "    y_axis_limits = VLLM_METRIC_SPECIAL_Y_AXIS_LIMITS.get(metric_name)\n",
    "\n",
    "    # Set y-axis limits\n",
    "    if y_axis_limits:\n",
    "        plt.ylim(y_axis_limits)\n",
    "        if y_axis_limits == [0, 1]:\n",
    "            plt.yticks(np.linspace(0, 1, num=11))\n",
    "    elif metric_type == \"counter\":\n",
    "        plt.ylim(bottom=0)\n",
    "\n",
    "    # Legend header: always include metric type\n",
    "    info_lines = [Line2D([], [], color=\"none\", label=f\"*Metric type: {metric_type}\")]\n",
    "\n",
    "    # Add window info only if averaging is enabled\n",
    "    if is_histogram:\n",
    "        if window_sec is not None:\n",
    "            window_label = f\"*Averaged over window={window_sec} seconds\"\n",
    "        else:\n",
    "            window_label = \"*Averaged over window=infinity\"\n",
    "        info_lines.append(Line2D([], [], color=\"none\", label=window_label))\n",
    "\n",
    "    # Combine dummy + real legend entries\n",
    "    real_lines, real_labels = plt.gca().get_legend_handles_labels()\n",
    "    plt.legend(\n",
    "        handles=real_lines+info_lines,\n",
    "        loc=\"upper left\",\n",
    "        bbox_to_anchor=(1.01, 1.0),\n",
    "        borderaxespad=0.\n",
    "    )\n",
    "\n",
    "    # Use plain style for y-axis (no scientific notation)\n",
    "    plt.ticklabel_format(axis=\"y\", style=\"plain\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(right=0.75)\n",
    "\n",
    "    return plt.gcf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_all_vllm_metrics_timeseries(directory, title, metric):\n",
    "    # Get all JSON files in the directory\n",
    "    json_files = glob.glob(os.path.join(directory, \"*.json\"))\n",
    "\n",
    "    # Create a dictionary to group files by strategy\n",
    "    strategy_files = {}\n",
    "    for file_path in json_files:\n",
    "        filename = os.path.basename(file_path)\n",
    "        strategy_prefix = filename.split(\"_\")[0]  # Get the part before the first underscore\n",
    "\n",
    "        if strategy_prefix not in strategy_files:\n",
    "            strategy_files[strategy_prefix] = []\n",
    "\n",
    "        strategy_files[strategy_prefix].append(file_path)\n",
    "\n",
    "    # Sort each group of files\n",
    "    for strategy in strategy_files:\n",
    "        strategy_files[strategy] = sorted(strategy_files[strategy])\n",
    "\n",
    "    # Process files in order of STRATEGY_PREFIXES, then by filename\n",
    "    for strategy_prefix in STRATEGY_PREFIXES.keys():\n",
    "        if strategy_prefix in strategy_files:\n",
    "            for file_path in strategy_files[strategy_prefix]:\n",
    "                if file_path.strip():  # Skip empty file paths\n",
    "                    # Get the full strategy name or use the prefix if not found\n",
    "                    strategy_name = STRATEGY_PREFIXES.get(strategy_prefix, strategy_prefix.capitalize())\n",
    "                    # For each file, plot the specified metrics\n",
    "                    file_name = os.path.basename(file_path)\n",
    "                    plot_title = f\"{strategy_name} Strategy - {file_name}\"\n",
    "                    plot_title += f\" - {title}\" if title else \"\"\n",
    "                    try:\n",
    "                        plot_vllm_metric_timeseries(file_path, metric, title=plot_title)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error plotting {metric} for {file_path}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_timing_data(json_file_path):\n",
    "    \"\"\"Parse JSONL timing data and calculate durations\"\"\"\n",
    "    # Define the stages as they appear in the logs\n",
    "    phase_definitions = [\n",
    "        {\"name\": \"prefix_match_router_to_deployment\", \"start\": \"before_calling_prefix_match\", \"end\": \"received_by_prefix_match\"},\n",
    "        {\"name\": \"prefix_match_processing\", \"start\": \"received_by_prefix_match\", \"end\": \"completed_prefix_match\"},\n",
    "        {\"name\": \"prefix_match_deployment_to_router\", \"start\": \"completed_prefix_match\", \"end\": \"after_calling_prefix_match\"},\n",
    "        {\"name\": \"prefix_match_total\", \"start\": \"before_calling_prefix_match\", \"end\": \"after_calling_prefix_match\"},\n",
    "        {\"name\": \"insert_router_to_deployment\", \"start\": \"before_calling_insert\", \"end\": \"received_by_insert\"},\n",
    "        {\"name\": \"insert_processing\", \"start\": \"received_by_insert\", \"end\": \"completed_insert\"},\n",
    "        {\"name\": \"insert_deployment_to_router\", \"start\": \"completed_insert\", \"end\": \"after_calling_insert\"},\n",
    "        {\"name\": \"insert_total\", \"start\": \"before_calling_insert\", \"end\": \"after_calling_insert\"},\n",
    "        {\"name\": \"total_request_time\", \"start\": \"before_calling_prefix_match\", \"end\": \"after_calling_insert\"},\n",
    "    ]\n",
    "\n",
    "    # Read the data directly since each line contains all stages for a request\n",
    "    request_data = []\n",
    "    with open(json_file_path, \"r\") as f:\n",
    "        for line in f:\n",
    "            entry = json.loads(line.strip())\n",
    "            request_data.append(entry)\n",
    "\n",
    "    # Calculate durations for each phase\n",
    "    durations = {phase[\"name\"]: [] for phase in phase_definitions}\n",
    "    for entry in request_data:\n",
    "        for phase in phase_definitions:\n",
    "            start, end = phase[\"start\"], phase[\"end\"]\n",
    "            if start in entry and end in entry:\n",
    "                durations[phase[\"name\"]].append(entry[end] - entry[start])\n",
    "\n",
    "    return durations, request_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_phase_durations(durations, title=None):\n",
    "    stage_descriptions = {\n",
    "        \"prefix_match_router_to_deployment\": \"Router to Deployment\",\n",
    "        \"prefix_match_processing\": \"Processing\",\n",
    "        \"prefix_match_deployment_to_router\": \"Deployment to Router\",\n",
    "        \"prefix_match_total\": \"Total Round Trip\",\n",
    "        \"insert_router_to_deployment\": \"Router to Deployment\",\n",
    "        \"insert_processing\": \"Processing\",\n",
    "        \"insert_deployment_to_router\": \"Deployment to Router\",\n",
    "        \"insert_total\": \"Total Round Trip\",\n",
    "        \"total_request_time\": \"Complete Operation\"\n",
    "    }\n",
    "\n",
    "    # Detailed chart\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    detailed_phases = [\n",
    "        \"prefix_match_router_to_deployment\", \"prefix_match_processing\", \"prefix_match_deployment_to_router\",\n",
    "        \"insert_router_to_deployment\", \"insert_processing\", \"insert_deployment_to_router\"\n",
    "    ]\n",
    "    detailed_labels = [stage_descriptions[p] + f\" ({p.split('_')[0].capitalize()})\" for p in detailed_phases]\n",
    "    detailed_durations = [np.mean(durations[p]) * 1000 for p in detailed_phases if p in durations and durations[p]]\n",
    "\n",
    "    # Calculate the total as sum of all other bars\n",
    "    total_duration = sum(detailed_durations)\n",
    "    detailed_labels.append(\"Total\")\n",
    "    detailed_durations.append(total_duration)\n",
    "\n",
    "    # Colors for the bars - blue for prefix match, orange for insert, green for total\n",
    "    colors = [\"#1f77b4\"] * 3 + [\"#ff7f0e\"] * 3 + [\"#2ca02c\"]\n",
    "\n",
    "    # Get number of requests\n",
    "    num_requests = len(next(iter(durations.values()))) if durations else 0\n",
    "\n",
    "    bars = plt.bar(detailed_labels, detailed_durations, color=colors)\n",
    "    for bar, duration in zip(bars, detailed_durations):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.05,\n",
    "                 f\"{duration:.2f} ms\", ha=\"center\")\n",
    "\n",
    "    plt.title(f'Deployment Overhead Breakdown ({num_requests} requests){\" - \" + title if title else \"\"}')\n",
    "    plt.ylabel(\"Duration (ms)\")\n",
    "    plt.xticks(rotation=15)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_deployment_overhead(json_file_path, title=None):\n",
    "    # Parse and visualize the data\n",
    "    durations, raw_data = parse_timing_data(json_file_path)\n",
    "    plot_phase_durations(durations, title=title)\n",
    "\n",
    "def plot_all_deployment_overhead(directory):\n",
    "    import os\n",
    "    import glob\n",
    "\n",
    "    # Get all timing files in the deployment_overhead directory\n",
    "    timing_files = glob.glob(os.path.join(directory, \"*.jsonl\"))\n",
    "\n",
    "    # Process each file\n",
    "    for timing_file in timing_files:\n",
    "        # Extract the filename without path and extension for the title\n",
    "        filename = os.path.basename(timing_file)\n",
    "        title = filename.replace(\".jsonl\", \"\").replace(\"_\", \" \").title()\n",
    "\n",
    "        # print(f\"Processing: {filename}\")\n",
    "\n",
    "        # Parse and visualize the data\n",
    "        durations, raw_data = parse_timing_data(timing_file)\n",
    "        plot_phase_durations(durations, title=title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_scheduling_matches(title, log_file):\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(log_file)\n",
    "\n",
    "    # Print the title\n",
    "    print(title)\n",
    "\n",
    "    # Group by scheduling_decision and count matches\n",
    "    for decision, group in df.groupby(\"scheduling_decision\"):\n",
    "        matches = sum(group[\"original_request_id\"] == group[\"matched_replica_request_id\"])\n",
    "        total = len(group)\n",
    "        print(f\"{decision}: {matches}/{total} matches\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_queue_overhead(file_path):\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # Extract the filename for the title\n",
    "    filename = os.path.basename(file_path)\n",
    "    title = filename.replace(\".csv\", \"\").replace(\"_\", \" \").title()\n",
    "\n",
    "    # Create figure with 2 subplots\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "    # Plot 1: Histogram of num_searched\n",
    "    ax1.hist(df[\"num_searched\"], bins=max(10, len(df[\"num_searched\"].unique())), alpha=0.7)\n",
    "    ax1.set_title(f\"{title} - Distribution of Search Length\")\n",
    "    ax1.set_xlabel(\"Number of Items Searched\")\n",
    "    ax1.set_ylabel(\"Frequency\")\n",
    "    ax1.set_yscale(\"log\")  # Set y-axis to log scale\n",
    "\n",
    "    # Plot 2: Bar chart of average search_duration for each num_searched\n",
    "    avg_duration = df.groupby(\"num_searched\")[\"search_duration\"].mean()\n",
    "    # Convert to milliseconds\n",
    "    avg_duration_ms = avg_duration * 1000\n",
    "    ax2.bar(avg_duration_ms.index, avg_duration_ms.values)\n",
    "    ax2.set_title(f\"{title} - Average Search Duration by Length\")\n",
    "    ax2.set_xlabel(\"Number of Items Searched\")\n",
    "    ax2.set_ylabel(\"Average Duration (milliseconds)\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Print statistics\n",
    "    avg_searched = df[\"num_searched\"].mean()\n",
    "    avg_duration = df[\"search_duration\"].mean() * 1000  # Convert to milliseconds\n",
    "    total_duration = df[\"search_duration\"].sum() * 1000  # Convert to milliseconds\n",
    "\n",
    "    print(f\"Average num_searched: {avg_searched:.2f}\")\n",
    "    print(f\"Average search duration: {avg_duration:.4f} milliseconds\")\n",
    "    print(f\"Total search duration: {total_duration:.4f} milliseconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_character_count_over_time(file_path):\n",
    "    import os\n",
    "    import json\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    # Read the JSON file\n",
    "    with open(file_path, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Initialize empty DataFrame\n",
    "    df_list = []\n",
    "\n",
    "    # Extract timestamps and char counts for each replica\n",
    "    for timestamp, replica_data in data.items():\n",
    "        for replica_id, char_count in replica_data.items():\n",
    "            # Extract a shorter, cleaner name from the replica ID\n",
    "            clean_name = replica_id.split(\"#\")[-1]  # Get the last part after the last #\n",
    "            df_list.append({\"timestamp\": float(timestamp), \"replica\": clean_name, \"char_count\": char_count})\n",
    "\n",
    "    # Create and sort DataFrame\n",
    "    df = pd.DataFrame(df_list).sort_values(\"timestamp\")\n",
    "\n",
    "    # Create plot\n",
    "    plt.figure(figsize=(9.2, 6))\n",
    "\n",
    "    # Plot each replica's character count\n",
    "    for replica, group in df.groupby(\"replica\"):\n",
    "        plt.plot(group[\"timestamp\"], group[\"char_count\"], marker=\"o\", linestyle=\"-\", label=replica)\n",
    "\n",
    "    # Add labels and title\n",
    "    plt.xlabel(\"Time (seconds)\")\n",
    "    plt.ylabel(\"Character Count\")\n",
    "    plt.title(f\"Character Count Over Time - {os.path.basename(file_path)}\")\n",
    "\n",
    "    # Add grid and legend\n",
    "    plt.grid(True, linestyle=\"--\", alpha=0.7)\n",
    "    plt.legend()\n",
    "    plt.ticklabel_format(axis=\"y\", style=\"plain\")\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_overlay_metrics(char_count_file_path, vllm_metrics_file_path, eviction_threshold_chars, eviction_target_chars, interval_secs, title):\n",
    "    import json\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "\n",
    "    # Read the character count JSON file\n",
    "    with open(char_count_file_path, \"r\") as f:\n",
    "        char_count_data = json.load(f)\n",
    "\n",
    "    # Read the vLLM metrics JSON file\n",
    "    with open(vllm_metrics_file_path, \"r\") as f:\n",
    "        vllm_metrics_data = json.load(f)\n",
    "\n",
    "    # Process character count data\n",
    "    char_count_df_list = []\n",
    "    for timestamp, replica_data in char_count_data.items():\n",
    "        for replica_id, char_count in replica_data.items():\n",
    "            char_count_df_list.append({\n",
    "                \"timestamp\": float(timestamp),\n",
    "                \"replica\": replica_id,\n",
    "                \"char_count\": char_count\n",
    "            })\n",
    "\n",
    "    char_count_df = pd.DataFrame(char_count_df_list)\n",
    "    char_count_df = char_count_df.sort_values(\"timestamp\")\n",
    "\n",
    "    # Normalize character count by dividing by 4 to approximate tokens\n",
    "    char_count_df[\"approx_tokens\"] = char_count_df[\"char_count\"] / 4\n",
    "\n",
    "    # Find the first timestamp where character count becomes non-zero\n",
    "    first_nonzero_time = char_count_df[char_count_df[\"approx_tokens\"] > 0][\"timestamp\"].min()\n",
    "\n",
    "    # Process vLLM metrics data\n",
    "    metrics_df_list = []\n",
    "\n",
    "    for timestamp in sorted([float(t) for t in vllm_metrics_data.keys()]):\n",
    "        timestamp_str = str(timestamp)\n",
    "        snapshot = vllm_metrics_data[timestamp_str]\n",
    "\n",
    "        for replica_id, metrics in snapshot.items():\n",
    "            # Extract metrics for each replica\n",
    "            prompt_tokens = next((v for k, v in metrics.items()\n",
    "                                if \"ray_vllm:prompt_tokens\" in k), 0)\n",
    "            generation_tokens = next((v for k, v in metrics.items()\n",
    "                                    if \"ray_vllm:generation_tokens\" in k), 0)\n",
    "            gpu_cache = next((v for k, v in metrics.items()\n",
    "                            if \"ray_vllm:gpu_cache_usage_perc\" in k), 0)\n",
    "\n",
    "            # Only include metrics after the first non-zero character count\n",
    "            if timestamp >= first_nonzero_time:\n",
    "                metrics_df_list.append({\n",
    "                    \"timestamp\": timestamp,\n",
    "                    \"replica\": replica_id,\n",
    "                    \"prompt_tokens\": prompt_tokens,\n",
    "                    \"generation_tokens\": generation_tokens,\n",
    "                    \"gpu_cache_usage\": gpu_cache\n",
    "                })\n",
    "\n",
    "    metrics_df = pd.DataFrame(metrics_df_list)\n",
    "    metrics_df[\"total_tokens\"] = metrics_df[\"prompt_tokens\"] + metrics_df[\"generation_tokens\"]\n",
    "\n",
    "    # Create figure with three y-axes\n",
    "    fig, ax1 = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "    # Set up colors\n",
    "    color1 = \"tab:blue\"\n",
    "    color2 = \"tab:red\"\n",
    "    color3 = \"tab:green\"\n",
    "    color4 = \"tab:purple\"\n",
    "    color5 = \"tab:orange\"\n",
    "\n",
    "    # Plot approximate tokens for each replica\n",
    "    ax1.set_xlabel(\"Time (seconds)\")\n",
    "    ax1.set_ylabel(\"Tokens\", color=color1)\n",
    "\n",
    "    # Create custom legend handles\n",
    "    tree_line = ax1.plot([], [], color=color1, marker=\"o\", markersize=3, linestyle=\"-\", label=\"Tokens stored in Tree\")[0]\n",
    "    total_line = ax1.plot([], [], color=color2, marker=\"s\", markersize=3, linestyle=\"-\", label=\"Total Tokens Processed\")[0]\n",
    "    gpu_line = ax1.plot([], [], color=color3, marker=\"^\", markersize=3, linestyle=\"-\", label=\"GPU Cache\")[0]\n",
    "\n",
    "    # Plot actual data with alpha but no labels\n",
    "    for replica, group in char_count_df.groupby(\"replica\"):\n",
    "        ax1.plot(group[\"timestamp\"], group[\"approx_tokens\"],\n",
    "                marker=\"o\", markersize=3, linestyle=\"-\", color=color1, alpha=0.5)\n",
    "\n",
    "    for replica, group in metrics_df.groupby(\"replica\"):\n",
    "        ax1.plot(group[\"timestamp\"], group[\"total_tokens\"],\n",
    "                marker=\"s\", markersize=3, linestyle=\"-\", color=color2, alpha=0.5)\n",
    "\n",
    "    ax1.tick_params(axis=\"y\", labelcolor=color1)\n",
    "\n",
    "    # Plot eviction threshold and target lines\n",
    "    x_min = char_count_df[\"timestamp\"].min()\n",
    "    x_max = char_count_df[\"timestamp\"].max()\n",
    "    threshold_tokens = eviction_threshold_chars / 4\n",
    "    target_tokens = eviction_target_chars / 4\n",
    "\n",
    "    # Plot threshold and target lines with increased visibility\n",
    "    ax1.axhline(y=threshold_tokens, color=color4, linestyle=\"--\",\n",
    "                               linewidth=2, alpha=0.8)\n",
    "    ax1.axhline(y=target_tokens, color=color5, linestyle=\"--\",\n",
    "                            linewidth=2, alpha=0.8)\n",
    "\n",
    "    # Add labels on the left side of the graph\n",
    "    ax1.text(x_min, threshold_tokens + 2000, f\"Eviction Threshold\\n{threshold_tokens:,.0f} tokens\",\n",
    "             color=color4, va=\"bottom\", ha=\"left\")\n",
    "    ax1.text(x_min, target_tokens - 2000, f\"Eviction Target\\n{target_tokens:,.0f} tokens\",\n",
    "             color=color5, va=\"top\", ha=\"left\")\n",
    "\n",
    "    # Create third y-axis for GPU cache usage\n",
    "    ax3 = ax1.twinx()\n",
    "    ax3.set_ylabel(\"GPU Cache Usage (%)\", color=color3)\n",
    "\n",
    "    for replica, group in metrics_df.groupby(\"replica\"):\n",
    "        ax3.plot(group[\"timestamp\"], group[\"gpu_cache_usage\"] * 100,\n",
    "                marker=\"^\", markersize=3, linestyle=\"-\", color=color3, alpha=0.5)\n",
    "\n",
    "    ax3.tick_params(axis=\"y\", labelcolor=color3)\n",
    "    ax3.set_ylim(0, 100)  # Set GPU cache usage scale from 0-100%\n",
    "\n",
    "    # Set y-axis minimum to 0 for tokens\n",
    "    ax1.set_ylim(bottom=0)\n",
    "\n",
    "    # Add grid\n",
    "    ax1.grid(True, linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "    # Add legend with custom handles\n",
    "    ax1.legend(handles=[total_line, tree_line, gpu_line], loc=\"upper left\")\n",
    "\n",
    "    # Format y-axes with plain style (no scientific notation)\n",
    "    ax1.ticklabel_format(axis=\"y\", style=\"plain\")\n",
    "\n",
    "    # Set x-axis ticks to show every 10 seconds with minor ticks every second\n",
    "    # Start at first multiple of 5 greater than x_min\n",
    "    start_tick = np.floor(x_min / 5) * 5\n",
    "    ax1.set_xticks(np.arange(start_tick, x_max + 1, 5))\n",
    "    ax1.set_xticks(np.arange(start_tick, x_max + 1, 1), minor=True)\n",
    "\n",
    "    # Extract filenames for the plot title\n",
    "    plt.title(f\"Eviction Policy: {title}\\nThreshold = {eviction_threshold_chars/4:,.0f} tokens, Target = {eviction_target_chars/4:,.0f} tokens, Interval = {interval_secs} seconds\")\n",
    "\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_benchmark_comparison(csv_path: str, title: str = None):\n",
    "    \"\"\"\n",
    "    Plot benchmark metrics comparison between pow2 and prefix-aware strategies.\n",
    "    Each metric's bars are scaled relative to pow2's height, with actual values labeled.\n",
    "    Args:\n",
    "        csv_path: Path to CSV containing benchmark results\n",
    "        title: Optional title for the plot\n",
    "    \"\"\"\n",
    "    # Read the data\n",
    "    df = pd.read_csv(csv_path)\n",
    "    df = df[df[\"gpu_type\"] != \"gpu_type\"]\n",
    "\n",
    "    # Calculate mean metrics for each strategy\n",
    "    metrics = [\"mean_ttft_ms\", \"mean_tpot_ms\", \"mean_e2e_latency_ms\"]  # Reordered metrics\n",
    "    strategy_means = df.groupby(\"scheduler_strategy\")[metrics].mean()\n",
    "\n",
    "    # Set up the plot\n",
    "    fig, ax = plt.subplots(figsize=(10, 4.5))\n",
    "\n",
    "    # Set width of bars\n",
    "    barWidth = 0.25\n",
    "\n",
    "    # Set positions of the bars on X axis\n",
    "    r1 = np.arange(len(metrics))\n",
    "    r2 = [x + barWidth for x in r1]\n",
    "\n",
    "    # Get pow2 values and calculate ratios for prefix-aware\n",
    "    pow2_values = strategy_means.loc[\"pow_of_2\"]\n",
    "    prefix_values = strategy_means.loc[\"prefix_aware\"]\n",
    "    ratios = prefix_values / pow2_values\n",
    "\n",
    "    # Create bars - pow2 all same height (1.0)\n",
    "    ax.bar(r1, [1.0] * len(metrics), width=barWidth, label=\"Power-of-two\", color=\"#2ecc71\")\n",
    "    # Prefix-aware bars scaled relative to pow2\n",
    "    ax.bar(r2, ratios, width=barWidth, label=\"Prefix-Aware\", color=\"#3498db\")\n",
    "\n",
    "    # Add value labels on top of bars\n",
    "    for i, (pow2_val, prefix_val) in enumerate(zip(pow2_values, prefix_values)):\n",
    "        # All metrics are in milliseconds\n",
    "        pow2_label = f\"{pow2_val:.0f} ms\"\n",
    "        prefix_label = f\"{prefix_val:.0f} ms\"\n",
    "\n",
    "        ax.text(r1[i], 1.02, pow2_label, ha=\"center\", va=\"bottom\", fontsize=10)  # Smaller font for measurements\n",
    "        ax.text(r2[i], ratios[i] + 0.02, prefix_label, ha=\"center\", va=\"bottom\", fontsize=10)  # Smaller font for measurements\n",
    "\n",
    "    # Add labels and title\n",
    "    # ax.set_xlabel('Metrics')\n",
    "    if title:\n",
    "        ax.set_title(title, fontsize=14)\n",
    "\n",
    "    # Add xticks on the middle of the group bars\n",
    "    ax.set_xticks([r + barWidth/2 for r in range(len(metrics))])\n",
    "    ax.set_xticklabels([\"Avg Time-to-first-token\", \"Avg Time-per-output-token\", \"Avg End-to-end Latency\"], fontsize=12)  # Bigger font for labels\n",
    "\n",
    "    # Add legend underneath title\n",
    "    ax.legend(\n",
    "        loc=\"upper center\",\n",
    "        bbox_to_anchor=(0.5, 1.00),  # Adjust this for exact vertical spacing\n",
    "        ncol=2,  # Puts legend entries side-by-side\n",
    "        frameon=False,  # Optional: remove border around legend\n",
    "        fontsize=14,  # Make legend text bigger\n",
    "        handleheight=1.0,  # Make legend markers square\n",
    "        handlelength=1.0  # Make legend markers square\n",
    "    )\n",
    "    # Remove y-axis since we're using relative heights\n",
    "    ax.set_yticks([])\n",
    "\n",
    "    # Set y-axis limits to accommodate labels\n",
    "    ax.set_ylim(0, 1.2)\n",
    "\n",
    "    # Remove all spines (borders)\n",
    "    for spine in ax.spines.values():\n",
    "        spine.set_visible(False)\n",
    "\n",
    "    # Turn off vertical gridlines and ticks\n",
    "    ax.grid(False, axis=\"x\")  # Disable x-axis gridlines\n",
    "    ax.tick_params(axis=\"x\", length=0)  # Remove x-axis tick marks\n",
    "    ax.tick_params(axis=\"y\", left=False)  # Remove y-axis tick marks\n",
    "\n",
    "    plt.tight_layout(pad=2.0)\n",
    "\n",
    "    plt.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
