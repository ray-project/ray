2025-05-30 14:25:59,671	INFO scripts.py:497 -- Running config file: 'temp_config.yaml'.
2025-05-30 14:26:03,648	SUCC scripts.py:548 -- [32mSubmitted deploy config successfully.[39m
[36m(build_serve_application pid=49278)[0m INFO 05-30 14:26:08 [__init__.py:239] Automatically detected platform cuda.
[36m(ServeReplica:llm_app:LLMRouter pid=49466)[0m INFO 05-30 14:26:15 [__init__.py:239] Automatically detected platform cuda.
[36m(ServeReplica:llm_app:LLMRouter pid=49463)[0m INFO 05-30 14:26:16 [__init__.py:239] Automatically detected platform cuda.
[36m(ServeReplica:llm_app:LLMRouter pid=49460)[0m INFO 05-30 14:26:16 [__init__.py:239] Automatically detected platform cuda.
[36m(ServeReplica:llm_app:LLMRouter pid=49465)[0m INFO 05-30 14:26:16 [__init__.py:239] Automatically detected platform cuda.
[36m(ServeReplica:llm_app:LLMDeploymentQwen--Qwen2_5-1_5B-Instruct pid=49458)[0m INFO 05-30 14:26:16 [__init__.py:239] Automatically detected platform cuda.
[36m(ServeReplica:llm_app:LLMDeploymentQwen--Qwen2_5-1_5B-Instruct pid=49459)[0m INFO 05-30 14:26:16 [__init__.py:239] Automatically detected platform cuda.
[36m(ServeReplica:llm_app:LLMRouter pid=49461)[0m INFO 05-30 14:26:16 [__init__.py:239] Automatically detected platform cuda.
[36m(ServeReplica:llm_app:LLMRouter pid=49467)[0m INFO 05-30 14:26:16 [__init__.py:239] Automatically detected platform cuda.
[36m(ServeReplica:llm_app:LLMDeploymentQwen--Qwen2_5-1_5B-Instruct pid=49457)[0m INFO 05-30 14:26:16 [__init__.py:239] Automatically detected platform cuda.
[36m(ServeReplica:llm_app:LLMRouter pid=49464)[0m INFO 05-30 14:26:16 [__init__.py:239] Automatically detected platform cuda.
[36m(ServeReplica:llm_app:LLMRouter pid=49462)[0m INFO 05-30 14:26:16 [__init__.py:239] Automatically detected platform cuda.
[36m(ServeReplica:llm_app:LLMDeploymentQwen--Qwen2_5-1_5B-Instruct pid=49456)[0m INFO 05-30 14:26:16 [__init__.py:239] Automatically detected platform cuda.
[36m(pid=50673)[0m INFO 05-30 14:26:25 [__init__.py:239] Automatically detected platform cuda.
[36m(pid=50675)[0m INFO 05-30 14:26:25 [__init__.py:239] Automatically detected platform cuda.
[36m(pid=50674)[0m INFO 05-30 14:26:25 [__init__.py:239] Automatically detected platform cuda.
[36m(pid=50677)[0m INFO 05-30 14:26:25 [__init__.py:239] Automatically detected platform cuda.
[36m(_get_vllm_engine_config pid=50673)[0m INFO 05-30 14:26:36 [config.py:717] This model supports multiple tasks: {'embed', 'score', 'generate', 'reward', 'classify'}. Defaulting to 'generate'.
[36m(_get_vllm_engine_config pid=50673)[0m INFO 05-30 14:26:36 [config.py:2003] Chunked prefill is enabled with max_num_batched_tokens=2048.
[36m(_get_vllm_engine_config pid=50675)[0m INFO 05-30 14:26:36 [config.py:717] This model supports multiple tasks: {'classify', 'generate', 'reward', 'score', 'embed'}. Defaulting to 'generate'.
[36m(_get_vllm_engine_config pid=50675)[0m INFO 05-30 14:26:36 [config.py:2003] Chunked prefill is enabled with max_num_batched_tokens=2048.
[36m(_get_vllm_engine_config pid=50674)[0m INFO 05-30 14:26:36 [config.py:717] This model supports multiple tasks: {'score', 'reward', 'embed', 'classify', 'generate'}. Defaulting to 'generate'.
[36m(_get_vllm_engine_config pid=50674)[0m INFO 05-30 14:26:36 [config.py:2003] Chunked prefill is enabled with max_num_batched_tokens=2048.
[36m(_get_vllm_engine_config pid=50677)[0m INFO 05-30 14:26:36 [config.py:717] This model supports multiple tasks: {'embed', 'reward', 'classify', 'generate', 'score'}. Defaulting to 'generate'.
[36m(_get_vllm_engine_config pid=50677)[0m INFO 05-30 14:26:36 [config.py:2003] Chunked prefill is enabled with max_num_batched_tokens=2048.
[36m(pid=51088)[0m INFO 05-30 14:26:41 [__init__.py:239] Automatically detected platform cuda.
[36m(pid=51090)[0m INFO 05-30 14:26:41 [__init__.py:239] Automatically detected platform cuda.
[36m(pid=51089)[0m INFO 05-30 14:26:41 [__init__.py:239] Automatically detected platform cuda.
[36m(pid=51178)[0m INFO 05-30 14:26:42 [__init__.py:239] Automatically detected platform cuda.
[36m(_EngineBackgroundProcess pid=51088)[0m INFO 05-30 14:26:43 [llm_engine.py:240] Initializing a V0 LLM engine (v0.8.5) with config: model='Qwen/Qwen2.5-1.5B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2.5-1.5B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=Qwen/Qwen2.5-1.5B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=True, 
[36m(_EngineBackgroundProcess pid=51090)[0m INFO 05-30 14:26:43 [llm_engine.py:240] Initializing a V0 LLM engine (v0.8.5) with config: model='Qwen/Qwen2.5-1.5B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2.5-1.5B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=Qwen/Qwen2.5-1.5B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=True, 
[36m(_EngineBackgroundProcess pid=51089)[0m INFO 05-30 14:26:43 [llm_engine.py:240] Initializing a V0 LLM engine (v0.8.5) with config: model='Qwen/Qwen2.5-1.5B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2.5-1.5B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=Qwen/Qwen2.5-1.5B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=True, 
[36m(_EngineBackgroundProcess pid=51088)[0m INFO 05-30 14:26:43 [ray_utils.py:288] Ray is already initialized. Skipping Ray initialization.
[36m(_EngineBackgroundProcess pid=51088)[0m INFO 05-30 14:26:43 [ray_utils.py:314] Using the existing placement group
[36m(_EngineBackgroundProcess pid=51088)[0m INFO 05-30 14:26:43 [ray_distributed_executor.py:176] use_ray_spmd_worker: False
[36m(_EngineBackgroundProcess pid=51089)[0m INFO 05-30 14:26:43 [ray_utils.py:288] Ray is already initialized. Skipping Ray initialization.
[36m(_EngineBackgroundProcess pid=51089)[0m INFO 05-30 14:26:43 [ray_utils.py:314] Using the existing placement group
[36m(_EngineBackgroundProcess pid=51089)[0m INFO 05-30 14:26:43 [ray_distributed_executor.py:176] use_ray_spmd_worker: False
[36m(_EngineBackgroundProcess pid=51090)[0m INFO 05-30 14:26:43 [ray_utils.py:288] Ray is already initialized. Skipping Ray initialization.
[36m(_EngineBackgroundProcess pid=51090)[0m INFO 05-30 14:26:43 [ray_utils.py:314] Using the existing placement group
[36m(_EngineBackgroundProcess pid=51090)[0m INFO 05-30 14:26:43 [ray_distributed_executor.py:176] use_ray_spmd_worker: False
[36m(_EngineBackgroundProcess pid=51178)[0m INFO 05-30 14:26:43 [llm_engine.py:240] Initializing a V0 LLM engine (v0.8.5) with config: model='Qwen/Qwen2.5-1.5B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2.5-1.5B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=Qwen/Qwen2.5-1.5B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=True, 
[36m(_EngineBackgroundProcess pid=51178)[0m INFO 05-30 14:26:44 [ray_utils.py:288] Ray is already initialized. Skipping Ray initialization.
[36m(_EngineBackgroundProcess pid=51178)[0m INFO 05-30 14:26:44 [ray_utils.py:314] Using the existing placement group
[36m(_EngineBackgroundProcess pid=51178)[0m INFO 05-30 14:26:44 [ray_distributed_executor.py:176] use_ray_spmd_worker: False
[36m(pid=51404)[0m INFO 05-30 14:26:47 [__init__.py:239] Automatically detected platform cuda.
[36m(pid=51412)[0m INFO 05-30 14:26:48 [__init__.py:239] Automatically detected platform cuda.
[36m(pid=51409)[0m INFO 05-30 14:26:48 [__init__.py:239] Automatically detected platform cuda.
[36m(pid=51494)[0m INFO 05-30 14:26:48 [__init__.py:239] Automatically detected platform cuda.
[36m(_EngineBackgroundProcess pid=51088)[0m INFO 05-30 14:26:49 [ray_distributed_executor.py:352] non_carry_over_env_vars from config: set()
[36m(_EngineBackgroundProcess pid=51088)[0m INFO 05-30 14:26:49 [ray_distributed_executor.py:354] Copying the following environment variables to workers: ['LD_LIBRARY_PATH', 'VLLM_USE_V1']
[36m(_EngineBackgroundProcess pid=51088)[0m INFO 05-30 14:26:49 [ray_distributed_executor.py:357] If certain env vars should NOT be copied to workers, add them to /home/ray/.config/vllm/ray_non_carry_over_env_vars.json file
[36m(_EngineBackgroundProcess pid=51089)[0m INFO 05-30 14:26:50 [ray_distributed_executor.py:352] non_carry_over_env_vars from config: set()
[36m(_EngineBackgroundProcess pid=51089)[0m INFO 05-30 14:26:50 [ray_distributed_executor.py:354] Copying the following environment variables to workers: ['LD_LIBRARY_PATH', 'VLLM_USE_V1']
[36m(_EngineBackgroundProcess pid=51089)[0m INFO 05-30 14:26:50 [ray_distributed_executor.py:357] If certain env vars should NOT be copied to workers, add them to /home/ray/.config/vllm/ray_non_carry_over_env_vars.json file
[36m(_EngineBackgroundProcess pid=51090)[0m INFO 05-30 14:26:50 [ray_distributed_executor.py:352] non_carry_over_env_vars from config: set()
[36m(_EngineBackgroundProcess pid=51090)[0m INFO 05-30 14:26:50 [ray_distributed_executor.py:354] Copying the following environment variables to workers: ['LD_LIBRARY_PATH', 'VLLM_USE_V1']
[36m(_EngineBackgroundProcess pid=51090)[0m INFO 05-30 14:26:50 [ray_distributed_executor.py:357] If certain env vars should NOT be copied to workers, add them to /home/ray/.config/vllm/ray_non_carry_over_env_vars.json file
[36m(_EngineBackgroundProcess pid=51178)[0m INFO 05-30 14:26:50 [ray_distributed_executor.py:352] non_carry_over_env_vars from config: set()
[36m(_EngineBackgroundProcess pid=51178)[0m INFO 05-30 14:26:50 [ray_distributed_executor.py:354] Copying the following environment variables to workers: ['LD_LIBRARY_PATH', 'VLLM_USE_V1']
[36m(_EngineBackgroundProcess pid=51178)[0m INFO 05-30 14:26:50 [ray_distributed_executor.py:357] If certain env vars should NOT be copied to workers, add them to /home/ray/.config/vllm/ray_non_carry_over_env_vars.json file
[36m(_EngineBackgroundProcess pid=51088)[0m INFO 05-30 14:26:50 [cuda.py:292] Using Flash Attention backend.
[36m(_EngineBackgroundProcess pid=51089)[0m INFO 05-30 14:26:51 [cuda.py:292] Using Flash Attention backend.
[36m(_EngineBackgroundProcess pid=51090)[0m INFO 05-30 14:26:51 [cuda.py:292] Using Flash Attention backend.
[36m(_EngineBackgroundProcess pid=51088)[0m INFO 05-30 14:26:51 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0
[36m(_EngineBackgroundProcess pid=51088)[0m INFO 05-30 14:26:51 [model_runner.py:1108] Starting to load model Qwen/Qwen2.5-1.5B-Instruct...
[36m(_EngineBackgroundProcess pid=51178)[0m INFO 05-30 14:26:51 [cuda.py:292] Using Flash Attention backend.
[36m(_EngineBackgroundProcess pid=51089)[0m INFO 05-30 14:26:52 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0
[36m(_EngineBackgroundProcess pid=51089)[0m INFO 05-30 14:26:52 [model_runner.py:1108] Starting to load model Qwen/Qwen2.5-1.5B-Instruct...
[36m(_EngineBackgroundProcess pid=51090)[0m INFO 05-30 14:26:52 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0
[36m(_EngineBackgroundProcess pid=51090)[0m INFO 05-30 14:26:52 [model_runner.py:1108] Starting to load model Qwen/Qwen2.5-1.5B-Instruct...
[36m(_EngineBackgroundProcess pid=51088)[0m INFO 05-30 14:26:52 [weight_utils.py:265] Using model weights format ['*.safetensors']
[36m(_EngineBackgroundProcess pid=51089)[0m INFO 05-30 14:26:52 [weight_utils.py:265] Using model weights format ['*.safetensors']
[36m(_EngineBackgroundProcess pid=51090)[0m INFO 05-30 14:26:52 [weight_utils.py:265] Using model weights format ['*.safetensors']
[36m(_EngineBackgroundProcess pid=51178)[0m INFO 05-30 14:26:52 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0
[36m(_EngineBackgroundProcess pid=51178)[0m INFO 05-30 14:26:52 [model_runner.py:1108] Starting to load model Qwen/Qwen2.5-1.5B-Instruct...
[36m(_EngineBackgroundProcess pid=51089)[0m INFO 05-30 14:26:52 [weight_utils.py:315] No model.safetensors.index.json found in remote.
[36m(_EngineBackgroundProcess pid=51178)[0m INFO 05-30 14:26:52 [weight_utils.py:265] Using model weights format ['*.safetensors']
[36m(_EngineBackgroundProcess pid=51088)[0m INFO 05-30 14:26:52 [weight_utils.py:315] No model.safetensors.index.json found in remote.
[36m(_EngineBackgroundProcess pid=51090)[0m INFO 05-30 14:26:52 [weight_utils.py:315] No model.safetensors.index.json found in remote.
[36m(_EngineBackgroundProcess pid=51089)[0m INFO 05-30 14:26:53 [loader.py:458] Loading weights took 0.49 seconds
[36m(_EngineBackgroundProcess pid=51088)[0m INFO 05-30 14:26:53 [loader.py:458] Loading weights took 0.48 seconds
[36m(_EngineBackgroundProcess pid=51089)[0m INFO 05-30 14:26:53 [model_runner.py:1140] Model loading took 2.8876 GiB and 0.988574 seconds
[36m(_EngineBackgroundProcess pid=51090)[0m INFO 05-30 14:26:53 [loader.py:458] Loading weights took 0.47 seconds
[36m(_EngineBackgroundProcess pid=51178)[0m INFO 05-30 14:26:53 [weight_utils.py:315] No model.safetensors.index.json found in remote.
[36m(_EngineBackgroundProcess pid=51088)[0m INFO 05-30 14:26:53 [model_runner.py:1140] Model loading took 2.8876 GiB and 1.558129 seconds
[36m(_EngineBackgroundProcess pid=51090)[0m INFO 05-30 14:26:53 [model_runner.py:1140] Model loading took 2.8876 GiB and 1.365246 seconds
[36m(_EngineBackgroundProcess pid=51178)[0m INFO 05-30 14:26:54 [loader.py:458] Loading weights took 0.49 seconds
[36m(_EngineBackgroundProcess pid=51089)[0m INFO 05-30 14:26:54 [worker.py:287] Memory profiling takes 0.64 seconds
[36m(_EngineBackgroundProcess pid=51089)[0m INFO 05-30 14:26:54 [worker.py:287] the current vLLM instance can use total_gpu_memory (21.95GiB) x gpu_memory_utilization (0.90) = 19.76GiB
[36m(_EngineBackgroundProcess pid=51089)[0m INFO 05-30 14:26:54 [worker.py:287] model weights take 2.89GiB; non_torch_memory takes 0.04GiB; PyTorch activation peak memory takes 1.39GiB; the rest of the memory reserved for KV Cache is 15.43GiB.
[36m(_EngineBackgroundProcess pid=51178)[0m INFO 05-30 14:26:54 [model_runner.py:1140] Model loading took 2.8876 GiB and 1.620411 seconds
[36m(_EngineBackgroundProcess pid=51088)[0m INFO 05-30 14:26:54 [worker.py:287] Memory profiling takes 0.66 seconds
[36m(_EngineBackgroundProcess pid=51088)[0m INFO 05-30 14:26:54 [worker.py:287] the current vLLM instance can use total_gpu_memory (21.95GiB) x gpu_memory_utilization (0.90) = 19.76GiB
[36m(_EngineBackgroundProcess pid=51088)[0m INFO 05-30 14:26:54 [worker.py:287] model weights take 2.89GiB; non_torch_memory takes 0.04GiB; PyTorch activation peak memory takes 1.39GiB; the rest of the memory reserved for KV Cache is 15.43GiB.
[36m(_EngineBackgroundProcess pid=51089)[0m INFO 05-30 14:26:54 [executor_base.py:112] # cuda blocks: 36121, # CPU blocks: 9362
[36m(_EngineBackgroundProcess pid=51089)[0m INFO 05-30 14:26:54 [executor_base.py:117] Maximum concurrency for 32768 tokens per request: 17.64x
[36m(_EngineBackgroundProcess pid=51090)[0m INFO 05-30 14:26:54 [worker.py:287] Memory profiling takes 0.63 seconds
[36m(_EngineBackgroundProcess pid=51090)[0m INFO 05-30 14:26:54 [worker.py:287] the current vLLM instance can use total_gpu_memory (21.95GiB) x gpu_memory_utilization (0.90) = 19.76GiB
[36m(_EngineBackgroundProcess pid=51090)[0m INFO 05-30 14:26:54 [worker.py:287] model weights take 2.89GiB; non_torch_memory takes 0.04GiB; PyTorch activation peak memory takes 1.39GiB; the rest of the memory reserved for KV Cache is 15.43GiB.
[36m(_EngineBackgroundProcess pid=51088)[0m INFO 05-30 14:26:54 [executor_base.py:112] # cuda blocks: 36121, # CPU blocks: 9362
[36m(_EngineBackgroundProcess pid=51088)[0m INFO 05-30 14:26:54 [executor_base.py:117] Maximum concurrency for 32768 tokens per request: 17.64x
[36m(_EngineBackgroundProcess pid=51090)[0m INFO 05-30 14:26:54 [executor_base.py:112] # cuda blocks: 36121, # CPU blocks: 9362
[36m(_EngineBackgroundProcess pid=51090)[0m INFO 05-30 14:26:54 [executor_base.py:117] Maximum concurrency for 32768 tokens per request: 17.64x
[36m(_EngineBackgroundProcess pid=51178)[0m INFO 05-30 14:26:55 [worker.py:287] Memory profiling takes 0.63 seconds
[36m(_EngineBackgroundProcess pid=51178)[0m INFO 05-30 14:26:55 [worker.py:287] the current vLLM instance can use total_gpu_memory (21.95GiB) x gpu_memory_utilization (0.90) = 19.76GiB
[36m(_EngineBackgroundProcess pid=51178)[0m INFO 05-30 14:26:55 [worker.py:287] model weights take 2.89GiB; non_torch_memory takes 0.04GiB; PyTorch activation peak memory takes 1.39GiB; the rest of the memory reserved for KV Cache is 15.43GiB.
[36m(_EngineBackgroundProcess pid=51178)[0m INFO 05-30 14:26:55 [executor_base.py:112] # cuda blocks: 36121, # CPU blocks: 9362
[36m(_EngineBackgroundProcess pid=51178)[0m INFO 05-30 14:26:55 [executor_base.py:117] Maximum concurrency for 32768 tokens per request: 17.64x
[36m(_EngineBackgroundProcess pid=51089)[0m INFO 05-30 14:26:59 [model_runner.py:1450] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[36m(_EngineBackgroundProcess pid=51088)[0m INFO 05-30 14:26:59 [model_runner.py:1450] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[36m(_EngineBackgroundProcess pid=51090)[0m INFO 05-30 14:26:59 [model_runner.py:1450] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[36m(_EngineBackgroundProcess pid=51178)[0m INFO 05-30 14:27:00 [model_runner.py:1450] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[36m(_EngineBackgroundProcess pid=51089)[0m INFO 05-30 14:27:19 [model_runner.py:1592] Graph capturing finished in 20 secs, took 0.20 GiB
[36m(_EngineBackgroundProcess pid=51089)[0m INFO 05-30 14:27:19 [llm_engine.py:437] init engine (profile, create kv cache, warmup model) took 26.11 seconds
[36m(_EngineBackgroundProcess pid=51090)[0m INFO 05-30 14:27:19 [model_runner.py:1592] Graph capturing finished in 20 secs, took 0.20 GiB
[36m(_EngineBackgroundProcess pid=51090)[0m INFO 05-30 14:27:19 [llm_engine.py:437] init engine (profile, create kv cache, warmup model) took 26.05 seconds
[36m(_EngineBackgroundProcess pid=51088)[0m INFO 05-30 14:27:19 [model_runner.py:1592] Graph capturing finished in 20 secs, took 0.20 GiB
[36m(_EngineBackgroundProcess pid=51088)[0m INFO 05-30 14:27:19 [llm_engine.py:437] init engine (profile, create kv cache, warmup model) took 26.39 seconds
[36m(_EngineBackgroundProcess pid=51178)[0m INFO 05-30 14:27:21 [model_runner.py:1592] Graph capturing finished in 21 secs, took 0.20 GiB
[36m(_EngineBackgroundProcess pid=51178)[0m INFO 05-30 14:27:21 [llm_engine.py:437] init engine (profile, create kv cache, warmup model) took 26.83 seconds
[36m(ServeReplica:llm_app:LLMDeploymentQwen--Qwen2_5-1_5B-Instruct pid=49457)[0m INFO 05-30 14:27:21 [chat_utils.py:397] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.
[36m(ServeReplica:llm_app:LLMDeploymentQwen--Qwen2_5-1_5B-Instruct pid=49459)[0m INFO 05-30 14:27:22 [chat_utils.py:397] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.
[36m(ServeReplica:llm_app:LLMDeploymentQwen--Qwen2_5-1_5B-Instruct pid=49458)[0m INFO 05-30 14:27:22 [chat_utils.py:397] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.