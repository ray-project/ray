2025-05-28 15:27:54,695	INFO scripts.py:497 -- Running config file: 'temp_config.yaml'.
2025-05-28 15:27:58,773	SUCC scripts.py:548 -- [32mSubmitted deploy config successfully.[39m
[36m(build_serve_application pid=166901)[0m INFO 05-28 15:28:03 [__init__.py:239] Automatically detected platform cuda.
[36m(ServeReplica:llm_app:LLMDeploymentQwen--Qwen2_5-1_5B-Instruct pid=167055)[0m INFO 05-28 15:28:11 [__init__.py:239] Automatically detected platform cuda.
[36m(ServeReplica:llm_app:LLMDeploymentQwen--Qwen2_5-1_5B-Instruct pid=167056)[0m INFO 05-28 15:28:11 [__init__.py:239] Automatically detected platform cuda.
[36m(ServeReplica:llm_app:LLMRouter pid=167062)[0m INFO 05-28 15:28:11 [__init__.py:239] Automatically detected platform cuda.
[36m(ServeReplica:llm_app:LLMDeploymentQwen--Qwen2_5-1_5B-Instruct pid=167054)[0m INFO 05-28 15:28:11 [__init__.py:239] Automatically detected platform cuda.
[36m(ServeReplica:llm_app:LLMRouter pid=167060)[0m INFO 05-28 15:28:11 [__init__.py:239] Automatically detected platform cuda.
[36m(ServeReplica:llm_app:LLMDeploymentQwen--Qwen2_5-1_5B-Instruct pid=167053)[0m INFO 05-28 15:28:11 [__init__.py:239] Automatically detected platform cuda.
[36m(ServeReplica:llm_app:LLMRouter pid=167059)[0m INFO 05-28 15:28:11 [__init__.py:239] Automatically detected platform cuda.
[36m(ServeReplica:llm_app:LLMRouter pid=167061)[0m INFO 05-28 15:28:11 [__init__.py:239] Automatically detected platform cuda.
[36m(ServeReplica:llm_app:LLMRouter pid=167063)[0m INFO 05-28 15:28:11 [__init__.py:239] Automatically detected platform cuda.
[36m(ServeReplica:llm_app:LLMRouter pid=167064)[0m INFO 05-28 15:28:11 [__init__.py:239] Automatically detected platform cuda.
[36m(ServeReplica:llm_app:LLMRouter pid=167057)[0m INFO 05-28 15:28:11 [__init__.py:239] Automatically detected platform cuda.
[36m(ServeReplica:llm_app:LLMRouter pid=167058)[0m INFO 05-28 15:28:11 [__init__.py:239] Automatically detected platform cuda.
[36m(pid=168254)[0m INFO 05-28 15:28:20 [__init__.py:239] Automatically detected platform cuda.
[36m(pid=168253)[0m INFO 05-28 15:28:20 [__init__.py:239] Automatically detected platform cuda.
[36m(pid=168252)[0m INFO 05-28 15:28:20 [__init__.py:239] Automatically detected platform cuda.
[36m(pid=168255)[0m INFO 05-28 15:28:20 [__init__.py:239] Automatically detected platform cuda.
[36m(_get_vllm_engine_config pid=168253)[0m INFO 05-28 15:28:31 [config.py:717] This model supports multiple tasks: {'classify', 'reward', 'generate', 'embed', 'score'}. Defaulting to 'generate'.
[36m(_get_vllm_engine_config pid=168253)[0m INFO 05-28 15:28:31 [config.py:2003] Chunked prefill is enabled with max_num_batched_tokens=2048.
[36m(_get_vllm_engine_config pid=168252)[0m INFO 05-28 15:28:31 [config.py:717] This model supports multiple tasks: {'reward', 'score', 'classify', 'generate', 'embed'}. Defaulting to 'generate'.
[36m(_get_vllm_engine_config pid=168252)[0m INFO 05-28 15:28:31 [config.py:2003] Chunked prefill is enabled with max_num_batched_tokens=2048.
[36m(_get_vllm_engine_config pid=168254)[0m INFO 05-28 15:28:31 [config.py:717] This model supports multiple tasks: {'classify', 'generate', 'score', 'embed', 'reward'}. Defaulting to 'generate'.
[36m(_get_vllm_engine_config pid=168254)[0m INFO 05-28 15:28:31 [config.py:2003] Chunked prefill is enabled with max_num_batched_tokens=2048.
[36m(_get_vllm_engine_config pid=168255)[0m INFO 05-28 15:28:32 [config.py:717] This model supports multiple tasks: {'generate', 'embed', 'reward', 'classify', 'score'}. Defaulting to 'generate'.
[36m(_get_vllm_engine_config pid=168255)[0m INFO 05-28 15:28:32 [config.py:2003] Chunked prefill is enabled with max_num_batched_tokens=2048.
[36m(pid=168664)[0m INFO 05-28 15:28:36 [__init__.py:239] Automatically detected platform cuda.
[36m(pid=168665)[0m INFO 05-28 15:28:36 [__init__.py:239] Automatically detected platform cuda.
[36m(pid=168666)[0m INFO 05-28 15:28:36 [__init__.py:239] Automatically detected platform cuda.
[36m(pid=168689)[0m INFO 05-28 15:28:37 [__init__.py:239] Automatically detected platform cuda.
[36m(_EngineBackgroundProcess pid=168664)[0m INFO 05-28 15:28:38 [llm_engine.py:240] Initializing a V0 LLM engine (v0.8.5) with config: model='Qwen/Qwen2.5-1.5B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2.5-1.5B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=Qwen/Qwen2.5-1.5B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=True, 
[36m(_EngineBackgroundProcess pid=168665)[0m INFO 05-28 15:28:38 [llm_engine.py:240] Initializing a V0 LLM engine (v0.8.5) with config: model='Qwen/Qwen2.5-1.5B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2.5-1.5B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=Qwen/Qwen2.5-1.5B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=True, 
[36m(_EngineBackgroundProcess pid=168666)[0m INFO 05-28 15:28:38 [llm_engine.py:240] Initializing a V0 LLM engine (v0.8.5) with config: model='Qwen/Qwen2.5-1.5B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2.5-1.5B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=Qwen/Qwen2.5-1.5B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=True, 
[36m(_EngineBackgroundProcess pid=168689)[0m INFO 05-28 15:28:39 [llm_engine.py:240] Initializing a V0 LLM engine (v0.8.5) with config: model='Qwen/Qwen2.5-1.5B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2.5-1.5B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=Qwen/Qwen2.5-1.5B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=True, 
[36m(_EngineBackgroundProcess pid=168664)[0m INFO 05-28 15:28:39 [ray_utils.py:288] Ray is already initialized. Skipping Ray initialization.
[36m(_EngineBackgroundProcess pid=168664)[0m INFO 05-28 15:28:39 [ray_utils.py:314] Using the existing placement group
[36m(_EngineBackgroundProcess pid=168664)[0m INFO 05-28 15:28:39 [ray_distributed_executor.py:176] use_ray_spmd_worker: False
[36m(_EngineBackgroundProcess pid=168665)[0m INFO 05-28 15:28:39 [ray_utils.py:288] Ray is already initialized. Skipping Ray initialization.
[36m(_EngineBackgroundProcess pid=168665)[0m INFO 05-28 15:28:39 [ray_utils.py:314] Using the existing placement group
[36m(_EngineBackgroundProcess pid=168665)[0m INFO 05-28 15:28:39 [ray_distributed_executor.py:176] use_ray_spmd_worker: False
[36m(_EngineBackgroundProcess pid=168666)[0m INFO 05-28 15:28:39 [ray_utils.py:288] Ray is already initialized. Skipping Ray initialization.
[36m(_EngineBackgroundProcess pid=168666)[0m INFO 05-28 15:28:39 [ray_utils.py:314] Using the existing placement group
[36m(_EngineBackgroundProcess pid=168666)[0m INFO 05-28 15:28:39 [ray_distributed_executor.py:176] use_ray_spmd_worker: False
[36m(_EngineBackgroundProcess pid=168689)[0m INFO 05-28 15:28:39 [ray_utils.py:288] Ray is already initialized. Skipping Ray initialization.
[36m(_EngineBackgroundProcess pid=168689)[0m INFO 05-28 15:28:39 [ray_utils.py:314] Using the existing placement group
[36m(_EngineBackgroundProcess pid=168689)[0m INFO 05-28 15:28:39 [ray_distributed_executor.py:176] use_ray_spmd_worker: False
[36m(pid=169024)[0m INFO 05-28 15:28:43 [__init__.py:239] Automatically detected platform cuda.
[36m(pid=169018)[0m INFO 05-28 15:28:43 [__init__.py:239] Automatically detected platform cuda.
[36m(pid=169027)[0m INFO 05-28 15:28:43 [__init__.py:239] Automatically detected platform cuda.
[36m(pid=169044)[0m INFO 05-28 15:28:44 [__init__.py:239] Automatically detected platform cuda.
[36m(_EngineBackgroundProcess pid=168664)[0m INFO 05-28 15:28:45 [ray_distributed_executor.py:352] non_carry_over_env_vars from config: set()
[36m(_EngineBackgroundProcess pid=168664)[0m INFO 05-28 15:28:45 [ray_distributed_executor.py:354] Copying the following environment variables to workers: ['LD_LIBRARY_PATH', 'VLLM_USE_V1']
[36m(_EngineBackgroundProcess pid=168664)[0m INFO 05-28 15:28:45 [ray_distributed_executor.py:357] If certain env vars should NOT be copied to workers, add them to /home/ray/.config/vllm/ray_non_carry_over_env_vars.json file
[36m(_EngineBackgroundProcess pid=168665)[0m INFO 05-28 15:28:45 [ray_distributed_executor.py:352] non_carry_over_env_vars from config: set()
[36m(_EngineBackgroundProcess pid=168665)[0m INFO 05-28 15:28:45 [ray_distributed_executor.py:354] Copying the following environment variables to workers: ['LD_LIBRARY_PATH', 'VLLM_USE_V1']
[36m(_EngineBackgroundProcess pid=168665)[0m INFO 05-28 15:28:45 [ray_distributed_executor.py:357] If certain env vars should NOT be copied to workers, add them to /home/ray/.config/vllm/ray_non_carry_over_env_vars.json file
[36m(_EngineBackgroundProcess pid=168666)[0m INFO 05-28 15:28:46 [ray_distributed_executor.py:352] non_carry_over_env_vars from config: set()
[36m(_EngineBackgroundProcess pid=168666)[0m INFO 05-28 15:28:46 [ray_distributed_executor.py:354] Copying the following environment variables to workers: ['LD_LIBRARY_PATH', 'VLLM_USE_V1']
[36m(_EngineBackgroundProcess pid=168666)[0m INFO 05-28 15:28:46 [ray_distributed_executor.py:357] If certain env vars should NOT be copied to workers, add them to /home/ray/.config/vllm/ray_non_carry_over_env_vars.json file
[36m(_EngineBackgroundProcess pid=168689)[0m INFO 05-28 15:28:46 [ray_distributed_executor.py:352] non_carry_over_env_vars from config: set()
[36m(_EngineBackgroundProcess pid=168689)[0m INFO 05-28 15:28:46 [ray_distributed_executor.py:354] Copying the following environment variables to workers: ['LD_LIBRARY_PATH', 'VLLM_USE_V1']
[36m(_EngineBackgroundProcess pid=168689)[0m INFO 05-28 15:28:46 [ray_distributed_executor.py:357] If certain env vars should NOT be copied to workers, add them to /home/ray/.config/vllm/ray_non_carry_over_env_vars.json file
[36m(_EngineBackgroundProcess pid=168664)[0m INFO 05-28 15:28:46 [cuda.py:292] Using Flash Attention backend.
[36m(_EngineBackgroundProcess pid=168665)[0m INFO 05-28 15:28:47 [cuda.py:292] Using Flash Attention backend.
[36m(_EngineBackgroundProcess pid=168666)[0m INFO 05-28 15:28:47 [cuda.py:292] Using Flash Attention backend.
[36m(_EngineBackgroundProcess pid=168689)[0m INFO 05-28 15:28:47 [cuda.py:292] Using Flash Attention backend.
[36m(_EngineBackgroundProcess pid=168664)[0m INFO 05-28 15:28:48 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0
[36m(_EngineBackgroundProcess pid=168664)[0m INFO 05-28 15:28:48 [model_runner.py:1108] Starting to load model Qwen/Qwen2.5-1.5B-Instruct...
[36m(_EngineBackgroundProcess pid=168665)[0m INFO 05-28 15:28:48 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0
[36m(_EngineBackgroundProcess pid=168665)[0m INFO 05-28 15:28:48 [model_runner.py:1108] Starting to load model Qwen/Qwen2.5-1.5B-Instruct...
[36m(_EngineBackgroundProcess pid=168666)[0m INFO 05-28 15:28:48 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0
[36m(_EngineBackgroundProcess pid=168666)[0m INFO 05-28 15:28:48 [model_runner.py:1108] Starting to load model Qwen/Qwen2.5-1.5B-Instruct...
[36m(_EngineBackgroundProcess pid=168664)[0m INFO 05-28 15:28:48 [weight_utils.py:265] Using model weights format ['*.safetensors']
[36m(_EngineBackgroundProcess pid=168665)[0m INFO 05-28 15:28:48 [weight_utils.py:265] Using model weights format ['*.safetensors']
[36m(_EngineBackgroundProcess pid=168689)[0m INFO 05-28 15:28:48 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0
[36m(_EngineBackgroundProcess pid=168689)[0m INFO 05-28 15:28:48 [model_runner.py:1108] Starting to load model Qwen/Qwen2.5-1.5B-Instruct...
[36m(_EngineBackgroundProcess pid=168664)[0m INFO 05-28 15:28:48 [weight_utils.py:315] No model.safetensors.index.json found in remote.
[36m(_EngineBackgroundProcess pid=168666)[0m INFO 05-28 15:28:48 [weight_utils.py:265] Using model weights format ['*.safetensors']
[36m(_EngineBackgroundProcess pid=168689)[0m INFO 05-28 15:28:48 [weight_utils.py:265] Using model weights format ['*.safetensors']
[36m(_EngineBackgroundProcess pid=168664)[0m INFO 05-28 15:28:49 [loader.py:458] Loading weights took 0.49 seconds
[36m(_EngineBackgroundProcess pid=168664)[0m INFO 05-28 15:28:49 [model_runner.py:1140] Model loading took 2.8876 GiB and 1.039894 seconds
[36m(_EngineBackgroundProcess pid=168664)[0m INFO 05-28 15:28:50 [worker.py:287] Memory profiling takes 0.63 seconds
[36m(_EngineBackgroundProcess pid=168664)[0m INFO 05-28 15:28:50 [worker.py:287] the current vLLM instance can use total_gpu_memory (21.95GiB) x gpu_memory_utilization (0.90) = 19.76GiB
[36m(_EngineBackgroundProcess pid=168664)[0m INFO 05-28 15:28:50 [worker.py:287] model weights take 2.89GiB; non_torch_memory takes 0.04GiB; PyTorch activation peak memory takes 1.39GiB; the rest of the memory reserved for KV Cache is 15.43GiB.
[36m(_EngineBackgroundProcess pid=168664)[0m INFO 05-28 15:28:50 [executor_base.py:112] # cuda blocks: 36121, # CPU blocks: 9362
[36m(_EngineBackgroundProcess pid=168664)[0m INFO 05-28 15:28:50 [executor_base.py:117] Maximum concurrency for 32768 tokens per request: 17.64x
[36m(_EngineBackgroundProcess pid=168665)[0m INFO 05-28 15:28:51 [weight_utils.py:315] No model.safetensors.index.json found in remote.
[36m(_EngineBackgroundProcess pid=168689)[0m INFO 05-28 15:28:51 [weight_utils.py:315] No model.safetensors.index.json found in remote.
[36m(_EngineBackgroundProcess pid=168666)[0m INFO 05-28 15:28:51 [weight_utils.py:315] No model.safetensors.index.json found in remote.
[36m(_EngineBackgroundProcess pid=168665)[0m INFO 05-28 15:28:51 [loader.py:458] Loading weights took 0.51 seconds
[36m(_EngineBackgroundProcess pid=168689)[0m INFO 05-28 15:28:51 [loader.py:458] Loading weights took 0.49 seconds
[36m(_EngineBackgroundProcess pid=168665)[0m INFO 05-28 15:28:51 [model_runner.py:1140] Model loading took 2.8876 GiB and 3.572203 seconds
[36m(_EngineBackgroundProcess pid=168689)[0m INFO 05-28 15:28:52 [model_runner.py:1140] Model loading took 2.8876 GiB and 3.414934 seconds
[36m(_EngineBackgroundProcess pid=168666)[0m INFO 05-28 15:28:52 [loader.py:458] Loading weights took 0.49 seconds
[36m(_EngineBackgroundProcess pid=168666)[0m INFO 05-28 15:28:52 [model_runner.py:1140] Model loading took 2.8876 GiB and 3.844736 seconds
[36m(_EngineBackgroundProcess pid=168665)[0m INFO 05-28 15:28:52 [worker.py:287] Memory profiling takes 0.65 seconds
[36m(_EngineBackgroundProcess pid=168665)[0m INFO 05-28 15:28:52 [worker.py:287] the current vLLM instance can use total_gpu_memory (21.95GiB) x gpu_memory_utilization (0.90) = 19.76GiB
[36m(_EngineBackgroundProcess pid=168665)[0m INFO 05-28 15:28:52 [worker.py:287] model weights take 2.89GiB; non_torch_memory takes 0.04GiB; PyTorch activation peak memory takes 1.39GiB; the rest of the memory reserved for KV Cache is 15.43GiB.
[36m(_EngineBackgroundProcess pid=168689)[0m INFO 05-28 15:28:53 [worker.py:287] Memory profiling takes 0.66 seconds
[36m(_EngineBackgroundProcess pid=168689)[0m INFO 05-28 15:28:53 [worker.py:287] the current vLLM instance can use total_gpu_memory (21.95GiB) x gpu_memory_utilization (0.90) = 19.76GiB
[36m(_EngineBackgroundProcess pid=168689)[0m INFO 05-28 15:28:53 [worker.py:287] model weights take 2.89GiB; non_torch_memory takes 0.04GiB; PyTorch activation peak memory takes 1.39GiB; the rest of the memory reserved for KV Cache is 15.43GiB.
[36m(_EngineBackgroundProcess pid=168665)[0m INFO 05-28 15:28:53 [executor_base.py:112] # cuda blocks: 36121, # CPU blocks: 9362
[36m(_EngineBackgroundProcess pid=168665)[0m INFO 05-28 15:28:53 [executor_base.py:117] Maximum concurrency for 32768 tokens per request: 17.64x
[36m(_EngineBackgroundProcess pid=168689)[0m INFO 05-28 15:28:53 [executor_base.py:112] # cuda blocks: 36121, # CPU blocks: 9362
[36m(_EngineBackgroundProcess pid=168689)[0m INFO 05-28 15:28:53 [executor_base.py:117] Maximum concurrency for 32768 tokens per request: 17.64x
[36m(_EngineBackgroundProcess pid=168666)[0m INFO 05-28 15:28:53 [worker.py:287] Memory profiling takes 0.65 seconds
[36m(_EngineBackgroundProcess pid=168666)[0m INFO 05-28 15:28:53 [worker.py:287] the current vLLM instance can use total_gpu_memory (21.95GiB) x gpu_memory_utilization (0.90) = 19.76GiB
[36m(_EngineBackgroundProcess pid=168666)[0m INFO 05-28 15:28:53 [worker.py:287] model weights take 2.89GiB; non_torch_memory takes 0.04GiB; PyTorch activation peak memory takes 1.39GiB; the rest of the memory reserved for KV Cache is 15.43GiB.
[36m(_EngineBackgroundProcess pid=168666)[0m INFO 05-28 15:28:53 [executor_base.py:112] # cuda blocks: 36121, # CPU blocks: 9362
[36m(_EngineBackgroundProcess pid=168666)[0m INFO 05-28 15:28:53 [executor_base.py:117] Maximum concurrency for 32768 tokens per request: 17.64x
[36m(_EngineBackgroundProcess pid=168664)[0m INFO 05-28 15:28:55 [model_runner.py:1450] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[36m(_EngineBackgroundProcess pid=168665)[0m INFO 05-28 15:28:58 [model_runner.py:1450] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[36m(_EngineBackgroundProcess pid=168689)[0m INFO 05-28 15:28:58 [model_runner.py:1450] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[36m(_EngineBackgroundProcess pid=168666)[0m INFO 05-28 15:28:58 [model_runner.py:1450] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
[36m(_EngineBackgroundProcess pid=168664)[0m INFO 05-28 15:29:15 [model_runner.py:1592] Graph capturing finished in 21 secs, took 0.20 GiB
[36m(_EngineBackgroundProcess pid=168664)[0m INFO 05-28 15:29:15 [llm_engine.py:437] init engine (profile, create kv cache, warmup model) took 26.39 seconds
[36m(ServeReplica:llm_app:LLMDeploymentQwen--Qwen2_5-1_5B-Instruct pid=167056)[0m INFO 05-28 15:29:17 [chat_utils.py:397] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.
[36m(_EngineBackgroundProcess pid=168689)[0m INFO 05-28 15:29:18 [model_runner.py:1592] Graph capturing finished in 20 secs, took 0.20 GiB
[36m(_EngineBackgroundProcess pid=168689)[0m INFO 05-28 15:29:18 [llm_engine.py:437] init engine (profile, create kv cache, warmup model) took 26.28 seconds
[36m(_EngineBackgroundProcess pid=168665)[0m INFO 05-28 15:29:19 [model_runner.py:1592] Graph capturing finished in 21 secs, took 0.20 GiB
[36m(_EngineBackgroundProcess pid=168665)[0m INFO 05-28 15:29:19 [llm_engine.py:437] init engine (profile, create kv cache, warmup model) took 27.39 seconds
[36m(_EngineBackgroundProcess pid=168666)[0m INFO 05-28 15:29:19 [model_runner.py:1592] Graph capturing finished in 21 secs, took 0.20 GiB
[36m(_EngineBackgroundProcess pid=168666)[0m INFO 05-28 15:29:19 [llm_engine.py:437] init engine (profile, create kv cache, warmup model) took 27.07 seconds
[36m(ServeReplica:llm_app:LLMDeploymentQwen--Qwen2_5-1_5B-Instruct pid=167055)[0m INFO 05-28 15:29:21 [chat_utils.py:397] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.
[36m(ServeReplica:llm_app:LLMDeploymentQwen--Qwen2_5-1_5B-Instruct pid=167054)[0m INFO 05-28 15:29:22 [chat_utils.py:397] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.