// Copyright 2017 The Ray Authors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//  http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

#pragma once

#include <thread>

#include "absl/base/thread_annotations.h"
#include "absl/synchronization/mutex.h"
#include "nlohmann/json.hpp"
#include "ray/common/task/task_spec.h"
#include "ray/core_worker/common.h"
using json = nlohmann::json;
namespace ray {
namespace core {

struct WorkerExecContext {
  explicit WorkerExecContext(const TaskID &task_id, bool is_driver = false)
      : current_task_id_(task_id),
        task_index_(0),
        put_counter_(0),
        is_driver_(is_driver) {
    SetCurrentTaskId(task_id, /* attempt_number */ 0);
  }

  uint64_t GetNextTaskIndex() { return ++task_index_; }

  uint64_t GetTaskIndex() { return task_index_; }

  /// Returns the next put object index. The index starts at the number of
  /// return values for the current task in order to keep the put indices from
  /// conflicting with return object indices. 1 <= idx <= NumReturns() is reserved for
  /// return objects, while idx > NumReturns is available for put objects.
  ObjectIDIndexType GetNextPutIndex() {
    // If current_task_ is nullptr, we assume that we're in the event loop thread and
    // are executing async tasks; in this case, we're using a fake, random task ID
    // for put objects, so there's no risk of creating put object IDs that conflict with
    // return object IDs (none of the latter are created). The put counter will never
    // reset and will therefore continue to increment for the lifetime of the event
    // loop thread (ResetCurrentTask and SetCurrenTask will never be called in the
    // thread), so there's no risk of conflicting put object IDs, either.
    // See https://github.com/ray-project/ray/issues/10324 for further details.
    auto num_returns = current_task_ != nullptr ? current_task_->NumReturns() : 0;
    return num_returns + ++put_counter_;
  }

  const TaskID &GetCurrentTaskID() const { return current_task_id_; }

  std::shared_ptr<const TaskSpecification> GetCurrentTask() const {
    return current_task_;
  }

  void SetCurrentTaskId(const TaskID &task_id, uint64_t attempt_number) {
    current_task_id_ = task_id;
    if (!current_task_id_.IsNil()) {
      current_internal_task_id_ = TaskID::ForExecutionAttempt(task_id, attempt_number);
    } else {
      current_internal_task_id_ = TaskID::Nil();
    }
  }

  const TaskID &GetCurrentInternalTaskId() const { return current_internal_task_id_; }

  const PlacementGroupID &GetCurrentPlacementGroupId() const {
    return current_placement_group_id_;
  }

  void SetCurrentPlacementGroupId(const PlacementGroupID &placement_group_id) {
    current_placement_group_id_ = placement_group_id;
  }

  void SetPlacementGroupCaptureChildTasks(bool placement_group_capture_child_tasks) {
    placement_group_capture_child_tasks_ = placement_group_capture_child_tasks;
  }

  bool PlacementGroupCaptureChildTasks() const {
    return placement_group_capture_child_tasks_;
  }

  void SetCurrentTask(const TaskSpecification &task_spec) {
    RAY_CHECK(task_index_ == 0);
    RAY_CHECK(put_counter_ == 0);
    SetCurrentTaskId(task_spec.TaskId(), task_spec.AttemptNumber());
    SetCurrentPlacementGroupId(task_spec.PlacementGroupBundleId().first);
    SetPlacementGroupCaptureChildTasks(task_spec.PlacementGroupCaptureChildTasks());
    current_task_ = std::make_shared<const TaskSpecification>(task_spec);
  }

  void ResetCurrentTask() {
    SetCurrentTaskId(TaskID::Nil(), /*attempt_number=*/0);
    task_index_ = 0;
    put_counter_ = 0;
  }

 private:
  /// The task ID for current task.
  TaskID current_task_id_;

  /// The internal task ID, used to seed any TaskIDs or ObjectIDs generated by
  /// the current task (via ray.put or .remote calls). As tasks can be executed
  /// multiple times, this ID represents the n-th execution of the current
  /// task.
  /// See https://github.com/ray-project/ray/issues/20713.
  TaskID current_internal_task_id_;

  /// The current task.
  std::shared_ptr<const TaskSpecification> current_task_;

  /// Number of tasks that have been submitted from current task.
  uint64_t task_index_;

  static_assert(sizeof(task_index_) == TaskID::Size() - ActorID::Size(),
                "Size of task_index_ doesn't match the unique bytes of a TaskID.");

  /// A running counter for the number of object puts carried out in the current task.
  /// Used to calculate the object index for put object ObjectIDs.
  ObjectIDIndexType put_counter_;

  static_assert(sizeof(put_counter_) == ObjectID::Size() - TaskID::Size(),
                "Size of put_counter_ doesn't match the unique bytes of an ObjectID.");

  /// Placement group id that the current task belongs to.
  /// NOTE: The top level `WorkerContext` will also have placement_group_id
  ///   which is set when actors are created. It is because we'd like to keep track
  ///   thread local placement group id for tasks, and the process placement group id for
  ///   actors.
  PlacementGroupID current_placement_group_id_;

  /// Whether or not child tasks are captured in the parent's placement group implicitly.
  bool placement_group_capture_child_tasks_ = false;

  // If this is a driver worker.
  const bool is_driver_;
};

class WorkerContext {
 public:
  WorkerContext(WorkerType worker_type, const WorkerID &worker_id, const JobID &job_id);

  const WorkerType GetWorkerType() const;

  const WorkerID &GetWorkerID() const;

  JobID GetCurrentJobID() const LOCKS_EXCLUDED(mutex_);
  rpc::JobConfig GetCurrentJobConfig() const LOCKS_EXCLUDED(mutex_);

  const TaskID &GetCurrentTaskID() const;

  const PlacementGroupID &GetCurrentPlacementGroupId() const LOCKS_EXCLUDED(mutex_);

  bool ShouldCaptureChildTasksInPlacementGroup() const LOCKS_EXCLUDED(mutex_);

  const std::shared_ptr<rpc::RuntimeEnvInfo> GetCurrentRuntimeEnvInfo() const
      LOCKS_EXCLUDED(mutex_);

  const std::string &GetCurrentSerializedRuntimeEnv() const LOCKS_EXCLUDED(mutex_);

  std::shared_ptr<json> GetCurrentRuntimeEnv() const LOCKS_EXCLUDED(mutex_);

  // Initialize worker's job_id and job_config if they haven't already.
  // Note a worker's job config can't be changed after initialization.
  void MaybeInitializeJobInfo(const JobID &job_id, const rpc::JobConfig &job_config)
      LOCKS_EXCLUDED(mutex_);

  // TODO(edoakes): remove this once Python core worker uses the task interfaces.
  void SetCurrentTaskId(const TaskID &task_id, uint64_t attempt_number);

  const TaskID &GetCurrentInternalTaskId() const;

  void SetCurrentActorId(const ActorID &actor_id) LOCKS_EXCLUDED(mutex_);

  void SetTaskDepth(int64_t depth) EXCLUSIVE_LOCKS_REQUIRED(mutex_);

  void SetCurrentTask(const TaskSpecification &task_spec) LOCKS_EXCLUDED(mutex_);

  void ResetCurrentTask();

  std::shared_ptr<const TaskSpecification> GetCurrentTask() const;

  const ActorID &GetCurrentActorID() const LOCKS_EXCLUDED(mutex_);

  /// Returns whether the current thread is the main worker thread.
  bool CurrentThreadIsMain() const;

  /// Returns whether we should Block/Unblock through the raylet on Get/Wait.
  /// This only applies to direct task calls.
  bool ShouldReleaseResourcesOnBlockingCalls() const;

  /// Returns whether we are in a direct call actor.
  bool CurrentActorIsDirectCall() const LOCKS_EXCLUDED(mutex_);

  /// Returns whether we are in a direct call task. This encompasses both direct
  /// actor and normal tasks.
  bool CurrentTaskIsDirectCall() const LOCKS_EXCLUDED(mutex_);

  int CurrentActorMaxConcurrency() const LOCKS_EXCLUDED(mutex_);

  bool CurrentActorIsAsync() const LOCKS_EXCLUDED(mutex_);

  bool CurrentActorDetached() const LOCKS_EXCLUDED(mutex_);

  uint64_t GetNextTaskIndex();

  uint64_t GetTaskIndex();

  // Returns the next put object index; used to calculate ObjectIDs for puts.
  ObjectIDIndexType GetNextPutIndex();

  int64_t GetTaskDepth() const;

 protected:
  // allow unit test to set.
  bool current_actor_is_direct_call_ = false;
  bool current_task_is_direct_call_ = false;

 private:
  WorkerExecContext &GetExecContext() const;

  std::shared_ptr<WorkerExecContext> GetExecContextInternal() const;

  void InitExecContext(const TaskID &task_id, bool is_driver) LOCKS_EXCLUDED(mutex_);

  const WorkerType worker_type_;
  const WorkerID worker_id_;

  // a worker's job infomation might be lazily initialized.
  JobID current_job_id_ GUARDED_BY(mutex_);
  std::optional<rpc::JobConfig> job_config_ GUARDED_BY(mutex_);

  int64_t task_depth_ GUARDED_BY(mutex_) = 0;
  ActorID current_actor_id_ GUARDED_BY(mutex_);
  int current_actor_max_concurrency_ GUARDED_BY(mutex_) = 1;
  bool current_actor_is_asyncio_ GUARDED_BY(mutex_) = false;
  // If this is an actor with multiple threads of execution. It could be an async actor or
  // threaded actor. This is true when `max_concurrency > 0` set on actor's options.
  bool current_actor_is_multi_threaded_ GUARDED_BY(mutex_) = false;
  bool is_detached_actor_ GUARDED_BY(mutex_) = false;
  // The placement group id that the current actor belongs to.
  PlacementGroupID current_actor_placement_group_id_ GUARDED_BY(mutex_);
  // Whether or not we should implicitly capture parent's placement group.
  bool placement_group_capture_child_tasks_ GUARDED_BY(mutex_);
  // The runtime env for the current actor or task.
  std::shared_ptr<json> runtime_env_ GUARDED_BY(mutex_);
  // The runtime env info.
  std::shared_ptr<rpc::RuntimeEnvInfo> runtime_env_info_ GUARDED_BY(mutex_);
  /// The id of the (main) thread that constructed this worker context.
  const std::thread::id main_thread_id_ GUARDED_BY(mutex_);
  // All thread contexts started by this CoreWorker.
  absl::flat_hash_map<std::thread::id, std::shared_ptr<WorkerExecContext>>
      all_exec_threads_contexts_ GUARDED_BY(mutex_);
  // The current running task's thread context.
  std::shared_ptr<WorkerExecContext> current_exec_context_ GUARDED_BY(mutex_);

  // To protect access to mutable members;
  mutable absl::Mutex mutex_;
};

}  // namespace core
}  // namespace ray
