// Copyright 2023 The Ray Authors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//  http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

syntax = "proto3";
option cc_enable_arenas = true;

package ray.autoscaler;

import "src/ray/protobuf/experimental/instance_manager.proto";

// ============= Cluster Resources ====================
//
// Following fields represents the Cluster Resources autoscaler interested
// in.

// Represents an anti-affinity constraint. A bundle with this constraint
// can't be allocated to a node that has a label with the same name and
// value. This is used to implement placement group anti-affinity.
//
// For placement group, the label_name is "_PG" (reserved),
// and the label_value is the placement group id.
message AntiAffinityConstraint {
  string label_name = 1;
  string label_value = 2;
  // If true, the label will be created on the node
  // where the request with this constraint is scheduled.
  bool create_label_on_schedule = 3;
}

message PlacementConstraint {
  AntiAffinityConstraint anti_affinity = 1;
}

message ResourceRequest {
  // resource requirements for the request.
  map<string, double> resources_bundle = 1;
  // placement constraint for the request. multiple constraints
  // form AND semantics.
  repeated PlacementConstraint placement_constraints = 2;
}

message ResourceRequestByCount {
  ResourceRequest request = 1;
  int64 count = 2;
}

// All bundles in the same resource request require gang
// allocation semantics: they should be allocated all or nothing.
message GangResourceRequest {
  // a map from bundles to the number of bundles requested.
  repeated ResourceRequest requests = 1;
}

// Cluster resource constraint represents minimial cluster size requirement,
// this is issued through ray.autoscaler.sdk.request_resources.
message ClusterResourceConstraint {
  // If not empty, the cluster should have the capacity (total resource) to
  // fit the min_resources.
  map<string, double> min_resources = 1;
  // If not emtpy, the cluster should have the capacity (total resource) to fit
  // the min_bundles.
  repeated ResourceRequest min_bundles = 2;
  // Id of the job who issued this constraint.
  string job_id = 3;
}

message NodeState {
  enum NodeStatus {
    // Node is alive.
    ALIVE = 0;
    // Node is dead.
    DEAD = 1;
    // Node is being drained.
    DRAIN_PENDING = 2;
    // Node is being drained.
    DRAIN_FAILED = 3;
    // Node is being drained.
    DRAINING = 4;
    // Node is already drained, and ready to be removed.
    DRAINED = 5;
  }
  // The node id internal to Ray.
  string node_id = 11;

  // The instance id that the node is running on.
  // This is passed in when the node is registered.
  string instance_id = 12;

  // The available resources on the node.
  // Reserved resource names: CPU, GPU, MEMORY, OBJECT_STORE_MEMORY
  map<string, double> available_resources = 13;

  // The corresponding total resources on the node.
  map<string, double> total_resources = 14;

  // Dynamic labels associated with the node.
  // Reserved dynamic label names: _PG
  map<string, string> dynamic_labels = 15;

  // A monotonic increasing version of the node resource state.
  int64 node_state_version = 16;

  // The status of the node.
  NodeStatus status = 17;
}

// ============= Autoscaling State Service API =======================
//
// Autoscaler periodically calls to
// two snapshot APIs, GetClusterResourceState
// and ReportAutoscalingState.
// The GetClusterResourceState will return a snapshot
// of Ray state that Autoscaler interested, along with
// the cluster_resource_state_version (version).
//
// Separately, autoscaler will constantly making decisions
// based on the latest Ray state, and also change its
// state based on the information from node provider.
// Autoscaler will periodically report its state to GCS
// through ReportAutoscalingState API.

message GetClusterResourceStateRequest {
  // The last seen cluster resource state version. The default value is reserved for if a
  // previous scheduling state has never been seen.
  int64 last_seen_cluster_resource_state_version = 1;
}

message GetClusterResourceStateReply {
  // an monotonically increasing version of the cluster resources.
  int64 cluster_resource_state_version = 1;
  // last seen autoscaler state.
  int64 last_seen_autoscaler_state_version = 2;
  // Current cluster resources.
  repeated NodeState node_states = 3;
  // Resource requests pending scheduling.
  repeated ResourceRequestByCount pending_resource_requests = 4;
  // Gang resource requests pending scheduling.
  repeated GangResourceRequest pending_gang_resource_requests = 5;
  // Cluster resource constraints.
  // There could be multiple constraints issued by different
  // jobs. Autoscaler to make sure all constraints are satisfied.
  repeated ClusterResourceConstraint cluster_resource_constraints = 6;
}

message ReportAutoscalingStateRequest {
  int64 last_seen_cluster_resource_state_version = 1;
  // A monotonically increasing version identifies
  // the state of autoscaler.
  // Note: for the same cluster resource state, the
  // autoscaler state might be different, since
  // the autoscaler's state could also be updated by
  // node provider.
  int64 autoscaler_state_version = 2;
  repeated Instance instances = 3;
  // infeasible resource requests.
  repeated ResourceRequest infeasible_resource_requests = 4;
  repeated ClusterResourceConstraint infeasible_gange_resource_requests = 5;
  repeated ClusterResourceConstraint infeasible_cluster_resource_constraints = 6;
}

message ReportAutoscalingStateReply {}

service AutoscalerStateService {
  rpc GetClusterResourceState(GetClusterResourceStateRequest)
      returns (GetClusterResourceStateReply);
  rpc ReportAutoscalingState(ReportAutoscalingStateRequest)
      returns (ReportAutoscalingStateReply);
}