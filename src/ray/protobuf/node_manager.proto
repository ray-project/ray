// Copyright 2017 The Ray Authors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//  http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

syntax = "proto3";
option cc_enable_arenas = true;

package ray.rpc;

import "src/ray/protobuf/common.proto";
import "src/ray/protobuf/gcs.proto";

message WorkerBacklogReport {
  // TaskSpec indicating the scheduling class.
  // Cannot send scheduling class directly
  // since it's local to each process.
  TaskSpec resource_spec = 1;
  // Size of the backlog for the above scheduling class.
  int64 backlog_size = 2;
}

message ReportWorkerBacklogRequest {
  // Unique id of the worker that's reporting the backlog
  bytes worker_id = 1;
  // Backlog report per scheduling class
  repeated WorkerBacklogReport backlog_reports = 2;
}

message ReportWorkerBacklogReply {}

// Request a worker from the raylet with the specified resources.
message RequestWorkerLeaseRequest {
  // TaskSpec containing the requested resources.
  TaskSpec resource_spec = 1;
  // Worker's backlog size for this spec's shape.
  int64 backlog_size = 2;
  // If it's true, either grant the lease if the task is
  // locally schedulable or reject the request.
  // Else, the raylet may return another raylet at which to retry the request.
  bool grant_or_reject = 3;
}

message RequestWorkerLeaseReply {
  // Address of the leased worker. If this is empty, then the request should be
  // retried at the provided raylet address.
  Address worker_address = 1;
  // Address of the raylet to spill back to, if any.
  Address retry_at_raylet_address = 2;
  // Resource mapping ids acquired by the leased worker.
  repeated ResourceMapEntry resource_mapping = 3;
  // Whether this lease request was canceled. In this case, the
  // client should try again if the resources are still required.
  bool canceled = 4;
  // Whether creating the runtime environment for this lease request failed.
  // If this is true, the corresponding task should be failed by the client.
  bool runtime_env_setup_failed = 5;
  // PID of the worker process.
  uint32 worker_pid = 6;
  // Whether the request was rejected because of insufficient resources.
  bool rejected = 7;
  // The (normal task) resources data to be carried by the Reply.
  ResourcesData resources_data = 8;
}

message PrepareBundleResourcesRequest {
  // Bundles that containing the requested resources.
  repeated Bundle bundle_specs = 1;
}

message PrepareBundleResourcesReply {
  // The status if prepare request was successful.
  bool success = 1;
}

message CommitBundleResourcesRequest {
  // Bundles containing the requested resources.
  repeated Bundle bundle_specs = 1;
}

message CommitBundleResourcesReply {
}

message CancelResourceReserveRequest {
  // Bundle containing the requested resources.
  Bundle bundle_spec = 1;
}

message CancelResourceReserveReply {
}

// Release a worker back to its raylet.
message ReturnWorkerRequest {
  // Port of the leased worker that we are now returning.
  int32 worker_port = 1;
  // Unique id of the leased worker we are now returning.
  bytes worker_id = 2;
  // If true, there was some unrecoverable error and the raylet should
  // disconnect the worker.
  bool disconnect_worker = 3;
}

message ReturnWorkerReply {
}

message ReleaseUnusedWorkersRequest {
  repeated bytes worker_ids_in_use = 1;
}

message ReleaseUnusedWorkersReply {
}

message ShutdownRayletRequest {
  /// Whether the shutdown request is graceful or not.
  bool graceful = 1;
}

message ShutdownRayletReply {}

message CancelWorkerLeaseRequest {
  // The task to cancel.
  bytes task_id = 1;
}

message CancelWorkerLeaseReply {
  // Whether the cancellation request was successful. Cancellation
  // succeeds if the node manager has the task queued upon receiving
  // the cancellation request, and the node manager has not yet
  // granted the lease.
  bool success = 1;
}

message PinObjectIDsRequest {
  // Address of the owner to ask when to unpin the objects.
  Address owner_address = 1;
  // ObjectIDs to pin.
  repeated bytes object_ids = 2;
}

message PinObjectIDsReply {
}

message GetNodeStatsRequest {
  // Whether to include memory stats. This could be large since it includes
  // metadata for all live object references.
  bool include_memory_info = 1;
}

// Object store stats, which may be reported per-node or aggregated across
// multiple nodes in the cluster (values are additive).
message ObjectStoreStats {
  // The amount of wall time total where spilling was happening.
  double spill_time_total_s = 1;
  // The number of bytes spilled total.
  int64 spilled_bytes_total = 2;
  // The number of objects spilled total.
  int64 spilled_objects_total = 3;
  // The amount of wall time total where object restore was happening.
  double restore_time_total_s = 4;
  // The number of bytes restored total.
  int64 restored_bytes_total = 5;
  // The number of objects restored total.
  int64 restored_objects_total = 6;
  // The current usage of the object store.
  int64 object_store_bytes_used = 7;
  // The max capacity of the object store.
  int64 object_store_bytes_avail = 8;
  // The number of bytes pinned as the primary copy of objects.
  int64 object_store_bytes_primary_copy = 9;
  // The number of bytes allocated from the filesystem (fallback allocs).
  int64 object_store_bytes_fallback = 10;
  // The number of local objects total.
  int64 num_local_objects = 11;
  // The number of plasma object bytes that are consumed by core workers.
  int64 consumed_bytes = 12;
  // Whether this node has object pulls queued. This can happen if
  // the node has more pull requests than available object store
  // memory.
  bool object_pulls_queued = 13;
}

message GetNodeStatsReply {
  repeated CoreWorkerStats core_workers_stats = 1;
  repeated ViewData view_data = 2;
  uint32 num_workers = 3;
  repeated TaskSpec infeasible_tasks = 4;
  repeated TaskSpec ready_tasks = 5;
  ObjectStoreStats store_stats = 6;
}

message GlobalGCRequest {
}

message GlobalGCReply {
}

// Accumulates memory info across all nodes. To access per-node memory info,
// use GetNodeStats() calls instead.
message FormatGlobalMemoryInfoRequest {
  // Whether or not the reply should include memory summary.
  // If it is true, it will add extra overhead to the system
  // because getting memory info requires to ping every core worker
  // in the cluster.
  bool include_memory_info = 1;
}

message FormatGlobalMemoryInfoReply {
  // A tabular summary of the memory stats. To get this data in structured form,
  // you can instead use GetNodeStats() directly.
  string memory_summary = 1;
  // Aggregate store stats across all nodes. To get the individual node data,
  // you can instead use GetNodeStats() directly.
  ObjectStoreStats store_stats = 2;
}

message RequestObjectSpillageRequest {
  // ObjectID to spill.
  bytes object_id = 1;
}

message RequestObjectSpillageReply {
  // Whether the object spilling was successful or not.
  bool success = 1;
  // Object URL where the object is spilled.
  string object_url = 2;
  // The node id of a node where the object is spilled.
  bytes spilled_node_id = 3;
}

message ReleaseUnusedBundlesRequest {
  repeated Bundle bundles_in_use = 1;
}

message ReleaseUnusedBundlesReply {
}

message GetSystemConfigRequest {
}

message GetSystemConfigReply {
  string system_config = 1;
}

message RequestResourceReportRequest {
}

message RequestResourceReportReply {
  ResourcesData resources = 1;
}

message UpdateResourceUsageRequest {
  // This is a serialized version of ResourceUsageBatchData. This extra layer of
  // serialization allows the sender to cache the expensive operation of serializing a
  // `ResourceUsageBatchData` when sending this request to all nodes.
  bytes serialized_resource_usage_batch = 1;
}

message UpdateResourceUsageReply {
}

message GetGcsServerAddressRequest {
}

message GetGcsServerAddressReply {
  string ip = 1;
  int32 port = 2;
}

// Service for inter-node-manager communication.
service NodeManagerService {
  // Update the node's view of the cluster resource usage
  rpc UpdateResourceUsage(UpdateResourceUsageRequest) returns (UpdateResourceUsageReply);
  // Request the current resource usage from this raylet
  rpc RequestResourceReport(RequestResourceReportRequest)
      returns (RequestResourceReportReply);
  // Request a worker from the raylet.
  rpc RequestWorkerLease(RequestWorkerLeaseRequest) returns (RequestWorkerLeaseReply);
  // Report task backlog information from a worker to the raylet
  rpc ReportWorkerBacklog(ReportWorkerBacklogRequest) returns (ReportWorkerBacklogReply);
  // Release a worker back to its raylet.
  rpc ReturnWorker(ReturnWorkerRequest) returns (ReturnWorkerReply);
  // This method is only used by GCS, and the purpose is to release leased workers
  // that may be leaked. When GCS restarts, it doesn't know which workers it has leased
  // in the previous lifecycle. In this case, GCS will send a list of worker ids that
  // are still needed. And Raylet will release other leased workers.
  rpc ReleaseUnusedWorkers(ReleaseUnusedWorkersRequest)
      returns (ReleaseUnusedWorkersReply);
  /// Shutdown the raylet (node manager) gracefully.
  rpc ShutdownRaylet(ShutdownRayletRequest) returns (ShutdownRayletReply);
  // Request a raylet to lock resources for a bundle.
  // This is the first phase of 2PC protocol for atomic placement group creation.
  rpc PrepareBundleResources(PrepareBundleResourcesRequest)
      returns (PrepareBundleResourcesReply);
  // Commit bundle resources to a raylet.
  // This is the second phase of 2PC protocol for atomic placement group creation.
  rpc CommitBundleResources(CommitBundleResourcesRequest)
      returns (CommitBundleResourcesReply);
  // Return resource for the raylet.
  rpc CancelResourceReserve(CancelResourceReserveRequest)
      returns (CancelResourceReserveReply);
  // Cancel a pending lease request. This only returns success if the
  // lease request was not yet granted.
  rpc CancelWorkerLease(CancelWorkerLeaseRequest) returns (CancelWorkerLeaseReply);
  // Pin the provided object IDs.
  rpc PinObjectIDs(PinObjectIDsRequest) returns (PinObjectIDsReply);
  // Get the current node stats.
  rpc GetNodeStats(GetNodeStatsRequest) returns (GetNodeStatsReply);
  // Trigger garbage collection in all workers across the cluster.
  rpc GlobalGC(GlobalGCRequest) returns (GlobalGCReply);
  // Get global object reference stats in formatted form.
  rpc FormatGlobalMemoryInfo(FormatGlobalMemoryInfoRequest)
      returns (FormatGlobalMemoryInfoReply);
  // Ask the raylet to spill an object to external storage.
  rpc RequestObjectSpillage(RequestObjectSpillageRequest)
      returns (RequestObjectSpillageReply);
  // This method is only used by GCS, and the purpose is to release bundles
  // that may be leaked. When GCS restarts, it doesn't know which bundles it has leased
  // in the previous lifecycle. In this case, GCS will send a list of bundles that
  // are still needed. And Raylet will release other bundles.
  rpc ReleaseUnusedBundles(ReleaseUnusedBundlesRequest)
      returns (ReleaseUnusedBundlesReply);
  // Get the system config.
  rpc GetSystemConfig(GetSystemConfigRequest) returns (GetSystemConfigReply);
  // Get gcs server address.
  rpc GetGcsServerAddress(GetGcsServerAddressRequest) returns (GetGcsServerAddressReply);
}
