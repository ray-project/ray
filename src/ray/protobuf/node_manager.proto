// Copyright 2017 The Ray Authors.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//  http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

syntax = "proto3";
option cc_enable_arenas = true;

package ray.rpc;

import "src/ray/protobuf/common.proto";
import "src/ray/protobuf/gcs.proto";

message WorkerBacklogReport {
  // TaskSpec indicating the scheduling class.
  // Cannot send scheduling class directly
  // since it's local to each process.
  TaskSpec resource_spec = 1;
  // Size of the backlog for the above scheduling class.
  int64 backlog_size = 2;
}

message ReportWorkerBacklogRequest {
  // Unique id of the worker that's reporting the backlog
  bytes worker_id = 1;
  // Backlog report per scheduling class
  repeated WorkerBacklogReport backlog_reports = 2;
}

message ReportWorkerBacklogReply {}

// Request a worker from the raylet with the specified resources.
message RequestWorkerLeaseRequest {
  // TaskSpec containing the requested resources.
  TaskSpec resource_spec = 1;
  // Worker's backlog size for this spec's shape.
  int64 backlog_size = 2;
  // If it's true, either grant the lease if the task is
  // locally schedulable or reject the request.
  // Else, the raylet may return another raylet at which to retry the request.
  bool grant_or_reject = 3;
  // If it's true, then the current raylet is selected
  // due to the locality of task arguments.
  bool is_selected_based_on_locality = 4;
}

message RequestWorkerLeaseReply {
  enum SchedulingFailureType {
    // The default failure type is "not failed".
    NOT_FAILED = 0;
    // Scheduling is failed on this node.
    SCHEDULING_FAILED = 1;
    // Scheduling is cancelled because task/actor's placement is removed.
    SCHEDULING_CANCELLED_PLACEMENT_GROUP_REMOVED = 2;
    // Scheduling is cancelled because task/actor's runtime environment setup is failed
    SCHEDULING_CANCELLED_RUNTIME_ENV_SETUP_FAILED = 3;
    // Scheduling is cancelled because task/actor is intentionally cancelled. E.g.,
    // ray.kill or ray.cancel
    SCHEDULING_CANCELLED_INTENDED = 4;
    // Scheduling is cancelled because the task/actor is no longer schedulable.
    SCHEDULING_CANCELLED_UNSCHEDULABLE = 5;
  }

  // Address of the leased worker. If this is empty, then the request should be
  // retried at the provided raylet address.
  Address worker_address = 1;
  // Address of the raylet to spill back to, if any.
  Address retry_at_raylet_address = 2;
  // Resource mapping ids acquired by the leased worker.
  repeated ResourceMapEntry resource_mapping = 3;
  // Whether this lease request was canceled.
  bool canceled = 4;
  // PID of the worker process.
  uint32 worker_pid = 6;
  // Whether the request was rejected because of insufficient resources.
  bool rejected = 7;
  // The (normal task) resources data to be carried by the Reply.
  ResourcesData resources_data = 8;
  // Scheduling failure type.
  // Must be set only when canceled is set.
  SchedulingFailureType failure_type = 9;
  // The error message explaining why scheduling has failed.
  // Must be an empty string if failure_type is `NOT_FAILED`.
  string scheduling_failure_message = 10;
}

message PrepareBundleResourcesRequest {
  // Bundles that containing the requested resources.
  repeated Bundle bundle_specs = 1;
}

message PrepareBundleResourcesReply {
  // The status if prepare request was successful.
  bool success = 1;
}

message CommitBundleResourcesRequest {
  // Bundles containing the requested resources.
  repeated Bundle bundle_specs = 1;
}

message CommitBundleResourcesReply {
}

message CancelResourceReserveRequest {
  // Bundle containing the requested resources.
  Bundle bundle_spec = 1;
}

message CancelResourceReserveReply {
}

// Release a worker back to its raylet.
message ReturnWorkerRequest {
  // Port of the leased worker that we are now returning.
  int32 worker_port = 1;
  // Unique id of the leased worker we are now returning.
  bytes worker_id = 2;
  // If true, there was some unrecoverable error and the raylet should
  // disconnect the worker.
  bool disconnect_worker = 3;
  // Whether the worker is exiting and cannot be reused.
  bool worker_exiting = 4;
}

message ReturnWorkerReply {
}

message ReleaseUnusedWorkersRequest {
  repeated bytes worker_ids_in_use = 1;
}

message ReleaseUnusedWorkersReply {
}

message ShutdownRayletRequest {
  /// Whether the shutdown request is graceful or not.
  bool graceful = 1;
}

message ShutdownRayletReply {}

message CancelWorkerLeaseRequest {
  // The task to cancel.
  bytes task_id = 1;
}

message CancelWorkerLeaseReply {
  // Whether the cancellation request was successful. Cancellation
  // succeeds if the node manager has the task queued upon receiving
  // the cancellation request, and the node manager has not yet
  // granted the lease.
  bool success = 1;
}

message PinObjectIDsRequest {
  // Address of the owner to ask when to unpin the objects.
  Address owner_address = 1;
  // ObjectIDs to pin.
  repeated bytes object_ids = 2;
  // For object IDs that were generated dynamically during task execution. The
  // owner learns of these ObjectRefs at task execution time instead of at
  // ray.put() or f.remote() time. This is the outer ObjectID that the
  // dynamically generated ObjectRefs are stored inside. We need this information
  // so that we can notify the owner about the nested ObjectRefs, in case the
  // owner does not know about them yet.
  optional bytes generator_id = 3;
}

message PinObjectIDsReply {
  // Whether pinning the corresponding object succeeded or not.
  // Pin can fail if the object is already evicted.
  repeated bool successes = 1;
}

message GetNodeStatsRequest {
  // Whether to include memory stats. This could be large since it includes
  // metadata for all live object references.
  bool include_memory_info = 1;
}

// Object store stats, which may be reported per-node or aggregated across
// multiple nodes in the cluster (values are additive).
message ObjectStoreStats {
  // The amount of wall time total where spilling was happening.
  double spill_time_total_s = 1;
  // The number of bytes spilled total.
  int64 spilled_bytes_total = 2;
  // The number of objects spilled total.
  int64 spilled_objects_total = 3;
  // The amount of wall time total where object restore was happening.
  double restore_time_total_s = 4;
  // The number of bytes restored total.
  int64 restored_bytes_total = 5;
  // The number of objects restored total.
  int64 restored_objects_total = 6;
  // The current usage of the object store.
  int64 object_store_bytes_used = 7;
  // The max capacity of the object store.
  int64 object_store_bytes_avail = 8;
  // The number of bytes pinned as the primary copy of objects.
  int64 object_store_bytes_primary_copy = 9;
  // The number of bytes allocated from the filesystem (fallback allocs).
  int64 object_store_bytes_fallback = 10;
  // The number of local objects total.
  int64 num_local_objects = 11;
  // The number of plasma object bytes that are consumed by core workers.
  int64 consumed_bytes = 12;
  // Whether this node has object pulls queued. This can happen if
  // the node has more pull requests than available object store
  // memory.
  bool object_pulls_queued = 13;
}

message GetNodeStatsReply {
  repeated CoreWorkerStats core_workers_stats = 1;
  uint32 num_workers = 3;
  ObjectStoreStats store_stats = 6;
}

message GlobalGCRequest {
}

message GlobalGCReply {
}

// Accumulates memory info across all nodes. To access per-node memory info,
// use GetNodeStats() calls instead.
message FormatGlobalMemoryInfoRequest {
  // Whether or not the reply should include memory summary.
  // If it is true, it will add extra overhead to the system
  // because getting memory info requires to ping every core worker
  // in the cluster.
  bool include_memory_info = 1;
}

message FormatGlobalMemoryInfoReply {
  // A tabular summary of the memory stats. To get this data in structured form,
  // you can instead use GetNodeStats() directly.
  string memory_summary = 1;
  // Aggregate store stats across all nodes. To get the individual node data,
  // you can instead use GetNodeStats() directly.
  ObjectStoreStats store_stats = 2;
}

message RequestObjectSpillageRequest {
  // ObjectID to spill.
  bytes object_id = 1;
}

message RequestObjectSpillageReply {
  // Whether the object spilling was successful or not.
  bool success = 1;
  // Object URL where the object is spilled.
  string object_url = 2;
  // The node id of a node where the object is spilled.
  bytes spilled_node_id = 3;
}

message ReleaseUnusedBundlesRequest {
  repeated Bundle bundles_in_use = 1;
}

message ReleaseUnusedBundlesReply {
}

message GetSystemConfigRequest {
}

message GetSystemConfigReply {
  string system_config = 1;
}

message RequestResourceReportRequest {
}

message RequestResourceReportReply {
  ResourcesData resources = 1;
}

message UpdateResourceUsageRequest {
  // This is a serialized version of ResourceUsageBatchData. This extra layer of
  // serialization allows the sender to cache the expensive operation of serializing a
  // `ResourceUsageBatchData` when sending this request to all nodes.
  bytes serialized_resource_usage_batch = 1;
}

message UpdateResourceUsageReply {
}

message GetTasksInfoRequest {
  // Maximum number of entries to return.
  // If not specified, return the whole entries without truncation.
  optional int64 limit = 1;
}

message GetTasksInfoReply {
  repeated TaskInfoEntry owned_task_info_entries = 1;
  // A list of task that's currently running.
  // All tasks must be treated as TaskStatus::RUNNING
  // from the caller side.
  repeated bytes running_task_ids = 2;
  // Length of the corresponding resource without truncation.
  int64 total = 3;
  // NOTE: we don't truncate running_task_ids because it is usually small.
  // However, there are edge cases it can be very large (e.g., when the
  // async actor tasks are used, it can have up to 1000 running tasks per
  // actor). We should handle this better.
}

message GetObjectsInfoRequest {
  // Maximum number of entries to return.
  // If not specified, return the whole entries without truncation.
  optional int64 limit = 1;
}

message GetObjectsInfoReply {
  repeated CoreWorkerStats core_workers_stats = 1;
  // Length of the corresponding resource without truncation.
  int64 total = 2;
}

message GetResourceLoadRequest {}

message GetResourceLoadReply {
  ResourcesData resources = 1;
}

message NotifyGCSRestartRequest {}

message NotifyGCSRestartReply {}

message GetTaskFailureCauseRequest {
  bytes task_id = 1;
}

message GetTaskFailureCauseReply {
  optional RayErrorInfo failure_cause = 1;
  bool fail_task_immediately = 2;
}

// Service for inter-node-manager communication.
service NodeManagerService {
  // Update the node's view of the cluster resource usage.
  rpc UpdateResourceUsage(UpdateResourceUsageRequest) returns (UpdateResourceUsageReply);
  // Handle the case when GCS restarted.
  rpc NotifyGCSRestart(NotifyGCSRestartRequest) returns (NotifyGCSRestartReply);
  // Get the resource load of the raylet.
  rpc GetResourceLoad(GetResourceLoadRequest) returns (GetResourceLoadReply);
  // Request the current resource usage from this raylet
  rpc RequestResourceReport(RequestResourceReportRequest)
      returns (RequestResourceReportReply);
  // Request a worker from the raylet.
  rpc RequestWorkerLease(RequestWorkerLeaseRequest) returns (RequestWorkerLeaseReply);
  // Report task backlog information from a worker to the raylet
  rpc ReportWorkerBacklog(ReportWorkerBacklogRequest) returns (ReportWorkerBacklogReply);
  // Release a worker back to its raylet.
  rpc ReturnWorker(ReturnWorkerRequest) returns (ReturnWorkerReply);
  // This method is only used by GCS, and the purpose is to release leased workers
  // that may be leaked. When GCS restarts, it doesn't know which workers it has leased
  // in the previous lifecycle. In this case, GCS will send a list of worker ids that
  // are still needed. And Raylet will release other leased workers.
  rpc ReleaseUnusedWorkers(ReleaseUnusedWorkersRequest)
      returns (ReleaseUnusedWorkersReply);
  /// Shutdown the raylet (node manager) gracefully.
  rpc ShutdownRaylet(ShutdownRayletRequest) returns (ShutdownRayletReply);
  // Request a raylet to lock resources for a bundle.
  // This is the first phase of 2PC protocol for atomic placement group creation.
  rpc PrepareBundleResources(PrepareBundleResourcesRequest)
      returns (PrepareBundleResourcesReply);
  // Commit bundle resources to a raylet.
  // This is the second phase of 2PC protocol for atomic placement group creation.
  rpc CommitBundleResources(CommitBundleResourcesRequest)
      returns (CommitBundleResourcesReply);
  // Return resource for the raylet.
  rpc CancelResourceReserve(CancelResourceReserveRequest)
      returns (CancelResourceReserveReply);
  // Cancel a pending lease request. This only returns success if the
  // lease request was not yet granted.
  rpc CancelWorkerLease(CancelWorkerLeaseRequest) returns (CancelWorkerLeaseReply);
  // Pin the provided object IDs.
  rpc PinObjectIDs(PinObjectIDsRequest) returns (PinObjectIDsReply);
  // Get the current node stats.
  rpc GetNodeStats(GetNodeStatsRequest) returns (GetNodeStatsReply);
  // Trigger garbage collection in all workers across the cluster.
  rpc GlobalGC(GlobalGCRequest) returns (GlobalGCReply);
  // Get global object reference stats in formatted form.
  rpc FormatGlobalMemoryInfo(FormatGlobalMemoryInfoRequest)
      returns (FormatGlobalMemoryInfoReply);
  // Ask the raylet to spill an object to external storage.
  rpc RequestObjectSpillage(RequestObjectSpillageRequest)
      returns (RequestObjectSpillageReply);
  // This method is only used by GCS, and the purpose is to release bundles
  // that may be leaked. When GCS restarts, it doesn't know which bundles it has leased
  // in the previous lifecycle. In this case, GCS will send a list of bundles that
  // are still needed. And Raylet will release other bundles.
  rpc ReleaseUnusedBundles(ReleaseUnusedBundlesRequest)
      returns (ReleaseUnusedBundlesReply);
  // Get the system config.
  rpc GetSystemConfig(GetSystemConfigRequest) returns (GetSystemConfigReply);
  // [State API] Get the all task information of the node.
  rpc GetTasksInfo(GetTasksInfoRequest) returns (GetTasksInfoReply);
  // [State API] Get the all object information of the node.
  rpc GetObjectsInfo(GetObjectsInfoRequest) returns (GetObjectsInfoReply);
  // Gets the task execution result. May contain a result if
  // the task completed in error.
  rpc GetTaskFailureCause(GetTaskFailureCauseRequest) returns (GetTaskFailureCauseReply);
}
