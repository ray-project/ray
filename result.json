{"traceEvents": [{"ph": "M", "pid": 56824, "tid": 56824, "name": "process_name", "args": {"name": "MainProcess"}}, {"ph": "M", "pid": 56824, "tid": 13435864, "name": "thread_name", "args": {"name": "MainThread"}}, {"pid": 56824, "tid": 13435864, "ts": 3673945087473.0, "ph": "X", "cat": "fee", "dur": 24.0, "name": "<module> (/tmp/a.json:1)"}, {"pid": 56824, "tid": 13435864, "ts": 3673945087472.0, "ph": "X", "cat": "fee", "dur": 27.0, "name": "builtins.exec"}, {"pid": 56824, "tid": 13435864, "ts": 3673945087502.0, "ph": "X", "cat": "fee", "dur": 0.02, "name": "info (/Users/sangcho/anaconda3/envs/core/lib/python3.9/multiprocessing/util.py:52)"}, {"pid": 56824, "tid": 13435864, "ts": 3673945087502.04, "ph": "X", "cat": "fee", "dur": 0.02, "name": "debug (/Users/sangcho/anaconda3/envs/core/lib/python3.9/multiprocessing/util.py:48)"}, {"pid": 56824, "tid": 13435864, "ts": 3673945087503.02, "ph": "X", "cat": "fee", "dur": 0.98, "name": "<lambda> (/Users/sangcho/anaconda3/envs/core/lib/python3.9/multiprocessing/util.py:284)"}, {"pid": 56824, "tid": 13435864, "ts": 3673945087503.0, "ph": "X", "cat": "fee", "dur": 1.02, "name": "<listcomp> (/Users/sangcho/anaconda3/envs/core/lib/python3.9/multiprocessing/util.py:291)"}, {"pid": 56824, "tid": 13435864, "ts": 3673945087504.04, "ph": "X", "cat": "fee", "dur": 0.02, "name": "list.sort"}, {"pid": 56824, "tid": 13435864, "ts": 3673945087502.08, "ph": "X", "cat": "fee", "dur": 2.0, "name": "_run_finalizers (/Users/sangcho/anaconda3/envs/core/lib/python3.9/multiprocessing/util.py:268)"}, {"pid": 56824, "tid": 13435864, "ts": 3673945087505.0, "ph": "X", "cat": "fee", "dur": 0.02, "name": "current_process (/Users/sangcho/anaconda3/envs/core/lib/python3.9/multiprocessing/process.py:37)"}, {"pid": 56824, "tid": 13435864, "ts": 3673945087505.06, "ph": "X", "cat": "fee", "dur": 0.94, "name": "_cleanup (/Users/sangcho/anaconda3/envs/core/lib/python3.9/multiprocessing/process.py:61)"}, {"pid": 56824, "tid": 13435864, "ts": 3673945087505.04, "ph": "X", "cat": "fee", "dur": 0.98, "name": "active_children (/Users/sangcho/anaconda3/envs/core/lib/python3.9/multiprocessing/process.py:43)"}, {"pid": 56824, "tid": 13435864, "ts": 3673945087506.06, "ph": "X", "cat": "fee", "dur": 0.02, "name": "_cleanup (/Users/sangcho/anaconda3/envs/core/lib/python3.9/multiprocessing/process.py:61)"}, {"pid": 56824, "tid": 13435864, "ts": 3673945087506.04, "ph": "X", "cat": "fee", "dur": 0.96, "name": "active_children (/Users/sangcho/anaconda3/envs/core/lib/python3.9/multiprocessing/process.py:43)"}, {"pid": 56824, "tid": 13435864, "ts": 3673945087507.02, "ph": "X", "cat": "fee", "dur": 0.02, "name": "debug (/Users/sangcho/anaconda3/envs/core/lib/python3.9/multiprocessing/util.py:48)"}, {"pid": 56824, "tid": 13435864, "ts": 3673945087507.1, "ph": "X", "cat": "fee", "dur": 0.02, "name": "<lambda> (/Users/sangcho/anaconda3/envs/core/lib/python3.9/multiprocessing/util.py:282)"}, {"pid": 56824, "tid": 13435864, "ts": 3673945087507.08, "ph": "X", "cat": "fee", "dur": 0.92, "name": "<listcomp> (/Users/sangcho/anaconda3/envs/core/lib/python3.9/multiprocessing/util.py:291)"}, {"pid": 56824, "tid": 13435864, "ts": 3673945087508.02, "ph": "X", "cat": "fee", "dur": 0.02, "name": "list.sort"}, {"pid": 56824, "tid": 13435864, "ts": 3673945087508.06, "ph": "X", "cat": "fee", "dur": 0.02, "name": "dict.get"}, {"pid": 56824, "tid": 13435864, "ts": 3673945087508.1, "ph": "X", "cat": "fee", "dur": 0.02, "name": "sub_debug (/Users/sangcho/anaconda3/envs/core/lib/python3.9/multiprocessing/util.py:44)"}, {"pid": 56824, "tid": 13435864, "ts": 3673945087509.02, "ph": "X", "cat": "fee", "dur": 0.02, "name": "posix.getpid"}, {"pid": 56824, "tid": 13435864, "ts": 3673945087510.0, "ph": "X", "cat": "fee", "dur": 0.02, "name": "sub_debug (/Users/sangcho/anaconda3/envs/core/lib/python3.9/multiprocessing/util.py:44)"}], "viztracer_metadata": {"overflow": false, "version": "0.16.0"}, "file_info": {"files": {"/tmp/a.json": ["{\"traceEvents\": [{\"ph\": \"M\", \"pid\": 56769, \"tid\": 56769, \"name\": \"process_name\", \"args\": {\"name\": \"MainProcess\"}}, {\"ph\": \"M\", \"pid\": 56769, \"tid\": 13435431, \"name\": \"thread_name\", \"args\": {\"name\": \"AsyncIO Thread: default\"}}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123137.0, \"dur\": 0.02, \"name\": \"connected (/Users/sangcho/work/ray/python/ray/_private/worker.py:471)\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123136.0, \"dur\": 2.0, \"name\": \"check_connected (/Users/sangcho/work/ray/python/ray/_private/worker.py:631)\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123155.0, \"dur\": 1.0, \"name\": \"_asyncio.get_running_loop\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123168.0, \"dur\": 0.02, \"name\": \"set.add\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123166.0, \"dur\": 2.04, \"name\": \"add (/Users/sangcho/anaconda3/envs/core/lib/python3.9/_weakrefset.py:86)\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123158.0, \"dur\": 11.0, \"name\": \"Loop.create_task\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123169.02, \"dur\": 0.02, \"name\": \"_set_task_name (/Users/sangcho/anaconda3/envs/core/lib/python3.9/asyncio/tasks.py:89)\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123154.0, \"dur\": 15.06, \"name\": \"create_task (/Users/sangcho/anaconda3/envs/core/lib/python3.9/asyncio/tasks.py:355)\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123176.0, \"dur\": 0.02, \"name\": \"_io.TextIOWrapper.write\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123176.04, \"dur\": 16.96, \"name\": \"_io.TextIOWrapper.flush\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123175.0, \"dur\": 18.02, \"name\": \"write (/Users/sangcho/work/ray/python/ray/_private/utils.py:405)\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123193.06, \"dur\": 5.94, \"name\": \"_io.TextIOWrapper.write\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123199.02, \"dur\": 0.02, \"name\": \"_io.TextIOWrapper.flush\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123193.04, \"dur\": 6.02, \"name\": \"write (/Users/sangcho/work/ray/python/ray/_private/utils.py:405)\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123205.0, \"dur\": 1.0, \"name\": \"builtins.hasattr\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123204.0, \"dur\": 2.02, \"name\": \"isfuture (/Users/sangcho/anaconda3/envs/core/lib/python3.9/asyncio/base_futures.py:14)\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123209.02, \"dur\": 0.98, \"name\": \"_abc._abc_instancecheck\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123209.0, \"dur\": 1.02, \"name\": \"__instancecheck__ (/Users/sangcho/anaconda3/envs/core/lib/python3.9/abc.py:117)\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123208.0, \"dur\": 2.04, \"name\": \"builtins.isinstance\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123207.0, \"dur\": 3.06, \"name\": \"iscoroutine (/Users/sangcho/anaconda3/envs/core/lib/python3.9/asyncio/coroutines.py:177)\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123210.08, \"dur\": 0.92, \"name\": \"_asyncio.get_running_loop\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123214.02, \"dur\": 0.02, \"name\": \"_abc._abc_instancecheck\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123214.0, \"dur\": 0.06, \"name\": \"__instancecheck__ (/Users/sangcho/anaconda3/envs/core/lib/python3.9/abc.py:117)\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123213.04, \"dur\": 1.04, \"name\": \"builtins.isinstance\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123213.02, \"dur\": 1.08, \"name\": \"iscoroutine (/Users/sangcho/anaconda3/envs/core/lib/python3.9/asyncio/coroutines.py:177)\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123213.0, \"dur\": 1.12, \"name\": \"<genexpr> (/Users/sangcho/anaconda3/envs/core/lib/python3.9/asyncio/tasks.py:405)\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123214.14, \"dur\": 0.02, \"name\": \"<genexpr> (/Users/sangcho/anaconda3/envs/core/lib/python3.9/asyncio/tasks.py:405)\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123212.0, \"dur\": 3.0, \"name\": \"builtins.any\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123216.08, \"dur\": 0.02, \"name\": \"_abc._abc_instancecheck\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123216.06, \"dur\": 0.06, \"name\": \"__instancecheck__ (/Users/sangcho/anaconda3/envs/core/lib/python3.9/abc.py:117)\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123216.04, \"dur\": 0.1, \"name\": \"builtins.isinstance\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123216.02, \"dur\": 0.14, \"name\": \"iscoroutine (/Users/sangcho/anaconda3/envs/core/lib/python3.9/asyncio/coroutines.py:177)\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123217.0, \"dur\": 0.02, \"name\": \"builtins.hasattr\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123216.18, \"dur\": 0.86, \"name\": \"isfuture (/Users/sangcho/anaconda3/envs/core/lib/python3.9/asyncio/base_futures.py:14)\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123218.0, \"dur\": 0.02, \"name\": \"_asyncio.Task.get_loop\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123217.06, \"dur\": 0.98, \"name\": \"_get_loop (/Users/sangcho/anaconda3/envs/core/lib/python3.9/asyncio/futures.py:296)\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123216.0, \"dur\": 2.06, \"name\": \"ensure_future (/Users/sangcho/anaconda3/envs/core/lib/python3.9/asyncio/tasks.py:657)\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123215.02, \"dur\": 3.98, \"name\": \"<setcomp> (/Users/sangcho/anaconda3/envs/core/lib/python3.9/asyncio/tasks.py:411)\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123220.02, \"dur\": 0.98, \"name\": \"Loop.create_future\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123221.02, \"dur\": 0.02, \"name\": \"builtins.len\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123221.06, \"dur\": 0.94, \"name\": \"_asyncio.Task.add_done_callback\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123220.0, \"dur\": 2.02, \"name\": \"_wait (/Users/sangcho/anaconda3/envs/core/lib/python3.9/asyncio/tasks.py:497)\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123203.0, \"dur\": 19.04, \"name\": \"wait (/Users/sangcho/anaconda3/envs/core/lib/python3.9/asyncio/tasks.py:373)\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123247.0, \"dur\": 1.0, \"name\": \"RLock (/Users/sangcho/anaconda3/envs/core/lib/python3.9/threading.py:82)\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123246.0, \"dur\": 4.0, \"name\": \"__init__ (/Users/sangcho/anaconda3/envs/core/lib/python3.9/threading.py:228)\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123245.0, \"dur\": 6.0, \"name\": \"__init__ (/Users/sangcho/anaconda3/envs/core/lib/python3.9/concurrent/futures/_base.py:318)\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123260.04, \"dur\": 0.96, \"name\": \"builtins.hasattr\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123260.02, \"dur\": 1.0, \"name\": \"isfuture (/Users/sangcho/anaconda3/envs/core/lib/python3.9/asyncio/base_futures.py:14)\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123261.04, \"dur\": 0.02, \"name\": \"builtins.isinstance\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123261.08, \"dur\": 0.92, \"name\": \"_asyncio.get_event_loop\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123262.02, \"dur\": 0.02, \"name\": \"Loop.create_future\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123263.02, \"dur\": 0.02, \"name\": \"builtins.hasattr\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123263.0, \"dur\": 0.06, \"name\": \"isfuture (/Users/sangcho/anaconda3/envs/core/lib/python3.9/asyncio/base_futures.py:14)\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123263.08, \"dur\": 0.02, \"name\": \"builtins.isinstance\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123264.0, \"dur\": 0.02, \"name\": \"builtins.hasattr\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123263.12, \"dur\": 0.92, \"name\": \"isfuture (/Users/sangcho/anaconda3/envs/core/lib/python3.9/asyncio/base_futures.py:14)\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123264.08, \"dur\": 0.02, \"name\": \"builtins.hasattr\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123264.06, \"dur\": 0.06, \"name\": \"isfuture (/Users/sangcho/anaconda3/envs/core/lib/python3.9/asyncio/base_futures.py:14)\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123264.16, \"dur\": 0.84, \"name\": \"builtins.hasattr\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123264.14, \"dur\": 0.88, \"name\": \"isfuture (/Users/sangcho/anaconda3/envs/core/lib/python3.9/asyncio/base_futures.py:14)\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123265.06, \"dur\": 0.02, \"name\": \"_asyncio.Future.get_loop\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123265.04, \"dur\": 0.06, \"name\": \"_get_loop (/Users/sangcho/anaconda3/envs/core/lib/python3.9/asyncio/futures.py:296)\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123265.12, \"dur\": 0.88, \"name\": \"_asyncio.Future.add_done_callback\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123267.0, \"dur\": 0.02, \"name\": \"_thread.RLock.__enter__\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123266.04, \"dur\": 1.0, \"name\": \"__enter__ (/Users/sangcho/anaconda3/envs/core/lib/python3.9/threading.py:256)\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123268.0, \"dur\": 0.02, \"name\": \"list.append\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123268.06, \"dur\": 0.02, \"name\": \"_thread.RLock.__exit__\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123268.04, \"dur\": 0.06, \"name\": \"__exit__ (/Users/sangcho/anaconda3/envs/core/lib/python3.9/threading.py:259)\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123266.02, \"dur\": 2.1, \"name\": \"add_done_callback (/Users/sangcho/anaconda3/envs/core/lib/python3.9/concurrent/futures/_base.py:398)\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123262.06, \"dur\": 6.08, \"name\": \"_chain_future (/Users/sangcho/anaconda3/envs/core/lib/python3.9/asyncio/futures.py:362)\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123260.0, \"dur\": 9.0, \"name\": \"wrap_future (/Users/sangcho/anaconda3/envs/core/lib/python3.9/asyncio/futures.py:404)\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123386.02, \"dur\": 0.02, \"name\": \"builtins.hasattr\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123386.0, \"dur\": 1.0, \"name\": \"isfuture (/Users/sangcho/anaconda3/envs/core/lib/python3.9/asyncio/base_futures.py:14)\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123388.02, \"dur\": 0.02, \"name\": \"_thread.RLock.__enter__\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123388.0, \"dur\": 0.06, \"name\": \"__enter__ (/Users/sangcho/anaconda3/envs/core/lib/python3.9/threading.py:256)\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123389.02, \"dur\": 0.02, \"name\": \"_thread.RLock.__exit__\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123389.0, \"dur\": 0.06, \"name\": \"__exit__ (/Users/sangcho/anaconda3/envs/core/lib/python3.9/threading.py:259)\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123387.04, \"dur\": 2.04, \"name\": \"done (/Users/sangcho/anaconda3/envs/core/lib/python3.9/concurrent/futures/_base.py:383)\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123390.0, \"dur\": 0.02, \"name\": \"_asyncio.Future.cancelled\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123390.04, \"dur\": 0.02, \"name\": \"_asyncio.Future.done\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123390.12, \"dur\": 0.02, \"name\": \"_thread.RLock.__enter__\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123390.1, \"dur\": 0.06, \"name\": \"__enter__ (/Users/sangcho/anaconda3/envs/core/lib/python3.9/threading.py:256)\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123391.02, \"dur\": 0.02, \"name\": \"_thread.RLock.__exit__\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123391.0, \"dur\": 0.06, \"name\": \"__exit__ (/Users/sangcho/anaconda3/envs/core/lib/python3.9/threading.py:259)\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123390.08, \"dur\": 1.0, \"name\": \"cancelled (/Users/sangcho/anaconda3/envs/core/lib/python3.9/concurrent/futures/_base.py:373)\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123391.14, \"dur\": 0.02, \"name\": \"_thread.RLock.__enter__\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123391.12, \"dur\": 0.06, \"name\": \"__enter__ (/Users/sangcho/anaconda3/envs/core/lib/python3.9/threading.py:256)\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123392.02, \"dur\": 0.02, \"name\": \"_thread.RLock.__exit__\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123392.0, \"dur\": 0.06, \"name\": \"__exit__ (/Users/sangcho/anaconda3/envs/core/lib/python3.9/threading.py:259)\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123391.1, \"dur\": 0.98, \"name\": \"exception (/Users/sangcho/anaconda3/envs/core/lib/python3.9/concurrent/futures/_base.py:453)\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123393.04, \"dur\": 0.02, \"name\": \"_thread.RLock.__enter__\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123393.02, \"dur\": 0.06, \"name\": \"__enter__ (/Users/sangcho/anaconda3/envs/core/lib/python3.9/threading.py:256)\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123393.1, \"dur\": 0.9, \"name\": \"__get_result (/Users/sangcho/anaconda3/envs/core/lib/python3.9/concurrent/futures/_base.py:388)\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123394.04, \"dur\": 0.02, \"name\": \"_thread.RLock.__exit__\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123394.02, \"dur\": 0.06, \"name\": \"__exit__ (/Users/sangcho/anaconda3/envs/core/lib/python3.9/threading.py:259)\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123393.0, \"dur\": 1.1, \"name\": \"result (/Users/sangcho/anaconda3/envs/core/lib/python3.9/concurrent/futures/_base.py:418)\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123394.12, \"dur\": 0.88, \"name\": \"_asyncio.Future.set_result\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123387.02, \"dur\": 8.0, \"name\": \"_copy_future_state (/Users/sangcho/anaconda3/envs/core/lib/python3.9/asyncio/futures.py:342)\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123385.0, \"dur\": 10.04, \"name\": \"_set_state (/Users/sangcho/anaconda3/envs/core/lib/python3.9/asyncio/futures.py:378)\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123408.02, \"dur\": 0.02, \"name\": \"_asyncio.Future.cancelled\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123408.0, \"dur\": 0.06, \"name\": \"_call_check_cancel (/Users/sangcho/anaconda3/envs/core/lib/python3.9/asyncio/futures.py:384)\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123424.0, \"dur\": 0.02, \"name\": \"_asyncio.Future.done\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123424.04, \"dur\": 0.96, \"name\": \"_asyncio.Future.set_result\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123423.0, \"dur\": 2.02, \"name\": \"_on_completion (/Users/sangcho/anaconda3/envs/core/lib/python3.9/asyncio/tasks.py:509)\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123438.0, \"dur\": 0.02, \"name\": \"_asyncio.Task.remove_done_callback\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123438.04, \"dur\": 0.02, \"name\": \"_asyncio.Task.done\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123438.08, \"dur\": 0.92, \"name\": \"set.add\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123437.04, \"dur\": 1.98, \"name\": \"_wait (/Users/sangcho/anaconda3/envs/core/lib/python3.9/asyncio/tasks.py:497)\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123437.02, \"dur\": 2.02, \"name\": \"wait (/Users/sangcho/anaconda3/envs/core/lib/python3.9/asyncio/tasks.py:373)\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123442.02, \"dur\": 0.02, \"name\": \"_io.TextIOWrapper.write\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123442.06, \"dur\": 5.94, \"name\": \"_io.TextIOWrapper.flush\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123442.0, \"dur\": 6.02, \"name\": \"write (/Users/sangcho/work/ray/python/ray/_private/utils.py:405)\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123448.06, \"dur\": 2.94, \"name\": \"_io.TextIOWrapper.write\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123451.02, \"dur\": 0.02, \"name\": \"_io.TextIOWrapper.flush\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123448.04, \"dur\": 3.02, \"name\": \"write (/Users/sangcho/work/ray/python/ray/_private/utils.py:405)\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123459.0, \"dur\": 1.0, \"name\": \"set.discard\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123458.0, \"dur\": 2.02, \"name\": \"_remove (/Users/sangcho/anaconda3/envs/core/lib/python3.9/_weakrefset.py:39)\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123461.0, \"dur\": 0.02, \"name\": \"time.time\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123464.0, \"dur\": 0.02, \"name\": \"_io.TextIOWrapper.write\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123464.04, \"dur\": 1.96, \"name\": \"_io.TextIOWrapper.flush\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123463.02, \"dur\": 3.0, \"name\": \"write (/Users/sangcho/work/ray/python/ray/_private/utils.py:405)\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123467.02, \"dur\": 1.98, \"name\": \"_io.TextIOWrapper.write\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123469.02, \"dur\": 0.02, \"name\": \"_io.TextIOWrapper.flush\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123467.0, \"dur\": 2.06, \"name\": \"write (/Users/sangcho/work/ray/python/ray/_private/utils.py:405)\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123463.0, \"dur\": 6.08, \"name\": \"builtins.print\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123469.1, \"dur\": 0.02, \"name\": \"time.time\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123471.02, \"dur\": 0.02, \"name\": \"RLock (/Users/sangcho/anaconda3/envs/core/lib/python3.9/threading.py:82)\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123471.0, \"dur\": 1.0, \"name\": \"__init__ (/Users/sangcho/anaconda3/envs/core/lib/python3.9/threading.py:228)\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123470.0, \"dur\": 3.0, \"name\": \"__init__ (/Users/sangcho/anaconda3/envs/core/lib/python3.9/concurrent/futures/_base.py:318)\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123477.0, \"dur\": 0.02, \"name\": \"builtins.hasattr\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123476.02, \"dur\": 1.02, \"name\": \"isfuture (/Users/sangcho/anaconda3/envs/core/lib/python3.9/asyncio/base_futures.py:14)\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123477.06, \"dur\": 0.02, \"name\": \"builtins.isinstance\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123478.0, \"dur\": 0.02, \"name\": \"_asyncio.get_event_loop\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123478.04, \"dur\": 0.02, \"name\": \"Loop.create_future\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123479.02, \"dur\": 0.02, \"name\": \"builtins.hasattr\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123479.0, \"dur\": 0.06, \"name\": \"isfuture (/Users/sangcho/anaconda3/envs/core/lib/python3.9/asyncio/base_futures.py:14)\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123479.08, \"dur\": 0.02, \"name\": \"builtins.isinstance\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123479.14, \"dur\": 0.02, \"name\": \"builtins.hasattr\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123479.12, \"dur\": 0.06, \"name\": \"isfuture (/Users/sangcho/anaconda3/envs/core/lib/python3.9/asyncio/base_futures.py:14)\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123479.22, \"dur\": 0.78, \"name\": \"builtins.hasattr\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123479.2, \"dur\": 0.82, \"name\": \"isfuture (/Users/sangcho/anaconda3/envs/core/lib/python3.9/asyncio/base_futures.py:14)\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123480.06, \"dur\": 0.02, \"name\": \"builtins.hasattr\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123480.04, \"dur\": 0.06, \"name\": \"isfuture (/Users/sangcho/anaconda3/envs/core/lib/python3.9/asyncio/base_futures.py:14)\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123480.14, \"dur\": 0.02, \"name\": \"_asyncio.Future.get_loop\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123480.12, \"dur\": 0.06, \"name\": \"_get_loop (/Users/sangcho/anaconda3/envs/core/lib/python3.9/asyncio/futures.py:296)\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123481.0, \"dur\": 0.02, \"name\": \"_asyncio.Future.add_done_callback\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123481.08, \"dur\": 0.02, \"name\": \"_thread.RLock.__enter__\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123481.06, \"dur\": 0.94, \"name\": \"__enter__ (/Users/sangcho/anaconda3/envs/core/lib/python3.9/threading.py:256)\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123482.02, \"dur\": 0.02, \"name\": \"list.append\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123482.08, \"dur\": 0.02, \"name\": \"_thread.RLock.__exit__\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123482.06, \"dur\": 0.06, \"name\": \"__exit__ (/Users/sangcho/anaconda3/envs/core/lib/python3.9/threading.py:259)\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123481.04, \"dur\": 1.1, \"name\": \"add_done_callback (/Users/sangcho/anaconda3/envs/core/lib/python3.9/concurrent/futures/_base.py:398)\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123478.08, \"dur\": 4.08, \"name\": \"_chain_future (/Users/sangcho/anaconda3/envs/core/lib/python3.9/asyncio/futures.py:362)\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123476.0, \"dur\": 6.18, \"name\": \"wrap_future (/Users/sangcho/anaconda3/envs/core/lib/python3.9/asyncio/futures.py:404)\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123437.0, \"dur\": 46.0, \"name\": \"stream (/Users/sangcho/work/ray/a.py:44)\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123514.02, \"dur\": 0.02, \"name\": \"builtins.hasattr\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123514.0, \"dur\": 0.06, \"name\": \"isfuture (/Users/sangcho/anaconda3/envs/core/lib/python3.9/asyncio/base_futures.py:14)\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123514.14, \"dur\": 0.86, \"name\": \"_thread.RLock.__enter__\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123514.12, \"dur\": 0.9, \"name\": \"__enter__ (/Users/sangcho/anaconda3/envs/core/lib/python3.9/threading.py:256)\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123515.06, \"dur\": 0.02, \"name\": \"_thread.RLock.__exit__\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123515.04, \"dur\": 0.06, \"name\": \"__exit__ (/Users/sangcho/anaconda3/envs/core/lib/python3.9/threading.py:259)\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123514.1, \"dur\": 1.02, \"name\": \"done (/Users/sangcho/anaconda3/envs/core/lib/python3.9/concurrent/futures/_base.py:383)\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123515.14, \"dur\": 0.02, \"name\": \"_asyncio.Future.cancelled\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123515.18, \"dur\": 0.02, \"name\": \"_asyncio.Future.done\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123516.02, \"dur\": 0.02, \"name\": \"_thread.RLock.__enter__\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123516.0, \"dur\": 0.06, \"name\": \"__enter__ (/Users/sangcho/anaconda3/envs/core/lib/python3.9/threading.py:256)\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123516.1, \"dur\": 0.02, \"name\": \"_thread.RLock.__exit__\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123516.08, \"dur\": 0.06, \"name\": \"__exit__ (/Users/sangcho/anaconda3/envs/core/lib/python3.9/threading.py:259)\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123515.22, \"dur\": 0.94, \"name\": \"cancelled (/Users/sangcho/anaconda3/envs/core/lib/python3.9/concurrent/futures/_base.py:373)\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123516.22, \"dur\": 0.78, \"name\": \"_thread.RLock.__enter__\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123516.2, \"dur\": 0.82, \"name\": \"__enter__ (/Users/sangcho/anaconda3/envs/core/lib/python3.9/threading.py:256)\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123517.06, \"dur\": 0.02, \"name\": \"_thread.RLock.__exit__\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123517.04, \"dur\": 0.06, \"name\": \"__exit__ (/Users/sangcho/anaconda3/envs/core/lib/python3.9/threading.py:259)\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123516.18, \"dur\": 0.94, \"name\": \"exception (/Users/sangcho/anaconda3/envs/core/lib/python3.9/concurrent/futures/_base.py:453)\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123517.18, \"dur\": 0.02, \"name\": \"_thread.RLock.__enter__\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123517.16, \"dur\": 0.06, \"name\": \"__enter__ (/Users/sangcho/anaconda3/envs/core/lib/python3.9/threading.py:256)\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123518.0, \"dur\": 0.02, \"name\": \"__get_result (/Users/sangcho/anaconda3/envs/core/lib/python3.9/concurrent/futures/_base.py:388)\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123518.06, \"dur\": 0.02, \"name\": \"_thread.RLock.__exit__\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123518.04, \"dur\": 0.06, \"name\": \"__exit__ (/Users/sangcho/anaconda3/envs/core/lib/python3.9/threading.py:259)\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123517.14, \"dur\": 0.98, \"name\": \"result (/Users/sangcho/anaconda3/envs/core/lib/python3.9/concurrent/futures/_base.py:418)\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123518.14, \"dur\": 0.86, \"name\": \"_asyncio.Future.set_result\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123514.08, \"dur\": 4.94, \"name\": \"_copy_future_state (/Users/sangcho/anaconda3/envs/core/lib/python3.9/asyncio/futures.py:342)\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123513.0, \"dur\": 6.04, \"name\": \"_set_state (/Users/sangcho/anaconda3/envs/core/lib/python3.9/asyncio/futures.py:378)\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123531.02, \"dur\": 0.02, \"name\": \"_asyncio.Future.cancelled\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123531.0, \"dur\": 0.06, \"name\": \"_call_check_cancel (/Users/sangcho/anaconda3/envs/core/lib/python3.9/asyncio/futures.py:384)\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123541.0, \"dur\": 0.02, \"name\": \"time.time\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123543.0, \"dur\": 0.02, \"name\": \"_io.TextIOWrapper.write\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123543.04, \"dur\": 2.96, \"name\": \"_io.TextIOWrapper.flush\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123542.02, \"dur\": 4.0, \"name\": \"write (/Users/sangcho/work/ray/python/ray/_private/utils.py:405)\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123546.06, \"dur\": 1.94, \"name\": \"_io.TextIOWrapper.write\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123549.0, \"dur\": 0.02, \"name\": \"_io.TextIOWrapper.flush\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123546.04, \"dur\": 3.0, \"name\": \"write (/Users/sangcho/work/ray/python/ray/_private/utils.py:405)\", \"ph\": \"X\", \"cat\": \"FEE\"}, {\"pid\": 56769, \"tid\": 13435431, \"ts\": 3673929123542.0, \"dur\": 7.06, \"name\": \"builtins.print\", \"ph\": \"X\", \"cat\": \"FEE\"}], \"viztracer_metadata\": {\"version\": \"0.16.0\", \"overflow\": false}, \"file_info\": {\"files\": {\"/Users/sangcho/work/ray/python/ray/_private/worker.py\": [\"import atexit\\nimport faulthandler\\nimport functools\\nimport inspect\\nimport io\\nimport json\\nimport logging\\nimport os\\nimport sys\\nimport threading\\nimport time\\nimport traceback\\nimport urllib\\nimport warnings\\nfrom abc import ABCMeta, abstractmethod\\nfrom collections.abc import Mapping\\nfrom contextlib import contextmanager\\nfrom dataclasses import dataclass\\nfrom typing import (\\n    IO,\\n    Any,\\n    AnyStr,\\n    Callable,\\n    Dict,\\n    Generic,\\n    Iterator,\\n    List,\\n    Optional,\\n    Sequence,\\n    Tuple,\\n    TypeVar,\\n    Union,\\n    overload,\\n)\\nfrom urllib.parse import urlparse\\n\\nimport colorama\\nimport setproctitle\\n\\nfrom typing import Literal, Protocol\\n\\nimport ray\\nimport ray._private.node\\nimport ray._private.parameter\\nimport ray._private.profiling as profiling\\nimport ray._private.ray_constants as ray_constants\\nimport ray._private.serialization as serialization\\nimport ray._private.services as services\\nimport ray._private.state\\nimport ray._private.storage as storage\\n\\n# Ray modules\\nimport ray.actor\\nimport ray.cloudpickle as pickle  # noqa\\nimport ray.job_config\\nimport ray.remote_function\\nfrom ray import ActorID, JobID, Language, ObjectRef\\nfrom ray._raylet import (\\n    StreamingObjectRefGenerator,\\n    raise_sys_exit_with_custom_error_message,\\n)\\nfrom ray.runtime_env.runtime_env import _merge_runtime_env\\nfrom ray._private import ray_option_utils\\nfrom ray._private.client_mode_hook import client_mode_hook\\nfrom ray._private.function_manager import FunctionActorManager\\n\\nfrom ray._private.inspect_util import is_cython\\nfrom ray._private.ray_logging import (\\n    global_worker_stdstream_dispatcher,\\n    stdout_deduplicator,\\n    stderr_deduplicator,\\n    setup_logger,\\n)\\nfrom ray._private.runtime_env.constants import RAY_JOB_CONFIG_JSON_ENV_VAR\\nfrom ray._private.runtime_env.py_modules import upload_py_modules_if_needed\\nfrom ray._private.runtime_env.working_dir import upload_working_dir_if_needed\\nfrom ray._private.runtime_env.setup_hook import (\\n    upload_worker_process_setup_hook_if_needed,\\n)\\nfrom ray._private.storage import _load_class\\nfrom ray._private.utils import get_ray_doc_version\\nfrom ray.exceptions import ObjectStoreFullError, RayError, RaySystemError, RayTaskError\\nfrom ray.experimental.internal_kv import (\\n    _initialize_internal_kv,\\n    _internal_kv_get,\\n    _internal_kv_initialized,\\n    _internal_kv_reset,\\n)\\nfrom ray.experimental import tqdm_ray\\nfrom ray.experimental.tqdm_ray import RAY_TQDM_MAGIC\\nfrom ray.util.annotations import Deprecated, DeveloperAPI, PublicAPI\\nfrom ray.util.debug import log_once\\nfrom ray.util.scheduling_strategies import PlacementGroupSchedulingStrategy\\nfrom ray.util.tracing.tracing_helper import _import_from_string\\nfrom ray.widgets import Template\\nfrom ray.widgets.util import repr_with_fallback\\n\\nSCRIPT_MODE = 0\\nWORKER_MODE = 1\\nLOCAL_MODE = 2\\nSPILL_WORKER_MODE = 3\\nRESTORE_WORKER_MODE = 4\\n\\n# Logger for this module. It should be configured at the entry point\\n# into the program using Ray. Ray provides a default configuration at\\n# entry/init points.\\nlogger = logging.getLogger(__name__)\\n\\n\\nT0 = TypeVar(\\\"T0\\\")\\nT1 = TypeVar(\\\"T1\\\")\\nT2 = TypeVar(\\\"T2\\\")\\nT3 = TypeVar(\\\"T3\\\")\\nT4 = TypeVar(\\\"T4\\\")\\nT5 = TypeVar(\\\"T5\\\")\\nT6 = TypeVar(\\\"T6\\\")\\nT7 = TypeVar(\\\"T7\\\")\\nT8 = TypeVar(\\\"T8\\\")\\nT9 = TypeVar(\\\"T9\\\")\\nR = TypeVar(\\\"R\\\")\\n\\nDAGNode = TypeVar(\\\"DAGNode\\\")\\n\\n\\nclass RemoteFunctionNoArgs(Generic[R]):\\n    def __init__(self, function: Callable[[], R]) -> None:\\n        pass\\n\\n    def remote(\\n        self,\\n    ) -> \\\"ObjectRef[R]\\\":\\n        ...\\n\\n    def bind(\\n        self,\\n    ) -> \\\"DAGNode[R]\\\":\\n        ...\\n\\n\\nclass RemoteFunction0(Generic[R, T0]):\\n    def __init__(self, function: Callable[[T0], R]) -> None:\\n        pass\\n\\n    def remote(\\n        self,\\n        __arg0: \\\"Union[T0, ObjectRef[T0]]\\\",\\n    ) -> \\\"ObjectRef[R]\\\":\\n        ...\\n\\n    def bind(\\n        self,\\n        __arg0: \\\"Union[T0, DAGNode[T0]]\\\",\\n    ) -> \\\"DAGNode[R]\\\":\\n        ...\\n\\n\\nclass RemoteFunction1(Generic[R, T0, T1]):\\n    def __init__(self, function: Callable[[T0, T1], R]) -> None:\\n        pass\\n\\n    def remote(\\n        self,\\n        __arg0: \\\"Union[T0, ObjectRef[T0]]\\\",\\n        __arg1: \\\"Union[T1, ObjectRef[T1]]\\\",\\n    ) -> \\\"ObjectRef[R]\\\":\\n        ...\\n\\n    def bind(\\n        self,\\n        __arg0: \\\"Union[T0, DAGNode[T0]]\\\",\\n        __arg1: \\\"Union[T1, DAGNode[T1]]\\\",\\n    ) -> \\\"DAGNode[R]\\\":\\n        ...\\n\\n\\nclass RemoteFunction2(Generic[R, T0, T1, T2]):\\n    def __init__(self, function: Callable[[T0, T1, T2], R]) -> None:\\n        pass\\n\\n    def remote(\\n        self,\\n        __arg0: \\\"Union[T0, ObjectRef[T0]]\\\",\\n        __arg1: \\\"Union[T1, ObjectRef[T1]]\\\",\\n        __arg2: \\\"Union[T2, ObjectRef[T2]]\\\",\\n    ) -> \\\"ObjectRef[R]\\\":\\n        ...\\n\\n    def bind(\\n        self,\\n        __arg0: \\\"Union[T0, DAGNode[T0]]\\\",\\n        __arg1: \\\"Union[T1, DAGNode[T1]]\\\",\\n        __arg2: \\\"Union[T2, DAGNode[T2]]\\\",\\n    ) -> \\\"DAGNode[R]\\\":\\n        ...\\n\\n\\nclass RemoteFunction3(Generic[R, T0, T1, T2, T3]):\\n    def __init__(self, function: Callable[[T0, T1, T2, T3], R]) -> None:\\n        pass\\n\\n    def remote(\\n        self,\\n        __arg0: \\\"Union[T0, ObjectRef[T0]]\\\",\\n        __arg1: \\\"Union[T1, ObjectRef[T1]]\\\",\\n        __arg2: \\\"Union[T2, ObjectRef[T2]]\\\",\\n        __arg3: \\\"Union[T3, ObjectRef[T3]]\\\",\\n    ) -> \\\"ObjectRef[R]\\\":\\n        ...\\n\\n    def bind(\\n        self,\\n        __arg0: \\\"Union[T0, DAGNode[T0]]\\\",\\n        __arg1: \\\"Union[T1, DAGNode[T1]]\\\",\\n        __arg2: \\\"Union[T2, DAGNode[T2]]\\\",\\n        __arg3: \\\"Union[T3, DAGNode[T3]]\\\",\\n    ) -> \\\"DAGNode[R]\\\":\\n        ...\\n\\n\\nclass RemoteFunction4(Generic[R, T0, T1, T2, T3, T4]):\\n    def __init__(self, function: Callable[[T0, T1, T2, T3, T4], R]) -> None:\\n        pass\\n\\n    def remote(\\n        self,\\n        __arg0: \\\"Union[T0, ObjectRef[T0]]\\\",\\n        __arg1: \\\"Union[T1, ObjectRef[T1]]\\\",\\n        __arg2: \\\"Union[T2, ObjectRef[T2]]\\\",\\n        __arg3: \\\"Union[T3, ObjectRef[T3]]\\\",\\n        __arg4: \\\"Union[T4, ObjectRef[T4]]\\\",\\n    ) -> \\\"ObjectRef[R]\\\":\\n        ...\\n\\n    def bind(\\n        self,\\n        __arg0: \\\"Union[T0, DAGNode[T0]]\\\",\\n        __arg1: \\\"Union[T1, DAGNode[T1]]\\\",\\n        __arg2: \\\"Union[T2, DAGNode[T2]]\\\",\\n        __arg3: \\\"Union[T3, DAGNode[T3]]\\\",\\n        __arg4: \\\"Union[T4, DAGNode[T4]]\\\",\\n    ) -> \\\"DAGNode[R]\\\":\\n        ...\\n\\n\\nclass RemoteFunction5(Generic[R, T0, T1, T2, T3, T4, T5]):\\n    def __init__(self, function: Callable[[T0, T1, T2, T3, T4, T5], R]) -> None:\\n        pass\\n\\n    def remote(\\n        self,\\n        __arg0: \\\"Union[T0, ObjectRef[T0]]\\\",\\n        __arg1: \\\"Union[T1, ObjectRef[T1]]\\\",\\n        __arg2: \\\"Union[T2, ObjectRef[T2]]\\\",\\n        __arg3: \\\"Union[T3, ObjectRef[T3]]\\\",\\n        __arg4: \\\"Union[T4, ObjectRef[T4]]\\\",\\n        __arg5: \\\"Union[T5, ObjectRef[T5]]\\\",\\n    ) -> \\\"ObjectRef[R]\\\":\\n        ...\\n\\n    def bind(\\n        self,\\n        __arg0: \\\"Union[T0, DAGNode[T0]]\\\",\\n        __arg1: \\\"Union[T1, DAGNode[T1]]\\\",\\n        __arg2: \\\"Union[T2, DAGNode[T2]]\\\",\\n        __arg3: \\\"Union[T3, DAGNode[T3]]\\\",\\n        __arg4: \\\"Union[T4, DAGNode[T4]]\\\",\\n        __arg5: \\\"Union[T5, DAGNode[T5]]\\\",\\n    ) -> \\\"DAGNode[R]\\\":\\n        ...\\n\\n\\nclass RemoteFunction6(Generic[R, T0, T1, T2, T3, T4, T5, T6]):\\n    def __init__(self, function: Callable[[T0, T1, T2, T3, T4, T5, T6], R]) -> None:\\n        pass\\n\\n    def remote(\\n        self,\\n        __arg0: \\\"Union[T0, ObjectRef[T0]]\\\",\\n        __arg1: \\\"Union[T1, ObjectRef[T1]]\\\",\\n        __arg2: \\\"Union[T2, ObjectRef[T2]]\\\",\\n        __arg3: \\\"Union[T3, ObjectRef[T3]]\\\",\\n        __arg4: \\\"Union[T4, ObjectRef[T4]]\\\",\\n        __arg5: \\\"Union[T5, ObjectRef[T5]]\\\",\\n        __arg6: \\\"Union[T6, ObjectRef[T6]]\\\",\\n    ) -> \\\"ObjectRef[R]\\\":\\n        ...\\n\\n    def bind(\\n        self,\\n        __arg0: \\\"Union[T0, DAGNode[T0]]\\\",\\n        __arg1: \\\"Union[T1, DAGNode[T1]]\\\",\\n        __arg2: \\\"Union[T2, DAGNode[T2]]\\\",\\n        __arg3: \\\"Union[T3, DAGNode[T3]]\\\",\\n        __arg4: \\\"Union[T4, DAGNode[T4]]\\\",\\n        __arg5: \\\"Union[T5, DAGNode[T5]]\\\",\\n        __arg6: \\\"Union[T6, DAGNode[T6]]\\\",\\n    ) -> \\\"DAGNode[R]\\\":\\n        ...\\n\\n\\nclass RemoteFunction7(Generic[R, T0, T1, T2, T3, T4, T5, T6, T7]):\\n    def __init__(self, function: Callable[[T0, T1, T2, T3, T4, T5, T6, T7], R]) -> None:\\n        pass\\n\\n    def remote(\\n        self,\\n        __arg0: \\\"Union[T0, ObjectRef[T0]]\\\",\\n        __arg1: \\\"Union[T1, ObjectRef[T1]]\\\",\\n        __arg2: \\\"Union[T2, ObjectRef[T2]]\\\",\\n        __arg3: \\\"Union[T3, ObjectRef[T3]]\\\",\\n        __arg4: \\\"Union[T4, ObjectRef[T4]]\\\",\\n        __arg5: \\\"Union[T5, ObjectRef[T5]]\\\",\\n        __arg6: \\\"Union[T6, ObjectRef[T6]]\\\",\\n        __arg7: \\\"Union[T7, ObjectRef[T7]]\\\",\\n    ) -> \\\"ObjectRef[R]\\\":\\n        ...\\n\\n    def bind(\\n        self,\\n        __arg0: \\\"Union[T0, DAGNode[T0]]\\\",\\n        __arg1: \\\"Union[T1, DAGNode[T1]]\\\",\\n        __arg2: \\\"Union[T2, DAGNode[T2]]\\\",\\n        __arg3: \\\"Union[T3, DAGNode[T3]]\\\",\\n        __arg4: \\\"Union[T4, DAGNode[T4]]\\\",\\n        __arg5: \\\"Union[T5, DAGNode[T5]]\\\",\\n        __arg6: \\\"Union[T6, DAGNode[T6]]\\\",\\n        __arg7: \\\"Union[T7, DAGNode[T7]]\\\",\\n    ) -> \\\"DAGNode[R]\\\":\\n        ...\\n\\n\\nclass RemoteFunction8(Generic[R, T0, T1, T2, T3, T4, T5, T6, T7, T8]):\\n    def __init__(\\n        self, function: Callable[[T0, T1, T2, T3, T4, T5, T6, T7, T8], R]\\n    ) -> None:\\n        pass\\n\\n    def remote(\\n        self,\\n        __arg0: \\\"Union[T0, ObjectRef[T0]]\\\",\\n        __arg1: \\\"Union[T1, ObjectRef[T1]]\\\",\\n        __arg2: \\\"Union[T2, ObjectRef[T2]]\\\",\\n        __arg3: \\\"Union[T3, ObjectRef[T3]]\\\",\\n        __arg4: \\\"Union[T4, ObjectRef[T4]]\\\",\\n        __arg5: \\\"Union[T5, ObjectRef[T5]]\\\",\\n        __arg6: \\\"Union[T6, ObjectRef[T6]]\\\",\\n        __arg7: \\\"Union[T7, ObjectRef[T7]]\\\",\\n        __arg8: \\\"Union[T8, ObjectRef[T8]]\\\",\\n    ) -> \\\"ObjectRef[R]\\\":\\n        ...\\n\\n    def bind(\\n        self,\\n        __arg0: \\\"Union[T0, DAGNode[T0]]\\\",\\n        __arg1: \\\"Union[T1, DAGNode[T1]]\\\",\\n        __arg2: \\\"Union[T2, DAGNode[T2]]\\\",\\n        __arg3: \\\"Union[T3, DAGNode[T3]]\\\",\\n        __arg4: \\\"Union[T4, DAGNode[T4]]\\\",\\n        __arg5: \\\"Union[T5, DAGNode[T5]]\\\",\\n        __arg6: \\\"Union[T6, DAGNode[T6]]\\\",\\n        __arg7: \\\"Union[T7, DAGNode[T7]]\\\",\\n        __arg8: \\\"Union[T8, DAGNode[T8]]\\\",\\n    ) -> \\\"DAGNode[R]\\\":\\n        ...\\n\\n\\nclass RemoteFunction9(Generic[R, T0, T1, T2, T3, T4, T5, T6, T7, T8, T9]):\\n    def __init__(\\n        self, function: Callable[[T0, T1, T2, T3, T4, T5, T6, T7, T8, T9], R]\\n    ) -> None:\\n        pass\\n\\n    def remote(\\n        self,\\n        __arg0: \\\"Union[T0, ObjectRef[T0]]\\\",\\n        __arg1: \\\"Union[T1, ObjectRef[T1]]\\\",\\n        __arg2: \\\"Union[T2, ObjectRef[T2]]\\\",\\n        __arg3: \\\"Union[T3, ObjectRef[T3]]\\\",\\n        __arg4: \\\"Union[T4, ObjectRef[T4]]\\\",\\n        __arg5: \\\"Union[T5, ObjectRef[T5]]\\\",\\n        __arg6: \\\"Union[T6, ObjectRef[T6]]\\\",\\n        __arg7: \\\"Union[T7, ObjectRef[T7]]\\\",\\n        __arg8: \\\"Union[T8, ObjectRef[T8]]\\\",\\n        __arg9: \\\"Union[T9, ObjectRef[T9]]\\\",\\n    ) -> \\\"ObjectRef[R]\\\":\\n        ...\\n\\n    def bind(\\n        self,\\n        __arg0: \\\"Union[T0, DAGNode[T0]]\\\",\\n        __arg1: \\\"Union[T1, DAGNode[T1]]\\\",\\n        __arg2: \\\"Union[T2, DAGNode[T2]]\\\",\\n        __arg3: \\\"Union[T3, DAGNode[T3]]\\\",\\n        __arg4: \\\"Union[T4, DAGNode[T4]]\\\",\\n        __arg5: \\\"Union[T5, DAGNode[T5]]\\\",\\n        __arg6: \\\"Union[T6, DAGNode[T6]]\\\",\\n        __arg7: \\\"Union[T7, DAGNode[T7]]\\\",\\n        __arg8: \\\"Union[T8, DAGNode[T8]]\\\",\\n        __arg9: \\\"Union[T9, DAGNode[T9]]\\\",\\n    ) -> \\\"DAGNode[R]\\\":\\n        ...\\n\\n\\n# Visible for testing.\\ndef _unhandled_error_handler(e: Exception):\\n    logger.error(\\n        f\\\"Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): {e}\\\"\\n    )\\n\\n\\nclass Worker:\\n    \\\"\\\"\\\"A class used to define the control flow of a worker process.\\n\\n    Note:\\n        The methods in this class are considered unexposed to the user. The\\n        functions outside of this class are considered exposed.\\n\\n    Attributes:\\n        node (ray._private.node.Node): The node this worker is attached to.\\n        mode: The mode of the worker. One of SCRIPT_MODE, LOCAL_MODE, and\\n            WORKER_MODE.\\n    \\\"\\\"\\\"\\n\\n    def __init__(self):\\n        \\\"\\\"\\\"Initialize a Worker object.\\\"\\\"\\\"\\n        self.node = None\\n        self.mode = None\\n        self.actors = {}\\n        # When the worker is constructed. Record the original value of the\\n        # (CUDA_VISIBLE_DEVICES, ONEAPI_DEVICE_SELECTOR, NEURON_RT_VISIBLE_CORES,\\n        # TPU_VISIBLE_CHIPS, ..) environment variables.\\n        self.original_visible_accelerator_ids = (\\n            ray._private.utils.get_visible_accelerator_ids()\\n        )\\n        # A dictionary that maps from driver id to SerializationContext\\n        # TODO: clean up the SerializationContext once the job finished.\\n        self.serialization_context_map = {}\\n        self.function_actor_manager = FunctionActorManager(self)\\n        # This event is checked regularly by all of the threads so that they\\n        # know when to exit.\\n        self.threads_stopped = threading.Event()\\n        # Index of the current session. This number will\\n        # increment every time when `ray.shutdown` is called.\\n        self._session_index = 0\\n        # If this is set, the next .remote call should drop into the\\n        # debugger, at the specified breakpoint ID.\\n        self.debugger_breakpoint = b\\\"\\\"\\n        # If this is set, ray.get calls invoked on the object ID returned\\n        # by the worker should drop into the debugger at the specified\\n        # breakpoint ID.\\n        self.debugger_get_breakpoint = b\\\"\\\"\\n        # If True, make the debugger external to the node this worker is\\n        # running on.\\n        self.ray_debugger_external = False\\n        self._load_code_from_local = False\\n        # Opened file descriptor to stdout/stderr for this python worker.\\n        self._enable_record_actor_task_log = (\\n            ray_constants.RAY_ENABLE_RECORD_ACTOR_TASK_LOGGING\\n        )\\n        self._out_file = None\\n        self._err_file = None\\n        # Create the lock here because the serializer will use it before\\n        # initializing Ray.\\n        self.lock = threading.RLock()\\n        # By default, don't show logs from other drivers. This is set to true by Serve\\n        # in order to stream logs from the controller and replica actors across\\n        # different drivers that connect to the same Serve instance.\\n        # See https://github.com/ray-project/ray/pull/35070.\\n        self._filter_logs_by_job = True\\n\\n    @property\\n    def connected(self):\\n        \\\"\\\"\\\"bool: True if Ray has been started and False otherwise.\\\"\\\"\\\"\\n        return self.node is not None\\n\\n    @property\\n    def node_ip_address(self):\\n        self.check_connected()\\n        return self.node.node_ip_address\\n\\n    @property\\n    def load_code_from_local(self):\\n        self.check_connected()\\n        return self._load_code_from_local\\n\\n    @property\\n    def current_job_id(self):\\n        if hasattr(self, \\\"core_worker\\\"):\\n            return self.core_worker.get_current_job_id()\\n        return JobID.nil()\\n\\n    @property\\n    def actor_id(self):\\n        if hasattr(self, \\\"core_worker\\\"):\\n            return self.core_worker.get_actor_id()\\n        return ActorID.nil()\\n\\n    @property\\n    def actor_name(self):\\n        if hasattr(self, \\\"core_worker\\\"):\\n            return self.core_worker.get_actor_name().decode(\\\"utf-8\\\")\\n        return None\\n\\n    @property\\n    def current_task_id(self):\\n        return self.core_worker.get_current_task_id()\\n\\n    @property\\n    def current_node_id(self):\\n        return self.core_worker.get_current_node_id()\\n\\n    @property\\n    def task_depth(self):\\n        return self.core_worker.get_task_depth()\\n\\n    @property\\n    def namespace(self):\\n        return self.core_worker.get_job_config().ray_namespace\\n\\n    @property\\n    def placement_group_id(self):\\n        return self.core_worker.get_placement_group_id()\\n\\n    @property\\n    def worker_id(self):\\n        return self.core_worker.get_worker_id().binary()\\n\\n    @property\\n    def should_capture_child_tasks_in_placement_group(self):\\n        return self.core_worker.should_capture_child_tasks_in_placement_group()\\n\\n    @property\\n    def current_session_and_job(self):\\n        \\\"\\\"\\\"Get the current session index and job id as pair.\\\"\\\"\\\"\\n        assert isinstance(self._session_index, int)\\n        assert isinstance(self.current_job_id, ray.JobID)\\n        return self._session_index, self.current_job_id\\n\\n    @property\\n    def runtime_env(self):\\n        \\\"\\\"\\\"Get the runtime env in json format\\\"\\\"\\\"\\n        return self.core_worker.get_current_runtime_env()\\n\\n    def set_err_file(self, err_file=Optional[IO[AnyStr]]) -> None:\\n        \\\"\\\"\\\"Set the worker's err file where stderr is redirected to\\\"\\\"\\\"\\n        self._err_file = err_file\\n\\n    def set_out_file(self, out_file=Optional[IO[AnyStr]]) -> None:\\n        \\\"\\\"\\\"Set the worker's out file where stdout is redirected to\\\"\\\"\\\"\\n        self._out_file = out_file\\n\\n    def record_task_log_start(self):\\n        \\\"\\\"\\\"Record the task log info when task starts executing for\\n        non concurrent actor tasks.\\\"\\\"\\\"\\n        if not self._enable_record_actor_task_log and not self.actor_id.is_nil():\\n            # We are not recording actor task log if not enabled explicitly.\\n            # Recording actor task log is expensive and should be enabled only\\n            # when needed.\\n            # https://github.com/ray-project/ray/issues/35598\\n            return\\n\\n        if not hasattr(self, \\\"core_worker\\\"):\\n            return\\n\\n        self.core_worker.record_task_log_start(\\n            self.get_out_file_path(),\\n            self.get_err_file_path(),\\n            self.get_current_out_offset(),\\n            self.get_current_err_offset(),\\n        )\\n\\n    def record_task_log_end(self):\\n        \\\"\\\"\\\"Record the task log info when task finishes executing for\\n        non concurrent actor tasks.\\\"\\\"\\\"\\n        if not self._enable_record_actor_task_log and not self.actor_id.is_nil():\\n            # We are not recording actor task log if not enabled explicitly.\\n            # Recording actor task log is expensive and should be enabled only\\n            # when needed.\\n            # https://github.com/ray-project/ray/issues/35598\\n            return\\n\\n        if not hasattr(self, \\\"core_worker\\\"):\\n            return\\n\\n        self.core_worker.record_task_log_end(\\n            self.get_current_out_offset(), self.get_current_err_offset()\\n        )\\n\\n    def get_err_file_path(self) -> str:\\n        \\\"\\\"\\\"Get the err log file path\\\"\\\"\\\"\\n        return self._err_file.name if self._err_file is not None else \\\"\\\"\\n\\n    def get_out_file_path(self) -> str:\\n        \\\"\\\"\\\"Get the out log file path\\\"\\\"\\\"\\n        return self._out_file.name if self._out_file is not None else \\\"\\\"\\n\\n    def get_current_out_offset(self) -> int:\\n        \\\"\\\"\\\"Get the current offset of the out file if seekable, else 0\\\"\\\"\\\"\\n        if self._out_file is not None and self._out_file.seekable():\\n            return self._out_file.tell()\\n        return 0\\n\\n    def get_current_err_offset(self) -> int:\\n        \\\"\\\"\\\"Get the current offset of the err file if seekable, else 0\\\"\\\"\\\"\\n        if self._err_file is not None and self._err_file.seekable():\\n            return self._err_file.tell()\\n        return 0\\n\\n    def get_serialization_context(self):\\n        \\\"\\\"\\\"Get the SerializationContext of the job that this worker is processing.\\n\\n        Returns:\\n            The serialization context of the given job.\\n        \\\"\\\"\\\"\\n        # This function needs to be protected by a lock, because it will be\\n        # called by`register_class_for_serialization`, as well as the import\\n        # thread, from different threads. Also, this function will recursively\\n        # call itself, so we use RLock here.\\n        job_id = self.current_job_id\\n        context_map = self.serialization_context_map\\n        with self.lock:\\n            if job_id not in context_map:\\n                # The job ID is nil before initializing Ray.\\n                if JobID.nil() in context_map:\\n                    # Transfer the serializer context used before initializing Ray.\\n                    context_map[job_id] = context_map.pop(JobID.nil())\\n                else:\\n                    context_map[job_id] = serialization.SerializationContext(self)\\n            return context_map[job_id]\\n\\n    def check_connected(self):\\n        \\\"\\\"\\\"Check if the worker is connected.\\n\\n        Raises:\\n          Exception: An exception is raised if the worker is not connected.\\n        \\\"\\\"\\\"\\n        if not self.connected:\\n            raise RaySystemError(\\n                \\\"Ray has not been started yet. You can start Ray with 'ray.init()'.\\\"\\n            )\\n\\n    def set_mode(self, mode):\\n        \\\"\\\"\\\"Set the mode of the worker.\\n\\n        The mode SCRIPT_MODE should be used if this Worker is a driver that is\\n        being run as a Python script or interactively in a shell. It will print\\n        information about task failures.\\n\\n        The mode WORKER_MODE should be used if this Worker is not a driver. It\\n        will not print information about tasks.\\n\\n        The mode LOCAL_MODE should be used if this Worker is a driver and if\\n        you want to run the driver in a manner equivalent to serial Python for\\n        debugging purposes. It will not send remote function calls to the\\n        scheduler and will instead execute them in a blocking fashion.\\n\\n        Args:\\n            mode: One of SCRIPT_MODE, WORKER_MODE, and LOCAL_MODE.\\n        \\\"\\\"\\\"\\n        self.mode = mode\\n\\n    def set_load_code_from_local(self, load_code_from_local):\\n        self._load_code_from_local = load_code_from_local\\n\\n    def put_object(self, value, object_ref=None, owner_address=None):\\n        \\\"\\\"\\\"Put value in the local object store with object reference `object_ref`.\\n\\n        This assumes that the value for `object_ref` has not yet been placed in\\n        the local object store. If the plasma store is full, the worker will\\n        automatically retry up to DEFAULT_PUT_OBJECT_RETRIES times. Each\\n        retry will delay for an exponentially doubling amount of time,\\n        starting with DEFAULT_PUT_OBJECT_DELAY. After this, exception\\n        will be raised.\\n\\n        Args:\\n            value: The value to put in the object store.\\n            object_ref: The object ref of the value to be\\n                put. If None, one will be generated.\\n            owner_address: The serialized address of object's owner.\\n\\n        Returns:\\n            ObjectRef: The object ref the object was put under.\\n\\n        Raises:\\n            ray.exceptions.ObjectStoreFullError: This is raised if the attempt\\n                to store the object fails because the object store is full even\\n                after multiple retries.\\n        \\\"\\\"\\\"\\n        # Make sure that the value is not an object ref.\\n        if isinstance(value, ObjectRef):\\n            raise TypeError(\\n                \\\"Calling 'put' on an ray.ObjectRef is not allowed. \\\"\\n                \\\"If you really want to do this, you can wrap the \\\"\\n                \\\"ray.ObjectRef in a list and call 'put' on it.\\\"\\n            )\\n\\n        if self.mode == LOCAL_MODE:\\n            assert (\\n                object_ref is None\\n            ), \\\"Local Mode does not support inserting with an ObjectRef\\\"\\n\\n        try:\\n            serialized_value = self.get_serialization_context().serialize(value)\\n        except TypeError as e:\\n            sio = io.StringIO()\\n            ray.util.inspect_serializability(value, print_file=sio)\\n            msg = (\\n                \\\"Could not serialize the put value \\\"\\n                f\\\"{repr(value)}:\\\\n\\\"\\n                f\\\"{sio.getvalue()}\\\"\\n            )\\n            raise TypeError(msg) from e\\n        # This *must* be the first place that we construct this python\\n        # ObjectRef because an entry with 0 local references is created when\\n        # the object is Put() in the core worker, expecting that this python\\n        # reference will be created. If another reference is created and\\n        # removed before this one, it will corrupt the state in the\\n        # reference counter.\\n        return ray.ObjectRef(\\n            self.core_worker.put_serialized_object_and_increment_local_ref(\\n                serialized_value, object_ref=object_ref, owner_address=owner_address\\n            ),\\n            # The initial local reference is already acquired internally.\\n            skip_adding_local_ref=True,\\n        )\\n\\n    def raise_errors(self, data_metadata_pairs, object_refs):\\n        out = self.deserialize_objects(data_metadata_pairs, object_refs)\\n        if \\\"RAY_IGNORE_UNHANDLED_ERRORS\\\" in os.environ:\\n            return\\n        for e in out:\\n            _unhandled_error_handler(e)\\n\\n    def deserialize_objects(self, data_metadata_pairs, object_refs):\\n        # Function actor manager or the import thread may call pickle.loads\\n        # at the same time which can lead to failed imports\\n        # TODO: We may be better off locking on all imports or injecting a lock\\n        # into pickle.loads (https://github.com/ray-project/ray/issues/16304)\\n        with self.function_actor_manager.lock:\\n            context = self.get_serialization_context()\\n            return context.deserialize_objects(data_metadata_pairs, object_refs)\\n\\n    def get_objects(self, object_refs: list, timeout: Optional[float] = None):\\n        \\\"\\\"\\\"Get the values in the object store associated with the IDs.\\n\\n        Return the values from the local object store for object_refs. This\\n        will block until all the values for object_refs have been written to\\n        the local object store.\\n\\n        Args:\\n            object_refs: A list of the object refs\\n                whose values should be retrieved.\\n            timeout: The maximum amount of time in\\n                seconds to wait before returning.\\n        Returns:\\n            list: List of deserialized objects\\n            bytes: UUID of the debugger breakpoint we should drop\\n                into or b\\\"\\\" if there is no breakpoint.\\n        \\\"\\\"\\\"\\n        # Make sure that the values are object refs.\\n        for object_ref in object_refs:\\n            if not isinstance(object_ref, ObjectRef):\\n                raise TypeError(\\n                    f\\\"Attempting to call `get` on the value {object_ref}, \\\"\\n                    \\\"which is not an ray.ObjectRef.\\\"\\n                )\\n\\n        timeout_ms = int(timeout * 1000) if timeout is not None else -1\\n        data_metadata_pairs = self.core_worker.get_objects(\\n            object_refs, self.current_task_id, timeout_ms\\n        )\\n        debugger_breakpoint = b\\\"\\\"\\n        for data, metadata in data_metadata_pairs:\\n            if metadata:\\n                metadata_fields = metadata.split(b\\\",\\\")\\n                if len(metadata_fields) >= 2 and metadata_fields[1].startswith(\\n                    ray_constants.OBJECT_METADATA_DEBUG_PREFIX\\n                ):\\n                    debugger_breakpoint = metadata_fields[1][\\n                        len(ray_constants.OBJECT_METADATA_DEBUG_PREFIX) :\\n                    ]\\n        return (\\n            self.deserialize_objects(data_metadata_pairs, object_refs),\\n            debugger_breakpoint,\\n        )\\n\\n    def main_loop(self):\\n        \\\"\\\"\\\"The main loop a worker runs to receive and execute tasks.\\\"\\\"\\\"\\n\\n        def sigterm_handler(signum, frame):\\n            raise_sys_exit_with_custom_error_message(\\n                \\\"The process receives a SIGTERM.\\\", exit_code=1\\n            )\\n            # Note: shutdown() function is called from atexit handler.\\n\\n        ray._private.utils.set_sigterm_handler(sigterm_handler)\\n        self.core_worker.run_task_loop()\\n        sys.exit(0)\\n\\n    def print_logs(self):\\n        \\\"\\\"\\\"Prints log messages from workers on all nodes in the same job.\\\"\\\"\\\"\\n        subscriber = self.gcs_log_subscriber\\n        subscriber.subscribe()\\n        exception_type = ray.exceptions.RpcError\\n        localhost = services.get_node_ip_address()\\n        try:\\n            # Number of messages received from the last polling. When the batch\\n            # size exceeds 100 and keeps increasing, the worker and the user\\n            # probably will not be able to consume the log messages as rapidly\\n            # as they are coming in.\\n            # This is meaningful only for GCS subscriber.\\n            last_polling_batch_size = 0\\n            job_id_hex = self.current_job_id.hex()\\n            while True:\\n                # Exit if we received a signal that we should stop.\\n                if self.threads_stopped.is_set():\\n                    return\\n\\n                data = subscriber.poll()\\n                # GCS subscriber only returns None on unavailability.\\n                if data is None:\\n                    last_polling_batch_size = 0\\n                    continue\\n\\n                if (\\n                    self._filter_logs_by_job\\n                    and data[\\\"job\\\"]\\n                    and data[\\\"job\\\"] != job_id_hex\\n                ):\\n                    last_polling_batch_size = 0\\n                    continue\\n\\n                data[\\\"localhost\\\"] = localhost\\n                global_worker_stdstream_dispatcher.emit(data)\\n\\n                lagging = 100 <= last_polling_batch_size < subscriber.last_batch_size\\n                if lagging:\\n                    logger.warning(\\n                        \\\"The driver may not be able to keep up with the \\\"\\n                        \\\"stdout/stderr of the workers. To avoid forwarding \\\"\\n                        \\\"logs to the driver, use \\\"\\n                        \\\"'ray.init(log_to_driver=False)'.\\\"\\n                    )\\n\\n                last_polling_batch_size = subscriber.last_batch_size\\n\\n        except (OSError, exception_type) as e:\\n            logger.error(f\\\"print_logs: {e}\\\")\\n        finally:\\n            # Close the pubsub client to avoid leaking file descriptors.\\n            subscriber.close()\\n\\n    def get_accelerator_ids_for_accelerator_resource(\\n        self, resource_name: str, resource_regex: str\\n    ) -> List[str]:\\n        \\\"\\\"\\\"Get the accelerator IDs that are assigned to the given accelerator resource.\\n\\n        Args:\\n            resource_name: The name of the resource.\\n            resource_regex: The regex of the resource.\\n\\n        Returns:\\n            (List[str]) The IDs that are assigned to the given resource.\\n        \\\"\\\"\\\"\\n        resource_ids = self.core_worker.resource_ids()\\n        assigned_ids = set()\\n        # Handle both normal and placement group accelerator resources.\\n        # Note: We should only get the accelerator ids from the placement\\n        # group resource that does not contain the bundle index!\\n        import re\\n\\n        for resource, assignment in resource_ids.items():\\n            if resource == resource_name or re.match(resource_regex, resource):\\n                for resource_id, _ in assignment:\\n                    assigned_ids.add(resource_id)\\n\\n        # If the user had already set the environment variables\\n        # (CUDA_VISIBLE_DEVICES, ONEAPI_DEVICE_SELECTOR, NEURON_RT_VISIBLE_CORES,\\n        # TPU_VISIBLE_CHIPS, ..) then respect that in the sense that only IDs\\n        # that appear in (CUDA_VISIBLE_DEVICES, ONEAPI_DEVICE_SELECTOR,\\n        # NEURON_RT_VISIBLE_CORES, TPU_VISIBLE_CHIPS, ..) should be returned.\\n        if self.original_visible_accelerator_ids.get(resource_name, None) is not None:\\n            original_ids = self.original_visible_accelerator_ids[resource_name]\\n            assigned_ids = {str(original_ids[i]) for i in assigned_ids}\\n            # Give all accelerator ids in local_mode.\\n            if self.mode == LOCAL_MODE:\\n                if resource_name == ray_constants.GPU:\\n                    max_accelerators = self.node.get_resource_spec().num_gpus\\n                else:\\n                    max_accelerators = self.node.get_resource_spec().resources.get(\\n                        resource_name, None\\n                    )\\n                if max_accelerators:\\n                    assigned_ids = original_ids[:max_accelerators]\\n        return [str(assigned_id) for assigned_id in assigned_ids]\\n\\n\\n@PublicAPI\\n@client_mode_hook\\ndef get_gpu_ids():\\n    \\\"\\\"\\\"Get the IDs of the GPUs that are available to the worker.\\n\\n    If the CUDA_VISIBLE_DEVICES environment variable was set when the worker\\n    started up, then the IDs returned by this method will be a subset of the\\n    IDs in CUDA_VISIBLE_DEVICES. If not, the IDs will fall in the range\\n    [0, NUM_GPUS - 1], where NUM_GPUS is the number of GPUs that the node has.\\n\\n    Returns:\\n        A list of GPU IDs.\\n    \\\"\\\"\\\"\\n    worker = global_worker\\n    worker.check_connected()\\n    return [\\n        int(i)\\n        for i in worker.get_accelerator_ids_for_accelerator_resource(\\n            ray_constants.GPU, f\\\"^{ray_constants.GPU}_group_[0-9A-Za-z]+$\\\"\\n        )\\n    ]\\n\\n\\n@Deprecated(\\n    message=\\\"Use ray.get_runtime_context().get_assigned_resources() instead.\\\",\\n    warning=True,\\n)\\ndef get_resource_ids():\\n    \\\"\\\"\\\"Get the IDs of the resources that are available to the worker.\\n\\n    Returns:\\n        A dictionary mapping the name of a resource to a list of pairs, where\\n        each pair consists of the ID of a resource and the fraction of that\\n        resource reserved for this worker.\\n    \\\"\\\"\\\"\\n    worker = global_worker\\n    worker.check_connected()\\n\\n    if _mode() == LOCAL_MODE:\\n        raise RuntimeError(\\n            \\\"ray._private.worker.get_resource_ids() does not work in local_mode.\\\"\\n        )\\n\\n    return global_worker.core_worker.resource_ids()\\n\\n\\n@Deprecated(message=\\\"Use ray.init().address_info['webui_url'] instead.\\\")\\ndef get_dashboard_url():\\n    \\\"\\\"\\\"Get the URL to access the Ray dashboard.\\n\\n    Note that the URL does not specify which node the dashboard is on.\\n\\n    Returns:\\n        The URL of the dashboard as a string.\\n    \\\"\\\"\\\"\\n    if ray_constants.RAY_OVERRIDE_DASHBOARD_URL in os.environ:\\n        return _remove_protocol_from_url(\\n            os.environ.get(ray_constants.RAY_OVERRIDE_DASHBOARD_URL)\\n        )\\n    else:\\n        worker = global_worker\\n        worker.check_connected()\\n        return _global_node.webui_url\\n\\n\\ndef _remove_protocol_from_url(url: Optional[str]) -> str:\\n    \\\"\\\"\\\"\\n    Helper function to remove protocol from URL if it exists.\\n    \\\"\\\"\\\"\\n    if not url:\\n        return url\\n    parsed_url = urllib.parse.urlparse(url)\\n    if parsed_url.scheme:\\n        # Construct URL without protocol\\n        scheme = f\\\"{parsed_url.scheme}://\\\"\\n        return parsed_url.geturl().replace(scheme, \\\"\\\", 1)\\n    return url\\n\\n\\nclass BaseContext(metaclass=ABCMeta):\\n    \\\"\\\"\\\"\\n    Base class for RayContext and ClientContext\\n    \\\"\\\"\\\"\\n\\n    dashboard_url: Optional[str]\\n    python_version: str\\n    ray_version: str\\n\\n    @abstractmethod\\n    def disconnect(self):\\n        \\\"\\\"\\\"\\n        If this context is for directly attaching to a cluster, disconnect\\n        will call ray.shutdown(). Otherwise, if the context is for a ray\\n        client connection, the client will be disconnected.\\n        \\\"\\\"\\\"\\n        pass\\n\\n    @abstractmethod\\n    def __enter__(self):\\n        pass\\n\\n    @abstractmethod\\n    def __exit__(self):\\n        pass\\n\\n    def _context_table_template(self):\\n        if self.dashboard_url:\\n            dashboard_row = Template(\\\"context_dashrow.html.j2\\\").render(\\n                dashboard_url=\\\"http://\\\" + self.dashboard_url\\n            )\\n        else:\\n            dashboard_row = None\\n\\n        return Template(\\\"context_table.html.j2\\\").render(\\n            python_version=self.python_version,\\n            ray_version=self.ray_version,\\n            dashboard_row=dashboard_row,\\n        )\\n\\n    def _repr_html_(self):\\n        return Template(\\\"context.html.j2\\\").render(\\n            context_logo=Template(\\\"context_logo.html.j2\\\").render(),\\n            context_table=self._context_table_template(),\\n        )\\n\\n    @repr_with_fallback([\\\"ipywidgets\\\", \\\"8\\\"])\\n    def _get_widget_bundle(self, **kwargs) -> Dict[str, Any]:\\n        \\\"\\\"\\\"Get the mimebundle for the widget representation of the context.\\n\\n        Args:\\n            **kwargs: Passed to the _repr_mimebundle_() function for the widget\\n\\n        Returns:\\n            Dictionary (\\\"mimebundle\\\") of the widget representation of the context.\\n        \\\"\\\"\\\"\\n        import ipywidgets\\n\\n        disconnect_button = ipywidgets.Button(\\n            description=\\\"Disconnect\\\",\\n            disabled=False,\\n            button_style=\\\"\\\",\\n            tooltip=\\\"Disconnect from the Ray cluster\\\",\\n            layout=ipywidgets.Layout(margin=\\\"auto 0px 0px 0px\\\"),\\n        )\\n\\n        def disconnect_callback(button):\\n            button.disabled = True\\n            button.description = \\\"Disconnecting...\\\"\\n            self.disconnect()\\n            button.description = \\\"Disconnected\\\"\\n\\n        disconnect_button.on_click(disconnect_callback)\\n        left_content = ipywidgets.VBox(\\n            [\\n                ipywidgets.HTML(Template(\\\"context_logo.html.j2\\\").render()),\\n                disconnect_button,\\n            ],\\n            layout=ipywidgets.Layout(),\\n        )\\n        right_content = ipywidgets.HTML(self._context_table_template())\\n        widget = ipywidgets.HBox(\\n            [left_content, right_content], layout=ipywidgets.Layout(width=\\\"100%\\\")\\n        )\\n        return widget._repr_mimebundle_(**kwargs)\\n\\n    def _repr_mimebundle_(self, **kwargs):\\n        bundle = self._get_widget_bundle(**kwargs)\\n\\n        # Overwrite the widget html repr and default repr with those of the BaseContext\\n        bundle.update({\\\"text/html\\\": self._repr_html_(), \\\"text/plain\\\": repr(self)})\\n        return bundle\\n\\n\\n@dataclass\\nclass RayContext(BaseContext, Mapping):\\n    \\\"\\\"\\\"\\n    Context manager for attached drivers.\\n    \\\"\\\"\\\"\\n\\n    dashboard_url: Optional[str]\\n    python_version: str\\n    ray_version: str\\n    ray_commit: str\\n    protocol_version: Optional[str]\\n\\n    def __init__(self, address_info: Dict[str, Optional[str]]):\\n        super().__init__()\\n        self.dashboard_url = get_dashboard_url()\\n        self.python_version = \\\"{}.{}.{}\\\".format(*sys.version_info[:3])\\n        self.ray_version = ray.__version__\\n        self.ray_commit = ray.__commit__\\n        # No client protocol version since this driver was intiialized\\n        # directly\\n        self.protocol_version = None\\n        self.address_info = address_info\\n\\n    def __getitem__(self, key):\\n        if log_once(\\\"ray_context_getitem\\\"):\\n            warnings.warn(\\n                f'Accessing values through ctx[\\\"{key}\\\"] is deprecated. '\\n                f'Use ctx.address_info[\\\"{key}\\\"] instead.',\\n                DeprecationWarning,\\n                stacklevel=2,\\n            )\\n        return self.address_info[key]\\n\\n    def __len__(self):\\n        if log_once(\\\"ray_context_len\\\"):\\n            warnings.warn(\\\"len(ctx) is deprecated. Use len(ctx.address_info) instead.\\\")\\n        return len(self.address_info)\\n\\n    def __iter__(self):\\n        if log_once(\\\"ray_context_len\\\"):\\n            warnings.warn(\\n                \\\"iter(ctx) is deprecated. Use iter(ctx.address_info) instead.\\\"\\n            )\\n        return iter(self.address_info)\\n\\n    def __enter__(self) -> \\\"RayContext\\\":\\n        return self\\n\\n    def __exit__(self, *exc):\\n        ray.shutdown()\\n\\n    def disconnect(self):\\n        # Include disconnect() to stay consistent with ClientContext\\n        ray.shutdown()\\n\\n\\nglobal_worker = Worker()\\n\\\"\\\"\\\"Worker: The global Worker object for this worker process.\\n\\nWe use a global Worker object to ensure that there is a single worker object\\nper worker process.\\n\\\"\\\"\\\"\\n\\n_global_node = None\\n\\\"\\\"\\\"ray._private.node.Node: The global node object that is created by ray.init().\\\"\\\"\\\"\\n\\n\\n@PublicAPI\\n@client_mode_hook\\ndef init(\\n    address: Optional[str] = None,\\n    *,\\n    num_cpus: Optional[int] = None,\\n    num_gpus: Optional[int] = None,\\n    resources: Optional[Dict[str, float]] = None,\\n    labels: Optional[Dict[str, str]] = None,\\n    object_store_memory: Optional[int] = None,\\n    local_mode: bool = False,\\n    ignore_reinit_error: bool = False,\\n    include_dashboard: Optional[bool] = None,\\n    dashboard_host: str = ray_constants.DEFAULT_DASHBOARD_IP,\\n    dashboard_port: Optional[int] = None,\\n    job_config: \\\"ray.job_config.JobConfig\\\" = None,\\n    configure_logging: bool = True,\\n    logging_level: int = ray_constants.LOGGER_LEVEL,\\n    logging_format: Optional[str] = None,\\n    log_to_driver: bool = True,\\n    namespace: Optional[str] = None,\\n    runtime_env: Optional[Union[Dict[str, Any], \\\"RuntimeEnv\\\"]] = None,  # noqa: F821\\n    storage: Optional[str] = None,\\n    **kwargs,\\n) -> BaseContext:\\n    \\\"\\\"\\\"\\n    Connect to an existing Ray cluster or start one and connect to it.\\n\\n    This method handles two cases; either a Ray cluster already exists and we\\n    just attach this driver to it or we start all of the processes associated\\n    with a Ray cluster and attach to the newly started cluster.\\n    Note: This method overwrite sigterm handler of the driver process.\\n\\n    In most cases, it is enough to just call this method with no arguments.\\n    This will autodetect an existing Ray cluster or start a new Ray instance if\\n    no existing cluster is found:\\n\\n    .. testcode::\\n\\n        ray.init()\\n\\n    To explicitly connect to an existing local cluster, use this as follows. A\\n    ConnectionError will be thrown if no existing local cluster is found.\\n\\n    .. testcode::\\n        :skipif: True\\n\\n        ray.init(address=\\\"auto\\\")\\n\\n    To connect to an existing remote cluster, use this as follows (substituting\\n    in the appropriate address). Note the addition of \\\"ray://\\\" at the beginning\\n    of the address. This requires `ray[client]`.\\n\\n    .. testcode::\\n        :skipif: True\\n\\n        ray.init(address=\\\"ray://123.45.67.89:10001\\\")\\n\\n    More details for starting and connecting to a remote cluster can be found\\n    here: https://docs.ray.io/en/master/cluster/getting-started.html\\n\\n    You can also define an environment variable called `RAY_ADDRESS` in\\n    the same format as the `address` parameter to connect to an existing\\n    cluster with ray.init() or ray.init(address=\\\"auto\\\").\\n\\n    Args:\\n        address: The address of the Ray cluster to connect to. The provided\\n            address is resolved as follows:\\n            1. If a concrete address (e.g., localhost:<port>) is provided, try to\\n            connect to it. Concrete addresses can be prefixed with \\\"ray://\\\" to\\n            connect to a remote cluster. For example, passing in the address\\n            \\\"ray://123.45.67.89:50005\\\" will connect to the cluster at the given\\n            address.\\n            2. If no address is provided, try to find an existing Ray instance\\n            to connect to. This is done by first checking the environment\\n            variable `RAY_ADDRESS`. If this is not defined, check the address\\n            of the latest cluster started (found in\\n            /tmp/ray/ray_current_cluster) if available. If this is also empty,\\n            then start a new local Ray instance.\\n            3. If the provided address is \\\"auto\\\", then follow the same process\\n            as above. However, if there is no existing cluster found, this will\\n            throw a ConnectionError instead of starting a new local Ray\\n            instance.\\n            4. If the provided address is \\\"local\\\", start a new local Ray\\n            instance, even if there is already an existing local Ray instance.\\n        num_cpus: Number of CPUs the user wishes to assign to each\\n            raylet. By default, this is set based on virtual cores.\\n        num_gpus: Number of GPUs the user wishes to assign to each\\n            raylet. By default, this is set based on detected GPUs.\\n        resources: A dictionary mapping the names of custom resources to the\\n            quantities for them available.\\n        labels: [Experimental] The key-value labels of the node.\\n        object_store_memory: The amount of memory (in bytes) to start the\\n            object store with. By default, this is automatically set based on\\n            available system memory.\\n        local_mode: Deprecated: consider using the Ray Debugger instead.\\n        ignore_reinit_error: If true, Ray suppresses errors from calling\\n            ray.init() a second time. Ray won't be restarted.\\n        include_dashboard: Boolean flag indicating whether or not to start the\\n            Ray dashboard, which displays the status of the Ray\\n            cluster. If this argument is None, then the UI will be started if\\n            the relevant dependencies are present.\\n        dashboard_host: The host to bind the dashboard server to. Can either be\\n            localhost (127.0.0.1) or 0.0.0.0 (available from all interfaces).\\n            By default, this is set to localhost to prevent access from\\n            external machines.\\n        dashboard_port(int, None): The port to bind the dashboard server to.\\n            Defaults to 8265 and Ray will automatically find a free port if\\n            8265 is not available.\\n        job_config (ray.job_config.JobConfig): The job configuration.\\n        configure_logging: True (default) if configuration of logging is\\n            allowed here. Otherwise, the user may want to configure it\\n            separately.\\n        logging_level: Logging level, defaults to logging.INFO. Ignored unless\\n            \\\"configure_logging\\\" is true.\\n        logging_format: Logging format, defaults to string containing a\\n            timestamp, filename, line number, and message. See the source file\\n            ray_constants.py for details. Ignored unless \\\"configure_logging\\\"\\n            is true.\\n        log_to_driver: If true, the output from all of the worker\\n            processes on all nodes will be directed to the driver.\\n        namespace: A namespace is a logical grouping of jobs and named actors.\\n        runtime_env: The runtime environment to use\\n            for this job (see :ref:`runtime-environments` for details).\\n        storage: [Experimental] Specify a URI for persistent cluster-wide storage.\\n            This storage path must be accessible by all nodes of the cluster, otherwise\\n            an error will be raised. This option can also be specified as the\\n            RAY_STORAGE env var.\\n        _enable_object_reconstruction: If True, when an object stored in\\n            the distributed plasma store is lost due to node failure, Ray will\\n            attempt to reconstruct the object by re-executing the task that\\n            created the object. Arguments to the task will be recursively\\n            reconstructed. If False, then ray.ObjectLostError will be\\n            thrown.\\n        _redis_max_memory: Redis max memory.\\n        _plasma_directory: Override the plasma mmap file directory.\\n        _node_ip_address: The IP address of the node that we are on.\\n        _driver_object_store_memory: Deprecated.\\n        _memory: Amount of reservable memory resource in bytes rounded\\n            down to the nearest integer.\\n        _redis_password: Prevents external clients without the password\\n            from connecting to Redis if provided.\\n        _temp_dir: If provided, specifies the root temporary\\n            directory for the Ray process. Must be an absolute path. Defaults to an\\n            OS-specific conventional location, e.g., \\\"/tmp/ray\\\".\\n        _metrics_export_port: Port number Ray exposes system metrics\\n            through a Prometheus endpoint. It is currently under active\\n            development, and the API is subject to change.\\n        _system_config: Configuration for overriding\\n            RayConfig defaults. For testing purposes ONLY.\\n        _tracing_startup_hook: If provided, turns on and sets up tracing\\n            for Ray. Must be the name of a function that takes no arguments and\\n            sets up a Tracer Provider, Remote Span Processors, and\\n            (optional) additional instruments. See more at\\n            docs.ray.io/tracing.html. It is currently under active development,\\n            and the API is subject to change.\\n        _node_name: User-provided node name or identifier. Defaults to\\n            the node IP address.\\n\\n    Returns:\\n        If the provided address includes a protocol, for example by prepending\\n        \\\"ray://\\\" to the address to get \\\"ray://1.2.3.4:10001\\\", then a\\n        ClientContext is returned with information such as settings, server\\n        versions for ray and python, and the dashboard_url. Otherwise,\\n        a RayContext is returned with ray and python versions, and address\\n        information about the started processes.\\n\\n    Raises:\\n        Exception: An exception is raised if an inappropriate combination of\\n            arguments is passed in.\\n    \\\"\\\"\\\"\\n    if configure_logging:\\n        setup_logger(logging_level, logging_format or ray_constants.LOGGER_FORMAT)\\n    else:\\n        logging.getLogger(\\\"ray\\\").handlers.clear()\\n\\n    # Parse the hidden options:\\n    _enable_object_reconstruction: bool = kwargs.pop(\\n        \\\"_enable_object_reconstruction\\\", False\\n    )\\n    _redis_max_memory: Optional[int] = kwargs.pop(\\\"_redis_max_memory\\\", None)\\n    _plasma_directory: Optional[str] = kwargs.pop(\\\"_plasma_directory\\\", None)\\n    _node_ip_address: str = kwargs.pop(\\\"_node_ip_address\\\", None)\\n    _driver_object_store_memory: Optional[int] = kwargs.pop(\\n        \\\"_driver_object_store_memory\\\", None\\n    )\\n    _memory: Optional[int] = kwargs.pop(\\\"_memory\\\", None)\\n    _redis_password: str = kwargs.pop(\\n        \\\"_redis_password\\\", ray_constants.REDIS_DEFAULT_PASSWORD\\n    )\\n    _temp_dir: Optional[str] = kwargs.pop(\\\"_temp_dir\\\", None)\\n    _metrics_export_port: Optional[int] = kwargs.pop(\\\"_metrics_export_port\\\", None)\\n    _system_config: Optional[Dict[str, str]] = kwargs.pop(\\\"_system_config\\\", None)\\n    _tracing_startup_hook: Optional[Callable] = kwargs.pop(\\n        \\\"_tracing_startup_hook\\\", None\\n    )\\n    _node_name: str = kwargs.pop(\\\"_node_name\\\", None)\\n    # Fix for https://github.com/ray-project/ray/issues/26729\\n    _skip_env_hook: bool = kwargs.pop(\\\"_skip_env_hook\\\", False)\\n\\n    # terminate any signal before connecting driver\\n    def sigterm_handler(signum, frame):\\n        sys.exit(signum)\\n\\n    if threading.current_thread() is threading.main_thread():\\n        ray._private.utils.set_sigterm_handler(sigterm_handler)\\n    else:\\n        logger.warning(\\n            \\\"SIGTERM handler is not set because current thread \\\"\\n            \\\"is not the main thread.\\\"\\n        )\\n\\n    # If available, use RAY_ADDRESS to override if the address was left\\n    # unspecified, or set to \\\"auto\\\" in the call to init\\n    address_env_var = os.environ.get(ray_constants.RAY_ADDRESS_ENVIRONMENT_VARIABLE)\\n    if address_env_var and (address is None or address == \\\"auto\\\"):\\n        address = address_env_var\\n        logger.info(\\n            f\\\"Using address {address_env_var} set in the environment \\\"\\n            f\\\"variable {ray_constants.RAY_ADDRESS_ENVIRONMENT_VARIABLE}\\\"\\n        )\\n\\n    if address is not None and \\\"://\\\" in address:\\n        # Address specified a protocol, use ray client\\n        builder = ray.client(address, _deprecation_warn_enabled=False)\\n\\n        # Forward any keyword arguments that were changed from their default\\n        # values to the builder\\n        init_sig = inspect.signature(init)\\n        passed_kwargs = {}\\n        for argument_name, param_obj in init_sig.parameters.items():\\n            if argument_name in {\\\"kwargs\\\", \\\"address\\\"}:\\n                # kwargs and address are handled separately\\n                continue\\n            default_value = param_obj.default\\n            passed_value = locals()[argument_name]\\n            if passed_value != default_value:\\n                # passed value is different than default, pass to the client\\n                # builder\\n                passed_kwargs[argument_name] = passed_value\\n        passed_kwargs.update(kwargs)\\n        builder._init_args(**passed_kwargs)\\n        ctx = builder.connect()\\n        from ray._private.usage import usage_lib\\n\\n        if passed_kwargs.get(\\\"allow_multiple\\\") is True:\\n            with ctx:\\n                usage_lib.put_pre_init_usage_stats()\\n        else:\\n            usage_lib.put_pre_init_usage_stats()\\n\\n        usage_lib.record_library_usage(\\\"client\\\")\\n        return ctx\\n\\n    if kwargs.get(\\\"allow_multiple\\\"):\\n        raise RuntimeError(\\n            \\\"`allow_multiple` argument is passed to `ray.init` when the \\\"\\n            \\\"ray client is not used (\\\"\\n            f\\\"https://docs.ray.io/en/{get_ray_doc_version()}/cluster\\\"\\n            \\\"/running-applications/job-submission\\\"\\n            \\\"/ray-client.html#connect-to-multiple-ray-clusters-experimental). \\\"\\n            \\\"Do not pass the `allow_multiple` to `ray.init` to fix the issue.\\\"\\n        )\\n\\n    if kwargs:\\n        # User passed in extra keyword arguments but isn't connecting through\\n        # ray client. Raise an error, since most likely a typo in keyword\\n        unknown = \\\", \\\".join(kwargs)\\n        raise RuntimeError(f\\\"Unknown keyword argument(s): {unknown}\\\")\\n\\n    # Try to increase the file descriptor limit, which is too low by\\n    # default for Ray: https://github.com/ray-project/ray/issues/11239\\n    try:\\n        import resource\\n\\n        soft, hard = resource.getrlimit(resource.RLIMIT_NOFILE)\\n        if soft < hard:\\n            # https://github.com/ray-project/ray/issues/12059\\n            soft = max(soft, min(hard, 65536))\\n            logger.debug(\\n                f\\\"Automatically increasing RLIMIT_NOFILE to max value of {hard}\\\"\\n            )\\n            try:\\n                resource.setrlimit(resource.RLIMIT_NOFILE, (soft, hard))\\n            except ValueError:\\n                logger.debug(\\\"Failed to raise limit.\\\")\\n        soft, _ = resource.getrlimit(resource.RLIMIT_NOFILE)\\n        if soft < 4096:\\n            logger.warning(\\n                \\\"File descriptor limit {} is too low for production \\\"\\n                \\\"servers and may result in connection errors. \\\"\\n                \\\"At least 8192 is recommended. --- \\\"\\n                \\\"Fix with 'ulimit -n 8192'\\\".format(soft)\\n            )\\n    except ImportError:\\n        logger.debug(\\\"Could not import resource module (on Windows)\\\")\\n        pass\\n\\n    if job_config is None:\\n        job_config = ray.job_config.JobConfig()\\n\\n    if RAY_JOB_CONFIG_JSON_ENV_VAR in os.environ:\\n        injected_job_config_json = json.loads(\\n            os.environ.get(RAY_JOB_CONFIG_JSON_ENV_VAR)\\n        )\\n        injected_job_config: ray.job_config.JobConfig = (\\n            ray.job_config.JobConfig.from_json(injected_job_config_json)\\n        )\\n        driver_runtime_env = runtime_env\\n        runtime_env = _merge_runtime_env(\\n            injected_job_config.runtime_env,\\n            driver_runtime_env,\\n            override=os.getenv(\\\"RAY_OVERRIDE_JOB_RUNTIME_ENV\\\") == \\\"1\\\",\\n        )\\n        if runtime_env is None:\\n            # None means there was a conflict.\\n            raise ValueError(\\n                \\\"Failed to merge the Job's runtime env \\\"\\n                f\\\"{injected_job_config.runtime_env} with \\\"\\n                f\\\"a ray.init's runtime env {driver_runtime_env} because \\\"\\n                \\\"of a conflict. Specifying the same runtime_env fields \\\"\\n                \\\"or the same environment variable keys is not allowed. \\\"\\n                \\\"Use RAY_OVERRIDE_JOB_RUNTIME_ENV=1 to instruct Ray to \\\"\\n                \\\"combine Job and Driver's runtime environment in the event of \\\"\\n                \\\"a conflict.\\\"\\n            )\\n\\n        if ray_constants.RAY_RUNTIME_ENV_HOOK in os.environ and not _skip_env_hook:\\n            runtime_env = _load_class(os.environ[ray_constants.RAY_RUNTIME_ENV_HOOK])(\\n                runtime_env\\n            )\\n        job_config.set_runtime_env(runtime_env)\\n        # Similarly, we prefer metadata provided via job submission API\\n        for key, value in injected_job_config.metadata.items():\\n            job_config.set_metadata(key, value)\\n\\n    # RAY_JOB_CONFIG_JSON_ENV_VAR is only set at ray job manager level and has\\n    # higher priority in case user also provided runtime_env for ray.init()\\n    else:\\n        if ray_constants.RAY_RUNTIME_ENV_HOOK in os.environ and not _skip_env_hook:\\n            runtime_env = _load_class(os.environ[ray_constants.RAY_RUNTIME_ENV_HOOK])(\\n                runtime_env\\n            )\\n\\n        if runtime_env:\\n            # Set runtime_env in job_config if passed in as part of ray.init()\\n            job_config.set_runtime_env(runtime_env)\\n\\n    redis_address, gcs_address = None, None\\n    bootstrap_address = services.canonicalize_bootstrap_address(address, _temp_dir)\\n    if bootstrap_address is not None:\\n        gcs_address = bootstrap_address\\n        logger.info(\\\"Connecting to existing Ray cluster at address: %s...\\\", gcs_address)\\n\\n    if local_mode:\\n        driver_mode = LOCAL_MODE\\n        warnings.warn(\\n            \\\"DeprecationWarning: local mode is an experimental feature that is no \\\"\\n            \\\"longer maintained and will be removed in the future.\\\"\\n            \\\"For debugging consider using Ray debugger. \\\",\\n            DeprecationWarning,\\n            stacklevel=2,\\n        )\\n    else:\\n        driver_mode = SCRIPT_MODE\\n\\n    global _global_node\\n\\n    if global_worker.connected:\\n        if ignore_reinit_error:\\n            logger.info(\\\"Calling ray.init() again after it has already been called.\\\")\\n            node_id = global_worker.core_worker.get_current_node_id()\\n            return RayContext(dict(_global_node.address_info, node_id=node_id.hex()))\\n        else:\\n            raise RuntimeError(\\n                \\\"Maybe you called ray.init twice by accident? \\\"\\n                \\\"This error can be suppressed by passing in \\\"\\n                \\\"'ignore_reinit_error=True' or by calling \\\"\\n                \\\"'ray.shutdown()' prior to 'ray.init()'.\\\"\\n            )\\n\\n    _system_config = _system_config or {}\\n    if not isinstance(_system_config, dict):\\n        raise TypeError(\\\"The _system_config must be a dict.\\\")\\n\\n    if bootstrap_address is None:\\n        # In this case, we need to start a new cluster.\\n\\n        # Don't collect usage stats in ray.init() unless it's a nightly wheel.\\n        from ray._private.usage import usage_lib\\n\\n        if usage_lib.is_nightly_wheel():\\n            usage_lib.show_usage_stats_prompt(cli=False)\\n        else:\\n            usage_lib.set_usage_stats_enabled_via_env_var(False)\\n\\n        # Use a random port by not specifying Redis port / GCS server port.\\n        ray_params = ray._private.parameter.RayParams(\\n            node_ip_address=_node_ip_address,\\n            object_ref_seed=None,\\n            driver_mode=driver_mode,\\n            redirect_output=None,\\n            num_cpus=num_cpus,\\n            num_gpus=num_gpus,\\n            resources=resources,\\n            labels=labels,\\n            num_redis_shards=None,\\n            redis_max_clients=None,\\n            redis_password=_redis_password,\\n            plasma_directory=_plasma_directory,\\n            huge_pages=None,\\n            include_dashboard=include_dashboard,\\n            dashboard_host=dashboard_host,\\n            dashboard_port=dashboard_port,\\n            memory=_memory,\\n            object_store_memory=object_store_memory,\\n            redis_max_memory=_redis_max_memory,\\n            plasma_store_socket_name=None,\\n            temp_dir=_temp_dir,\\n            storage=storage,\\n            _system_config=_system_config,\\n            enable_object_reconstruction=_enable_object_reconstruction,\\n            metrics_export_port=_metrics_export_port,\\n            tracing_startup_hook=_tracing_startup_hook,\\n            node_name=_node_name,\\n        )\\n        # Start the Ray processes. We set shutdown_at_exit=False because we\\n        # shutdown the node in the ray.shutdown call that happens in the atexit\\n        # handler. We still spawn a reaper process in case the atexit handler\\n        # isn't called.\\n        _global_node = ray._private.node.Node(\\n            head=True,\\n            shutdown_at_exit=False,\\n            spawn_reaper=True,\\n            ray_params=ray_params,\\n        )\\n    else:\\n        # In this case, we are connecting to an existing cluster.\\n        if num_cpus is not None or num_gpus is not None:\\n            raise ValueError(\\n                \\\"When connecting to an existing cluster, num_cpus \\\"\\n                \\\"and num_gpus must not be provided.\\\"\\n            )\\n        if resources is not None:\\n            raise ValueError(\\n                \\\"When connecting to an existing cluster, \\\"\\n                \\\"resources must not be provided.\\\"\\n            )\\n        if labels is not None:\\n            raise ValueError(\\n                \\\"When connecting to an existing cluster, \\\"\\n                \\\"labels must not be provided.\\\"\\n            )\\n        if object_store_memory is not None:\\n            raise ValueError(\\n                \\\"When connecting to an existing cluster, \\\"\\n                \\\"object_store_memory must not be provided.\\\"\\n            )\\n        if storage is not None:\\n            raise ValueError(\\n                \\\"When connecting to an existing cluster, \\\"\\n                \\\"storage must not be provided.\\\"\\n            )\\n        if _system_config is not None and len(_system_config) != 0:\\n            raise ValueError(\\n                \\\"When connecting to an existing cluster, \\\"\\n                \\\"_system_config must not be provided.\\\"\\n            )\\n        if _enable_object_reconstruction:\\n            raise ValueError(\\n                \\\"When connecting to an existing cluster, \\\"\\n                \\\"_enable_object_reconstruction must not be provided.\\\"\\n            )\\n        if _node_name is not None:\\n            raise ValueError(\\n                \\\"_node_name cannot be configured when connecting to \\\"\\n                \\\"an existing cluster.\\\"\\n            )\\n\\n        # In this case, we only need to connect the node.\\n        ray_params = ray._private.parameter.RayParams(\\n            node_ip_address=_node_ip_address,\\n            gcs_address=gcs_address,\\n            redis_address=redis_address,\\n            redis_password=_redis_password,\\n            object_ref_seed=None,\\n            temp_dir=_temp_dir,\\n            _system_config=_system_config,\\n            enable_object_reconstruction=_enable_object_reconstruction,\\n            metrics_export_port=_metrics_export_port,\\n        )\\n        try:\\n            _global_node = ray._private.node.Node(\\n                ray_params,\\n                head=False,\\n                shutdown_at_exit=False,\\n                spawn_reaper=False,\\n                connect_only=True,\\n            )\\n        except (ConnectionError, RuntimeError):\\n            if gcs_address == ray._private.utils.read_ray_address(_temp_dir):\\n                logger.info(\\n                    \\\"Failed to connect to the default Ray cluster address at \\\"\\n                    f\\\"{gcs_address}. This is most likely due to a previous Ray \\\"\\n                    \\\"instance that has since crashed. To reset the default \\\"\\n                    \\\"address to connect to, run `ray stop` or restart Ray with \\\"\\n                    \\\"`ray start`.\\\"\\n                )\\n            raise ConnectionError\\n\\n    # Log a message to find the Ray address that we connected to and the\\n    # dashboard URL.\\n    if ray_constants.RAY_OVERRIDE_DASHBOARD_URL in os.environ:\\n        dashboard_url = os.environ.get(ray_constants.RAY_OVERRIDE_DASHBOARD_URL)\\n    else:\\n        dashboard_url = _global_node.webui_url\\n    # Add http protocol to dashboard URL if it doesn't\\n    # already contain a protocol.\\n    if dashboard_url and not urlparse(dashboard_url).scheme:\\n        dashboard_url = \\\"http://\\\" + dashboard_url\\n\\n    # We logged the address before attempting the connection, so we don't need\\n    # to log it again.\\n    info_str = \\\"Connected to Ray cluster.\\\"\\n    if gcs_address is None:\\n        info_str = \\\"Started a local Ray instance.\\\"\\n    if dashboard_url:\\n        logger.info(\\n            info_str + \\\" View the dashboard at %s%s%s %s%s\\\",\\n            colorama.Style.BRIGHT,\\n            colorama.Fore.GREEN,\\n            dashboard_url,\\n            colorama.Fore.RESET,\\n            colorama.Style.NORMAL,\\n        )\\n    else:\\n        logger.info(info_str)\\n\\n    connect(\\n        _global_node,\\n        _global_node.session_name,\\n        mode=driver_mode,\\n        log_to_driver=log_to_driver,\\n        worker=global_worker,\\n        driver_object_store_memory=_driver_object_store_memory,\\n        job_id=None,\\n        namespace=namespace,\\n        job_config=job_config,\\n        entrypoint=ray._private.utils.get_entrypoint_name(),\\n    )\\n    if job_config and job_config.code_search_path:\\n        global_worker.set_load_code_from_local(True)\\n    else:\\n        # Because `ray.shutdown()` doesn't reset this flag, for multiple\\n        # sessions in one process, the 2nd `ray.init()` will reuse the\\n        # flag of last session. For example:\\n        #     ray.init(load_code_from_local=True)\\n        #     ray.shutdown()\\n        #     ray.init()\\n        #     # Here the flag `load_code_from_local` is still True if we\\n        #     # doesn't have this `else` branch.\\n        #     ray.shutdown()\\n        global_worker.set_load_code_from_local(False)\\n\\n    for hook in _post_init_hooks:\\n        hook()\\n\\n    node_id = global_worker.core_worker.get_current_node_id()\\n    global_node_address_info = _global_node.address_info.copy()\\n    global_node_address_info[\\\"webui_url\\\"] = _remove_protocol_from_url(dashboard_url)\\n    return RayContext(dict(global_node_address_info, node_id=node_id.hex()))\\n\\n\\n# Functions to run as callback after a successful ray init.\\n_post_init_hooks = []\\n\\n\\n@PublicAPI\\n@client_mode_hook\\ndef shutdown(_exiting_interpreter: bool = False):\\n    \\\"\\\"\\\"Disconnect the worker, and terminate processes started by ray.init().\\n\\n    This will automatically run at the end when a Python process that uses Ray\\n    exits. It is ok to run this twice in a row. The primary use case for this\\n    function is to cleanup state between tests.\\n\\n    Note that this will clear any remote function definitions, actor\\n    definitions, and existing actors, so if you wish to use any previously\\n    defined remote functions or actors after calling ray.shutdown(), then you\\n    need to redefine them. If they were defined in an imported module, then you\\n    will need to reload the module.\\n\\n    Args:\\n        _exiting_interpreter: True if this is called by the atexit hook\\n            and false otherwise. If we are exiting the interpreter, we will\\n            wait a little while to print any extra error messages.\\n    \\\"\\\"\\\"\\n    if _exiting_interpreter and global_worker.mode == SCRIPT_MODE:\\n        # This is a duration to sleep before shutting down everything in order\\n        # to make sure that log messages finish printing.\\n        time.sleep(0.5)\\n    disconnect(_exiting_interpreter)\\n\\n    # disconnect internal kv\\n    if hasattr(global_worker, \\\"gcs_client\\\"):\\n        del global_worker.gcs_client\\n    _internal_kv_reset()\\n\\n    # We need to destruct the core worker here because after this function,\\n    # we will tear down any processes spawned by ray.init() and the background\\n    # IO thread in the core worker doesn't currently handle that gracefully.\\n    if hasattr(global_worker, \\\"core_worker\\\"):\\n        if global_worker.mode == SCRIPT_MODE or global_worker.mode == LOCAL_MODE:\\n            global_worker.core_worker.shutdown_driver()\\n        del global_worker.core_worker\\n    # We need to reset function actor manager to clear the context\\n    global_worker.function_actor_manager = FunctionActorManager(global_worker)\\n    # Disconnect global state from GCS.\\n    ray._private.state.state.disconnect()\\n\\n    # Shut down the Ray processes.\\n    global _global_node\\n    if _global_node is not None:\\n        if _global_node.is_head():\\n            _global_node.destroy_external_storage()\\n        _global_node.kill_all_processes(check_alive=False, allow_graceful=True)\\n        _global_node = None\\n    storage._reset()\\n\\n    # TODO(rkn): Instead of manually resetting some of the worker fields, we\\n    # should simply set \\\"global_worker\\\" to equal \\\"None\\\" or something like that.\\n    global_worker.set_mode(None)\\n\\n\\natexit.register(shutdown, True)\\n\\n# Define a custom excepthook so that if the driver exits with an exception, we\\n# can push that exception to Redis.\\nnormal_excepthook = sys.excepthook\\n\\n\\ndef custom_excepthook(type, value, tb):\\n    import ray.core.generated.common_pb2 as common_pb2\\n\\n    # If this is a driver, push the exception to GCS worker table.\\n    if global_worker.mode == SCRIPT_MODE and hasattr(global_worker, \\\"worker_id\\\"):\\n        error_message = \\\"\\\".join(traceback.format_tb(tb))\\n        worker_id = global_worker.worker_id\\n        worker_type = common_pb2.DRIVER\\n        worker_info = {\\\"exception\\\": error_message}\\n\\n        ray._private.state.state._check_connected()\\n        ray._private.state.state.add_worker(worker_id, worker_type, worker_info)\\n    # Call the normal excepthook.\\n    normal_excepthook(type, value, tb)\\n\\n\\nsys.excepthook = custom_excepthook\\n\\n\\ndef print_to_stdstream(data):\\n    should_dedup = data.get(\\\"pid\\\") not in [\\\"autoscaler\\\"]\\n\\n    if data[\\\"is_err\\\"]:\\n        if should_dedup:\\n            batches = stderr_deduplicator.deduplicate(data)\\n        else:\\n            batches = [data]\\n        sink = sys.stderr\\n    else:\\n        if should_dedup:\\n            batches = stdout_deduplicator.deduplicate(data)\\n        else:\\n            batches = [data]\\n        sink = sys.stdout\\n\\n    for batch in batches:\\n        print_worker_logs(batch, sink)\\n\\n\\n# Start time of this process, used for relative time logs.\\nt0 = time.time()\\nautoscaler_log_fyi_printed = False\\n\\n\\ndef filter_autoscaler_events(lines: List[str]) -> Iterator[str]:\\n    \\\"\\\"\\\"Given raw log lines from the monitor, return only autoscaler events.\\n\\n    For Autoscaler V1:\\n        Autoscaler events are denoted by the \\\":event_summary:\\\" magic token.\\n    For Autoscaler V2:\\n        Autoscaler events are published from log_monitor.py which read\\n        them from the `event_AUTOSCALER.log`.\\n    \\\"\\\"\\\"\\n\\n    if not ray_constants.AUTOSCALER_EVENTS:\\n        return\\n\\n    AUTOSCALER_LOG_FYI = (\\n        \\\"Tip: use `ray status` to view detailed \\\"\\n        \\\"cluster status. To disable these \\\"\\n        \\\"messages, set RAY_SCHEDULER_EVENTS=0.\\\"\\n    )\\n\\n    def autoscaler_log_fyi_needed() -> bool:\\n        global autoscaler_log_fyi_printed\\n        if not autoscaler_log_fyi_printed:\\n            autoscaler_log_fyi_printed = True\\n            return True\\n        return False\\n\\n    from ray.autoscaler.v2.utils import is_autoscaler_v2\\n\\n    if is_autoscaler_v2():\\n        from ray._private.event.event_logger import parse_event, filter_event_by_level\\n\\n        for event_line in lines:\\n            if autoscaler_log_fyi_needed():\\n                yield AUTOSCALER_LOG_FYI\\n\\n            event = parse_event(event_line)\\n            if not event or not event.message:\\n                continue\\n\\n            if filter_event_by_level(\\n                event, ray_constants.RAY_LOG_TO_DRIVER_EVENT_LEVEL\\n            ):\\n                continue\\n\\n            yield event.message\\n    else:\\n        # Print out autoscaler events only, ignoring other messages.\\n        for line in lines:\\n            if ray_constants.LOG_PREFIX_EVENT_SUMMARY in line:\\n                if autoscaler_log_fyi_needed():\\n                    yield AUTOSCALER_LOG_FYI\\n                # The event text immediately follows the \\\":event_summary:\\\"\\n                # magic token.\\n                yield line.split(ray_constants.LOG_PREFIX_EVENT_SUMMARY)[1]\\n\\n\\ndef time_string() -> str:\\n    \\\"\\\"\\\"Return the relative time from the start of this job.\\n\\n    For example, 15m30s.\\n    \\\"\\\"\\\"\\n    delta = time.time() - t0\\n    hours = 0\\n    minutes = 0\\n    while delta > 3600:\\n        hours += 1\\n        delta -= 3600\\n    while delta > 60:\\n        minutes += 1\\n        delta -= 60\\n    output = \\\"\\\"\\n    if hours:\\n        output += f\\\"{hours}h\\\"\\n    if minutes:\\n        output += f\\\"{minutes}m\\\"\\n    output += f\\\"{int(delta)}s\\\"\\n    return output\\n\\n\\n# When we enter a breakpoint, worker logs are automatically disabled via this.\\n_worker_logs_enabled = True\\n\\n\\ndef print_worker_logs(data: Dict[str, str], print_file: Any):\\n    if not _worker_logs_enabled:\\n        return\\n\\n    def prefix_for(data: Dict[str, str]) -> str:\\n        \\\"\\\"\\\"The PID prefix for this log line.\\\"\\\"\\\"\\n        if data.get(\\\"pid\\\") in [\\\"autoscaler\\\", \\\"raylet\\\"]:\\n            return \\\"\\\"\\n        else:\\n            res = \\\"pid=\\\"\\n            if data.get(\\\"actor_name\\\"):\\n                res = f\\\"{data['actor_name']} {res}\\\"\\n            elif data.get(\\\"task_name\\\"):\\n                res = f\\\"{data['task_name']} {res}\\\"\\n            return res\\n\\n    def message_for(data: Dict[str, str], line: str) -> str:\\n        \\\"\\\"\\\"The printed message of this log line.\\\"\\\"\\\"\\n        if ray_constants.LOG_PREFIX_INFO_MESSAGE in line:\\n            return line.split(ray_constants.LOG_PREFIX_INFO_MESSAGE)[1]\\n        return line\\n\\n    def color_for(data: Dict[str, str], line: str) -> str:\\n        \\\"\\\"\\\"The color for this log line.\\\"\\\"\\\"\\n        if (\\n            data.get(\\\"pid\\\") == \\\"raylet\\\"\\n            and ray_constants.LOG_PREFIX_INFO_MESSAGE not in line\\n        ):\\n            return colorama.Fore.YELLOW\\n        elif data.get(\\\"pid\\\") == \\\"autoscaler\\\":\\n            if \\\"Error:\\\" in line or \\\"Warning:\\\" in line:\\n                return colorama.Fore.YELLOW\\n            else:\\n                return colorama.Fore.CYAN\\n        elif os.getenv(\\\"RAY_COLOR_PREFIX\\\") == \\\"1\\\":\\n            colors = [\\n                # colorama.Fore.BLUE, # Too dark\\n                colorama.Fore.MAGENTA,\\n                colorama.Fore.CYAN,\\n                colorama.Fore.GREEN,\\n                # colorama.Fore.WHITE, # Too light\\n                # colorama.Fore.RED,\\n                colorama.Fore.LIGHTBLACK_EX,\\n                colorama.Fore.LIGHTBLUE_EX,\\n                # colorama.Fore.LIGHTCYAN_EX, # Too light\\n                # colorama.Fore.LIGHTGREEN_EX, # Too light\\n                colorama.Fore.LIGHTMAGENTA_EX,\\n                # colorama.Fore.LIGHTWHITE_EX, # Too light\\n                # colorama.Fore.LIGHTYELLOW_EX, # Too light\\n            ]\\n            pid = data.get(\\\"pid\\\", 0)\\n            try:\\n                i = int(pid)\\n            except ValueError:\\n                i = 0\\n            return colors[i % len(colors)]\\n        else:\\n            return colorama.Fore.CYAN\\n\\n    if data.get(\\\"pid\\\") == \\\"autoscaler\\\":\\n        pid = \\\"autoscaler +{}\\\".format(time_string())\\n        lines = filter_autoscaler_events(data.get(\\\"lines\\\", []))\\n    else:\\n        pid = data.get(\\\"pid\\\")\\n        lines = data.get(\\\"lines\\\", [])\\n\\n    if data.get(\\\"ip\\\") == data.get(\\\"localhost\\\"):\\n        for line in lines:\\n            if RAY_TQDM_MAGIC in line:\\n                process_tqdm(line)\\n            else:\\n                hide_tqdm()\\n                print(\\n                    \\\"{}({}{}){} {}\\\".format(\\n                        color_for(data, line),\\n                        prefix_for(data),\\n                        pid,\\n                        colorama.Style.RESET_ALL,\\n                        message_for(data, line),\\n                    ),\\n                    file=print_file,\\n                )\\n    else:\\n        for line in lines:\\n            if RAY_TQDM_MAGIC in line:\\n                process_tqdm(line)\\n            else:\\n                hide_tqdm()\\n                print(\\n                    \\\"{}({}{}, ip={}){} {}\\\".format(\\n                        color_for(data, line),\\n                        prefix_for(data),\\n                        pid,\\n                        data.get(\\\"ip\\\"),\\n                        colorama.Style.RESET_ALL,\\n                        message_for(data, line),\\n                    ),\\n                    file=print_file,\\n                )\\n    # Restore once at end of batch to avoid excess hiding/unhiding of tqdm.\\n    restore_tqdm()\\n\\n\\ndef process_tqdm(line):\\n    \\\"\\\"\\\"Experimental distributed tqdm: see ray.experimental.tqdm_ray.\\\"\\\"\\\"\\n    try:\\n        data = json.loads(line)\\n        tqdm_ray.instance().process_state_update(data)\\n    except Exception:\\n        if log_once(\\\"tqdm_corruption\\\"):\\n            logger.warning(\\n                f\\\"[tqdm_ray] Failed to decode {line}, this may be due to \\\"\\n                \\\"logging too fast. This warning will not be printed again.\\\"\\n            )\\n\\n\\ndef hide_tqdm():\\n    \\\"\\\"\\\"Hide distributed tqdm bars temporarily to avoid conflicts with other logs.\\\"\\\"\\\"\\n    tqdm_ray.instance().hide_bars()\\n\\n\\ndef restore_tqdm():\\n    \\\"\\\"\\\"Undo hide_tqdm().\\\"\\\"\\\"\\n    tqdm_ray.instance().unhide_bars()\\n\\n\\ndef listen_error_messages(worker, threads_stopped):\\n    \\\"\\\"\\\"Listen to error messages in the background on the driver.\\n\\n    This runs in a separate thread on the driver and pushes (error, time)\\n    tuples to be published.\\n\\n    Args:\\n        worker: The worker class that this thread belongs to.\\n        threads_stopped (threading.Event): A threading event used to signal to\\n            the thread that it should exit.\\n    \\\"\\\"\\\"\\n\\n    # TODO: we should just subscribe to the errors for this specific job.\\n    worker.gcs_error_subscriber.subscribe()\\n\\n    try:\\n        if _internal_kv_initialized():\\n            # Get any autoscaler errors that occurred before the call to\\n            # subscribe.\\n            error_message = _internal_kv_get(ray_constants.DEBUG_AUTOSCALING_ERROR)\\n            if error_message is not None:\\n                logger.warning(error_message.decode())\\n\\n        while True:\\n            # Exit if received a signal that the thread should stop.\\n            if threads_stopped.is_set():\\n                return\\n\\n            _, error_data = worker.gcs_error_subscriber.poll()\\n            if error_data is None:\\n                continue\\n            if error_data[\\\"job_id\\\"] not in [\\n                worker.current_job_id.binary(),\\n                JobID.nil().binary(),\\n            ]:\\n                continue\\n\\n            error_message = error_data[\\\"error_message\\\"]\\n            if error_data[\\\"type\\\"] == ray_constants.TASK_PUSH_ERROR:\\n                # TODO(ekl) remove task push errors entirely now that we have\\n                # the separate unhandled exception handler.\\n                pass\\n            else:\\n                print_to_stdstream(\\n                    {\\n                        \\\"lines\\\": [error_message],\\n                        \\\"pid\\\": \\\"raylet\\\",\\n                        \\\"is_err\\\": False,\\n                    }\\n                )\\n    except (OSError, ConnectionError) as e:\\n        logger.error(f\\\"listen_error_messages: {e}\\\")\\n\\n\\n@PublicAPI\\n@client_mode_hook\\ndef is_initialized() -> bool:\\n    \\\"\\\"\\\"Check if ray.init has been called yet.\\n\\n    Returns:\\n        True if ray.init has already been called and false otherwise.\\n    \\\"\\\"\\\"\\n    return ray._private.worker.global_worker.connected\\n\\n\\ndef connect(\\n    node,\\n    session_name: str,\\n    mode=WORKER_MODE,\\n    log_to_driver: bool = False,\\n    worker=global_worker,\\n    driver_object_store_memory: Optional[int] = None,\\n    job_id=None,\\n    namespace: Optional[str] = None,\\n    job_config=None,\\n    runtime_env_hash: int = 0,\\n    startup_token: int = 0,\\n    ray_debugger_external: bool = False,\\n    entrypoint: str = \\\"\\\",\\n    worker_launch_time_ms: int = -1,\\n    worker_launched_time_ms: int = -1,\\n):\\n    \\\"\\\"\\\"Connect this worker to the raylet, to Plasma, and to GCS.\\n\\n    Args:\\n        node (ray._private.node.Node): The node to connect.\\n        session_name: The session name (cluster id) of this cluster.\\n        mode: The mode of the worker. One of SCRIPT_MODE, WORKER_MODE, and LOCAL_MODE.\\n        log_to_driver: If true, then output from all of the worker\\n            processes on all nodes will be directed to the driver.\\n        worker: The ray.Worker instance.\\n        driver_object_store_memory: Deprecated.\\n        job_id: The ID of job. If it's None, then we will generate one.\\n        namespace: Namespace to use.\\n        job_config (ray.job_config.JobConfig): The job configuration.\\n        runtime_env_hash: The hash of the runtime env for this worker.\\n        startup_token: The startup token of the process assigned to\\n            it during startup as a command line argument.\\n        ray_debugger_external: If True, make the debugger external to the\\n            node this worker is running on.\\n        entrypoint: The name of the entrypoint script. Ignored if the\\n            mode != SCRIPT_MODE\\n        worker_launch_time_ms: The time when the worker process for this worker\\n            is launched. If the worker is not launched by raylet (e.g.,\\n            driver), this must be -1 (default value).\\n        worker_launched_time_ms: The time when the worker process for this worker\\n            finshes launching. If the worker is not launched by raylet (e.g.,\\n            driver), this must be -1 (default value).\\n    \\\"\\\"\\\"\\n    # Do some basic checking to make sure we didn't call ray.init twice.\\n    error_message = \\\"Perhaps you called ray.init twice by accident?\\\"\\n    assert not worker.connected, error_message\\n\\n    # Enable nice stack traces on SIGSEGV etc.\\n    try:\\n        if not faulthandler.is_enabled():\\n            faulthandler.enable(all_threads=False)\\n    except io.UnsupportedOperation:\\n        pass  # ignore\\n\\n    worker.gcs_client = node.get_gcs_client()\\n    assert worker.gcs_client is not None\\n    _initialize_internal_kv(worker.gcs_client)\\n    ray._private.state.state._initialize_global_state(\\n        ray._raylet.GcsClientOptions.from_gcs_address(node.gcs_address)\\n    )\\n    worker.gcs_publisher = ray._raylet.GcsPublisher(address=worker.gcs_client.address)\\n    # Initialize some fields.\\n    if mode in (WORKER_MODE, RESTORE_WORKER_MODE, SPILL_WORKER_MODE):\\n        # We should not specify the job_id if it's `WORKER_MODE`.\\n        assert job_id is None\\n        job_id = JobID.nil()\\n    else:\\n        # This is the code path of driver mode.\\n        if job_id is None:\\n            job_id = ray._private.state.next_job_id()\\n\\n    if mode is not SCRIPT_MODE and mode is not LOCAL_MODE and setproctitle:\\n        process_name = ray_constants.WORKER_PROCESS_TYPE_IDLE_WORKER\\n        if mode is SPILL_WORKER_MODE:\\n            process_name = ray_constants.WORKER_PROCESS_TYPE_SPILL_WORKER_IDLE\\n        elif mode is RESTORE_WORKER_MODE:\\n            process_name = ray_constants.WORKER_PROCESS_TYPE_RESTORE_WORKER_IDLE\\n        setproctitle.setproctitle(process_name)\\n\\n    if not isinstance(job_id, JobID):\\n        raise TypeError(\\\"The type of given job id must be JobID.\\\")\\n\\n    # All workers start out as non-actors. A worker can be turned into an actor\\n    # after it is created.\\n    worker.node = node\\n    worker.set_mode(mode)\\n\\n    # For driver's check that the version information matches the version\\n    # information that the Ray cluster was started with.\\n    try:\\n        node.check_version_info()\\n    except Exception as e:\\n        if mode == SCRIPT_MODE:\\n            raise e\\n        elif mode == WORKER_MODE:\\n            traceback_str = traceback.format_exc()\\n            ray._private.utils.publish_error_to_driver(\\n                ray_constants.VERSION_MISMATCH_PUSH_ERROR,\\n                traceback_str,\\n                gcs_publisher=worker.gcs_publisher,\\n                num_retries=1,\\n            )\\n\\n    driver_name = \\\"\\\"\\n    log_stdout_file_path = \\\"\\\"\\n    log_stderr_file_path = \\\"\\\"\\n    interactive_mode = False\\n    if mode == SCRIPT_MODE:\\n        import __main__ as main\\n\\n        if hasattr(main, \\\"__file__\\\"):\\n            driver_name = main.__file__\\n        else:\\n            interactive_mode = True\\n            driver_name = \\\"INTERACTIVE MODE\\\"\\n    elif not LOCAL_MODE:\\n        raise ValueError(\\\"Invalid worker mode. Expected DRIVER, WORKER or LOCAL.\\\")\\n\\n    gcs_options = ray._raylet.GcsClientOptions.from_gcs_address(node.gcs_address)\\n    if job_config is None:\\n        job_config = ray.job_config.JobConfig()\\n\\n    if namespace is not None:\\n        ray._private.utils.validate_namespace(namespace)\\n\\n        # The namespace field of job config may have already been set in code\\n        # paths such as the client.\\n        job_config.set_ray_namespace(namespace)\\n\\n    # Make sure breakpoint() in the user's code will\\n    # invoke the Ray debugger if we are in a worker or actor process\\n    # (but not on the driver).\\n    if mode == WORKER_MODE:\\n        os.environ[\\\"PYTHONBREAKPOINT\\\"] = \\\"ray.util.rpdb.set_trace\\\"\\n    else:\\n        # Add hook to suppress worker logs during breakpoint.\\n        os.environ[\\\"PYTHONBREAKPOINT\\\"] = \\\"ray.util.rpdb._driver_set_trace\\\"\\n\\n    worker.ray_debugger_external = ray_debugger_external\\n\\n    # If it's a driver and it's not coming from ray client, we'll prepare the\\n    # environment here. If it's ray client, the environment will be prepared\\n    # at the server side.\\n    if mode == SCRIPT_MODE and not job_config._client_job and job_config.runtime_env:\\n        scratch_dir: str = worker.node.get_runtime_env_dir_path()\\n        runtime_env = job_config.runtime_env or {}\\n        runtime_env = upload_py_modules_if_needed(\\n            runtime_env, scratch_dir, logger=logger\\n        )\\n        runtime_env = upload_working_dir_if_needed(\\n            runtime_env, scratch_dir, logger=logger\\n        )\\n        runtime_env = upload_worker_process_setup_hook_if_needed(\\n            runtime_env,\\n            worker,\\n        )\\n        # Remove excludes, it isn't relevant after the upload step.\\n        runtime_env.pop(\\\"excludes\\\", None)\\n        job_config.set_runtime_env(runtime_env)\\n\\n    if mode == SCRIPT_MODE:\\n        # Add the directory containing the script that is running to the Python\\n        # paths of the workers. Also add the current directory. Note that this\\n        # assumes that the directory structures on the machines in the clusters\\n        # are the same.\\n        # When using an interactive shell, there is no script directory.\\n        # We also want to skip adding script directory when running from dashboard.\\n        code_paths = []\\n        if not interactive_mode and not (\\n            namespace and namespace == ray_constants.RAY_INTERNAL_DASHBOARD_NAMESPACE\\n        ):\\n            script_directory = os.path.dirname(os.path.realpath(sys.argv[0]))\\n            # If driver's sys.path doesn't include the script directory\\n            # (e.g driver is started via `python -m`,\\n            # see https://peps.python.org/pep-0338/),\\n            # then we shouldn't add it to the workers.\\n            if script_directory in sys.path:\\n                code_paths.append(script_directory)\\n        # In client mode, if we use runtime envs with \\\"working_dir\\\", then\\n        # it'll be handled automatically.  Otherwise, add the current dir.\\n        if not job_config._client_job and not job_config._runtime_env_has_working_dir():\\n            current_directory = os.path.abspath(os.path.curdir)\\n            code_paths.append(current_directory)\\n        if len(code_paths) != 0:\\n            job_config._py_driver_sys_path.extend(code_paths)\\n\\n    serialized_job_config = job_config._serialize()\\n    if not node.should_redirect_logs():\\n        # Logging to stderr, so give core worker empty logs directory.\\n        logs_dir = \\\"\\\"\\n    else:\\n        logs_dir = node.get_logs_dir_path()\\n\\n    worker.core_worker = ray._raylet.CoreWorker(\\n        mode,\\n        node.plasma_store_socket_name,\\n        node.raylet_socket_name,\\n        job_id,\\n        gcs_options,\\n        logs_dir,\\n        node.node_ip_address,\\n        node.node_manager_port,\\n        node.raylet_ip_address,\\n        (mode == LOCAL_MODE),\\n        driver_name,\\n        log_stdout_file_path,\\n        log_stderr_file_path,\\n        serialized_job_config,\\n        node.metrics_agent_port,\\n        runtime_env_hash,\\n        startup_token,\\n        session_name,\\n        node.cluster_id,\\n        \\\"\\\" if mode != SCRIPT_MODE else entrypoint,\\n        worker_launch_time_ms,\\n        worker_launched_time_ms,\\n    )\\n\\n    # Notify raylet that the core worker is ready.\\n    worker.core_worker.notify_raylet()\\n\\n    if mode == SCRIPT_MODE:\\n        worker_id = worker.worker_id\\n        worker.gcs_error_subscriber = ray._raylet.GcsErrorSubscriber(\\n            worker_id=worker_id, address=worker.gcs_client.address\\n        )\\n        worker.gcs_log_subscriber = ray._raylet.GcsLogSubscriber(\\n            worker_id=worker_id, address=worker.gcs_client.address\\n        )\\n\\n    if driver_object_store_memory is not None:\\n        logger.warning(\\n            \\\"`driver_object_store_memory` is deprecated\\\"\\n            \\\" and will be removed in the future.\\\"\\n        )\\n\\n    # If this is a driver running in SCRIPT_MODE, start a thread to print error\\n    # messages asynchronously in the background. Ideally the scheduler would\\n    # push messages to the driver's worker service, but we ran into bugs when\\n    # trying to properly shutdown the driver's worker service, so we are\\n    # temporarily using this implementation which constantly queries the\\n    # scheduler for new error messages.\\n    if mode == SCRIPT_MODE:\\n        worker.listener_thread = threading.Thread(\\n            target=listen_error_messages,\\n            name=\\\"ray_listen_error_messages\\\",\\n            args=(worker, worker.threads_stopped),\\n        )\\n        worker.listener_thread.daemon = True\\n        worker.listener_thread.start()\\n        if log_to_driver:\\n            global_worker_stdstream_dispatcher.add_handler(\\n                \\\"ray_print_logs\\\", print_to_stdstream\\n            )\\n            worker.logger_thread = threading.Thread(\\n                target=worker.print_logs, name=\\\"ray_print_logs\\\"\\n            )\\n            worker.logger_thread.daemon = True\\n            worker.logger_thread.start()\\n\\n    # Setup tracing here\\n    tracing_hook_val = worker.gcs_client.internal_kv_get(\\n        b\\\"tracing_startup_hook\\\", ray_constants.KV_NAMESPACE_TRACING\\n    )\\n    if tracing_hook_val is not None:\\n        ray.util.tracing.tracing_helper._enable_tracing()\\n        if not getattr(ray, \\\"__traced__\\\", False):\\n            _setup_tracing = _import_from_string(tracing_hook_val.decode(\\\"utf-8\\\"))\\n            _setup_tracing()\\n            ray.__traced__ = True\\n\\n\\ndef disconnect(exiting_interpreter=False):\\n    \\\"\\\"\\\"Disconnect this worker from the raylet and object store.\\\"\\\"\\\"\\n    # Reset the list of cached remote functions and actors so that if more\\n    # remote functions or actors are defined and then connect is called again,\\n    # the remote functions will be exported. This is mostly relevant for the\\n    # tests.\\n    worker = global_worker\\n    if worker.connected:\\n        # Shutdown all of the threads that we've started. TODO(rkn): This\\n        # should be handled cleanly in the worker object's destructor and not\\n        # in this disconnect method.\\n        worker.threads_stopped.set()\\n        if hasattr(worker, \\\"gcs_error_subscriber\\\"):\\n            worker.gcs_error_subscriber.close()\\n        if hasattr(worker, \\\"gcs_log_subscriber\\\"):\\n            worker.gcs_log_subscriber.close()\\n        if hasattr(worker, \\\"listener_thread\\\"):\\n            worker.listener_thread.join()\\n        if hasattr(worker, \\\"logger_thread\\\"):\\n            worker.logger_thread.join()\\n        worker.threads_stopped.clear()\\n\\n        worker._session_index += 1\\n\\n        for leftover in stdout_deduplicator.flush():\\n            print_worker_logs(leftover, sys.stdout)\\n        for leftover in stderr_deduplicator.flush():\\n            print_worker_logs(leftover, sys.stderr)\\n        global_worker_stdstream_dispatcher.remove_handler(\\\"ray_print_logs\\\")\\n\\n    worker.node = None  # Disconnect the worker from the node.\\n    worker.serialization_context_map.clear()\\n    try:\\n        ray_actor = ray.actor\\n    except AttributeError:\\n        ray_actor = None  # This can occur during program termination\\n    if ray_actor is not None:\\n        ray_actor._ActorClassMethodMetadata.reset_cache()\\n\\n\\ndef start_import_thread():\\n    \\\"\\\"\\\"Start the import thread if the worker is connected.\\\"\\\"\\\"\\n    worker = global_worker\\n    worker.check_connected()\\n\\n    assert _mode() not in (\\n        RESTORE_WORKER_MODE,\\n        SPILL_WORKER_MODE,\\n    ), \\\"import thread can not be used in IO workers.\\\"\\n    if worker.import_thread and ray._raylet.Config.start_python_importer_thread():\\n        worker.import_thread.start()\\n\\n\\n@contextmanager\\ndef _changeproctitle(title, next_title):\\n    if _mode() is not LOCAL_MODE:\\n        setproctitle.setproctitle(title)\\n    try:\\n        yield\\n    finally:\\n        if _mode() is not LOCAL_MODE:\\n            setproctitle.setproctitle(next_title)\\n\\n\\n@DeveloperAPI\\ndef show_in_dashboard(message: str, key: str = \\\"\\\", dtype: str = \\\"text\\\"):\\n    \\\"\\\"\\\"Display message in dashboard.\\n\\n    Display message for the current task or actor in the dashboard.\\n    For example, this can be used to display the status of a long-running\\n    computation.\\n\\n    Args:\\n        message: Message to be displayed.\\n        key: The key name for the message. Multiple message under\\n            different keys will be displayed at the same time. Messages\\n            under the same key will be overridden.\\n        dtype: The type of message for rendering. One of the\\n            following: text, html.\\n    \\\"\\\"\\\"\\n    worker = global_worker\\n    worker.check_connected()\\n\\n    acceptable_dtypes = {\\\"text\\\", \\\"html\\\"}\\n    assert dtype in acceptable_dtypes, f\\\"dtype accepts only: {acceptable_dtypes}\\\"\\n\\n    message_wrapped = {\\\"message\\\": message, \\\"dtype\\\": dtype}\\n    message_encoded = json.dumps(message_wrapped).encode()\\n\\n    worker.core_worker.set_webui_display(key.encode(), message_encoded)\\n\\n\\n# Global variable to make sure we only send out the warning once.\\nblocking_get_inside_async_warned = False\\n\\n\\n@overload\\ndef get(\\n    object_refs: \\\"Sequence[ObjectRef[Any]]\\\", *, timeout: Optional[float] = None\\n) -> List[Any]:\\n    ...\\n\\n\\n@overload\\ndef get(\\n    object_refs: \\\"Sequence[ObjectRef[R]]\\\", *, timeout: Optional[float] = None\\n) -> List[R]:\\n    ...\\n\\n\\n@overload\\ndef get(object_refs: \\\"ObjectRef[R]\\\", *, timeout: Optional[float] = None) -> R:\\n    ...\\n\\n\\n@PublicAPI\\n@client_mode_hook\\ndef get(\\n    object_refs: Union[ray.ObjectRef, Sequence[ray.ObjectRef]],\\n    *,\\n    timeout: Optional[float] = None,\\n) -> Union[Any, List[Any]]:\\n    \\\"\\\"\\\"Get a remote object or a list of remote objects from the object store.\\n\\n    This method blocks until the object corresponding to the object ref is\\n    available in the local object store. If this object is not in the local\\n    object store, it will be shipped from an object store that has it (once the\\n    object has been created). If object_refs is a list, then the objects\\n    corresponding to each object in the list will be returned.\\n\\n    Ordering for an input list of object refs is preserved for each object\\n    returned. That is, if an object ref to A precedes an object ref to B in the\\n    input list, then A will precede B in the returned list.\\n\\n    This method will issue a warning if it's running inside async context,\\n    you can use ``await object_ref`` instead of ``ray.get(object_ref)``. For\\n    a list of object refs, you can use ``await asyncio.gather(*object_refs)``.\\n\\n    Related patterns and anti-patterns:\\n\\n    - :doc:`/ray-core/patterns/ray-get-loop`\\n    - :doc:`/ray-core/patterns/unnecessary-ray-get`\\n    - :doc:`/ray-core/patterns/ray-get-submission-order`\\n    - :doc:`/ray-core/patterns/ray-get-too-many-objects`\\n\\n\\n    Args:\\n        object_refs: Object ref of the object to get or a list of object refs\\n            to get.\\n        timeout (Optional[float]): The maximum amount of time in seconds to\\n            wait before returning. Set this to None will block until the\\n            corresponding object becomes available. Setting ``timeout=0`` will\\n            return the object immediately if it's available, else raise\\n            GetTimeoutError in accordance with the above docstring.\\n\\n    Returns:\\n        A Python object or a list of Python objects.\\n\\n    Raises:\\n        GetTimeoutError: A GetTimeoutError is raised if a timeout is set and\\n            the get takes longer than timeout to return.\\n        Exception: An exception is raised if the task that created the object\\n            or that created one of the objects raised an exception.\\n    \\\"\\\"\\\"\\n    worker = global_worker\\n    worker.check_connected()\\n\\n    if hasattr(worker, \\\"core_worker\\\") and worker.core_worker.current_actor_is_asyncio():\\n        global blocking_get_inside_async_warned\\n        if not blocking_get_inside_async_warned:\\n            logger.warning(\\n                \\\"Using blocking ray.get inside async actor. \\\"\\n                \\\"This blocks the event loop. Please use `await` \\\"\\n                \\\"on object ref with asyncio.gather if you want to \\\"\\n                \\\"yield execution to the event loop instead.\\\"\\n            )\\n            blocking_get_inside_async_warned = True\\n\\n    with profiling.profile(\\\"ray.get\\\"):\\n        # TODO(sang): Should make StreamingObjectRefGenerator\\n        # compatible to ray.get for dataset.\\n        if isinstance(object_refs, StreamingObjectRefGenerator):\\n            return object_refs\\n\\n        is_individual_id = isinstance(object_refs, ray.ObjectRef)\\n        if is_individual_id:\\n            object_refs = [object_refs]\\n\\n        if not isinstance(object_refs, list):\\n            raise ValueError(\\n                \\\"'object_refs' must either be an ObjectRef or a list of ObjectRefs.\\\"\\n            )\\n\\n        # TODO(ujvl): Consider how to allow user to retrieve the ready objects.\\n        values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\\n        for i, value in enumerate(values):\\n            if isinstance(value, RayError):\\n                if isinstance(value, ray.exceptions.ObjectLostError):\\n                    worker.core_worker.dump_object_store_memory_usage()\\n                if isinstance(value, RayTaskError):\\n                    raise value.as_instanceof_cause()\\n                else:\\n                    raise value\\n\\n        if is_individual_id:\\n            values = values[0]\\n\\n        if debugger_breakpoint != b\\\"\\\":\\n            frame = sys._getframe().f_back\\n            rdb = ray.util.pdb._connect_ray_pdb(\\n                host=None,\\n                port=None,\\n                patch_stdstreams=False,\\n                quiet=None,\\n                breakpoint_uuid=debugger_breakpoint.decode()\\n                if debugger_breakpoint\\n                else None,\\n                debugger_external=worker.ray_debugger_external,\\n            )\\n            rdb.set_trace(frame=frame)\\n\\n        return values\\n\\n\\n@PublicAPI\\n@client_mode_hook\\ndef put(\\n    value: Any, *, _owner: Optional[\\\"ray.actor.ActorHandle\\\"] = None\\n) -> \\\"ray.ObjectRef\\\":\\n    \\\"\\\"\\\"Store an object in the object store.\\n\\n    The object may not be evicted while a reference to the returned ID exists.\\n\\n    Related patterns and anti-patterns:\\n\\n    - :doc:`/ray-core/patterns/return-ray-put`\\n    - :doc:`/ray-core/patterns/pass-large-arg-by-value`\\n    - :doc:`/ray-core/patterns/closure-capture-large-objects`\\n\\n    Args:\\n        value: The Python object to be stored.\\n        _owner [Experimental]: The actor that should own this object. This\\n            allows creating objects with lifetimes decoupled from that of the\\n            creating process. The owner actor must be passed a reference to the\\n            object prior to the object creator exiting, otherwise the reference\\n            will still be lost. *Note that this argument is an experimental API\\n            and should be avoided if possible.*\\n\\n    Returns:\\n        The object ref assigned to this value.\\n    \\\"\\\"\\\"\\n    worker = global_worker\\n    worker.check_connected()\\n\\n    if _owner is None:\\n        serialize_owner_address = None\\n    elif isinstance(_owner, ray.actor.ActorHandle):\\n        # Ensure `ray._private.state.state.global_state_accessor` is not None\\n        ray._private.state.state._check_connected()\\n        serialize_owner_address = (\\n            ray._raylet._get_actor_serialized_owner_address_or_none(\\n                ray._private.state.state.global_state_accessor.get_actor_info(\\n                    _owner._actor_id\\n                )\\n            )\\n        )\\n        if not serialize_owner_address:\\n            raise RuntimeError(f\\\"{_owner} is not alive, it's worker_id is empty!\\\")\\n    else:\\n        raise TypeError(f\\\"Expect an `ray.actor.ActorHandle`, but got: {type(_owner)}\\\")\\n\\n    with profiling.profile(\\\"ray.put\\\"):\\n        try:\\n            object_ref = worker.put_object(value, owner_address=serialize_owner_address)\\n        except ObjectStoreFullError:\\n            logger.info(\\n                \\\"Put failed since the value was either too large or the \\\"\\n                \\\"store was full of pinned objects.\\\"\\n            )\\n            raise\\n        return object_ref\\n\\n\\n# Global variable to make sure we only send out the warning once.\\nblocking_wait_inside_async_warned = False\\n\\n\\n@PublicAPI\\n@client_mode_hook\\ndef wait(\\n    object_refs: List[\\\"ray.ObjectRef\\\"],\\n    *,\\n    num_returns: int = 1,\\n    timeout: Optional[float] = None,\\n    fetch_local: bool = True,\\n) -> Tuple[List[\\\"ray.ObjectRef\\\"], List[\\\"ray.ObjectRef\\\"]]:\\n    \\\"\\\"\\\"Return a list of IDs that are ready and a list of IDs that are not.\\n\\n    If timeout is set, the function returns either when the requested number of\\n    IDs are ready or when the timeout is reached, whichever occurs first. If it\\n    is not set, the function simply waits until that number of objects is ready\\n    and returns that exact number of object refs.\\n\\n    This method returns two lists. The first list consists of object refs that\\n    correspond to objects that are available in the object store. The second\\n    list corresponds to the rest of the object refs (which may or may not be\\n    ready).\\n\\n    Ordering of the input list of object refs is preserved. That is, if A\\n    precedes B in the input list, and both are in the ready list, then A will\\n    precede B in the ready list. This also holds true if A and B are both in\\n    the remaining list.\\n\\n    This method will issue a warning if it's running inside an async context.\\n    Instead of ``ray.wait(object_refs)``, you can use\\n    ``await asyncio.wait(object_refs)``.\\n\\n    Related patterns and anti-patterns:\\n\\n    - :doc:`/ray-core/patterns/limit-pending-tasks`\\n    - :doc:`/ray-core/patterns/ray-get-submission-order`\\n\\n    Args:\\n        object_refs: List of :class:`~ObjectRefs` or\\n            :class:`~StreamingObjectRefGenerators` for objects that may or may\\n            not be ready. Note that these must be unique.\\n        num_returns: The number of object refs that should be returned.\\n        timeout: The maximum amount of time in seconds to wait before\\n            returning.\\n        fetch_local: If True, wait for the object to be downloaded onto\\n            the local node before returning it as ready. If False, ray.wait()\\n            will not trigger fetching of objects to the local node and will\\n            return immediately once the object is available anywhere in the\\n            cluster.\\n\\n    Returns:\\n        A list of object refs that are ready and a list of the remaining object\\n        IDs.\\n    \\\"\\\"\\\"\\n    worker = global_worker\\n    worker.check_connected()\\n\\n    if (\\n        hasattr(worker, \\\"core_worker\\\")\\n        and worker.core_worker.current_actor_is_asyncio()\\n        and timeout != 0\\n    ):\\n        global blocking_wait_inside_async_warned\\n        if not blocking_wait_inside_async_warned:\\n            logger.debug(\\n                \\\"Using blocking ray.wait inside async method. \\\"\\n                \\\"This blocks the event loop. Please use `await` \\\"\\n                \\\"on object ref with asyncio.wait. \\\"\\n            )\\n            blocking_wait_inside_async_warned = True\\n\\n    if isinstance(object_refs, ObjectRef) or isinstance(\\n        object_refs, StreamingObjectRefGenerator\\n    ):\\n        raise TypeError(\\n            \\\"wait() expected a list of ray.ObjectRef or ray.StreamingObjectRefGenerator\\\"\\n            \\\", got a single ray.ObjectRef or ray.StreamingObjectRefGenerator \\\"\\n            f\\\"{object_refs}\\\"\\n        )\\n\\n    if not isinstance(object_refs, list):\\n        raise TypeError(\\n            \\\"wait() expected a list of ray.ObjectRef or \\\"\\n            \\\"ray.StreamingObjectRefGenerator, \\\"\\n            f\\\"got {type(object_refs)}\\\"\\n        )\\n\\n    if timeout is not None and timeout < 0:\\n        raise ValueError(\\n            \\\"The 'timeout' argument must be nonnegative. \\\" f\\\"Received {timeout}\\\"\\n        )\\n\\n    for object_ref in object_refs:\\n        if not isinstance(object_ref, ObjectRef) and not isinstance(\\n            object_ref, StreamingObjectRefGenerator\\n        ):\\n            raise TypeError(\\n                \\\"wait() expected a list of ray.ObjectRef or \\\"\\n                \\\"ray.StreamingObjectRefGenerator, \\\"\\n                f\\\"got list containing {type(object_ref)}\\\"\\n            )\\n    worker.check_connected()\\n\\n    # TODO(swang): Check main thread.\\n    with profiling.profile(\\\"ray.wait\\\"):\\n        # TODO(rkn): This is a temporary workaround for\\n        # https://github.com/ray-project/ray/issues/997. However, it should be\\n        # fixed in Arrow instead of here.\\n        if len(object_refs) == 0:\\n            return [], []\\n\\n        if len(object_refs) != len(set(object_refs)):\\n            raise ValueError(\\\"Wait requires a list of unique object refs.\\\")\\n        if num_returns <= 0:\\n            raise ValueError(\\\"Invalid number of objects to return %d.\\\" % num_returns)\\n        if num_returns > len(object_refs):\\n            raise ValueError(\\n                \\\"num_returns cannot be greater than the number \\\"\\n                \\\"of objects provided to ray.wait.\\\"\\n            )\\n\\n        timeout = timeout if timeout is not None else 10**6\\n        timeout_milliseconds = int(timeout * 1000)\\n        ready_ids, remaining_ids = worker.core_worker.wait(\\n            object_refs,\\n            num_returns,\\n            timeout_milliseconds,\\n            worker.current_task_id,\\n            fetch_local,\\n        )\\n        return ready_ids, remaining_ids\\n\\n\\n@PublicAPI\\n@client_mode_hook\\ndef get_actor(name: str, namespace: Optional[str] = None) -> \\\"ray.actor.ActorHandle\\\":\\n    \\\"\\\"\\\"Get a handle to a named actor.\\n\\n    Gets a handle to an actor with the given name. The actor must\\n    have been created with Actor.options(name=\\\"name\\\").remote(). This\\n    works for both detached & non-detached actors.\\n\\n    This method is a sync call and it'll timeout after 60s. This can be modified\\n    by setting OS env RAY_gcs_server_request_timeout_seconds before starting\\n    the cluster.\\n\\n    Args:\\n        name: The name of the actor.\\n        namespace: The namespace of the actor, or None to specify the current\\n            namespace.\\n\\n    Returns:\\n        ActorHandle to the actor.\\n\\n    Raises:\\n        ValueError if the named actor does not exist.\\n    \\\"\\\"\\\"\\n    if not name:\\n        raise ValueError(\\\"Please supply a non-empty value to get_actor\\\")\\n\\n    if namespace is not None:\\n        ray._private.utils.validate_namespace(namespace)\\n\\n    worker = global_worker\\n    worker.check_connected()\\n    return worker.core_worker.get_named_actor_handle(name, namespace or \\\"\\\")\\n\\n\\n@PublicAPI\\n@client_mode_hook\\ndef kill(actor: \\\"ray.actor.ActorHandle\\\", *, no_restart: bool = True):\\n    \\\"\\\"\\\"Kill an actor forcefully.\\n\\n    This will interrupt any running tasks on the actor, causing them to fail\\n    immediately. ``atexit`` handlers installed in the actor will not be run.\\n\\n    If you want to kill the actor but let pending tasks finish,\\n    you can call ``actor.__ray_terminate__.remote()`` instead to queue a\\n    termination task. Any ``atexit`` handlers installed in the actor *will*\\n    be run in this case.\\n\\n    If the actor is a detached actor, subsequent calls to get its handle via\\n    ray.get_actor will fail.\\n\\n    Args:\\n        actor: Handle to the actor to kill.\\n        no_restart: Whether or not this actor should be restarted if\\n            it's a restartable actor.\\n    \\\"\\\"\\\"\\n    worker = global_worker\\n    worker.check_connected()\\n    if not isinstance(actor, ray.actor.ActorHandle):\\n        raise ValueError(\\n            \\\"ray.kill() only supported for actors. For tasks, try ray.cancel(). \\\"\\n            \\\"Got: {}.\\\".format(type(actor))\\n        )\\n    worker.core_worker.kill_actor(actor._ray_actor_id, no_restart)\\n\\n\\n@PublicAPI\\n@client_mode_hook\\ndef cancel(\\n    object_ref: \\\"ray.ObjectRef\\\", *, force: bool = False, recursive: bool = True\\n) -> None:\\n    \\\"\\\"\\\"Cancels a task.\\n\\n    Cancel API has a different behavior depending on if it is a remote function\\n    (Task) or a remote Actor method (Actor Task).\\n\\n    Task:\\n        If the specified Task is pending execution, it is cancelled and not\\n        executed. If the Task is currently executing, the behavior depends\\n        on the `force` flag. When `force=False`, a KeyboardInterrupt is\\n        raised in Python and when `force=True`, the executing Task\\n        immediately exits. If the Task is already finished, nothing happens.\\n\\n        Cancelled Tasks aren't retried. `max_task_retries` aren't respected.\\n\\n        Calling ray.get on a cancelled Task raises a TaskCancelledError\\n        if the Task has been scheduled or interrupted.\\n        It raises a WorkerCrashedError if `force=True`.\\n\\n        If `recursive=True`, all the child Tasks and Actor Tasks\\n        are cancelled. If `force=True` and `recursive=True`, `force=True`\\n        is ignored for child Actor Tasks.\\n\\n    Actor Task:\\n        If the specified Task is pending execution, it is cancelled and not\\n        executed. If the Task is currently executing, the behavior depends\\n        on the execution model of an Actor. If it is a regular Actor\\n        or a threaded Actor, the execution isn't cancelled.\\n        Actor Tasks cannot be interrupted because Actors have\\n        states. If it is an async Actor, Ray cancels a `asyncio.Task`.\\n        The semantic of cancellation is equivalent to asyncio's cancellation.\\n        https://docs.python.org/3/library/asyncio-task.html#task-cancellation\\n        If the Task has finished, nothing happens.\\n\\n        Only `force=False` is allowed for an Actor Task. Otherwise, it raises\\n        `ValueError`. Use `ray.kill(actor)` instead to kill an Actor.\\n\\n        Cancelled Tasks aren't retried. `max_task_retries` aren't respected.\\n\\n        Calling ray.get on a cancelled Task raises a TaskCancelledError\\n        if the Task has been scheduled or interrupted. Also note that\\n        only async actor tasks can be interrupted.\\n\\n        If `recursive=True`, all the child Tasks and actor Tasks\\n        are cancelled.\\n\\n    Args:\\n        object_ref: ObjectRef returned by the Task\\n            that should be cancelled.\\n        force: Whether to force-kill a running Task by killing\\n            the worker that is running the Task.\\n        recursive: Whether to try to cancel Tasks submitted by the\\n            Task specified.\\n    \\\"\\\"\\\"\\n    worker = ray._private.worker.global_worker\\n    worker.check_connected()\\n\\n    if isinstance(object_ref, ray._raylet.StreamingObjectRefGenerator):\\n        assert hasattr(object_ref, \\\"_generator_ref\\\")\\n        object_ref = object_ref._generator_ref\\n\\n    if not isinstance(object_ref, ray.ObjectRef):\\n        raise TypeError(\\n            \\\"ray.cancel() only supported for object refs. \\\"\\n            f\\\"For actors, try ray.kill(). Got: {type(object_ref)}.\\\"\\n        )\\n    return worker.core_worker.cancel_task(object_ref, force, recursive)\\n\\n\\ndef _mode(worker=global_worker):\\n    \\\"\\\"\\\"This is a wrapper around worker.mode.\\n\\n    We use this wrapper so that in the remote decorator, we can call _mode()\\n    instead of worker.mode. The difference is that when we attempt to\\n    serialize remote functions, we don't attempt to serialize the worker\\n    object, which cannot be serialized.\\n    \\\"\\\"\\\"\\n    return worker.mode\\n\\n\\ndef _make_remote(function_or_class, options):\\n    if not function_or_class.__module__:\\n        function_or_class.__module__ = \\\"global\\\"\\n\\n    if inspect.isfunction(function_or_class) or is_cython(function_or_class):\\n        ray_option_utils.validate_task_options(options, in_options=False)\\n        return ray.remote_function.RemoteFunction(\\n            Language.PYTHON,\\n            function_or_class,\\n            None,\\n            options,\\n        )\\n\\n    if inspect.isclass(function_or_class):\\n        ray_option_utils.validate_actor_options(options, in_options=False)\\n        return ray.actor._make_actor(function_or_class, options)\\n\\n    raise TypeError(\\n        \\\"The @ray.remote decorator must be applied to either a function or a class.\\\"\\n    )\\n\\n\\nclass RemoteDecorator(Protocol):\\n    @overload\\n    def __call__(self, __function: Callable[[], R]) -> RemoteFunctionNoArgs[R]:\\n        ...\\n\\n    @overload\\n    def __call__(self, __function: Callable[[T0], R]) -> RemoteFunction0[R, T0]:\\n        ...\\n\\n    @overload\\n    def __call__(self, __function: Callable[[T0, T1], R]) -> RemoteFunction1[R, T0, T1]:\\n        ...\\n\\n    @overload\\n    def __call__(\\n        self, __function: Callable[[T0, T1, T2], R]\\n    ) -> RemoteFunction2[R, T0, T1, T2]:\\n        ...\\n\\n    @overload\\n    def __call__(\\n        self, __function: Callable[[T0, T1, T2, T3], R]\\n    ) -> RemoteFunction3[R, T0, T1, T2, T3]:\\n        ...\\n\\n    @overload\\n    def __call__(\\n        self, __function: Callable[[T0, T1, T2, T3, T4], R]\\n    ) -> RemoteFunction4[R, T0, T1, T2, T3, T4]:\\n        ...\\n\\n    @overload\\n    def __call__(\\n        self, __function: Callable[[T0, T1, T2, T3, T4, T5], R]\\n    ) -> RemoteFunction5[R, T0, T1, T2, T3, T4, T5]:\\n        ...\\n\\n    @overload\\n    def __call__(\\n        self, __function: Callable[[T0, T1, T2, T3, T4, T5, T6], R]\\n    ) -> RemoteFunction6[R, T0, T1, T2, T3, T4, T5, T6]:\\n        ...\\n\\n    @overload\\n    def __call__(\\n        self, __function: Callable[[T0, T1, T2, T3, T4, T5, T6, T7], R]\\n    ) -> RemoteFunction7[R, T0, T1, T2, T3, T4, T5, T6, T7]:\\n        ...\\n\\n    @overload\\n    def __call__(\\n        self, __function: Callable[[T0, T1, T2, T3, T4, T5, T6, T7, T8], R]\\n    ) -> RemoteFunction8[R, T0, T1, T2, T3, T4, T5, T6, T7, T8]:\\n        ...\\n\\n    @overload\\n    def __call__(\\n        self, __function: Callable[[T0, T1, T2, T3, T4, T5, T6, T7, T8, T9], R]\\n    ) -> RemoteFunction9[R, T0, T1, T2, T3, T4, T5, T6, T7, T8, T9]:\\n        ...\\n\\n    # Pass on typing actors for now. The following makes it so no type errors\\n    # are generated for actors.\\n    @overload\\n    def __call__(self, __t: type) -> Any:\\n        ...\\n\\n\\n# Only used for type annotations as a placeholder\\nUndefined: Any = object()\\n\\n\\n@overload\\ndef remote(__function: Callable[[], R]) -> RemoteFunctionNoArgs[R]:\\n    ...\\n\\n\\n@overload\\ndef remote(__function: Callable[[T0], R]) -> RemoteFunction0[R, T0]:\\n    ...\\n\\n\\n@overload\\ndef remote(__function: Callable[[T0, T1], R]) -> RemoteFunction1[R, T0, T1]:\\n    ...\\n\\n\\n@overload\\ndef remote(__function: Callable[[T0, T1, T2], R]) -> RemoteFunction2[R, T0, T1, T2]:\\n    ...\\n\\n\\n@overload\\ndef remote(\\n    __function: Callable[[T0, T1, T2, T3], R]\\n) -> RemoteFunction3[R, T0, T1, T2, T3]:\\n    ...\\n\\n\\n@overload\\ndef remote(\\n    __function: Callable[[T0, T1, T2, T3, T4], R]\\n) -> RemoteFunction4[R, T0, T1, T2, T3, T4]:\\n    ...\\n\\n\\n@overload\\ndef remote(\\n    __function: Callable[[T0, T1, T2, T3, T4, T5], R]\\n) -> RemoteFunction5[R, T0, T1, T2, T3, T4, T5]:\\n    ...\\n\\n\\n@overload\\ndef remote(\\n    __function: Callable[[T0, T1, T2, T3, T4, T5, T6], R]\\n) -> RemoteFunction6[R, T0, T1, T2, T3, T4, T5, T6]:\\n    ...\\n\\n\\n@overload\\ndef remote(\\n    __function: Callable[[T0, T1, T2, T3, T4, T5, T6, T7], R]\\n) -> RemoteFunction7[R, T0, T1, T2, T3, T4, T5, T6, T7]:\\n    ...\\n\\n\\n@overload\\ndef remote(\\n    __function: Callable[[T0, T1, T2, T3, T4, T5, T6, T7, T8], R]\\n) -> RemoteFunction8[R, T0, T1, T2, T3, T4, T5, T6, T7, T8]:\\n    ...\\n\\n\\n@overload\\ndef remote(\\n    __function: Callable[[T0, T1, T2, T3, T4, T5, T6, T7, T8, T9], R]\\n) -> RemoteFunction9[R, T0, T1, T2, T3, T4, T5, T6, T7, T8, T9]:\\n    ...\\n\\n\\n# Pass on typing actors for now. The following makes it so no type errors\\n# are generated for actors.\\n@overload\\ndef remote(__t: type) -> Any:\\n    ...\\n\\n\\n# Passing options\\n@overload\\ndef remote(\\n    *,\\n    num_returns: Union[int, float] = Undefined,\\n    num_cpus: Union[int, float] = Undefined,\\n    num_gpus: Union[int, float] = Undefined,\\n    resources: Dict[str, float] = Undefined,\\n    accelerator_type: str = Undefined,\\n    memory: Union[int, float] = Undefined,\\n    max_calls: int = Undefined,\\n    max_restarts: int = Undefined,\\n    max_task_retries: int = Undefined,\\n    max_retries: int = Undefined,\\n    runtime_env: Dict[str, Any] = Undefined,\\n    retry_exceptions: bool = Undefined,\\n    scheduling_strategy: Union[\\n        None, Literal[\\\"DEFAULT\\\"], Literal[\\\"SPREAD\\\"], PlacementGroupSchedulingStrategy\\n    ] = Undefined,\\n) -> RemoteDecorator:\\n    ...\\n\\n\\n@PublicAPI\\ndef remote(\\n    *args, **kwargs\\n) -> Union[ray.remote_function.RemoteFunction, ray.actor.ActorClass]:\\n    \\\"\\\"\\\"Defines a remote function or an actor class.\\n\\n    This function can be used as a decorator with no arguments\\n    to define a remote function or actor as follows:\\n\\n    .. testcode::\\n\\n        import ray\\n\\n        @ray.remote\\n        def f(a, b, c):\\n            return a + b + c\\n\\n        object_ref = f.remote(1, 2, 3)\\n        result = ray.get(object_ref)\\n        assert result == (1 + 2 + 3)\\n\\n        @ray.remote\\n        class Foo:\\n            def __init__(self, arg):\\n                self.x = arg\\n\\n            def method(self, a):\\n                return self.x + a\\n\\n        actor_handle = Foo.remote(123)\\n        object_ref = actor_handle.method.remote(321)\\n        result = ray.get(object_ref)\\n        assert result == (123 + 321)\\n\\n    Equivalently, use a function call to create a remote function or actor.\\n\\n    .. testcode::\\n\\n        def g(a, b, c):\\n            return a + b + c\\n\\n        remote_g = ray.remote(g)\\n        object_ref = remote_g.remote(1, 2, 3)\\n        assert ray.get(object_ref) == (1 + 2 + 3)\\n\\n        class Bar:\\n            def __init__(self, arg):\\n                self.x = arg\\n\\n            def method(self, a):\\n                return self.x + a\\n\\n        RemoteBar = ray.remote(Bar)\\n        actor_handle = RemoteBar.remote(123)\\n        object_ref = actor_handle.method.remote(321)\\n        result = ray.get(object_ref)\\n        assert result == (123 + 321)\\n\\n\\n    It can also be used with specific keyword arguments as follows:\\n\\n    .. testcode::\\n\\n        @ray.remote(num_gpus=1, max_calls=1, num_returns=2)\\n        def f():\\n            return 1, 2\\n\\n        @ray.remote(num_cpus=2, resources={\\\"CustomResource\\\": 1})\\n        class Foo:\\n            def method(self):\\n                return 1\\n\\n    Remote task and actor objects returned by @ray.remote can also be\\n    dynamically modified with the same arguments as above using\\n    ``.options()`` as follows:\\n\\n    .. testcode::\\n        :hide:\\n\\n        ray.shutdown()\\n\\n        ray.init(num_cpus=5, num_gpus=5)\\n\\n    .. testcode::\\n\\n        @ray.remote(num_gpus=1, max_calls=1, num_returns=2)\\n        def f():\\n            return 1, 2\\n\\n        f_with_2_gpus = f.options(num_gpus=2)\\n        object_refs = f_with_2_gpus.remote()\\n        assert ray.get(object_refs) == [1, 2]\\n\\n        @ray.remote(num_cpus=2, resources={\\\"CustomResource\\\": 1})\\n        class Foo:\\n            def method(self):\\n                return 1\\n\\n        Foo_with_no_resources = Foo.options(num_cpus=1, resources=None)\\n        foo_actor = Foo_with_no_resources.remote()\\n        assert ray.get(foo_actor.method.remote()) == 1\\n\\n\\n    A remote actor will be terminated when all actor handle to it\\n    in Python is deleted, which will cause them to complete any outstanding\\n    work and then shut down. If you only have 1 reference to an actor handle,\\n    calling ``del actor`` *could* trigger actor deletion. Note that your program\\n    may have multiple references to the same ActorHandle, and actor termination\\n    will not occur until the reference count goes to 0. See the Python\\n    documentation for more context about object deletion.\\n    https://docs.python.org/3.9/reference/datamodel.html#object.__del__\\n\\n    If you want to kill actors immediately, you can also call ``ray.kill(actor)``.\\n\\n    .. tip::\\n        Avoid repeatedly passing in large arguments to remote task or method calls.\\n\\n        Instead, use ray.put to create a copy of the object in the object store.\\n\\n        See :ref:`more info here <ray-pass-large-arg-by-value>`.\\n\\n    Args:\\n        num_returns: This is only for *remote functions*. It specifies\\n            the number of object refs returned by the remote function\\n            invocation. The default value is 1.\\n            Pass \\\"dynamic\\\" to allow the task to decide how many\\n            return values to return during execution, and the caller will\\n            receive an ObjectRef[ObjectRefGenerator].\\n            See :ref:`dynamic generators <dynamic-generators>` for more details.\\n        num_cpus: The quantity of CPU resources to reserve\\n            for this task or for the lifetime of the actor.\\n            By default, tasks use 1 CPU resource and actors use 1 CPU\\n            for scheduling and 0 CPU for running\\n            (This means, by default, actors cannot get scheduled on a zero-cpu node,\\n            but an infinite number of them can run on any non-zero cpu node.\\n            The default value for actors was chosen for historical reasons.\\n            It\\u2019s recommended to always explicitly set num_cpus for actors\\n            to avoid any surprises.\\n            If resources are specified explicitly,\\n            they are required for both scheduling and running.)\\n            See :ref:`specifying resource requirements <resource-requirements>`\\n            for more details.\\n        num_gpus: The quantity of GPU resources to reserve\\n            for this task or for the lifetime of the actor.\\n            The default value is 0.\\n            See :ref:`Ray GPU support <gpu-support>` for more details.\\n        resources (Dict[str, float]): The quantity of various\\n            :ref:`custom resources <custom-resources>`\\n            to reserve for this task or for the lifetime of the actor.\\n            This is a dictionary mapping strings (resource names) to floats.\\n            By default it is empty.\\n        accelerator_type: If specified, requires that the task or actor run\\n            on a node with the specified type of accelerator.\\n            See `ray.util.accelerators` for accelerator types.\\n        memory: The heap memory request in bytes for this task/actor,\\n            rounded down to the nearest integer.\\n        max_calls: Only for *remote functions*. This specifies the\\n            maximum number of times that a given worker can execute\\n            the given remote function before it must exit\\n            (this can be used to address :ref:`memory leaks <gpu-leak>` in third-party\\n            libraries or to reclaim resources that cannot easily be\\n            released, e.g., GPU memory that was acquired by TensorFlow).\\n            By default this is infinite for CPU tasks and 1 for GPU tasks\\n            (to force GPU tasks to release resources after finishing).\\n        max_restarts: Only for *actors*. This specifies the maximum\\n            number of times that the actor should be restarted when it dies\\n            unexpectedly. The minimum valid value is 0 (default),\\n            which indicates that the actor doesn't need to be restarted.\\n            A value of -1 indicates that an actor should be restarted\\n            indefinitely.\\n            See :ref:`actor fault tolerance <fault-tolerance-actors>` for more details.\\n        max_task_retries: Only for *actors*. How many times to\\n            retry an actor task if the task fails due to a system error,\\n            e.g., the actor has died. If set to -1, the system will\\n            retry the failed task until the task succeeds, or the actor\\n            has reached its max_restarts limit. If set to `n > 0`, the\\n            system will retry the failed task up to n times, after which the\\n            task will throw a `RayActorError` exception upon :obj:`ray.get`.\\n            Note that Python exceptions are not considered system errors\\n            and will not trigger retries.\\n            The default value is 0.\\n            See :ref:`actor fault tolerance <fault-tolerance-actors>` for more details.\\n        max_retries: Only for *remote functions*. This specifies\\n            the maximum number of times that the remote function\\n            should be rerun when the worker process executing it\\n            crashes unexpectedly. The minimum valid value is 0,\\n            the default value is 3, and a value of -1 indicates\\n            infinite retries.\\n            See :ref:`task fault tolerance <fault-tolerance-tasks>` for more details.\\n        runtime_env (Dict[str, Any]): Specifies the runtime environment for\\n            this actor or task and its children. See\\n            :ref:`runtime-environments` for detailed documentation.\\n        retry_exceptions: Only for *remote functions*. This specifies whether\\n            application-level errors should be retried up to max_retries times.\\n            This can be a boolean or a list of exceptions that should be retried.\\n            See :ref:`task fault tolerance <fault-tolerance-tasks>` for more details.\\n        scheduling_strategy: Strategy about how to\\n            schedule a remote function or actor. Possible values are\\n            None: ray will figure out the scheduling strategy to use, it\\n            will either be the PlacementGroupSchedulingStrategy using parent's\\n            placement group if parent has one and has\\n            placement_group_capture_child_tasks set to true,\\n            or \\\"DEFAULT\\\";\\n            \\\"DEFAULT\\\": default hybrid scheduling;\\n            \\\"SPREAD\\\": best effort spread scheduling;\\n            `PlacementGroupSchedulingStrategy`:\\n            placement group based scheduling;\\n            `NodeAffinitySchedulingStrategy`:\\n            node id based affinity scheduling.\\n            See :ref:`Ray scheduling strategies <ray-scheduling-strategies>`\\n            for more details.\\n        _metadata: Extended options for Ray libraries. For example,\\n            _metadata={\\\"workflows.io/options\\\": <workflow options>} for Ray workflows.\\n\\n    \\\"\\\"\\\"\\n    # \\\"callable\\\" returns true for both function and class.\\n    if len(args) == 1 and len(kwargs) == 0 and callable(args[0]):\\n        # This is the case where the decorator is just @ray.remote.\\n        # \\\"args[0]\\\" is the class or function under the decorator.\\n        return _make_remote(args[0], {})\\n    assert len(args) == 0 and len(kwargs) > 0, ray_option_utils.remote_args_error_string\\n    return functools.partial(_make_remote, options=kwargs)\\n\", 3352], \"/Users/sangcho/anaconda3/envs/core/lib/python3.9/_weakrefset.py\": [\"# Access WeakSet through the weakref module.\\n# This code is separated-out because it is needed\\n# by abc.py to load everything else at startup.\\n\\nfrom _weakref import ref\\nfrom types import GenericAlias\\n\\n__all__ = ['WeakSet']\\n\\n\\nclass _IterationGuard:\\n    # This context manager registers itself in the current iterators of the\\n    # weak container, such as to delay all removals until the context manager\\n    # exits.\\n    # This technique should be relatively thread-safe (since sets are).\\n\\n    def __init__(self, weakcontainer):\\n        # Don't create cycles\\n        self.weakcontainer = ref(weakcontainer)\\n\\n    def __enter__(self):\\n        w = self.weakcontainer()\\n        if w is not None:\\n            w._iterating.add(self)\\n        return self\\n\\n    def __exit__(self, e, t, b):\\n        w = self.weakcontainer()\\n        if w is not None:\\n            s = w._iterating\\n            s.remove(self)\\n            if not s:\\n                w._commit_removals()\\n\\n\\nclass WeakSet:\\n    def __init__(self, data=None):\\n        self.data = set()\\n        def _remove(item, selfref=ref(self)):\\n            self = selfref()\\n            if self is not None:\\n                if self._iterating:\\n                    self._pending_removals.append(item)\\n                else:\\n                    self.data.discard(item)\\n        self._remove = _remove\\n        # A list of keys to be removed\\n        self._pending_removals = []\\n        self._iterating = set()\\n        if data is not None:\\n            self.update(data)\\n\\n    def _commit_removals(self):\\n        pop = self._pending_removals.pop\\n        discard = self.data.discard\\n        while True:\\n            try:\\n                item = pop()\\n            except IndexError:\\n                return\\n            discard(item)\\n\\n    def __iter__(self):\\n        with _IterationGuard(self):\\n            for itemref in self.data:\\n                item = itemref()\\n                if item is not None:\\n                    # Caveat: the iterator will keep a strong reference to\\n                    # `item` until it is resumed or closed.\\n                    yield item\\n\\n    def __len__(self):\\n        return len(self.data) - len(self._pending_removals)\\n\\n    def __contains__(self, item):\\n        try:\\n            wr = ref(item)\\n        except TypeError:\\n            return False\\n        return wr in self.data\\n\\n    def __reduce__(self):\\n        return (self.__class__, (list(self),),\\n                getattr(self, '__dict__', None))\\n\\n    def add(self, item):\\n        if self._pending_removals:\\n            self._commit_removals()\\n        self.data.add(ref(item, self._remove))\\n\\n    def clear(self):\\n        if self._pending_removals:\\n            self._commit_removals()\\n        self.data.clear()\\n\\n    def copy(self):\\n        return self.__class__(self)\\n\\n    def pop(self):\\n        if self._pending_removals:\\n            self._commit_removals()\\n        while True:\\n            try:\\n                itemref = self.data.pop()\\n            except KeyError:\\n                raise KeyError('pop from empty WeakSet') from None\\n            item = itemref()\\n            if item is not None:\\n                return item\\n\\n    def remove(self, item):\\n        if self._pending_removals:\\n            self._commit_removals()\\n        self.data.remove(ref(item))\\n\\n    def discard(self, item):\\n        if self._pending_removals:\\n            self._commit_removals()\\n        self.data.discard(ref(item))\\n\\n    def update(self, other):\\n        if self._pending_removals:\\n            self._commit_removals()\\n        for element in other:\\n            self.add(element)\\n\\n    def __ior__(self, other):\\n        self.update(other)\\n        return self\\n\\n    def difference(self, other):\\n        newset = self.copy()\\n        newset.difference_update(other)\\n        return newset\\n    __sub__ = difference\\n\\n    def difference_update(self, other):\\n        self.__isub__(other)\\n    def __isub__(self, other):\\n        if self._pending_removals:\\n            self._commit_removals()\\n        if self is other:\\n            self.data.clear()\\n        else:\\n            self.data.difference_update(ref(item) for item in other)\\n        return self\\n\\n    def intersection(self, other):\\n        return self.__class__(item for item in other if item in self)\\n    __and__ = intersection\\n\\n    def intersection_update(self, other):\\n        self.__iand__(other)\\n    def __iand__(self, other):\\n        if self._pending_removals:\\n            self._commit_removals()\\n        self.data.intersection_update(ref(item) for item in other)\\n        return self\\n\\n    def issubset(self, other):\\n        return self.data.issubset(ref(item) for item in other)\\n    __le__ = issubset\\n\\n    def __lt__(self, other):\\n        return self.data < set(map(ref, other))\\n\\n    def issuperset(self, other):\\n        return self.data.issuperset(ref(item) for item in other)\\n    __ge__ = issuperset\\n\\n    def __gt__(self, other):\\n        return self.data > set(map(ref, other))\\n\\n    def __eq__(self, other):\\n        if not isinstance(other, self.__class__):\\n            return NotImplemented\\n        return self.data == set(map(ref, other))\\n\\n    def symmetric_difference(self, other):\\n        newset = self.copy()\\n        newset.symmetric_difference_update(other)\\n        return newset\\n    __xor__ = symmetric_difference\\n\\n    def symmetric_difference_update(self, other):\\n        self.__ixor__(other)\\n    def __ixor__(self, other):\\n        if self._pending_removals:\\n            self._commit_removals()\\n        if self is other:\\n            self.data.clear()\\n        else:\\n            self.data.symmetric_difference_update(ref(item, self._remove) for item in other)\\n        return self\\n\\n    def union(self, other):\\n        return self.__class__(e for s in (self, other) for e in s)\\n    __or__ = union\\n\\n    def isdisjoint(self, other):\\n        return len(self.intersection(other)) == 0\\n\\n    def __repr__(self):\\n        return repr(self.data)\\n\\n    __class_getitem__ = classmethod(GenericAlias)\\n\", 206], \"/Users/sangcho/anaconda3/envs/core/lib/python3.9/asyncio/tasks.py\": [\"\\\"\\\"\\\"Support for tasks, coroutines and the scheduler.\\\"\\\"\\\"\\n\\n__all__ = (\\n    'Task', 'create_task',\\n    'FIRST_COMPLETED', 'FIRST_EXCEPTION', 'ALL_COMPLETED',\\n    'wait', 'wait_for', 'as_completed', 'sleep',\\n    'gather', 'shield', 'ensure_future', 'run_coroutine_threadsafe',\\n    'current_task', 'all_tasks',\\n    '_register_task', '_unregister_task', '_enter_task', '_leave_task',\\n)\\n\\nimport concurrent.futures\\nimport contextvars\\nimport functools\\nimport inspect\\nimport itertools\\nimport types\\nimport warnings\\nimport weakref\\nfrom types import GenericAlias\\n\\nfrom . import base_tasks\\nfrom . import coroutines\\nfrom . import events\\nfrom . import exceptions\\nfrom . import futures\\nfrom .coroutines import _is_coroutine\\n\\n# Helper to generate new task names\\n# This uses itertools.count() instead of a \\\"+= 1\\\" operation because the latter\\n# is not thread safe. See bpo-11866 for a longer explanation.\\n_task_name_counter = itertools.count(1).__next__\\n\\n\\ndef current_task(loop=None):\\n    \\\"\\\"\\\"Return a currently executed task.\\\"\\\"\\\"\\n    if loop is None:\\n        loop = events.get_running_loop()\\n    return _current_tasks.get(loop)\\n\\n\\ndef all_tasks(loop=None):\\n    \\\"\\\"\\\"Return a set of all tasks for the loop.\\\"\\\"\\\"\\n    if loop is None:\\n        loop = events.get_running_loop()\\n    # Looping over a WeakSet (_all_tasks) isn't safe as it can be updated from another\\n    # thread while we do so. Therefore we cast it to list prior to filtering. The list\\n    # cast itself requires iteration, so we repeat it several times ignoring\\n    # RuntimeErrors (which are not very likely to occur). See issues 34970 and 36607 for\\n    # details.\\n    i = 0\\n    while True:\\n        try:\\n            tasks = list(_all_tasks)\\n        except RuntimeError:\\n            i += 1\\n            if i >= 1000:\\n                raise\\n        else:\\n            break\\n    return {t for t in tasks\\n            if futures._get_loop(t) is loop and not t.done()}\\n\\n\\ndef _all_tasks_compat(loop=None):\\n    # Different from \\\"all_task()\\\" by returning *all* Tasks, including\\n    # the completed ones.  Used to implement deprecated \\\"Tasks.all_task()\\\"\\n    # method.\\n    if loop is None:\\n        loop = events.get_event_loop()\\n    # Looping over a WeakSet (_all_tasks) isn't safe as it can be updated from another\\n    # thread while we do so. Therefore we cast it to list prior to filtering. The list\\n    # cast itself requires iteration, so we repeat it several times ignoring\\n    # RuntimeErrors (which are not very likely to occur). See issues 34970 and 36607 for\\n    # details.\\n    i = 0\\n    while True:\\n        try:\\n            tasks = list(_all_tasks)\\n        except RuntimeError:\\n            i += 1\\n            if i >= 1000:\\n                raise\\n        else:\\n            break\\n    return {t for t in tasks if futures._get_loop(t) is loop}\\n\\n\\ndef _set_task_name(task, name):\\n    if name is not None:\\n        try:\\n            set_name = task.set_name\\n        except AttributeError:\\n            pass\\n        else:\\n            set_name(name)\\n\\n\\nclass Task(futures._PyFuture):  # Inherit Python Task implementation\\n                                # from a Python Future implementation.\\n\\n    \\\"\\\"\\\"A coroutine wrapped in a Future.\\\"\\\"\\\"\\n\\n    # An important invariant maintained while a Task not done:\\n    #\\n    # - Either _fut_waiter is None, and _step() is scheduled;\\n    # - or _fut_waiter is some Future, and _step() is *not* scheduled.\\n    #\\n    # The only transition from the latter to the former is through\\n    # _wakeup().  When _fut_waiter is not None, one of its callbacks\\n    # must be _wakeup().\\n\\n    # If False, don't log a message if the task is destroyed whereas its\\n    # status is still pending\\n    _log_destroy_pending = True\\n\\n    def __init__(self, coro, *, loop=None, name=None):\\n        super().__init__(loop=loop)\\n        if self._source_traceback:\\n            del self._source_traceback[-1]\\n        if not coroutines.iscoroutine(coro):\\n            # raise after Future.__init__(), attrs are required for __del__\\n            # prevent logging for pending task in __del__\\n            self._log_destroy_pending = False\\n            raise TypeError(f\\\"a coroutine was expected, got {coro!r}\\\")\\n\\n        if name is None:\\n            self._name = f'Task-{_task_name_counter()}'\\n        else:\\n            self._name = str(name)\\n\\n        self._must_cancel = False\\n        self._fut_waiter = None\\n        self._coro = coro\\n        self._context = contextvars.copy_context()\\n\\n        self._loop.call_soon(self.__step, context=self._context)\\n        _register_task(self)\\n\\n    def __del__(self):\\n        if self._state == futures._PENDING and self._log_destroy_pending:\\n            context = {\\n                'task': self,\\n                'message': 'Task was destroyed but it is pending!',\\n            }\\n            if self._source_traceback:\\n                context['source_traceback'] = self._source_traceback\\n            self._loop.call_exception_handler(context)\\n        super().__del__()\\n\\n    __class_getitem__ = classmethod(GenericAlias)\\n\\n    def _repr_info(self):\\n        return base_tasks._task_repr_info(self)\\n\\n    def get_coro(self):\\n        return self._coro\\n\\n    def get_name(self):\\n        return self._name\\n\\n    def set_name(self, value):\\n        self._name = str(value)\\n\\n    def set_result(self, result):\\n        raise RuntimeError('Task does not support set_result operation')\\n\\n    def set_exception(self, exception):\\n        raise RuntimeError('Task does not support set_exception operation')\\n\\n    def get_stack(self, *, limit=None):\\n        \\\"\\\"\\\"Return the list of stack frames for this task's coroutine.\\n\\n        If the coroutine is not done, this returns the stack where it is\\n        suspended.  If the coroutine has completed successfully or was\\n        cancelled, this returns an empty list.  If the coroutine was\\n        terminated by an exception, this returns the list of traceback\\n        frames.\\n\\n        The frames are always ordered from oldest to newest.\\n\\n        The optional limit gives the maximum number of frames to\\n        return; by default all available frames are returned.  Its\\n        meaning differs depending on whether a stack or a traceback is\\n        returned: the newest frames of a stack are returned, but the\\n        oldest frames of a traceback are returned.  (This matches the\\n        behavior of the traceback module.)\\n\\n        For reasons beyond our control, only one stack frame is\\n        returned for a suspended coroutine.\\n        \\\"\\\"\\\"\\n        return base_tasks._task_get_stack(self, limit)\\n\\n    def print_stack(self, *, limit=None, file=None):\\n        \\\"\\\"\\\"Print the stack or traceback for this task's coroutine.\\n\\n        This produces output similar to that of the traceback module,\\n        for the frames retrieved by get_stack().  The limit argument\\n        is passed to get_stack().  The file argument is an I/O stream\\n        to which the output is written; by default output is written\\n        to sys.stderr.\\n        \\\"\\\"\\\"\\n        return base_tasks._task_print_stack(self, limit, file)\\n\\n    def cancel(self, msg=None):\\n        \\\"\\\"\\\"Request that this task cancel itself.\\n\\n        This arranges for a CancelledError to be thrown into the\\n        wrapped coroutine on the next cycle through the event loop.\\n        The coroutine then has a chance to clean up or even deny\\n        the request using try/except/finally.\\n\\n        Unlike Future.cancel, this does not guarantee that the\\n        task will be cancelled: the exception might be caught and\\n        acted upon, delaying cancellation of the task or preventing\\n        cancellation completely.  The task may also return a value or\\n        raise a different exception.\\n\\n        Immediately after this method is called, Task.cancelled() will\\n        not return True (unless the task was already cancelled).  A\\n        task will be marked as cancelled when the wrapped coroutine\\n        terminates with a CancelledError exception (even if cancel()\\n        was not called).\\n        \\\"\\\"\\\"\\n        self._log_traceback = False\\n        if self.done():\\n            return False\\n        if self._fut_waiter is not None:\\n            if self._fut_waiter.cancel(msg=msg):\\n                # Leave self._fut_waiter; it may be a Task that\\n                # catches and ignores the cancellation so we may have\\n                # to cancel it again later.\\n                return True\\n        # It must be the case that self.__step is already scheduled.\\n        self._must_cancel = True\\n        self._cancel_message = msg\\n        return True\\n\\n    def __step(self, exc=None):\\n        if self.done():\\n            raise exceptions.InvalidStateError(\\n                f'_step(): already done: {self!r}, {exc!r}')\\n        if self._must_cancel:\\n            if not isinstance(exc, exceptions.CancelledError):\\n                exc = self._make_cancelled_error()\\n            self._must_cancel = False\\n        coro = self._coro\\n        self._fut_waiter = None\\n\\n        _enter_task(self._loop, self)\\n        # Call either coro.throw(exc) or coro.send(None).\\n        try:\\n            if exc is None:\\n                # We use the `send` method directly, because coroutines\\n                # don't have `__iter__` and `__next__` methods.\\n                result = coro.send(None)\\n            else:\\n                result = coro.throw(exc)\\n        except StopIteration as exc:\\n            if self._must_cancel:\\n                # Task is cancelled right before coro stops.\\n                self._must_cancel = False\\n                super().cancel(msg=self._cancel_message)\\n            else:\\n                super().set_result(exc.value)\\n        except exceptions.CancelledError as exc:\\n            # Save the original exception so we can chain it later.\\n            self._cancelled_exc = exc\\n            super().cancel()  # I.e., Future.cancel(self).\\n        except (KeyboardInterrupt, SystemExit) as exc:\\n            super().set_exception(exc)\\n            raise\\n        except BaseException as exc:\\n            super().set_exception(exc)\\n        else:\\n            blocking = getattr(result, '_asyncio_future_blocking', None)\\n            if blocking is not None:\\n                # Yielded Future must come from Future.__iter__().\\n                if futures._get_loop(result) is not self._loop:\\n                    new_exc = RuntimeError(\\n                        f'Task {self!r} got Future '\\n                        f'{result!r} attached to a different loop')\\n                    self._loop.call_soon(\\n                        self.__step, new_exc, context=self._context)\\n                elif blocking:\\n                    if result is self:\\n                        new_exc = RuntimeError(\\n                            f'Task cannot await on itself: {self!r}')\\n                        self._loop.call_soon(\\n                            self.__step, new_exc, context=self._context)\\n                    else:\\n                        result._asyncio_future_blocking = False\\n                        result.add_done_callback(\\n                            self.__wakeup, context=self._context)\\n                        self._fut_waiter = result\\n                        if self._must_cancel:\\n                            if self._fut_waiter.cancel(\\n                                    msg=self._cancel_message):\\n                                self._must_cancel = False\\n                else:\\n                    new_exc = RuntimeError(\\n                        f'yield was used instead of yield from '\\n                        f'in task {self!r} with {result!r}')\\n                    self._loop.call_soon(\\n                        self.__step, new_exc, context=self._context)\\n\\n            elif result is None:\\n                # Bare yield relinquishes control for one event loop iteration.\\n                self._loop.call_soon(self.__step, context=self._context)\\n            elif inspect.isgenerator(result):\\n                # Yielding a generator is just wrong.\\n                new_exc = RuntimeError(\\n                    f'yield was used instead of yield from for '\\n                    f'generator in task {self!r} with {result!r}')\\n                self._loop.call_soon(\\n                    self.__step, new_exc, context=self._context)\\n            else:\\n                # Yielding something else is an error.\\n                new_exc = RuntimeError(f'Task got bad yield: {result!r}')\\n                self._loop.call_soon(\\n                    self.__step, new_exc, context=self._context)\\n        finally:\\n            _leave_task(self._loop, self)\\n            self = None  # Needed to break cycles when an exception occurs.\\n\\n    def __wakeup(self, future):\\n        try:\\n            future.result()\\n        except BaseException as exc:\\n            # This may also be a cancellation.\\n            self.__step(exc)\\n        else:\\n            # Don't pass the value of `future.result()` explicitly,\\n            # as `Future.__iter__` and `Future.__await__` don't need it.\\n            # If we call `_step(value, None)` instead of `_step()`,\\n            # Python eval loop would use `.send(value)` method call,\\n            # instead of `__next__()`, which is slower for futures\\n            # that return non-generator iterators from their `__iter__`.\\n            self.__step()\\n        self = None  # Needed to break cycles when an exception occurs.\\n\\n\\n_PyTask = Task\\n\\n\\ntry:\\n    import _asyncio\\nexcept ImportError:\\n    pass\\nelse:\\n    # _CTask is needed for tests.\\n    Task = _CTask = _asyncio.Task\\n\\n\\ndef create_task(coro, *, name=None):\\n    \\\"\\\"\\\"Schedule the execution of a coroutine object in a spawn task.\\n\\n    Return a Task object.\\n    \\\"\\\"\\\"\\n    loop = events.get_running_loop()\\n    task = loop.create_task(coro)\\n    _set_task_name(task, name)\\n    return task\\n\\n\\n# wait() and as_completed() similar to those in PEP 3148.\\n\\nFIRST_COMPLETED = concurrent.futures.FIRST_COMPLETED\\nFIRST_EXCEPTION = concurrent.futures.FIRST_EXCEPTION\\nALL_COMPLETED = concurrent.futures.ALL_COMPLETED\\n\\n\\nasync def wait(fs, *, loop=None, timeout=None, return_when=ALL_COMPLETED):\\n    \\\"\\\"\\\"Wait for the Futures and coroutines given by fs to complete.\\n\\n    The fs iterable must not be empty.\\n\\n    Coroutines will be wrapped in Tasks.\\n\\n    Returns two sets of Future: (done, pending).\\n\\n    Usage:\\n\\n        done, pending = await asyncio.wait(fs)\\n\\n    Note: This does not raise TimeoutError! Futures that aren't done\\n    when the timeout occurs are returned in the second set.\\n    \\\"\\\"\\\"\\n    if futures.isfuture(fs) or coroutines.iscoroutine(fs):\\n        raise TypeError(f\\\"expect a list of futures, not {type(fs).__name__}\\\")\\n    if not fs:\\n        raise ValueError('Set of coroutines/Futures is empty.')\\n    if return_when not in (FIRST_COMPLETED, FIRST_EXCEPTION, ALL_COMPLETED):\\n        raise ValueError(f'Invalid return_when value: {return_when}')\\n\\n    if loop is None:\\n        loop = events.get_running_loop()\\n    else:\\n        warnings.warn(\\\"The loop argument is deprecated since Python 3.8, \\\"\\n                      \\\"and scheduled for removal in Python 3.10.\\\",\\n                      DeprecationWarning, stacklevel=2)\\n\\n    fs = set(fs)\\n\\n    if any(coroutines.iscoroutine(f) for f in fs):\\n        warnings.warn(\\\"The explicit passing of coroutine objects to \\\"\\n                      \\\"asyncio.wait() is deprecated since Python 3.8, and \\\"\\n                      \\\"scheduled for removal in Python 3.11.\\\",\\n                      DeprecationWarning, stacklevel=2)\\n\\n    fs = {ensure_future(f, loop=loop) for f in fs}\\n\\n    return await _wait(fs, timeout, return_when, loop)\\n\\n\\ndef _release_waiter(waiter, *args):\\n    if not waiter.done():\\n        waiter.set_result(None)\\n\\n\\nasync def wait_for(fut, timeout, *, loop=None):\\n    \\\"\\\"\\\"Wait for the single Future or coroutine to complete, with timeout.\\n\\n    Coroutine will be wrapped in Task.\\n\\n    Returns result of the Future or coroutine.  When a timeout occurs,\\n    it cancels the task and raises TimeoutError.  To avoid the task\\n    cancellation, wrap it in shield().\\n\\n    If the wait is cancelled, the task is also cancelled.\\n\\n    This function is a coroutine.\\n    \\\"\\\"\\\"\\n    if loop is None:\\n        loop = events.get_running_loop()\\n    else:\\n        warnings.warn(\\\"The loop argument is deprecated since Python 3.8, \\\"\\n                      \\\"and scheduled for removal in Python 3.10.\\\",\\n                      DeprecationWarning, stacklevel=2)\\n\\n    if timeout is None:\\n        return await fut\\n\\n    if timeout <= 0:\\n        fut = ensure_future(fut, loop=loop)\\n\\n        if fut.done():\\n            return fut.result()\\n\\n        await _cancel_and_wait(fut, loop=loop)\\n        try:\\n            return fut.result()\\n        except exceptions.CancelledError as exc:\\n            raise exceptions.TimeoutError() from exc\\n\\n    waiter = loop.create_future()\\n    timeout_handle = loop.call_later(timeout, _release_waiter, waiter)\\n    cb = functools.partial(_release_waiter, waiter)\\n\\n    fut = ensure_future(fut, loop=loop)\\n    fut.add_done_callback(cb)\\n\\n    try:\\n        # wait until the future completes or the timeout\\n        try:\\n            await waiter\\n        except exceptions.CancelledError:\\n            if fut.done():\\n                return fut.result()\\n            else:\\n                fut.remove_done_callback(cb)\\n                # We must ensure that the task is not running\\n                # after wait_for() returns.\\n                # See https://bugs.python.org/issue32751\\n                await _cancel_and_wait(fut, loop=loop)\\n                raise\\n\\n        if fut.done():\\n            return fut.result()\\n        else:\\n            fut.remove_done_callback(cb)\\n            # We must ensure that the task is not running\\n            # after wait_for() returns.\\n            # See https://bugs.python.org/issue32751\\n            await _cancel_and_wait(fut, loop=loop)\\n            # In case task cancellation failed with some\\n            # exception, we should re-raise it\\n            # See https://bugs.python.org/issue40607\\n            try:\\n                return fut.result()\\n            except exceptions.CancelledError as exc:\\n                raise exceptions.TimeoutError() from exc\\n    finally:\\n        timeout_handle.cancel()\\n\\n\\nasync def _wait(fs, timeout, return_when, loop):\\n    \\\"\\\"\\\"Internal helper for wait().\\n\\n    The fs argument must be a collection of Futures.\\n    \\\"\\\"\\\"\\n    assert fs, 'Set of Futures is empty.'\\n    waiter = loop.create_future()\\n    timeout_handle = None\\n    if timeout is not None:\\n        timeout_handle = loop.call_later(timeout, _release_waiter, waiter)\\n    counter = len(fs)\\n\\n    def _on_completion(f):\\n        nonlocal counter\\n        counter -= 1\\n        if (counter <= 0 or\\n            return_when == FIRST_COMPLETED or\\n            return_when == FIRST_EXCEPTION and (not f.cancelled() and\\n                                                f.exception() is not None)):\\n            if timeout_handle is not None:\\n                timeout_handle.cancel()\\n            if not waiter.done():\\n                waiter.set_result(None)\\n\\n    for f in fs:\\n        f.add_done_callback(_on_completion)\\n\\n    try:\\n        await waiter\\n    finally:\\n        if timeout_handle is not None:\\n            timeout_handle.cancel()\\n        for f in fs:\\n            f.remove_done_callback(_on_completion)\\n\\n    done, pending = set(), set()\\n    for f in fs:\\n        if f.done():\\n            done.add(f)\\n        else:\\n            pending.add(f)\\n    return done, pending\\n\\n\\nasync def _cancel_and_wait(fut, loop):\\n    \\\"\\\"\\\"Cancel the *fut* future or task and wait until it completes.\\\"\\\"\\\"\\n\\n    waiter = loop.create_future()\\n    cb = functools.partial(_release_waiter, waiter)\\n    fut.add_done_callback(cb)\\n\\n    try:\\n        fut.cancel()\\n        # We cannot wait on *fut* directly to make\\n        # sure _cancel_and_wait itself is reliably cancellable.\\n        await waiter\\n    finally:\\n        fut.remove_done_callback(cb)\\n\\n\\n# This is *not* a @coroutine!  It is just an iterator (yielding Futures).\\ndef as_completed(fs, *, loop=None, timeout=None):\\n    \\\"\\\"\\\"Return an iterator whose values are coroutines.\\n\\n    When waiting for the yielded coroutines you'll get the results (or\\n    exceptions!) of the original Futures (or coroutines), in the order\\n    in which and as soon as they complete.\\n\\n    This differs from PEP 3148; the proper way to use this is:\\n\\n        for f in as_completed(fs):\\n            result = await f  # The 'await' may raise.\\n            # Use result.\\n\\n    If a timeout is specified, the 'await' will raise\\n    TimeoutError when the timeout occurs before all Futures are done.\\n\\n    Note: The futures 'f' are not necessarily members of fs.\\n    \\\"\\\"\\\"\\n    if futures.isfuture(fs) or coroutines.iscoroutine(fs):\\n        raise TypeError(f\\\"expect an iterable of futures, not {type(fs).__name__}\\\")\\n\\n    if loop is not None:\\n        warnings.warn(\\\"The loop argument is deprecated since Python 3.8, \\\"\\n                      \\\"and scheduled for removal in Python 3.10.\\\",\\n                      DeprecationWarning, stacklevel=2)\\n\\n    from .queues import Queue  # Import here to avoid circular import problem.\\n    done = Queue(loop=loop)\\n\\n    if loop is None:\\n        loop = events.get_event_loop()\\n    todo = {ensure_future(f, loop=loop) for f in set(fs)}\\n    timeout_handle = None\\n\\n    def _on_timeout():\\n        for f in todo:\\n            f.remove_done_callback(_on_completion)\\n            done.put_nowait(None)  # Queue a dummy value for _wait_for_one().\\n        todo.clear()  # Can't do todo.remove(f) in the loop.\\n\\n    def _on_completion(f):\\n        if not todo:\\n            return  # _on_timeout() was here first.\\n        todo.remove(f)\\n        done.put_nowait(f)\\n        if not todo and timeout_handle is not None:\\n            timeout_handle.cancel()\\n\\n    async def _wait_for_one():\\n        f = await done.get()\\n        if f is None:\\n            # Dummy value from _on_timeout().\\n            raise exceptions.TimeoutError\\n        return f.result()  # May raise f.exception().\\n\\n    for f in todo:\\n        f.add_done_callback(_on_completion)\\n    if todo and timeout is not None:\\n        timeout_handle = loop.call_later(timeout, _on_timeout)\\n    for _ in range(len(todo)):\\n        yield _wait_for_one()\\n\\n\\n@types.coroutine\\ndef __sleep0():\\n    \\\"\\\"\\\"Skip one event loop run cycle.\\n\\n    This is a private helper for 'asyncio.sleep()', used\\n    when the 'delay' is set to 0.  It uses a bare 'yield'\\n    expression (which Task.__step knows how to handle)\\n    instead of creating a Future object.\\n    \\\"\\\"\\\"\\n    yield\\n\\n\\nasync def sleep(delay, result=None, *, loop=None):\\n    \\\"\\\"\\\"Coroutine that completes after a given time (in seconds).\\\"\\\"\\\"\\n    if loop is not None:\\n        warnings.warn(\\\"The loop argument is deprecated since Python 3.8, \\\"\\n                      \\\"and scheduled for removal in Python 3.10.\\\",\\n                      DeprecationWarning, stacklevel=2)\\n\\n    if delay <= 0:\\n        await __sleep0()\\n        return result\\n\\n    if loop is None:\\n        loop = events.get_running_loop()\\n\\n    future = loop.create_future()\\n    h = loop.call_later(delay,\\n                        futures._set_result_unless_cancelled,\\n                        future, result)\\n    try:\\n        return await future\\n    finally:\\n        h.cancel()\\n\\n\\ndef ensure_future(coro_or_future, *, loop=None):\\n    \\\"\\\"\\\"Wrap a coroutine or an awaitable in a future.\\n\\n    If the argument is a Future, it is returned directly.\\n    \\\"\\\"\\\"\\n    if coroutines.iscoroutine(coro_or_future):\\n        if loop is None:\\n            loop = events.get_event_loop()\\n        task = loop.create_task(coro_or_future)\\n        if task._source_traceback:\\n            del task._source_traceback[-1]\\n        return task\\n    elif futures.isfuture(coro_or_future):\\n        if loop is not None and loop is not futures._get_loop(coro_or_future):\\n            raise ValueError('The future belongs to a different loop than '\\n                             'the one specified as the loop argument')\\n        return coro_or_future\\n    elif inspect.isawaitable(coro_or_future):\\n        return ensure_future(_wrap_awaitable(coro_or_future), loop=loop)\\n    else:\\n        raise TypeError('An asyncio.Future, a coroutine or an awaitable is '\\n                        'required')\\n\\n\\n@types.coroutine\\ndef _wrap_awaitable(awaitable):\\n    \\\"\\\"\\\"Helper for asyncio.ensure_future().\\n\\n    Wraps awaitable (an object with __await__) into a coroutine\\n    that will later be wrapped in a Task by ensure_future().\\n    \\\"\\\"\\\"\\n    return (yield from awaitable.__await__())\\n\\n_wrap_awaitable._is_coroutine = _is_coroutine\\n\\n\\nclass _GatheringFuture(futures.Future):\\n    \\\"\\\"\\\"Helper for gather().\\n\\n    This overrides cancel() to cancel all the children and act more\\n    like Task.cancel(), which doesn't immediately mark itself as\\n    cancelled.\\n    \\\"\\\"\\\"\\n\\n    def __init__(self, children, *, loop=None):\\n        super().__init__(loop=loop)\\n        self._children = children\\n        self._cancel_requested = False\\n\\n    def cancel(self, msg=None):\\n        if self.done():\\n            return False\\n        ret = False\\n        for child in self._children:\\n            if child.cancel(msg=msg):\\n                ret = True\\n        if ret:\\n            # If any child tasks were actually cancelled, we should\\n            # propagate the cancellation request regardless of\\n            # *return_exceptions* argument.  See issue 32684.\\n            self._cancel_requested = True\\n        return ret\\n\\n\\ndef gather(*coros_or_futures, loop=None, return_exceptions=False):\\n    \\\"\\\"\\\"Return a future aggregating results from the given coroutines/futures.\\n\\n    Coroutines will be wrapped in a future and scheduled in the event\\n    loop. They will not necessarily be scheduled in the same order as\\n    passed in.\\n\\n    All futures must share the same event loop.  If all the tasks are\\n    done successfully, the returned future's result is the list of\\n    results (in the order of the original sequence, not necessarily\\n    the order of results arrival).  If *return_exceptions* is True,\\n    exceptions in the tasks are treated the same as successful\\n    results, and gathered in the result list; otherwise, the first\\n    raised exception will be immediately propagated to the returned\\n    future.\\n\\n    Cancellation: if the outer Future is cancelled, all children (that\\n    have not completed yet) are also cancelled.  If any child is\\n    cancelled, this is treated as if it raised CancelledError --\\n    the outer Future is *not* cancelled in this case.  (This is to\\n    prevent the cancellation of one child to cause other children to\\n    be cancelled.)\\n\\n    If *return_exceptions* is False, cancelling gather() after it\\n    has been marked done won't cancel any submitted awaitables.\\n    For instance, gather can be marked done after propagating an\\n    exception to the caller, therefore, calling ``gather.cancel()``\\n    after catching an exception (raised by one of the awaitables) from\\n    gather won't cancel any other awaitables.\\n    \\\"\\\"\\\"\\n    if loop is not None:\\n        warnings.warn(\\\"The loop argument is deprecated since Python 3.8, \\\"\\n                      \\\"and scheduled for removal in Python 3.10.\\\",\\n                      DeprecationWarning, stacklevel=2)\\n\\n    return _gather(*coros_or_futures, loop=loop, return_exceptions=return_exceptions)\\n\\n\\ndef _gather(*coros_or_futures, loop=None, return_exceptions=False):\\n    if not coros_or_futures:\\n        if loop is None:\\n            loop = events.get_event_loop()\\n        outer = loop.create_future()\\n        outer.set_result([])\\n        return outer\\n\\n    def _done_callback(fut):\\n        nonlocal nfinished\\n        nfinished += 1\\n\\n        if outer is None or outer.done():\\n            if not fut.cancelled():\\n                # Mark exception retrieved.\\n                fut.exception()\\n            return\\n\\n        if not return_exceptions:\\n            if fut.cancelled():\\n                # Check if 'fut' is cancelled first, as\\n                # 'fut.exception()' will *raise* a CancelledError\\n                # instead of returning it.\\n                exc = fut._make_cancelled_error()\\n                outer.set_exception(exc)\\n                return\\n            else:\\n                exc = fut.exception()\\n                if exc is not None:\\n                    outer.set_exception(exc)\\n                    return\\n\\n        if nfinished == nfuts:\\n            # All futures are done; create a list of results\\n            # and set it to the 'outer' future.\\n            results = []\\n\\n            for fut in children:\\n                if fut.cancelled():\\n                    # Check if 'fut' is cancelled first, as 'fut.exception()'\\n                    # will *raise* a CancelledError instead of returning it.\\n                    # Also, since we're adding the exception return value\\n                    # to 'results' instead of raising it, don't bother\\n                    # setting __context__.  This also lets us preserve\\n                    # calling '_make_cancelled_error()' at most once.\\n                    res = exceptions.CancelledError(\\n                        '' if fut._cancel_message is None else\\n                        fut._cancel_message)\\n                else:\\n                    res = fut.exception()\\n                    if res is None:\\n                        res = fut.result()\\n                results.append(res)\\n\\n            if outer._cancel_requested:\\n                # If gather is being cancelled we must propagate the\\n                # cancellation regardless of *return_exceptions* argument.\\n                # See issue 32684.\\n                exc = fut._make_cancelled_error()\\n                outer.set_exception(exc)\\n            else:\\n                outer.set_result(results)\\n\\n    arg_to_fut = {}\\n    children = []\\n    nfuts = 0\\n    nfinished = 0\\n    outer = None  # bpo-46672\\n    for arg in coros_or_futures:\\n        if arg not in arg_to_fut:\\n            fut = ensure_future(arg, loop=loop)\\n            if loop is None:\\n                loop = futures._get_loop(fut)\\n            if fut is not arg:\\n                # 'arg' was not a Future, therefore, 'fut' is a new\\n                # Future created specifically for 'arg'.  Since the caller\\n                # can't control it, disable the \\\"destroy pending task\\\"\\n                # warning.\\n                fut._log_destroy_pending = False\\n\\n            nfuts += 1\\n            arg_to_fut[arg] = fut\\n            fut.add_done_callback(_done_callback)\\n\\n        else:\\n            # There's a duplicate Future object in coros_or_futures.\\n            fut = arg_to_fut[arg]\\n\\n        children.append(fut)\\n\\n    outer = _GatheringFuture(children, loop=loop)\\n    return outer\\n\\n\\ndef shield(arg, *, loop=None):\\n    \\\"\\\"\\\"Wait for a future, shielding it from cancellation.\\n\\n    The statement\\n\\n        res = await shield(something())\\n\\n    is exactly equivalent to the statement\\n\\n        res = await something()\\n\\n    *except* that if the coroutine containing it is cancelled, the\\n    task running in something() is not cancelled.  From the POV of\\n    something(), the cancellation did not happen.  But its caller is\\n    still cancelled, so the yield-from expression still raises\\n    CancelledError.  Note: If something() is cancelled by other means\\n    this will still cancel shield().\\n\\n    If you want to completely ignore cancellation (not recommended)\\n    you can combine shield() with a try/except clause, as follows:\\n\\n        try:\\n            res = await shield(something())\\n        except CancelledError:\\n            res = None\\n    \\\"\\\"\\\"\\n    if loop is not None:\\n        warnings.warn(\\\"The loop argument is deprecated since Python 3.8, \\\"\\n                      \\\"and scheduled for removal in Python 3.10.\\\",\\n                      DeprecationWarning, stacklevel=2)\\n    inner = ensure_future(arg, loop=loop)\\n    if inner.done():\\n        # Shortcut.\\n        return inner\\n    loop = futures._get_loop(inner)\\n    outer = loop.create_future()\\n\\n    def _inner_done_callback(inner):\\n        if outer.cancelled():\\n            if not inner.cancelled():\\n                # Mark inner's result as retrieved.\\n                inner.exception()\\n            return\\n\\n        if inner.cancelled():\\n            outer.cancel()\\n        else:\\n            exc = inner.exception()\\n            if exc is not None:\\n                outer.set_exception(exc)\\n            else:\\n                outer.set_result(inner.result())\\n\\n\\n    def _outer_done_callback(outer):\\n        if not inner.done():\\n            inner.remove_done_callback(_inner_done_callback)\\n\\n    inner.add_done_callback(_inner_done_callback)\\n    outer.add_done_callback(_outer_done_callback)\\n    return outer\\n\\n\\ndef run_coroutine_threadsafe(coro, loop):\\n    \\\"\\\"\\\"Submit a coroutine object to a given event loop.\\n\\n    Return a concurrent.futures.Future to access the result.\\n    \\\"\\\"\\\"\\n    if not coroutines.iscoroutine(coro):\\n        raise TypeError('A coroutine object is required')\\n    future = concurrent.futures.Future()\\n\\n    def callback():\\n        try:\\n            futures._chain_future(ensure_future(coro, loop=loop), future)\\n        except (SystemExit, KeyboardInterrupt):\\n            raise\\n        except BaseException as exc:\\n            if future.set_running_or_notify_cancel():\\n                future.set_exception(exc)\\n            raise\\n\\n    loop.call_soon_threadsafe(callback)\\n    return future\\n\\n\\n# WeakSet containing all alive tasks.\\n_all_tasks = weakref.WeakSet()\\n\\n# Dictionary containing tasks that are currently active in\\n# all running event loops.  {EventLoop: Task}\\n_current_tasks = {}\\n\\n\\ndef _register_task(task):\\n    \\\"\\\"\\\"Register a new task in asyncio as executed by loop.\\\"\\\"\\\"\\n    _all_tasks.add(task)\\n\\n\\ndef _enter_task(loop, task):\\n    current_task = _current_tasks.get(loop)\\n    if current_task is not None:\\n        raise RuntimeError(f\\\"Cannot enter into task {task!r} while another \\\"\\n                           f\\\"task {current_task!r} is being executed.\\\")\\n    _current_tasks[loop] = task\\n\\n\\ndef _leave_task(loop, task):\\n    current_task = _current_tasks.get(loop)\\n    if current_task is not task:\\n        raise RuntimeError(f\\\"Leaving task {task!r} does not match \\\"\\n                           f\\\"the current task {current_task!r}.\\\")\\n    del _current_tasks[loop]\\n\\n\\ndef _unregister_task(task):\\n    \\\"\\\"\\\"Unregister a task.\\\"\\\"\\\"\\n    _all_tasks.discard(task)\\n\\n\\n_py_register_task = _register_task\\n_py_unregister_task = _unregister_task\\n_py_enter_task = _enter_task\\n_py_leave_task = _leave_task\\n\\n\\ntry:\\n    from _asyncio import (_register_task, _unregister_task,\\n                          _enter_task, _leave_task,\\n                          _all_tasks, _current_tasks)\\nexcept ImportError:\\n    pass\\nelse:\\n    _c_register_task = _register_task\\n    _c_unregister_task = _unregister_task\\n    _c_enter_task = _enter_task\\n    _c_leave_task = _leave_task\\n\", 989], \"/Users/sangcho/work/ray/python/ray/_private/utils.py\": [\"import asyncio\\nimport binascii\\nfrom collections import defaultdict\\nimport contextlib\\nimport errno\\nimport functools\\nimport importlib\\nimport inspect\\nimport json\\nimport logging\\nimport multiprocessing\\nimport os\\nimport platform\\nimport re\\nimport signal\\nimport subprocess\\nimport sys\\nimport tempfile\\nimport threading\\nimport time\\nfrom urllib.parse import urlencode, unquote, urlparse, parse_qsl, urlunparse\\nimport warnings\\nfrom inspect import signature\\nfrom pathlib import Path\\nfrom subprocess import list2cmdline\\nfrom typing import (\\n    TYPE_CHECKING,\\n    Any,\\n    Dict,\\n    Optional,\\n    Sequence,\\n    Tuple,\\n    Union,\\n    Coroutine,\\n    List,\\n    Mapping,\\n)\\n\\n# Import psutil after ray so the packaged version is used.\\nimport psutil\\nfrom google.protobuf import json_format\\n\\nimport ray\\nimport ray._private.ray_constants as ray_constants\\nfrom ray.core.generated.runtime_env_common_pb2 import (\\n    RuntimeEnvInfo as ProtoRuntimeEnvInfo,\\n)\\n\\nif TYPE_CHECKING:\\n    from ray.runtime_env import RuntimeEnv\\n\\npwd = None\\nif sys.platform != \\\"win32\\\":\\n    import pwd\\n\\nlogger = logging.getLogger(__name__)\\n\\n# Linux can bind child processes' lifetimes to that of their parents via prctl.\\n# prctl support is detected dynamically once, and assumed thereafter.\\nlinux_prctl = None\\n\\n# Windows can bind processes' lifetimes to that of kernel-level \\\"job objects\\\".\\n# We keep a global job object to tie its lifetime to that of our own process.\\nwin32_job = None\\nwin32_AssignProcessToJobObject = None\\n\\nENV_DISABLE_DOCKER_CPU_WARNING = \\\"RAY_DISABLE_DOCKER_CPU_WARNING\\\" in os.environ\\n_PYARROW_VERSION = None\\n\\n# This global variable is used for testing only\\n_CALLED_FREQ = defaultdict(lambda: 0)\\n_CALLED_FREQ_LOCK = threading.Lock()\\n\\nPLACEMENT_GROUP_INDEXED_BUNDLED_RESOURCE_PATTERN = re.compile(\\n    r\\\"(.+)_group_(\\\\d+)_([0-9a-zA-Z]+)\\\"\\n)\\nPLACEMENT_GROUP_WILDCARD_RESOURCE_PATTERN = re.compile(r\\\"(.+)_group_([0-9a-zA-Z]+)\\\")\\n\\n\\ndef get_user_temp_dir():\\n    if \\\"RAY_TMPDIR\\\" in os.environ:\\n        return os.environ[\\\"RAY_TMPDIR\\\"]\\n    elif sys.platform.startswith(\\\"linux\\\") and \\\"TMPDIR\\\" in os.environ:\\n        return os.environ[\\\"TMPDIR\\\"]\\n    elif sys.platform.startswith(\\\"darwin\\\") or sys.platform.startswith(\\\"linux\\\"):\\n        # Ideally we wouldn't need this fallback, but keep it for now for\\n        # for compatibility\\n        tempdir = os.path.join(os.sep, \\\"tmp\\\")\\n    else:\\n        tempdir = tempfile.gettempdir()\\n    return tempdir\\n\\n\\ndef get_ray_temp_dir():\\n    return os.path.join(get_user_temp_dir(), \\\"ray\\\")\\n\\n\\ndef get_ray_address_file(temp_dir: Optional[str]):\\n    if temp_dir is None:\\n        temp_dir = get_ray_temp_dir()\\n    return os.path.join(temp_dir, \\\"ray_current_cluster\\\")\\n\\n\\ndef write_ray_address(ray_address: str, temp_dir: Optional[str] = None):\\n    address_file = get_ray_address_file(temp_dir)\\n    if os.path.exists(address_file):\\n        with open(address_file, \\\"r\\\") as f:\\n            prev_address = f.read()\\n        if prev_address == ray_address:\\n            return\\n\\n        logger.info(\\n            f\\\"Overwriting previous Ray address ({prev_address}). \\\"\\n            \\\"Running ray.init() on this node will now connect to the new \\\"\\n            f\\\"instance at {ray_address}. To override this behavior, pass \\\"\\n            f\\\"address={prev_address} to ray.init().\\\"\\n        )\\n\\n    with open(address_file, \\\"w+\\\") as f:\\n        f.write(ray_address)\\n\\n\\ndef reset_ray_address(temp_dir: Optional[str] = None):\\n    address_file = get_ray_address_file(temp_dir)\\n    if os.path.exists(address_file):\\n        try:\\n            os.remove(address_file)\\n        except OSError:\\n            pass\\n\\n\\ndef read_ray_address(temp_dir: Optional[str] = None) -> str:\\n    address_file = get_ray_address_file(temp_dir)\\n    if not os.path.exists(address_file):\\n        return None\\n    with open(address_file, \\\"r\\\") as f:\\n        return f.read().strip()\\n\\n\\ndef format_error_message(exception_message: str, task_exception: bool = False):\\n    \\\"\\\"\\\"Improve the formatting of an exception thrown by a remote function.\\n\\n    This method takes a traceback from an exception and makes it nicer by\\n    removing a few uninformative lines and adding some space to indent the\\n    remaining lines nicely.\\n\\n    Args:\\n        exception_message: A message generated by traceback.format_exc().\\n\\n    Returns:\\n        A string of the formatted exception message.\\n    \\\"\\\"\\\"\\n    lines = exception_message.split(\\\"\\\\n\\\")\\n    if task_exception:\\n        # For errors that occur inside of tasks, remove lines 1 and 2 which are\\n        # always the same, they just contain information about the worker code.\\n        lines = lines[0:1] + lines[3:]\\n        pass\\n    return \\\"\\\\n\\\".join(lines)\\n\\n\\ndef push_error_to_driver(\\n    worker, error_type: str, message: str, job_id: Optional[str] = None\\n):\\n    \\\"\\\"\\\"Push an error message to the driver to be printed in the background.\\n\\n    Args:\\n        worker: The worker to use.\\n        error_type: The type of the error.\\n        message: The message that will be printed in the background\\n            on the driver.\\n        job_id: The ID of the driver to push the error message to. If this\\n            is None, then the message will be pushed to all drivers.\\n    \\\"\\\"\\\"\\n    if job_id is None:\\n        job_id = ray.JobID.nil()\\n    assert isinstance(job_id, ray.JobID)\\n    worker.core_worker.push_error(job_id, error_type, message, time.time())\\n\\n\\ndef publish_error_to_driver(\\n    error_type: str,\\n    message: str,\\n    gcs_publisher,\\n    job_id=None,\\n    num_retries=None,\\n):\\n    \\\"\\\"\\\"Push an error message to the driver to be printed in the background.\\n\\n    Normally the push_error_to_driver function should be used. However, in some\\n    instances, the raylet client is not available, e.g., because the\\n    error happens in Python before the driver or worker has connected to the\\n    backend processes.\\n\\n    Args:\\n        error_type: The type of the error.\\n        message: The message that will be printed in the background\\n            on the driver.\\n        gcs_publisher: The GCS publisher to use.\\n        job_id: The ID of the driver to push the error message to. If this\\n            is None, then the message will be pushed to all drivers.\\n    \\\"\\\"\\\"\\n    if job_id is None:\\n        job_id = ray.JobID.nil()\\n    assert isinstance(job_id, ray.JobID)\\n    try:\\n        gcs_publisher.publish_error(\\n            job_id.hex().encode(), error_type, message, job_id, num_retries\\n        )\\n    except Exception:\\n        logger.exception(f\\\"Failed to publish error: {message} [type {error_type}]\\\")\\n\\n\\ndef decode(byte_str: str, allow_none: bool = False, encode_type: str = \\\"utf-8\\\"):\\n    \\\"\\\"\\\"Make this unicode in Python 3, otherwise leave it as bytes.\\n\\n    Args:\\n        byte_str: The byte string to decode.\\n        allow_none: If true, then we will allow byte_str to be None in which\\n            case we will return an empty string. TODO(rkn): Remove this flag.\\n            This is only here to simplify upgrading to flatbuffers 1.10.0.\\n\\n    Returns:\\n        A byte string in Python 2 and a unicode string in Python 3.\\n    \\\"\\\"\\\"\\n    if byte_str is None and allow_none:\\n        return \\\"\\\"\\n\\n    if not isinstance(byte_str, bytes):\\n        raise ValueError(f\\\"The argument {byte_str} must be a bytes object.\\\")\\n    return byte_str.decode(encode_type)\\n\\n\\ndef ensure_str(s, encoding=\\\"utf-8\\\", errors=\\\"strict\\\"):\\n    \\\"\\\"\\\"Coerce *s* to `str`.\\n\\n    - `str` -> `str`\\n    - `bytes` -> decoded to `str`\\n    \\\"\\\"\\\"\\n    if isinstance(s, str):\\n        return s\\n    else:\\n        assert isinstance(s, bytes)\\n        return s.decode(encoding, errors)\\n\\n\\ndef binary_to_object_ref(binary_object_ref):\\n    return ray.ObjectRef(binary_object_ref)\\n\\n\\ndef binary_to_task_id(binary_task_id):\\n    return ray.TaskID(binary_task_id)\\n\\n\\ndef binary_to_hex(identifier):\\n    hex_identifier = binascii.hexlify(identifier)\\n    hex_identifier = hex_identifier.decode()\\n    return hex_identifier\\n\\n\\ndef hex_to_binary(hex_identifier):\\n    return binascii.unhexlify(hex_identifier)\\n\\n\\n# TODO(qwang): Remove these hepler functions\\n# once we separate `WorkerID` from `UniqueID`.\\ndef compute_job_id_from_driver(driver_id):\\n    assert isinstance(driver_id, ray.WorkerID)\\n    return ray.JobID(driver_id.binary()[0 : ray.JobID.size()])\\n\\n\\ndef compute_driver_id_from_job(job_id):\\n    assert isinstance(job_id, ray.JobID)\\n    rest_length = ray_constants.ID_SIZE - job_id.size()\\n    driver_id_str = job_id.binary() + (rest_length * b\\\"\\\\xff\\\")\\n    return ray.WorkerID(driver_id_str)\\n\\n\\ndef get_visible_accelerator_ids() -> Mapping[str, Optional[List[str]]]:\\n    \\\"\\\"\\\"Get the mapping from accelerator resource name\\n    to the visible ids.\\\"\\\"\\\"\\n\\n    from ray._private.accelerators import (\\n        get_all_accelerator_resource_names,\\n        get_accelerator_manager_for_resource,\\n    )\\n\\n    return {\\n        accelerator_resource_name: get_accelerator_manager_for_resource(\\n            accelerator_resource_name\\n        ).get_current_process_visible_accelerator_ids()\\n        for accelerator_resource_name in get_all_accelerator_resource_names()\\n    }\\n\\n\\ndef set_omp_num_threads_if_unset() -> bool:\\n    \\\"\\\"\\\"Set the OMP_NUM_THREADS to default to num cpus assigned to the worker\\n\\n    This function sets the environment variable OMP_NUM_THREADS for the worker,\\n    if the env is not previously set and it's running in worker (WORKER_MODE).\\n\\n    Returns True if OMP_NUM_THREADS is set in this function.\\n\\n    \\\"\\\"\\\"\\n    num_threads_from_env = os.environ.get(\\\"OMP_NUM_THREADS\\\")\\n    if num_threads_from_env is not None:\\n        # No ops if it's set\\n        return False\\n\\n    # If unset, try setting the correct CPU count assigned.\\n    runtime_ctx = ray.get_runtime_context()\\n    if runtime_ctx.worker.mode != ray._private.worker.WORKER_MODE:\\n        # Non worker mode, no ops.\\n        return False\\n\\n    num_assigned_cpus = runtime_ctx.get_assigned_resources().get(\\\"CPU\\\")\\n\\n    if num_assigned_cpus is None:\\n        # This is an actor task w/o any num_cpus specified, set it to 1\\n        logger.debug(\\n            \\\"[ray] Forcing OMP_NUM_THREADS=1 to avoid performance \\\"\\n            \\\"degradation with many workers (issue #6998). You can override this \\\"\\n            \\\"by explicitly setting OMP_NUM_THREADS, or changing num_cpus.\\\"\\n        )\\n        num_assigned_cpus = 1\\n\\n    import math\\n\\n    # For num_cpu < 1: Set to 1.\\n    # For num_cpus >= 1: Set to the floor of the actual assigned cpus.\\n    omp_num_threads = max(math.floor(num_assigned_cpus), 1)\\n    os.environ[\\\"OMP_NUM_THREADS\\\"] = str(omp_num_threads)\\n    return True\\n\\n\\ndef set_visible_accelerator_ids() -> None:\\n    \\\"\\\"\\\"Set (CUDA_VISIBLE_DEVICES, ONEAPI_DEVICE_SELECTOR, NEURON_RT_VISIBLE_CORES,\\n    TPU_VISIBLE_CHIPS ,...) environment variables based on the accelerator runtime.\\n    \\\"\\\"\\\"\\n    for resource_name, accelerator_ids in (\\n        ray.get_runtime_context().get_resource_ids().items()\\n    ):\\n        ray._private.accelerators.get_accelerator_manager_for_resource(\\n            resource_name\\n        ).set_current_process_visible_accelerator_ids(accelerator_ids)\\n\\n\\ndef resources_from_ray_options(options_dict: Dict[str, Any]) -> Dict[str, Any]:\\n    \\\"\\\"\\\"Determine a task's resource requirements.\\n\\n    Args:\\n        options_dict: The dictionary that contains resources requirements.\\n\\n    Returns:\\n        A dictionary of the resource requirements for the task.\\n    \\\"\\\"\\\"\\n    resources = (options_dict.get(\\\"resources\\\") or {}).copy()\\n\\n    if \\\"CPU\\\" in resources or \\\"GPU\\\" in resources:\\n        raise ValueError(\\n            \\\"The resources dictionary must not contain the key 'CPU' or 'GPU'\\\"\\n        )\\n    elif \\\"memory\\\" in resources or \\\"object_store_memory\\\" in resources:\\n        raise ValueError(\\n            \\\"The resources dictionary must not \\\"\\n            \\\"contain the key 'memory' or 'object_store_memory'\\\"\\n        )\\n\\n    num_cpus = options_dict.get(\\\"num_cpus\\\")\\n    num_gpus = options_dict.get(\\\"num_gpus\\\")\\n    memory = options_dict.get(\\\"memory\\\")\\n    object_store_memory = options_dict.get(\\\"object_store_memory\\\")\\n    accelerator_type = options_dict.get(\\\"accelerator_type\\\")\\n\\n    if num_cpus is not None:\\n        resources[\\\"CPU\\\"] = num_cpus\\n    if num_gpus is not None:\\n        resources[\\\"GPU\\\"] = num_gpus\\n    if memory is not None:\\n        resources[\\\"memory\\\"] = int(memory)\\n    if object_store_memory is not None:\\n        resources[\\\"object_store_memory\\\"] = object_store_memory\\n    if accelerator_type is not None:\\n        resources[\\n            f\\\"{ray_constants.RESOURCE_CONSTRAINT_PREFIX}{accelerator_type}\\\"\\n        ] = 0.001\\n\\n    return resources\\n\\n\\nclass Unbuffered(object):\\n    \\\"\\\"\\\"There's no \\\"built-in\\\" solution to programatically disabling buffering of\\n    text files. Ray expects stdout/err to be text files, so creating an\\n    unbuffered binary file is unacceptable.\\n\\n    See\\n    https://mail.python.org/pipermail/tutor/2003-November/026645.html.\\n    https://docs.python.org/3/library/functions.html#open\\n\\n    \\\"\\\"\\\"\\n\\n    def __init__(self, stream):\\n        self.stream = stream\\n\\n    def write(self, data):\\n        self.stream.write(data)\\n        self.stream.flush()\\n\\n    def writelines(self, datas):\\n        self.stream.writelines(datas)\\n        self.stream.flush()\\n\\n    def __getattr__(self, attr):\\n        return getattr(self.stream, attr)\\n\\n\\ndef open_log(path, unbuffered=False, **kwargs):\\n    \\\"\\\"\\\"\\n    Opens the log file at `path`, with the provided kwargs being given to\\n    `open`.\\n    \\\"\\\"\\\"\\n    # Disable buffering, see test_advanced_3.py::test_logging_to_driver\\n    kwargs.setdefault(\\\"buffering\\\", 1)\\n    kwargs.setdefault(\\\"mode\\\", \\\"a\\\")\\n    kwargs.setdefault(\\\"encoding\\\", \\\"utf-8\\\")\\n    stream = open(path, **kwargs)\\n    if unbuffered:\\n        return Unbuffered(stream)\\n    else:\\n        return stream\\n\\n\\ndef get_system_memory(\\n    # For cgroups v1:\\n    memory_limit_filename=\\\"/sys/fs/cgroup/memory/memory.limit_in_bytes\\\",\\n    # For cgroups v2:\\n    memory_limit_filename_v2=\\\"/sys/fs/cgroup/memory.max\\\",\\n):\\n    \\\"\\\"\\\"Return the total amount of system memory in bytes.\\n\\n    Returns:\\n        The total amount of system memory in bytes.\\n    \\\"\\\"\\\"\\n    # Try to accurately figure out the memory limit if we are in a docker\\n    # container. Note that this file is not specific to Docker and its value is\\n    # often much larger than the actual amount of memory.\\n    docker_limit = None\\n    if os.path.exists(memory_limit_filename):\\n        with open(memory_limit_filename, \\\"r\\\") as f:\\n            docker_limit = int(f.read().strip())\\n    elif os.path.exists(memory_limit_filename_v2):\\n        with open(memory_limit_filename_v2, \\\"r\\\") as f:\\n            # Don't forget to strip() the newline:\\n            max_file = f.read().strip()\\n            if max_file.isnumeric():\\n                docker_limit = int(max_file)\\n            else:\\n                # max_file is \\\"max\\\", i.e. is unset.\\n                docker_limit = None\\n\\n    # Use psutil if it is available.\\n    psutil_memory_in_bytes = psutil.virtual_memory().total\\n\\n    if docker_limit is not None:\\n        # We take the min because the cgroup limit is very large if we aren't\\n        # in Docker.\\n        return min(docker_limit, psutil_memory_in_bytes)\\n\\n    return psutil_memory_in_bytes\\n\\n\\ndef _get_docker_cpus(\\n    cpu_quota_file_name=\\\"/sys/fs/cgroup/cpu/cpu.cfs_quota_us\\\",\\n    cpu_period_file_name=\\\"/sys/fs/cgroup/cpu/cpu.cfs_period_us\\\",\\n    cpuset_file_name=\\\"/sys/fs/cgroup/cpuset/cpuset.cpus\\\",\\n    cpu_max_file_name=\\\"/sys/fs/cgroup/cpu.max\\\",\\n) -> Optional[float]:\\n    # TODO (Alex): Don't implement this logic oursleves.\\n    # Docker has 2 underyling ways of implementing CPU limits:\\n    # https://docs.docker.com/config/containers/resource_constraints/#configure-the-default-cfs-scheduler\\n    # 1. --cpuset-cpus 2. --cpus or --cpu-quota/--cpu-period (--cpu-shares is a\\n    # soft limit so we don't worry about it). For Ray's purposes, if we use\\n    # docker, the number of vCPUs on a machine is whichever is set (ties broken\\n    # by smaller value).\\n\\n    cpu_quota = None\\n    # See: https://bugs.openjdk.java.net/browse/JDK-8146115\\n    if os.path.exists(cpu_quota_file_name) and os.path.exists(cpu_period_file_name):\\n        try:\\n            with open(cpu_quota_file_name, \\\"r\\\") as quota_file, open(\\n                cpu_period_file_name, \\\"r\\\"\\n            ) as period_file:\\n                cpu_quota = float(quota_file.read()) / float(period_file.read())\\n        except Exception:\\n            logger.exception(\\\"Unexpected error calculating docker cpu quota.\\\")\\n    # Look at cpu.max for cgroups v2\\n    elif os.path.exists(cpu_max_file_name):\\n        try:\\n            max_file = open(cpu_max_file_name).read()\\n            quota_str, period_str = max_file.split()\\n            if quota_str.isnumeric() and period_str.isnumeric():\\n                cpu_quota = float(quota_str) / float(period_str)\\n            else:\\n                # quota_str is \\\"max\\\" meaning the cpu quota is unset\\n                cpu_quota = None\\n        except Exception:\\n            logger.exception(\\\"Unexpected error calculating docker cpu quota.\\\")\\n    if (cpu_quota is not None) and (cpu_quota < 0):\\n        cpu_quota = None\\n    elif cpu_quota == 0:\\n        # Round up in case the cpu limit is less than 1.\\n        cpu_quota = 1\\n\\n    cpuset_num = None\\n    if os.path.exists(cpuset_file_name):\\n        try:\\n            with open(cpuset_file_name) as cpuset_file:\\n                ranges_as_string = cpuset_file.read()\\n                ranges = ranges_as_string.split(\\\",\\\")\\n                cpu_ids = []\\n                for num_or_range in ranges:\\n                    if \\\"-\\\" in num_or_range:\\n                        start, end = num_or_range.split(\\\"-\\\")\\n                        cpu_ids.extend(list(range(int(start), int(end) + 1)))\\n                    else:\\n                        cpu_ids.append(int(num_or_range))\\n                cpuset_num = len(cpu_ids)\\n        except Exception:\\n            logger.exception(\\\"Unexpected error calculating docker cpuset ids.\\\")\\n    # Possible to-do: Parse cgroups v2's cpuset.cpus.effective for the number\\n    # of accessible CPUs.\\n\\n    if cpu_quota and cpuset_num:\\n        return min(cpu_quota, cpuset_num)\\n    return cpu_quota or cpuset_num\\n\\n\\ndef get_num_cpus(\\n    override_docker_cpu_warning: bool = ENV_DISABLE_DOCKER_CPU_WARNING,\\n) -> int:\\n    \\\"\\\"\\\"\\n    Get the number of CPUs available on this node.\\n    Depending on the situation, use multiprocessing.cpu_count() or cgroups.\\n\\n    Args:\\n        override_docker_cpu_warning: An extra flag to explicitly turn off the Docker\\n            warning. Setting this flag True has the same effect as setting the env\\n            RAY_DISABLE_DOCKER_CPU_WARNING. By default, whether or not to log\\n            the warning is determined by the env variable\\n            RAY_DISABLE_DOCKER_CPU_WARNING.\\n    \\\"\\\"\\\"\\n    cpu_count = multiprocessing.cpu_count()\\n    if os.environ.get(\\\"RAY_USE_MULTIPROCESSING_CPU_COUNT\\\"):\\n        logger.info(\\n            \\\"Detected RAY_USE_MULTIPROCESSING_CPU_COUNT=1: Using \\\"\\n            \\\"multiprocessing.cpu_count() to detect the number of CPUs. \\\"\\n            \\\"This may be inconsistent when used inside docker. \\\"\\n            \\\"To correctly detect CPUs, unset the env var: \\\"\\n            \\\"`RAY_USE_MULTIPROCESSING_CPU_COUNT`.\\\"\\n        )\\n        return cpu_count\\n    try:\\n        # Not easy to get cpu count in docker, see:\\n        # https://bugs.python.org/issue36054\\n        docker_count = _get_docker_cpus()\\n        if docker_count is not None and docker_count != cpu_count:\\n            # Don't log this warning if we're on K8s or if the warning is\\n            # explicitly disabled.\\n            if (\\n                \\\"KUBERNETES_SERVICE_HOST\\\" not in os.environ\\n                and not ENV_DISABLE_DOCKER_CPU_WARNING\\n                and not override_docker_cpu_warning\\n            ):\\n                logger.warning(\\n                    \\\"Detecting docker specified CPUs. In \\\"\\n                    \\\"previous versions of Ray, CPU detection in containers \\\"\\n                    \\\"was incorrect. Please ensure that Ray has enough CPUs \\\"\\n                    \\\"allocated. As a temporary workaround to revert to the \\\"\\n                    \\\"prior behavior, set \\\"\\n                    \\\"`RAY_USE_MULTIPROCESSING_CPU_COUNT=1` as an env var \\\"\\n                    \\\"before starting Ray. Set the env var: \\\"\\n                    \\\"`RAY_DISABLE_DOCKER_CPU_WARNING=1` to mute this warning.\\\"\\n                )\\n            # TODO (Alex): We should probably add support for fractional cpus.\\n            if int(docker_count) != float(docker_count):\\n                logger.warning(\\n                    f\\\"Ray currently does not support initializing Ray \\\"\\n                    f\\\"with fractional cpus. Your num_cpus will be \\\"\\n                    f\\\"truncated from {docker_count} to \\\"\\n                    f\\\"{int(docker_count)}.\\\"\\n                )\\n            docker_count = int(docker_count)\\n            cpu_count = docker_count\\n\\n    except Exception:\\n        # `nproc` and cgroup are linux-only. If docker only works on linux\\n        # (will run in a linux VM on other platforms), so this is fine.\\n        pass\\n\\n    return cpu_count\\n\\n\\n# TODO(clarng): merge code with c++\\ndef get_cgroupv1_used_memory(filename):\\n    with open(filename, \\\"r\\\") as f:\\n        lines = f.readlines()\\n        cache_bytes = -1\\n        rss_bytes = -1\\n        inactive_file_bytes = -1\\n        working_set = -1\\n        for line in lines:\\n            if \\\"total_rss \\\" in line:\\n                rss_bytes = int(line.split()[1])\\n            elif \\\"cache \\\" in line:\\n                cache_bytes = int(line.split()[1])\\n            elif \\\"inactive_file\\\" in line:\\n                inactive_file_bytes = int(line.split()[1])\\n        if cache_bytes >= 0 and rss_bytes >= 0 and inactive_file_bytes >= 0:\\n            working_set = rss_bytes + cache_bytes - inactive_file_bytes\\n            assert working_set >= 0\\n            return working_set\\n        return None\\n\\n\\ndef get_cgroupv2_used_memory(stat_file, usage_file):\\n    # Uses same calculation as libcontainer, that is:\\n    # memory.current - memory.stat[inactive_file]\\n    # Source: https://github.com/google/cadvisor/blob/24dd1de08a72cfee661f6178454db995900c0fee/container/libcontainer/handler.go#L836  # noqa: E501\\n    inactive_file_bytes = -1\\n    current_usage = -1\\n    with open(usage_file, \\\"r\\\") as f:\\n        current_usage = int(f.read().strip())\\n    with open(stat_file, \\\"r\\\") as f:\\n        lines = f.readlines()\\n        for line in lines:\\n            if \\\"inactive_file\\\" in line:\\n                inactive_file_bytes = int(line.split()[1])\\n        if current_usage >= 0 and inactive_file_bytes >= 0:\\n            working_set = current_usage - inactive_file_bytes\\n            assert working_set >= 0\\n            return working_set\\n        return None\\n\\n\\ndef get_used_memory():\\n    \\\"\\\"\\\"Return the currently used system memory in bytes\\n\\n    Returns:\\n        The total amount of used memory\\n    \\\"\\\"\\\"\\n    # Try to accurately figure out the memory usage if we are in a docker\\n    # container.\\n    docker_usage = None\\n    # For cgroups v1:\\n    memory_usage_filename = \\\"/sys/fs/cgroup/memory/memory.stat\\\"\\n    # For cgroups v2:\\n    memory_usage_filename_v2 = \\\"/sys/fs/cgroup/memory.current\\\"\\n    memory_stat_filename_v2 = \\\"/sys/fs/cgroup/memory.stat\\\"\\n    if os.path.exists(memory_usage_filename):\\n        docker_usage = get_cgroupv1_used_memory(memory_usage_filename)\\n    elif os.path.exists(memory_usage_filename_v2) and os.path.exists(\\n        memory_stat_filename_v2\\n    ):\\n        docker_usage = get_cgroupv2_used_memory(\\n            memory_stat_filename_v2, memory_usage_filename_v2\\n        )\\n\\n    if docker_usage is not None:\\n        return docker_usage\\n    return psutil.virtual_memory().used\\n\\n\\ndef estimate_available_memory():\\n    \\\"\\\"\\\"Return the currently available amount of system memory in bytes.\\n\\n    Returns:\\n        The total amount of available memory in bytes. Based on the used\\n        and total memory.\\n\\n    \\\"\\\"\\\"\\n    return get_system_memory() - get_used_memory()\\n\\n\\ndef get_shared_memory_bytes():\\n    \\\"\\\"\\\"Get the size of the shared memory file system.\\n\\n    Returns:\\n        The size of the shared memory file system in bytes.\\n    \\\"\\\"\\\"\\n    # Make sure this is only called on Linux.\\n    assert sys.platform == \\\"linux\\\" or sys.platform == \\\"linux2\\\"\\n\\n    shm_fd = os.open(\\\"/dev/shm\\\", os.O_RDONLY)\\n    try:\\n        shm_fs_stats = os.fstatvfs(shm_fd)\\n        # The value shm_fs_stats.f_bsize is the block size and the\\n        # value shm_fs_stats.f_bavail is the number of available\\n        # blocks.\\n        shm_avail = shm_fs_stats.f_bsize * shm_fs_stats.f_bavail\\n    finally:\\n        os.close(shm_fd)\\n\\n    return shm_avail\\n\\n\\ndef check_oversized_function(\\n    pickled: bytes, name: str, obj_type: str, worker: \\\"ray.Worker\\\"\\n) -> None:\\n    \\\"\\\"\\\"Send a warning message if the pickled function is too large.\\n\\n    Args:\\n        pickled: the pickled function.\\n        name: name of the pickled object.\\n        obj_type: type of the pickled object, can be 'function',\\n            'remote function', or 'actor'.\\n        worker: the worker used to send warning message. message will be logged\\n            locally if None.\\n    \\\"\\\"\\\"\\n    length = len(pickled)\\n    if length <= ray_constants.FUNCTION_SIZE_WARN_THRESHOLD:\\n        return\\n    elif length < ray_constants.FUNCTION_SIZE_ERROR_THRESHOLD:\\n        warning_message = (\\n            \\\"The {} {} is very large ({} MiB). \\\"\\n            \\\"Check that its definition is not implicitly capturing a large \\\"\\n            \\\"array or other object in scope. Tip: use ray.put() to put large \\\"\\n            \\\"objects in the Ray object store.\\\"\\n        ).format(obj_type, name, length // (1024 * 1024))\\n        if worker:\\n            push_error_to_driver(\\n                worker,\\n                ray_constants.PICKLING_LARGE_OBJECT_PUSH_ERROR,\\n                \\\"Warning: \\\" + warning_message,\\n                job_id=worker.current_job_id,\\n            )\\n    else:\\n        error = (\\n            \\\"The {} {} is too large ({} MiB > FUNCTION_SIZE_ERROR_THRESHOLD={}\\\"\\n            \\\" MiB). Check that its definition is not implicitly capturing a \\\"\\n            \\\"large array or other object in scope. Tip: use ray.put() to \\\"\\n            \\\"put large objects in the Ray object store.\\\"\\n        ).format(\\n            obj_type,\\n            name,\\n            length // (1024 * 1024),\\n            ray_constants.FUNCTION_SIZE_ERROR_THRESHOLD // (1024 * 1024),\\n        )\\n        raise ValueError(error)\\n\\n\\ndef is_main_thread():\\n    return threading.current_thread().getName() == \\\"MainThread\\\"\\n\\n\\ndef detect_fate_sharing_support_win32():\\n    global win32_job, win32_AssignProcessToJobObject\\n    if win32_job is None and sys.platform == \\\"win32\\\":\\n        import ctypes\\n\\n        try:\\n            from ctypes.wintypes import BOOL, DWORD, HANDLE, LPCWSTR, LPVOID\\n\\n            kernel32 = ctypes.WinDLL(\\\"kernel32\\\")\\n            kernel32.CreateJobObjectW.argtypes = (LPVOID, LPCWSTR)\\n            kernel32.CreateJobObjectW.restype = HANDLE\\n            sijo_argtypes = (HANDLE, ctypes.c_int, LPVOID, DWORD)\\n            kernel32.SetInformationJobObject.argtypes = sijo_argtypes\\n            kernel32.SetInformationJobObject.restype = BOOL\\n            kernel32.AssignProcessToJobObject.argtypes = (HANDLE, HANDLE)\\n            kernel32.AssignProcessToJobObject.restype = BOOL\\n            kernel32.IsDebuggerPresent.argtypes = ()\\n            kernel32.IsDebuggerPresent.restype = BOOL\\n        except (AttributeError, TypeError, ImportError):\\n            kernel32 = None\\n        job = kernel32.CreateJobObjectW(None, None) if kernel32 else None\\n        job = subprocess.Handle(job) if job else job\\n        if job:\\n            from ctypes.wintypes import DWORD, LARGE_INTEGER, ULARGE_INTEGER\\n\\n            class JOBOBJECT_BASIC_LIMIT_INFORMATION(ctypes.Structure):\\n                _fields_ = [\\n                    (\\\"PerProcessUserTimeLimit\\\", LARGE_INTEGER),\\n                    (\\\"PerJobUserTimeLimit\\\", LARGE_INTEGER),\\n                    (\\\"LimitFlags\\\", DWORD),\\n                    (\\\"MinimumWorkingSetSize\\\", ctypes.c_size_t),\\n                    (\\\"MaximumWorkingSetSize\\\", ctypes.c_size_t),\\n                    (\\\"ActiveProcessLimit\\\", DWORD),\\n                    (\\\"Affinity\\\", ctypes.c_size_t),\\n                    (\\\"PriorityClass\\\", DWORD),\\n                    (\\\"SchedulingClass\\\", DWORD),\\n                ]\\n\\n            class IO_COUNTERS(ctypes.Structure):\\n                _fields_ = [\\n                    (\\\"ReadOperationCount\\\", ULARGE_INTEGER),\\n                    (\\\"WriteOperationCount\\\", ULARGE_INTEGER),\\n                    (\\\"OtherOperationCount\\\", ULARGE_INTEGER),\\n                    (\\\"ReadTransferCount\\\", ULARGE_INTEGER),\\n                    (\\\"WriteTransferCount\\\", ULARGE_INTEGER),\\n                    (\\\"OtherTransferCount\\\", ULARGE_INTEGER),\\n                ]\\n\\n            class JOBOBJECT_EXTENDED_LIMIT_INFORMATION(ctypes.Structure):\\n                _fields_ = [\\n                    (\\\"BasicLimitInformation\\\", JOBOBJECT_BASIC_LIMIT_INFORMATION),\\n                    (\\\"IoInfo\\\", IO_COUNTERS),\\n                    (\\\"ProcessMemoryLimit\\\", ctypes.c_size_t),\\n                    (\\\"JobMemoryLimit\\\", ctypes.c_size_t),\\n                    (\\\"PeakProcessMemoryUsed\\\", ctypes.c_size_t),\\n                    (\\\"PeakJobMemoryUsed\\\", ctypes.c_size_t),\\n                ]\\n\\n            debug = kernel32.IsDebuggerPresent()\\n\\n            # Defined in <WinNT.h>; also available here:\\n            # https://docs.microsoft.com/en-us/windows/win32/api/jobapi2/nf-jobapi2-setinformationjobobject\\n            JobObjectExtendedLimitInformation = 9\\n            JOB_OBJECT_LIMIT_BREAKAWAY_OK = 0x00000800\\n            JOB_OBJECT_LIMIT_DIE_ON_UNHANDLED_EXCEPTION = 0x00000400\\n            JOB_OBJECT_LIMIT_KILL_ON_JOB_CLOSE = 0x00002000\\n            buf = JOBOBJECT_EXTENDED_LIMIT_INFORMATION()\\n            buf.BasicLimitInformation.LimitFlags = (\\n                (0 if debug else JOB_OBJECT_LIMIT_KILL_ON_JOB_CLOSE)\\n                | JOB_OBJECT_LIMIT_DIE_ON_UNHANDLED_EXCEPTION\\n                | JOB_OBJECT_LIMIT_BREAKAWAY_OK\\n            )\\n            infoclass = JobObjectExtendedLimitInformation\\n            if not kernel32.SetInformationJobObject(\\n                job, infoclass, ctypes.byref(buf), ctypes.sizeof(buf)\\n            ):\\n                job = None\\n        win32_AssignProcessToJobObject = (\\n            kernel32.AssignProcessToJobObject if kernel32 is not None else False\\n        )\\n        win32_job = job if job else False\\n    return bool(win32_job)\\n\\n\\ndef detect_fate_sharing_support_linux():\\n    global linux_prctl\\n    if linux_prctl is None and sys.platform.startswith(\\\"linux\\\"):\\n        try:\\n            from ctypes import CDLL, c_int, c_ulong\\n\\n            prctl = CDLL(None).prctl\\n            prctl.restype = c_int\\n            prctl.argtypes = [c_int, c_ulong, c_ulong, c_ulong, c_ulong]\\n        except (AttributeError, TypeError):\\n            prctl = None\\n        linux_prctl = prctl if prctl else False\\n    return bool(linux_prctl)\\n\\n\\ndef detect_fate_sharing_support():\\n    result = None\\n    if sys.platform == \\\"win32\\\":\\n        result = detect_fate_sharing_support_win32()\\n    elif sys.platform.startswith(\\\"linux\\\"):\\n        result = detect_fate_sharing_support_linux()\\n    return result\\n\\n\\ndef set_kill_on_parent_death_linux():\\n    \\\"\\\"\\\"Ensures this process dies if its parent dies (fate-sharing).\\n\\n    Linux-only. Must be called in preexec_fn (i.e. by the child).\\n    \\\"\\\"\\\"\\n    if detect_fate_sharing_support_linux():\\n        import signal\\n\\n        PR_SET_PDEATHSIG = 1\\n        if linux_prctl(PR_SET_PDEATHSIG, signal.SIGKILL, 0, 0, 0) != 0:\\n            import ctypes\\n\\n            raise OSError(ctypes.get_errno(), \\\"prctl(PR_SET_PDEATHSIG) failed\\\")\\n    else:\\n        assert False, \\\"PR_SET_PDEATHSIG used despite being unavailable\\\"\\n\\n\\ndef set_kill_child_on_death_win32(child_proc):\\n    \\\"\\\"\\\"Ensures the child process dies if this process dies (fate-sharing).\\n\\n    Windows-only. Must be called by the parent, after spawning the child.\\n\\n    Args:\\n        child_proc: The subprocess.Popen or subprocess.Handle object.\\n    \\\"\\\"\\\"\\n\\n    if isinstance(child_proc, subprocess.Popen):\\n        child_proc = child_proc._handle\\n    assert isinstance(child_proc, subprocess.Handle)\\n\\n    if detect_fate_sharing_support_win32():\\n        if not win32_AssignProcessToJobObject(win32_job, int(child_proc)):\\n            import ctypes\\n\\n            raise OSError(ctypes.get_last_error(), \\\"AssignProcessToJobObject() failed\\\")\\n    else:\\n        assert False, \\\"AssignProcessToJobObject used despite being unavailable\\\"\\n\\n\\ndef set_sigterm_handler(sigterm_handler):\\n    \\\"\\\"\\\"Registers a handler for SIGTERM in a platform-compatible manner.\\\"\\\"\\\"\\n    if sys.platform == \\\"win32\\\":\\n        # Note that these signal handlers only work for console applications.\\n        # TODO(mehrdadn): implement graceful process termination mechanism\\n        # SIGINT is Ctrl+C, SIGBREAK is Ctrl+Break.\\n        signal.signal(signal.SIGBREAK, sigterm_handler)\\n    else:\\n        signal.signal(signal.SIGTERM, sigterm_handler)\\n\\n\\ndef try_make_directory_shared(directory_path):\\n    try:\\n        os.chmod(directory_path, 0o0777)\\n    except OSError as e:\\n        # Silently suppress the PermissionError that is thrown by the chmod.\\n        # This is done because the user attempting to change the permissions\\n        # on a directory may not own it. The chmod is attempted whether the\\n        # directory is new or not to avoid race conditions.\\n        # ray-project/ray/#3591\\n        if e.errno in [errno.EACCES, errno.EPERM]:\\n            pass\\n        else:\\n            raise\\n\\n\\ndef try_to_create_directory(directory_path):\\n    \\\"\\\"\\\"Attempt to create a directory that is globally readable/writable.\\n\\n    Args:\\n        directory_path: The path of the directory to create.\\n    \\\"\\\"\\\"\\n    directory_path = os.path.expanduser(directory_path)\\n    os.makedirs(directory_path, exist_ok=True)\\n    # Change the log directory permissions so others can use it. This is\\n    # important when multiple people are using the same machine.\\n    try_make_directory_shared(directory_path)\\n\\n\\ndef try_to_symlink(symlink_path, target_path):\\n    \\\"\\\"\\\"Attempt to create a symlink.\\n\\n    If the symlink path exists and isn't a symlink, the symlink will not be\\n    created. If a symlink exists in the path, it will be attempted to be\\n    removed and replaced.\\n\\n    Args:\\n        symlink_path: The path at which to create the symlink.\\n        target_path: The path the symlink should point to.\\n    \\\"\\\"\\\"\\n    symlink_path = os.path.expanduser(symlink_path)\\n    target_path = os.path.expanduser(target_path)\\n\\n    if os.path.exists(symlink_path):\\n        if os.path.islink(symlink_path):\\n            # Try to remove existing symlink.\\n            try:\\n                os.remove(symlink_path)\\n            except OSError:\\n                return\\n        else:\\n            # There's an existing non-symlink file, don't overwrite it.\\n            return\\n\\n    try:\\n        os.symlink(target_path, symlink_path)\\n    except OSError:\\n        return\\n\\n\\ndef get_user():\\n    if pwd is None:\\n        return \\\"\\\"\\n    try:\\n        return pwd.getpwuid(os.getuid()).pw_name\\n    except Exception:\\n        return \\\"\\\"\\n\\n\\ndef get_function_args(callable):\\n    all_parameters = frozenset(signature(callable).parameters)\\n    return list(all_parameters)\\n\\n\\ndef get_conda_bin_executable(executable_name):\\n    \\\"\\\"\\\"\\n    Return path to the specified executable, assumed to be discoverable within\\n    the 'bin' subdirectory of a conda installation.  Adapted from\\n    https://github.com/mlflow/mlflow.\\n    \\\"\\\"\\\"\\n\\n    # Use CONDA_EXE as per https://github.com/conda/conda/issues/7126\\n    if \\\"CONDA_EXE\\\" in os.environ:\\n        conda_bin_dir = os.path.dirname(os.environ[\\\"CONDA_EXE\\\"])\\n        return os.path.join(conda_bin_dir, executable_name)\\n    return executable_name\\n\\n\\ndef get_conda_env_dir(env_name):\\n    \\\"\\\"\\\"Find and validate the conda directory for a given conda environment.\\n\\n    For example, given the environment name `tf1`, this function checks\\n    the existence of the corresponding conda directory, e.g.\\n    `/Users/scaly/anaconda3/envs/tf1`, and returns it.\\n    \\\"\\\"\\\"\\n    conda_prefix = os.environ.get(\\\"CONDA_PREFIX\\\")\\n    if conda_prefix is None:\\n        # The caller is neither in a conda env or in (base) env.  This is rare\\n        # because by default, new terminals start in (base), but we can still\\n        # support this case.\\n        conda_exe = os.environ.get(\\\"CONDA_EXE\\\")\\n        if conda_exe is None:\\n            raise ValueError(\\n                \\\"Cannot find environment variables set by conda. \\\"\\n                \\\"Please verify conda is installed.\\\"\\n            )\\n        # Example: CONDA_EXE=$HOME/anaconda3/bin/python\\n        # Strip out /bin/python by going up two parent directories.\\n        conda_prefix = str(Path(conda_exe).parent.parent)\\n\\n    # There are two cases:\\n    # 1. We are in a conda (base) env: CONDA_DEFAULT_ENV=base and\\n    #    CONDA_PREFIX=$HOME/anaconda3\\n    # 2. We are in a user-created conda env: CONDA_DEFAULT_ENV=$env_name and\\n    #    CONDA_PREFIX=$HOME/anaconda3/envs/$current_env_name\\n    if os.environ.get(\\\"CONDA_DEFAULT_ENV\\\") == \\\"base\\\":\\n        # Caller's curent environment is (base).\\n        # Not recommended by conda, but we can still support it.\\n        if env_name == \\\"base\\\":\\n            # Desired environment is (base), located at e.g. $HOME/anaconda3\\n            env_dir = conda_prefix\\n        else:\\n            # Desired environment is user-created, e.g.\\n            # $HOME/anaconda3/envs/$env_name\\n            env_dir = os.path.join(conda_prefix, \\\"envs\\\", env_name)\\n    else:\\n        # Now `conda_prefix` should be something like\\n        # $HOME/anaconda3/envs/$current_env_name\\n        # We want to replace the last component with the desired env name.\\n        conda_envs_dir = os.path.split(conda_prefix)[0]\\n        env_dir = os.path.join(conda_envs_dir, env_name)\\n    if not os.path.isdir(env_dir):\\n        raise ValueError(\\n            \\\"conda env \\\"\\n            + env_name\\n            + \\\" not found in conda envs directory. Run `conda env list` to \\\"\\n            + \\\"verify the name is correct.\\\"\\n        )\\n    return env_dir\\n\\n\\ndef get_call_location(back: int = 1):\\n    \\\"\\\"\\\"\\n    Get the location (filename and line number) of a function caller, `back`\\n    frames up the stack.\\n\\n    Args:\\n        back: The number of frames to go up the stack, not including this\\n            function.\\n    \\\"\\\"\\\"\\n    stack = inspect.stack()\\n    try:\\n        frame = stack[back + 1]\\n        return f\\\"{frame.filename}:{frame.lineno}\\\"\\n    except IndexError:\\n        return \\\"UNKNOWN\\\"\\n\\n\\ndef get_ray_doc_version():\\n    \\\"\\\"\\\"Get the docs.ray.io version corresponding to the ray.__version__.\\\"\\\"\\\"\\n    # The ray.__version__ can be official Ray release (such as 1.12.0), or\\n    # dev (3.0.0dev0) or release candidate (2.0.0rc0). For the later we map\\n    # to the master doc version at docs.ray.io.\\n    if re.match(r\\\"^\\\\d+\\\\.\\\\d+\\\\.\\\\d+$\\\", ray.__version__) is None:\\n        return \\\"master\\\"\\n    # For the former (official Ray release), we have corresponding doc version\\n    # released as well.\\n    return f\\\"releases-{ray.__version__}\\\"\\n\\n\\n# Used to only print a deprecation warning once for a given function if we\\n# don't wish to spam the caller.\\n_PRINTED_WARNING = set()\\n\\n\\n# The following is inspired by\\n# https://github.com/tensorflow/tensorflow/blob/dec8e0b11f4f87693b67e125e67dfbc68d26c205/tensorflow/python/util/deprecation.py#L274-L329\\ndef deprecated(\\n    instructions: Optional[str] = None,\\n    removal_release: Optional[str] = None,\\n    removal_date: Optional[str] = None,\\n    warn_once: bool = True,\\n    stacklevel=2,\\n):\\n    \\\"\\\"\\\"\\n    Creates a decorator for marking functions as deprecated. The decorator\\n    will log a deprecation warning on the first (or all, see `warn_once` arg)\\n    invocations, and will otherwise leave the wrapped function unchanged.\\n\\n    Args:\\n        instructions: Instructions for the caller to update their code.\\n        removal_release: The release in which this deprecated function\\n            will be removed. Only one of removal_release and removal_date\\n            should be specified. If neither is specfieid, we'll warning that\\n            the function will be removed \\\"in a future release\\\".\\n        removal_date: The date on which this deprecated function will be\\n            removed. Only one of removal_release and removal_date should be\\n            specified. If neither is specfieid, we'll warning that\\n            the function will be removed \\\"in a future release\\\".\\n        warn_once: If true, the deprecation warning will only be logged\\n            on the first invocation. Otherwise, the deprecation warning will\\n            be logged on every invocation. Defaults to True.\\n        stacklevel: adjust the warnings stacklevel to trace the source call\\n\\n    Returns:\\n        A decorator to be used for wrapping deprecated functions.\\n    \\\"\\\"\\\"\\n    if removal_release is not None and removal_date is not None:\\n        raise ValueError(\\n            \\\"Only one of removal_release and removal_date should be specified.\\\"\\n        )\\n\\n    def deprecated_wrapper(func):\\n        @functools.wraps(func)\\n        def new_func(*args, **kwargs):\\n            global _PRINTED_WARNING\\n            if func not in _PRINTED_WARNING:\\n                if warn_once:\\n                    _PRINTED_WARNING.add(func)\\n                msg = (\\n                    \\\"From {}: {} (from {}) is deprecated and will \\\".format(\\n                        get_call_location(), func.__name__, func.__module__\\n                    )\\n                    + \\\"be removed \\\"\\n                    + (\\n                        f\\\"in version {removal_release}.\\\"\\n                        if removal_release is not None\\n                        else f\\\"after {removal_date}\\\"\\n                        if removal_date is not None\\n                        else \\\"in a future version\\\"\\n                    )\\n                    + (f\\\" {instructions}\\\" if instructions is not None else \\\"\\\")\\n                )\\n                warnings.warn(msg, stacklevel=stacklevel)\\n            return func(*args, **kwargs)\\n\\n        return new_func\\n\\n    return deprecated_wrapper\\n\\n\\ndef import_attr(full_path: str, *, reload_module: bool = False):\\n    \\\"\\\"\\\"Given a full import path to a module attr, return the imported attr.\\n\\n    If `reload_module` is set, the module will be reloaded using `importlib.reload`.\\n\\n    For example, the following are equivalent:\\n        MyClass = import_attr(\\\"module.submodule:MyClass\\\")\\n        MyClass = import_attr(\\\"module.submodule.MyClass\\\")\\n        from module.submodule import MyClass\\n\\n    Returns:\\n        Imported attr\\n    \\\"\\\"\\\"\\n    if full_path is None:\\n        raise TypeError(\\\"import path cannot be None\\\")\\n\\n    if \\\":\\\" in full_path:\\n        if full_path.count(\\\":\\\") > 1:\\n            raise ValueError(\\n                f'Got invalid import path \\\"{full_path}\\\". An '\\n                \\\"import path may have at most one colon.\\\"\\n            )\\n        module_name, attr_name = full_path.split(\\\":\\\")\\n    else:\\n        last_period_idx = full_path.rfind(\\\".\\\")\\n        module_name = full_path[:last_period_idx]\\n        attr_name = full_path[last_period_idx + 1 :]\\n\\n    module = importlib.import_module(module_name)\\n    if reload_module:\\n        importlib.reload(module)\\n    return getattr(module, attr_name)\\n\\n\\ndef get_wheel_filename(\\n    sys_platform: str = sys.platform,\\n    ray_version: str = ray.__version__,\\n    py_version: Tuple[int, int] = (sys.version_info.major, sys.version_info.minor),\\n    architecture: Optional[str] = None,\\n) -> str:\\n    \\\"\\\"\\\"Returns the filename used for the nightly Ray wheel.\\n\\n    Args:\\n        sys_platform: The platform as returned by sys.platform. Examples:\\n            \\\"darwin\\\", \\\"linux\\\", \\\"win32\\\"\\n        ray_version: The Ray version as returned by ray.__version__ or\\n            `ray --version`.  Examples: \\\"3.0.0.dev0\\\"\\n        py_version: The Python version as returned by sys.version_info. A\\n            tuple of (major, minor). Examples: (3, 8)\\n        architecture: Architecture, e.g. ``x86_64`` or ``aarch64``. If None, will\\n            be determined by calling ``platform.processor()``.\\n\\n    Returns:\\n        The wheel file name.  Examples:\\n            ray-3.0.0.dev0-cp38-cp38-manylinux2014_x86_64.whl\\n    \\\"\\\"\\\"\\n    assert py_version in ray_constants.RUNTIME_ENV_CONDA_PY_VERSIONS, py_version\\n\\n    py_version_str = \\\"\\\".join(map(str, py_version))\\n\\n    architecture = architecture or platform.processor()\\n\\n    if py_version_str in [\\\"311\\\", \\\"310\\\", \\\"39\\\", \\\"38\\\"] and architecture == \\\"arm64\\\":\\n        darwin_os_string = \\\"macosx_11_0_arm64\\\"\\n    else:\\n        darwin_os_string = \\\"macosx_10_15_x86_64\\\"\\n\\n    if architecture == \\\"aarch64\\\":\\n        linux_os_string = \\\"manylinux2014_aarch64\\\"\\n    else:\\n        linux_os_string = \\\"manylinux2014_x86_64\\\"\\n\\n    os_strings = {\\n        \\\"darwin\\\": darwin_os_string,\\n        \\\"linux\\\": linux_os_string,\\n        \\\"win32\\\": \\\"win_amd64\\\",\\n    }\\n\\n    assert sys_platform in os_strings, sys_platform\\n\\n    wheel_filename = (\\n        f\\\"ray-{ray_version}-cp{py_version_str}-\\\"\\n        f\\\"cp{py_version_str}{'m' if py_version_str in ['37'] else ''}\\\"\\n        f\\\"-{os_strings[sys_platform]}.whl\\\"\\n    )\\n\\n    return wheel_filename\\n\\n\\ndef get_master_wheel_url(\\n    ray_commit: str = ray.__commit__,\\n    sys_platform: str = sys.platform,\\n    ray_version: str = ray.__version__,\\n    py_version: Tuple[int, int] = sys.version_info[:2],\\n) -> str:\\n    \\\"\\\"\\\"Return the URL for the wheel from a specific commit.\\\"\\\"\\\"\\n    filename = get_wheel_filename(\\n        sys_platform=sys_platform, ray_version=ray_version, py_version=py_version\\n    )\\n    return (\\n        f\\\"https://s3-us-west-2.amazonaws.com/ray-wheels/master/\\\"\\n        f\\\"{ray_commit}/{filename}\\\"\\n    )\\n\\n\\ndef get_release_wheel_url(\\n    ray_commit: str = ray.__commit__,\\n    sys_platform: str = sys.platform,\\n    ray_version: str = ray.__version__,\\n    py_version: Tuple[int, int] = sys.version_info[:2],\\n) -> str:\\n    \\\"\\\"\\\"Return the URL for the wheel for a specific release.\\\"\\\"\\\"\\n    filename = get_wheel_filename(\\n        sys_platform=sys_platform, ray_version=ray_version, py_version=py_version\\n    )\\n    return (\\n        f\\\"https://ray-wheels.s3-us-west-2.amazonaws.com/releases/\\\"\\n        f\\\"{ray_version}/{ray_commit}/{filename}\\\"\\n    )\\n    # e.g. https://ray-wheels.s3-us-west-2.amazonaws.com/releases/1.4.0rc1/e7c7\\n    # f6371a69eb727fa469e4cd6f4fbefd143b4c/ray-1.4.0rc1-cp36-cp36m-manylinux201\\n    # 4_x86_64.whl\\n\\n\\ndef validate_namespace(namespace: str):\\n    if not isinstance(namespace, str):\\n        raise TypeError(\\\"namespace must be None or a string.\\\")\\n    elif namespace == \\\"\\\":\\n        raise ValueError(\\n            '\\\"\\\" is not a valid namespace. ' \\\"Pass None to not specify a namespace.\\\"\\n        )\\n\\n\\ndef init_grpc_channel(\\n    address: str,\\n    options: Optional[Sequence[Tuple[str, Any]]] = None,\\n    asynchronous: bool = False,\\n):\\n    import grpc\\n\\n    try:\\n        from grpc import aio as aiogrpc\\n    except ImportError:\\n        from grpc.experimental import aio as aiogrpc\\n\\n    from ray._private.tls_utils import load_certs_from_env\\n\\n    grpc_module = aiogrpc if asynchronous else grpc\\n\\n    options = options or []\\n    options_dict = dict(options)\\n    options_dict[\\\"grpc.keepalive_time_ms\\\"] = options_dict.get(\\n        \\\"grpc.keepalive_time_ms\\\", ray._config.grpc_client_keepalive_time_ms()\\n    )\\n    options_dict[\\\"grpc.keepalive_timeout_ms\\\"] = options_dict.get(\\n        \\\"grpc.keepalive_timeout_ms\\\", ray._config.grpc_client_keepalive_timeout_ms()\\n    )\\n    options = options_dict.items()\\n\\n    if os.environ.get(\\\"RAY_USE_TLS\\\", \\\"0\\\").lower() in (\\\"1\\\", \\\"true\\\"):\\n        server_cert_chain, private_key, ca_cert = load_certs_from_env()\\n        credentials = grpc.ssl_channel_credentials(\\n            certificate_chain=server_cert_chain,\\n            private_key=private_key,\\n            root_certificates=ca_cert,\\n        )\\n        channel = grpc_module.secure_channel(address, credentials, options=options)\\n    else:\\n        channel = grpc_module.insecure_channel(address, options=options)\\n\\n    return channel\\n\\n\\ndef check_dashboard_dependencies_installed() -> bool:\\n    \\\"\\\"\\\"Returns True if Ray Dashboard dependencies are installed.\\n\\n    Checks to see if we should start the dashboard agent or not based on the\\n    Ray installation version the user has installed (ray vs. ray[default]).\\n    Unfortunately there doesn't seem to be a cleaner way to detect this other\\n    than just blindly importing the relevant packages.\\n\\n    \\\"\\\"\\\"\\n    try:\\n        import ray.dashboard.optional_deps  # noqa: F401\\n\\n        return True\\n    except ImportError:\\n        return False\\n\\n\\ndef check_ray_client_dependencies_installed() -> bool:\\n    \\\"\\\"\\\"Returns True if Ray Client dependencies are installed.\\n\\n    See documents for check_dashboard_dependencies_installed.\\n    \\\"\\\"\\\"\\n    try:\\n        import grpc  # noqa: F401\\n\\n        return True\\n    except ImportError:\\n        return False\\n\\n\\nconnect_error = (\\n    \\\"Unable to connect to GCS (ray head) at {}. \\\"\\n    \\\"Check that (1) Ray with matching version started \\\"\\n    \\\"successfully at the specified address, (2) this \\\"\\n    \\\"node can reach the specified address, and (3) there is \\\"\\n    \\\"no firewall setting preventing access.\\\"\\n)\\n\\n\\ndef internal_kv_list_with_retry(gcs_client, prefix, namespace, num_retries=20):\\n    result = None\\n    if isinstance(prefix, str):\\n        prefix = prefix.encode()\\n    if isinstance(namespace, str):\\n        namespace = namespace.encode()\\n    for _ in range(num_retries):\\n        try:\\n            result = gcs_client.internal_kv_keys(prefix, namespace)\\n        except Exception as e:\\n            if isinstance(e, ray.exceptions.RpcError) and e.rpc_code in (\\n                ray._raylet.GRPC_STATUS_CODE_UNAVAILABLE,\\n                ray._raylet.GRPC_STATUS_CODE_UNKNOWN,\\n            ):\\n                logger.warning(connect_error.format(gcs_client.address))\\n            else:\\n                logger.exception(\\\"Internal KV List failed\\\")\\n            result = None\\n\\n        if result is not None:\\n            break\\n        else:\\n            logger.debug(f\\\"Fetched {prefix}=None from KV. Retrying.\\\")\\n            time.sleep(2)\\n    if result is None:\\n        raise ConnectionError(\\n            f\\\"Could not list '{prefix}' from GCS. Did GCS start successfully?\\\"\\n        )\\n    return result\\n\\n\\ndef internal_kv_get_with_retry(gcs_client, key, namespace, num_retries=20):\\n    result = None\\n    if isinstance(key, str):\\n        key = key.encode()\\n    for _ in range(num_retries):\\n        try:\\n            result = gcs_client.internal_kv_get(key, namespace)\\n        except Exception as e:\\n            if isinstance(e, ray.exceptions.RpcError) and e.rpc_code in (\\n                ray._raylet.GRPC_STATUS_CODE_UNAVAILABLE,\\n                ray._raylet.GRPC_STATUS_CODE_UNKNOWN,\\n            ):\\n                logger.warning(connect_error.format(gcs_client.address))\\n            else:\\n                logger.exception(\\\"Internal KV Get failed\\\")\\n            result = None\\n\\n        if result is not None:\\n            break\\n        else:\\n            logger.debug(f\\\"Fetched {key}=None from KV. Retrying.\\\")\\n            time.sleep(2)\\n    if not result:\\n        raise ConnectionError(\\n            f\\\"Could not read '{key.decode()}' from GCS. Did GCS start successfully?\\\"\\n        )\\n    return result\\n\\n\\ndef parse_resources_json(\\n    resources: str, cli_logger, cf, command_arg=\\\"--resources\\\"\\n) -> Dict[str, float]:\\n    try:\\n        resources = json.loads(resources)\\n        if not isinstance(resources, dict):\\n            raise ValueError(\\\"The format after deserialization is not a dict\\\")\\n    except Exception as e:\\n        cli_logger.error(\\n            \\\"`{}` is not a valid JSON string, detail error:{}\\\",\\n            cf.bold(f\\\"{command_arg}={resources}\\\"),\\n            str(e),\\n        )\\n        cli_logger.abort(\\n            \\\"Valid values look like this: `{}`\\\",\\n            cf.bold(\\n                f'{command_arg}=\\\\'{{\\\"CustomResource3\\\": 1, \\\"CustomResource2\\\": 2}}\\\\''\\n            ),\\n        )\\n    return resources\\n\\n\\ndef parse_metadata_json(\\n    metadata: str, cli_logger, cf, command_arg=\\\"--metadata-json\\\"\\n) -> Dict[str, str]:\\n    try:\\n        metadata = json.loads(metadata)\\n        if not isinstance(metadata, dict):\\n            raise ValueError(\\\"The format after deserialization is not a dict\\\")\\n    except Exception as e:\\n        cli_logger.error(\\n            \\\"`{}` is not a valid JSON string, detail error:{}\\\",\\n            cf.bold(f\\\"{command_arg}={metadata}\\\"),\\n            str(e),\\n        )\\n        cli_logger.abort(\\n            \\\"Valid values look like this: `{}`\\\",\\n            cf.bold(f'{command_arg}=\\\\'{{\\\"key1\\\": \\\"value1\\\", \\\"key2\\\": \\\"value2\\\"}}\\\\''),\\n        )\\n    return metadata\\n\\n\\ndef internal_kv_put_with_retry(gcs_client, key, value, namespace, num_retries=20):\\n    if isinstance(key, str):\\n        key = key.encode()\\n    if isinstance(value, str):\\n        value = value.encode()\\n    if isinstance(namespace, str):\\n        namespace = namespace.encode()\\n    error = None\\n    for _ in range(num_retries):\\n        try:\\n            return gcs_client.internal_kv_put(\\n                key, value, overwrite=True, namespace=namespace\\n            )\\n        except ray.exceptions.RpcError as e:\\n            if e.rpc_code in (\\n                ray._raylet.GRPC_STATUS_CODE_UNAVAILABLE,\\n                ray._raylet.GRPC_STATUS_CODE_UNKNOWN,\\n            ):\\n                logger.warning(connect_error.format(gcs_client.address))\\n            else:\\n                logger.exception(\\\"Internal KV Put failed\\\")\\n            time.sleep(2)\\n            error = e\\n    # Reraise the last error.\\n    raise error\\n\\n\\ndef compute_version_info():\\n    \\\"\\\"\\\"Compute the versions of Python, and Ray.\\n\\n    Returns:\\n        A tuple containing the version information.\\n    \\\"\\\"\\\"\\n    ray_version = ray.__version__\\n    python_version = \\\".\\\".join(map(str, sys.version_info[:3]))\\n    return ray_version, python_version\\n\\n\\ndef get_directory_size_bytes(path: Union[str, Path] = \\\".\\\") -> int:\\n    \\\"\\\"\\\"Get the total size of a directory in bytes, including subdirectories.\\\"\\\"\\\"\\n    total_size_bytes = 0\\n    for dirpath, dirnames, filenames in os.walk(path):\\n        for f in filenames:\\n            fp = os.path.join(dirpath, f)\\n            # skip if it is a symbolic link or a .pyc file\\n            if not os.path.islink(fp) and not f.endswith(\\\".pyc\\\"):\\n                total_size_bytes += os.path.getsize(fp)\\n\\n    return total_size_bytes\\n\\n\\ndef check_version_info(cluster_metadata):\\n    \\\"\\\"\\\"Check if the Python and Ray versions stored in GCS matches this process.\\n    Args:\\n        cluster_metadata: Ray cluster metadata from GCS.\\n\\n    Raises:\\n        Exception: An exception is raised if there is a version mismatch.\\n    \\\"\\\"\\\"\\n    cluster_version_info = (\\n        cluster_metadata[\\\"ray_version\\\"],\\n        cluster_metadata[\\\"python_version\\\"],\\n    )\\n    version_info = compute_version_info()\\n    if version_info != cluster_version_info:\\n        node_ip_address = ray._private.services.get_node_ip_address()\\n        error_message = (\\n            \\\"Version mismatch: The cluster was started with:\\\\n\\\"\\n            \\\"    Ray: \\\" + cluster_version_info[0] + \\\"\\\\n\\\"\\n            \\\"    Python: \\\" + cluster_version_info[1] + \\\"\\\\n\\\"\\n            \\\"This process on node \\\" + node_ip_address + \\\" was started with:\\\" + \\\"\\\\n\\\"\\n            \\\"    Ray: \\\" + version_info[0] + \\\"\\\\n\\\"\\n            \\\"    Python: \\\" + version_info[1] + \\\"\\\\n\\\"\\n        )\\n        raise RuntimeError(error_message)\\n\\n\\ndef get_runtime_env_info(\\n    runtime_env: \\\"RuntimeEnv\\\",\\n    *,\\n    is_job_runtime_env: bool = False,\\n    serialize: bool = False,\\n):\\n    \\\"\\\"\\\"Create runtime env info from runtime env.\\n\\n    In the user interface, the argument `runtime_env` contains some fields\\n    which not contained in `ProtoRuntimeEnv` but in `ProtoRuntimeEnvInfo`,\\n    such as `eager_install`. This function will extract those fields from\\n    `RuntimeEnv` and create a new `ProtoRuntimeEnvInfo`, and serialize it.\\n    \\\"\\\"\\\"\\n    from ray.runtime_env import RuntimeEnvConfig\\n\\n    proto_runtime_env_info = ProtoRuntimeEnvInfo()\\n\\n    if runtime_env.working_dir_uri():\\n        proto_runtime_env_info.uris.working_dir_uri = runtime_env.working_dir_uri()\\n    if len(runtime_env.py_modules_uris()) > 0:\\n        proto_runtime_env_info.uris.py_modules_uris[:] = runtime_env.py_modules_uris()\\n\\n    # TODO(Catch-Bull): overload `__setitem__` for `RuntimeEnv`, change the\\n    # runtime_env of all internal code from dict to RuntimeEnv.\\n\\n    runtime_env_config = runtime_env.get(\\\"config\\\")\\n    if runtime_env_config is None:\\n        runtime_env_config = RuntimeEnvConfig.default_config()\\n    else:\\n        runtime_env_config = RuntimeEnvConfig.parse_and_validate_runtime_env_config(\\n            runtime_env_config\\n        )\\n\\n    proto_runtime_env_info.runtime_env_config.CopyFrom(\\n        runtime_env_config.build_proto_runtime_env_config()\\n    )\\n\\n    # Normally, `RuntimeEnv` should guarantee the accuracy of field eager_install,\\n    # but so far, the internal code has not completely prohibited direct\\n    # modification of fields in RuntimeEnv, so we should check it for insurance.\\n    eager_install = (\\n        runtime_env_config.get(\\\"eager_install\\\")\\n        if runtime_env_config is not None\\n        else None\\n    )\\n    if is_job_runtime_env or eager_install is not None:\\n        if eager_install is None:\\n            eager_install = True\\n        elif not isinstance(eager_install, bool):\\n            raise TypeError(\\n                f\\\"eager_install must be a boolean. got {type(eager_install)}\\\"\\n            )\\n        proto_runtime_env_info.runtime_env_config.eager_install = eager_install\\n\\n    proto_runtime_env_info.serialized_runtime_env = runtime_env.serialize()\\n\\n    if not serialize:\\n        return proto_runtime_env_info\\n\\n    return json_format.MessageToJson(proto_runtime_env_info)\\n\\n\\ndef parse_runtime_env(runtime_env: Optional[Union[Dict, \\\"RuntimeEnv\\\"]]):\\n    from ray.runtime_env import RuntimeEnv\\n\\n    # Parse local pip/conda config files here. If we instead did it in\\n    # .remote(), it would get run in the Ray Client server, which runs on\\n    # a remote node where the files aren't available.\\n    if runtime_env:\\n        if isinstance(runtime_env, dict):\\n            return RuntimeEnv(**(runtime_env or {}))\\n        raise TypeError(\\n            \\\"runtime_env must be dict or RuntimeEnv, \\\",\\n            f\\\"but got: {type(runtime_env)}\\\",\\n        )\\n    else:\\n        # Keep the new_runtime_env as None.  In .remote(), we need to know\\n        # if runtime_env is None to know whether or not to fall back to the\\n        # runtime_env specified in the @ray.remote decorator.\\n        return None\\n\\n\\ndef split_address(address: str) -> Tuple[str, str]:\\n    \\\"\\\"\\\"Splits address into a module string (scheme) and an inner_address.\\n\\n    We use a custom splitting function instead of urllib because\\n    PEP allows \\\"underscores\\\" in a module names, while URL schemes do not\\n    allow them.\\n\\n    Args:\\n        address: The address to split.\\n\\n    Returns:\\n        A tuple of (scheme, inner_address).\\n\\n    Raises:\\n        ValueError: If the address does not contain '://'.\\n\\n    Examples:\\n        >>> split_address(\\\"ray://my_cluster\\\")\\n        ('ray', 'my_cluster')\\n    \\\"\\\"\\\"\\n    if \\\"://\\\" not in address:\\n        raise ValueError(\\\"Address must contain '://'\\\")\\n\\n    module_string, inner_address = address.split(\\\"://\\\", maxsplit=1)\\n    return (module_string, inner_address)\\n\\n\\ndef get_or_create_event_loop() -> asyncio.BaseEventLoop:\\n    \\\"\\\"\\\"Get a running async event loop if one exists, otherwise create one.\\n\\n    This function serves as a proxy for the deprecating get_event_loop().\\n    It tries to get the running loop first, and if no running loop\\n    could be retrieved:\\n    - For python version <3.10: it falls back to the get_event_loop\\n        call.\\n    - For python version >= 3.10: it uses the same python implementation\\n        of _get_event_loop() at asyncio/events.py.\\n\\n    Ideally, one should use high level APIs like asyncio.run() with python\\n    version >= 3.7, if not possible, one should create and manage the event\\n    loops explicitly.\\n    \\\"\\\"\\\"\\n    vers_info = sys.version_info\\n    if vers_info.major >= 3 and vers_info.minor >= 10:\\n        # This follows the implementation of the deprecating `get_event_loop`\\n        # in python3.10's asyncio. See python3.10/asyncio/events.py\\n        # _get_event_loop()\\n        loop = None\\n        try:\\n            loop = asyncio.get_running_loop()\\n            assert loop is not None\\n            return loop\\n        except RuntimeError as e:\\n            # No running loop, relying on the error message as for now to\\n            # differentiate runtime errors.\\n            assert \\\"no running event loop\\\" in str(e)\\n            return asyncio.get_event_loop_policy().get_event_loop()\\n\\n    return asyncio.get_event_loop()\\n\\n\\ndef get_entrypoint_name():\\n    \\\"\\\"\\\"Get the entrypoint of the current script.\\\"\\\"\\\"\\n    prefix = \\\"\\\"\\n    try:\\n        curr = psutil.Process()\\n        # Prepend `interactive_shell` for interactive shell scripts.\\n        # https://stackoverflow.com/questions/2356399/tell-if-python-is-in-interactive-mode # noqa\\n        if hasattr(sys, \\\"ps1\\\"):\\n            prefix = \\\"(interactive_shell) \\\"\\n\\n        return prefix + list2cmdline(curr.cmdline())\\n    except Exception:\\n        return \\\"unknown\\\"\\n\\n\\ndef _add_url_query_params(url: str, params: Dict[str, str]) -> str:\\n    \\\"\\\"\\\"Add params to the provided url as query parameters.\\n\\n    If url already contains query parameters, they will be merged with params, with the\\n    existing query parameters overriding any in params with the same parameter name.\\n\\n    Args:\\n        url: The URL to add query parameters to.\\n        params: The query parameters to add.\\n\\n    Returns:\\n        URL with params added as query parameters.\\n    \\\"\\\"\\\"\\n    # Unquote URL first so we don't lose existing args.\\n    url = unquote(url)\\n    # Parse URL.\\n    parsed_url = urlparse(url)\\n    # Merge URL query string arguments dict with new params.\\n    base_params = params\\n    params = dict(parse_qsl(parsed_url.query))\\n    base_params.update(params)\\n    # bool and dict values should be converted to json-friendly values.\\n    base_params.update(\\n        {\\n            k: json.dumps(v)\\n            for k, v in base_params.items()\\n            if isinstance(v, (bool, dict))\\n        }\\n    )\\n\\n    # Convert URL arguments to proper query string.\\n    encoded_params = urlencode(base_params, doseq=True)\\n    # Replace query string in parsed URL with updated query string.\\n    parsed_url = parsed_url._replace(query=encoded_params)\\n    # Convert back to URL.\\n    return urlunparse(parsed_url)\\n\\n\\ndef _add_creatable_buckets_param_if_s3_uri(uri: str) -> str:\\n    \\\"\\\"\\\"If the provided URI is an S3 URL, add allow_bucket_creation=true as a query\\n    parameter. For pyarrow >= 9.0.0, this is required in order to allow\\n    ``S3FileSystem.create_dir()`` to create S3 buckets.\\n\\n    If the provided URI is not an S3 URL or if pyarrow < 9.0.0 is installed, we return\\n    the URI unchanged.\\n\\n    Args:\\n        uri: The URI that we'll add the query parameter to, if it's an S3 URL.\\n\\n    Returns:\\n        A URI with the added allow_bucket_creation=true query parameter, if the provided\\n        URI is an S3 URL; uri will be returned unchanged otherwise.\\n    \\\"\\\"\\\"\\n    from pkg_resources._vendor.packaging.version import parse as parse_version\\n\\n    pyarrow_version = _get_pyarrow_version()\\n    if pyarrow_version is not None:\\n        pyarrow_version = parse_version(pyarrow_version)\\n    if pyarrow_version is not None and pyarrow_version < parse_version(\\\"9.0.0\\\"):\\n        # This bucket creation query parameter is not required for pyarrow < 9.0.0.\\n        return uri\\n    parsed_uri = urlparse(uri)\\n    if parsed_uri.scheme == \\\"s3\\\":\\n        uri = _add_url_query_params(uri, {\\\"allow_bucket_creation\\\": True})\\n    return uri\\n\\n\\ndef _get_pyarrow_version() -> Optional[str]:\\n    \\\"\\\"\\\"Get the version of the installed pyarrow package, returned as a tuple of ints.\\n    Returns None if the package is not found.\\n    \\\"\\\"\\\"\\n    global _PYARROW_VERSION\\n    if _PYARROW_VERSION is None:\\n        try:\\n            import pyarrow\\n        except ModuleNotFoundError:\\n            # pyarrow not installed, short-circuit.\\n            pass\\n        else:\\n            if hasattr(pyarrow, \\\"__version__\\\"):\\n                _PYARROW_VERSION = pyarrow.__version__\\n    return _PYARROW_VERSION\\n\\n\\nclass DeferSigint(contextlib.AbstractContextManager):\\n    \\\"\\\"\\\"Context manager that defers SIGINT signals until the the context is left.\\\"\\\"\\\"\\n\\n    # This is used by Ray's task cancellation to defer cancellation interrupts during\\n    # problematic areas, e.g. task argument deserialization.\\n    def __init__(self):\\n        # Whether the task has been cancelled while in the context.\\n        self.task_cancelled = False\\n        # The original SIGINT handler.\\n        self.orig_sigint_handler = None\\n        # The original signal method.\\n        self.orig_signal = None\\n\\n    @classmethod\\n    def create_if_main_thread(cls) -> contextlib.AbstractContextManager:\\n        \\\"\\\"\\\"Creates a DeferSigint context manager if running on the main thread,\\n        returns a no-op context manager otherwise.\\n        \\\"\\\"\\\"\\n        if threading.current_thread() == threading.main_thread():\\n            return cls()\\n        else:\\n            return contextlib.nullcontext()\\n\\n    def _set_task_cancelled(self, signum, frame):\\n        \\\"\\\"\\\"SIGINT handler that defers the signal.\\\"\\\"\\\"\\n        self.task_cancelled = True\\n\\n    def _signal_monkey_patch(self, signum, handler):\\n        \\\"\\\"\\\"Monkey patch for signal.signal that raises an error if a SIGINT handler is\\n        registered within the DeferSigint context.\\n        \\\"\\\"\\\"\\n        # Only raise an error if setting a SIGINT handler in the main thread; if setting\\n        # a handler in a non-main thread, signal.signal will raise an error anyway\\n        # indicating that Python does not allow that.\\n        if (\\n            threading.current_thread() == threading.main_thread()\\n            and signum == signal.SIGINT\\n        ):\\n            raise ValueError(\\n                \\\"Can't set signal handler for SIGINT while SIGINT is being deferred \\\"\\n                \\\"within a DeferSigint context.\\\"\\n            )\\n        return self.orig_signal(signum, handler)\\n\\n    def __enter__(self):\\n        # Save original SIGINT handler for later restoration.\\n        self.orig_sigint_handler = signal.getsignal(signal.SIGINT)\\n        # Set SIGINT signal handler that defers the signal.\\n        signal.signal(signal.SIGINT, self._set_task_cancelled)\\n        # Monkey patch signal.signal to raise an error if a SIGINT handler is registered\\n        # within the context.\\n        self.orig_signal = signal.signal\\n        signal.signal = self._signal_monkey_patch\\n        return self\\n\\n    def __exit__(self, exc_type, exc, exc_tb):\\n        assert self.orig_sigint_handler is not None\\n        assert self.orig_signal is not None\\n        # Restore original signal.signal function.\\n        signal.signal = self.orig_signal\\n        # Restore original SIGINT handler.\\n        signal.signal(signal.SIGINT, self.orig_sigint_handler)\\n        if exc_type is None and self.task_cancelled:\\n            # No exception raised in context but task has been cancelled, so we raise\\n            # KeyboardInterrupt to go through the task cancellation path.\\n            raise KeyboardInterrupt\\n        else:\\n            # If exception was raised in context, returning False will cause it to be\\n            # reraised.\\n            return False\\n\\n\\nbackground_tasks = set()\\n\\n\\ndef run_background_task(coroutine: Coroutine) -> asyncio.Task:\\n    \\\"\\\"\\\"Schedule a task reliably to the event loop.\\n\\n    This API is used when you don't want to cache the reference of `asyncio.Task`.\\n    For example,\\n\\n    ```\\n    get_event_loop().create_task(coroutine(*args))\\n    ```\\n\\n    The above code doesn't guarantee to schedule the coroutine to the event loops\\n\\n    When using create_task in a  \\\"fire and forget\\\" way, we should keep the references\\n    alive for the reliable execution. This API is used to fire and forget\\n    asynchronous execution.\\n\\n    https://docs.python.org/3/library/asyncio-task.html#creating-tasks\\n    \\\"\\\"\\\"\\n    task = get_or_create_event_loop().create_task(coroutine)\\n    # Add task to the set. This creates a strong reference.\\n    background_tasks.add(task)\\n\\n    # To prevent keeping references to finished tasks forever,\\n    # make each task remove its own reference from the set after\\n    # completion:\\n    task.add_done_callback(background_tasks.discard)\\n    return task\\n\\n\\ndef try_import_each_module(module_names_to_import: List[str]) -> None:\\n    \\\"\\\"\\\"\\n    Make a best-effort attempt to import each named Python module.\\n    This is used by the Python default_worker.py to preload modules.\\n    \\\"\\\"\\\"\\n    for module_to_preload in module_names_to_import:\\n        try:\\n            importlib.import_module(module_to_preload)\\n        except ImportError:\\n            logger.exception(f'Failed to preload the module \\\"{module_to_preload}\\\"')\\n\\n\\ndef update_envs(env_vars: Dict[str, str]):\\n    \\\"\\\"\\\"\\n    When updating the environment variable, if there is ${X},\\n    it will be replaced with the current environment variable.\\n    \\\"\\\"\\\"\\n    if not env_vars:\\n        return\\n\\n    for key, value in env_vars.items():\\n        expanded = os.path.expandvars(value)\\n        # Replace non-existant env vars with an empty string.\\n        result = re.sub(r\\\"\\\\$\\\\{[A-Z0-9_]+\\\\}\\\", \\\"\\\", expanded)\\n        os.environ[key] = result\\n\\n\\ndef parse_node_labels_json(\\n    labels_json: str, cli_logger, cf, command_arg=\\\"--labels\\\"\\n) -> Dict[str, str]:\\n    try:\\n        labels = json.loads(labels_json)\\n        if not isinstance(labels, dict):\\n            raise ValueError(\\n                \\\"The format after deserialization is not a key-value pair map\\\"\\n            )\\n        for key, value in labels.items():\\n            if not isinstance(key, str):\\n                raise ValueError(\\\"The key is not string type.\\\")\\n            if not isinstance(value, str):\\n                raise ValueError(f'The value of the \\\"{key}\\\" is not string type')\\n    except Exception as e:\\n        cli_logger.abort(\\n            \\\"`{}` is not a valid JSON string, detail error:{}\\\"\\n            \\\"Valid values look like this: `{}`\\\",\\n            cf.bold(f\\\"{command_arg}={labels_json}\\\"),\\n            str(e),\\n            cf.bold(f'{command_arg}=\\\\'{{\\\"gpu_type\\\": \\\"A100\\\", \\\"region\\\": \\\"us\\\"}}\\\\''),\\n        )\\n    return labels\\n\\n\\ndef validate_node_labels(labels: Dict[str, str]):\\n    if labels is None:\\n        return\\n    for key in labels.keys():\\n        if key.startswith(ray_constants.RAY_DEFAULT_LABEL_KEYS_PREFIX):\\n            raise ValueError(\\n                f\\\"Custom label keys `{key}` cannot start with the prefix \\\"\\n                f\\\"`{ray_constants.RAY_DEFAULT_LABEL_KEYS_PREFIX}`. \\\"\\n                f\\\"This is reserved for Ray defined labels.\\\"\\n            )\\n\\n\\ndef pasre_pg_formatted_resources_to_original(\\n    pg_formatted_resources: Dict[str, float]\\n) -> Dict[str, float]:\\n    original_resources = {}\\n\\n    for key, value in pg_formatted_resources.items():\\n        result = PLACEMENT_GROUP_WILDCARD_RESOURCE_PATTERN.match(key)\\n        if result and len(result.groups()) == 2:\\n            original_resources[result.group(1)] = value\\n            continue\\n        result = PLACEMENT_GROUP_INDEXED_BUNDLED_RESOURCE_PATTERN.match(key)\\n        if result and len(result.groups()) == 3:\\n            original_resources[result.group(1)] = value\\n            continue\\n        original_resources[key] = value\\n\\n    return original_resources\\n\\n\\ndef load_class(path):\\n    \\\"\\\"\\\"Load a class at runtime given a full path.\\n\\n    Example of the path: mypkg.mysubpkg.myclass\\n    \\\"\\\"\\\"\\n    class_data = path.split(\\\".\\\")\\n    if len(class_data) < 2:\\n        raise ValueError(\\\"You need to pass a valid path like mymodule.provider_class\\\")\\n    module_path = \\\".\\\".join(class_data[:-1])\\n    class_str = class_data[-1]\\n    module = importlib.import_module(module_path)\\n    return getattr(module, class_str)\\n\\n\\ndef validate_actor_state_name(actor_state_name):\\n    if actor_state_name is None:\\n        return\\n    actor_state_names = [\\n        \\\"DEPENDENCIES_UNREADY\\\",\\n        \\\"PENDING_CREATION\\\",\\n        \\\"ALIVE\\\",\\n        \\\"RESTARTING\\\",\\n        \\\"DEAD\\\",\\n    ]\\n    if actor_state_name not in actor_state_names:\\n        raise ValueError(\\n            f'\\\"{actor_state_name}\\\" is not a valid actor state name, '\\n            'it must be one of the following: \\\"DEPENDENCIES_UNREADY\\\", '\\n            '\\\"PENDING_CREATION\\\", \\\"ALIVE\\\", \\\"RESTARTING\\\", or \\\"DEAD\\\"'\\n        )\\n\", 2002], \"/Users/sangcho/anaconda3/envs/core/lib/python3.9/asyncio/base_futures.py\": [\"__all__ = ()\\n\\nimport reprlib\\nfrom _thread import get_ident\\n\\nfrom . import format_helpers\\n\\n# States for Future.\\n_PENDING = 'PENDING'\\n_CANCELLED = 'CANCELLED'\\n_FINISHED = 'FINISHED'\\n\\n\\ndef isfuture(obj):\\n    \\\"\\\"\\\"Check for a Future.\\n\\n    This returns True when obj is a Future instance or is advertising\\n    itself as duck-type compatible by setting _asyncio_future_blocking.\\n    See comment in Future for more details.\\n    \\\"\\\"\\\"\\n    return (hasattr(obj.__class__, '_asyncio_future_blocking') and\\n            obj._asyncio_future_blocking is not None)\\n\\n\\ndef _format_callbacks(cb):\\n    \\\"\\\"\\\"helper function for Future.__repr__\\\"\\\"\\\"\\n    size = len(cb)\\n    if not size:\\n        cb = ''\\n\\n    def format_cb(callback):\\n        return format_helpers._format_callback_source(callback, ())\\n\\n    if size == 1:\\n        cb = format_cb(cb[0][0])\\n    elif size == 2:\\n        cb = '{}, {}'.format(format_cb(cb[0][0]), format_cb(cb[1][0]))\\n    elif size > 2:\\n        cb = '{}, <{} more>, {}'.format(format_cb(cb[0][0]),\\n                                        size - 2,\\n                                        format_cb(cb[-1][0]))\\n    return f'cb=[{cb}]'\\n\\n\\n# bpo-42183: _repr_running is needed for repr protection\\n# when a Future or Task result contains itself directly or indirectly.\\n# The logic is borrowed from @reprlib.recursive_repr decorator.\\n# Unfortunately, the direct decorator usage is impossible because of\\n# AttributeError: '_asyncio.Task' object has no attribute '__module__' error.\\n#\\n# After fixing this thing we can return to the decorator based approach.\\n_repr_running = set()\\n\\n\\ndef _future_repr_info(future):\\n    # (Future) -> str\\n    \\\"\\\"\\\"helper function for Future.__repr__\\\"\\\"\\\"\\n    info = [future._state.lower()]\\n    if future._state == _FINISHED:\\n        if future._exception is not None:\\n            info.append(f'exception={future._exception!r}')\\n        else:\\n            key = id(future), get_ident()\\n            if key in _repr_running:\\n                result = '...'\\n            else:\\n                _repr_running.add(key)\\n                try:\\n                    # use reprlib to limit the length of the output, especially\\n                    # for very long strings\\n                    result = reprlib.repr(future._result)\\n                finally:\\n                    _repr_running.discard(key)\\n            info.append(f'result={result}')\\n    if future._callbacks:\\n        info.append(_format_callbacks(future._callbacks))\\n    if future._source_traceback:\\n        frame = future._source_traceback[-1]\\n        info.append(f'created at {frame[0]}:{frame[1]}')\\n    return info\\n\", 80], \"/Users/sangcho/anaconda3/envs/core/lib/python3.9/abc.py\": [\"# Copyright 2007 Google, Inc. All Rights Reserved.\\n# Licensed to PSF under a Contributor Agreement.\\n\\n\\\"\\\"\\\"Abstract Base Classes (ABCs) according to PEP 3119.\\\"\\\"\\\"\\n\\n\\ndef abstractmethod(funcobj):\\n    \\\"\\\"\\\"A decorator indicating abstract methods.\\n\\n    Requires that the metaclass is ABCMeta or derived from it.  A\\n    class that has a metaclass derived from ABCMeta cannot be\\n    instantiated unless all of its abstract methods are overridden.\\n    The abstract methods can be called using any of the normal\\n    'super' call mechanisms.  abstractmethod() may be used to declare\\n    abstract methods for properties and descriptors.\\n\\n    Usage:\\n\\n        class C(metaclass=ABCMeta):\\n            @abstractmethod\\n            def my_abstract_method(self, ...):\\n                ...\\n    \\\"\\\"\\\"\\n    funcobj.__isabstractmethod__ = True\\n    return funcobj\\n\\n\\nclass abstractclassmethod(classmethod):\\n    \\\"\\\"\\\"A decorator indicating abstract classmethods.\\n\\n    Deprecated, use 'classmethod' with 'abstractmethod' instead:\\n\\n        class C(ABC):\\n            @classmethod\\n            @abstractmethod\\n            def my_abstract_classmethod(cls, ...):\\n                ...\\n\\n    \\\"\\\"\\\"\\n\\n    __isabstractmethod__ = True\\n\\n    def __init__(self, callable):\\n        callable.__isabstractmethod__ = True\\n        super().__init__(callable)\\n\\n\\nclass abstractstaticmethod(staticmethod):\\n    \\\"\\\"\\\"A decorator indicating abstract staticmethods.\\n\\n    Deprecated, use 'staticmethod' with 'abstractmethod' instead:\\n\\n        class C(ABC):\\n            @staticmethod\\n            @abstractmethod\\n            def my_abstract_staticmethod(...):\\n                ...\\n\\n    \\\"\\\"\\\"\\n\\n    __isabstractmethod__ = True\\n\\n    def __init__(self, callable):\\n        callable.__isabstractmethod__ = True\\n        super().__init__(callable)\\n\\n\\nclass abstractproperty(property):\\n    \\\"\\\"\\\"A decorator indicating abstract properties.\\n\\n    Deprecated, use 'property' with 'abstractmethod' instead:\\n\\n        class C(ABC):\\n            @property\\n            @abstractmethod\\n            def my_abstract_property(self):\\n                ...\\n\\n    \\\"\\\"\\\"\\n\\n    __isabstractmethod__ = True\\n\\n\\ntry:\\n    from _abc import (get_cache_token, _abc_init, _abc_register,\\n                      _abc_instancecheck, _abc_subclasscheck, _get_dump,\\n                      _reset_registry, _reset_caches)\\nexcept ImportError:\\n    from _py_abc import ABCMeta, get_cache_token\\n    ABCMeta.__module__ = 'abc'\\nelse:\\n    class ABCMeta(type):\\n        \\\"\\\"\\\"Metaclass for defining Abstract Base Classes (ABCs).\\n\\n        Use this metaclass to create an ABC.  An ABC can be subclassed\\n        directly, and then acts as a mix-in class.  You can also register\\n        unrelated concrete classes (even built-in classes) and unrelated\\n        ABCs as 'virtual subclasses' -- these and their descendants will\\n        be considered subclasses of the registering ABC by the built-in\\n        issubclass() function, but the registering ABC won't show up in\\n        their MRO (Method Resolution Order) nor will method\\n        implementations defined by the registering ABC be callable (not\\n        even via super()).\\n        \\\"\\\"\\\"\\n        def __new__(mcls, name, bases, namespace, **kwargs):\\n            cls = super().__new__(mcls, name, bases, namespace, **kwargs)\\n            _abc_init(cls)\\n            return cls\\n\\n        def register(cls, subclass):\\n            \\\"\\\"\\\"Register a virtual subclass of an ABC.\\n\\n            Returns the subclass, to allow usage as a class decorator.\\n            \\\"\\\"\\\"\\n            return _abc_register(cls, subclass)\\n\\n        def __instancecheck__(cls, instance):\\n            \\\"\\\"\\\"Override for isinstance(instance, cls).\\\"\\\"\\\"\\n            return _abc_instancecheck(cls, instance)\\n\\n        def __subclasscheck__(cls, subclass):\\n            \\\"\\\"\\\"Override for issubclass(subclass, cls).\\\"\\\"\\\"\\n            return _abc_subclasscheck(cls, subclass)\\n\\n        def _dump_registry(cls, file=None):\\n            \\\"\\\"\\\"Debug helper to print the ABC registry.\\\"\\\"\\\"\\n            print(f\\\"Class: {cls.__module__}.{cls.__qualname__}\\\", file=file)\\n            print(f\\\"Inv. counter: {get_cache_token()}\\\", file=file)\\n            (_abc_registry, _abc_cache, _abc_negative_cache,\\n             _abc_negative_cache_version) = _get_dump(cls)\\n            print(f\\\"_abc_registry: {_abc_registry!r}\\\", file=file)\\n            print(f\\\"_abc_cache: {_abc_cache!r}\\\", file=file)\\n            print(f\\\"_abc_negative_cache: {_abc_negative_cache!r}\\\", file=file)\\n            print(f\\\"_abc_negative_cache_version: {_abc_negative_cache_version!r}\\\",\\n                  file=file)\\n\\n        def _abc_registry_clear(cls):\\n            \\\"\\\"\\\"Clear the registry (for debugging or testing).\\\"\\\"\\\"\\n            _reset_registry(cls)\\n\\n        def _abc_caches_clear(cls):\\n            \\\"\\\"\\\"Clear the caches (for debugging or testing).\\\"\\\"\\\"\\n            _reset_caches(cls)\\n\\n\\nclass ABC(metaclass=ABCMeta):\\n    \\\"\\\"\\\"Helper class that provides a standard way to create an ABC using\\n    inheritance.\\n    \\\"\\\"\\\"\\n    __slots__ = ()\\n\", 150], \"/Users/sangcho/anaconda3/envs/core/lib/python3.9/asyncio/coroutines.py\": [\"__all__ = 'coroutine', 'iscoroutinefunction', 'iscoroutine'\\n\\nimport collections.abc\\nimport functools\\nimport inspect\\nimport os\\nimport sys\\nimport traceback\\nimport types\\nimport warnings\\n\\nfrom . import base_futures\\nfrom . import constants\\nfrom . import format_helpers\\nfrom .log import logger\\n\\n\\ndef _is_debug_mode():\\n    # If you set _DEBUG to true, @coroutine will wrap the resulting\\n    # generator objects in a CoroWrapper instance (defined below).  That\\n    # instance will log a message when the generator is never iterated\\n    # over, which may happen when you forget to use \\\"await\\\" or \\\"yield from\\\"\\n    # with a coroutine call.\\n    # Note that the value of the _DEBUG flag is taken\\n    # when the decorator is used, so to be of any use it must be set\\n    # before you define your coroutines.  A downside of using this feature\\n    # is that tracebacks show entries for the CoroWrapper.__next__ method\\n    # when _DEBUG is true.\\n    return sys.flags.dev_mode or (not sys.flags.ignore_environment and\\n                                  bool(os.environ.get('PYTHONASYNCIODEBUG')))\\n\\n\\n_DEBUG = _is_debug_mode()\\n\\n\\nclass CoroWrapper:\\n    # Wrapper for coroutine object in _DEBUG mode.\\n\\n    def __init__(self, gen, func=None):\\n        assert inspect.isgenerator(gen) or inspect.iscoroutine(gen), gen\\n        self.gen = gen\\n        self.func = func  # Used to unwrap @coroutine decorator\\n        self._source_traceback = format_helpers.extract_stack(sys._getframe(1))\\n        self.__name__ = getattr(gen, '__name__', None)\\n        self.__qualname__ = getattr(gen, '__qualname__', None)\\n\\n    def __repr__(self):\\n        coro_repr = _format_coroutine(self)\\n        if self._source_traceback:\\n            frame = self._source_traceback[-1]\\n            coro_repr += f', created at {frame[0]}:{frame[1]}'\\n\\n        return f'<{self.__class__.__name__} {coro_repr}>'\\n\\n    def __iter__(self):\\n        return self\\n\\n    def __next__(self):\\n        return self.gen.send(None)\\n\\n    def send(self, value):\\n        return self.gen.send(value)\\n\\n    def throw(self, type, value=None, traceback=None):\\n        return self.gen.throw(type, value, traceback)\\n\\n    def close(self):\\n        return self.gen.close()\\n\\n    @property\\n    def gi_frame(self):\\n        return self.gen.gi_frame\\n\\n    @property\\n    def gi_running(self):\\n        return self.gen.gi_running\\n\\n    @property\\n    def gi_code(self):\\n        return self.gen.gi_code\\n\\n    def __await__(self):\\n        return self\\n\\n    @property\\n    def gi_yieldfrom(self):\\n        return self.gen.gi_yieldfrom\\n\\n    def __del__(self):\\n        # Be careful accessing self.gen.frame -- self.gen might not exist.\\n        gen = getattr(self, 'gen', None)\\n        frame = getattr(gen, 'gi_frame', None)\\n        if frame is not None and frame.f_lasti == -1:\\n            msg = f'{self!r} was never yielded from'\\n            tb = getattr(self, '_source_traceback', ())\\n            if tb:\\n                tb = ''.join(traceback.format_list(tb))\\n                msg += (f'\\\\nCoroutine object created at '\\n                        f'(most recent call last, truncated to '\\n                        f'{constants.DEBUG_STACK_DEPTH} last lines):\\\\n')\\n                msg += tb.rstrip()\\n            logger.error(msg)\\n\\n\\ndef coroutine(func):\\n    \\\"\\\"\\\"Decorator to mark coroutines.\\n\\n    If the coroutine is not yielded from before it is destroyed,\\n    an error message is logged.\\n    \\\"\\\"\\\"\\n    warnings.warn('\\\"@coroutine\\\" decorator is deprecated since Python 3.8, use \\\"async def\\\" instead',\\n                  DeprecationWarning,\\n                  stacklevel=2)\\n    if inspect.iscoroutinefunction(func):\\n        # In Python 3.5 that's all we need to do for coroutines\\n        # defined with \\\"async def\\\".\\n        return func\\n\\n    if inspect.isgeneratorfunction(func):\\n        coro = func\\n    else:\\n        @functools.wraps(func)\\n        def coro(*args, **kw):\\n            res = func(*args, **kw)\\n            if (base_futures.isfuture(res) or inspect.isgenerator(res) or\\n                    isinstance(res, CoroWrapper)):\\n                res = yield from res\\n            else:\\n                # If 'res' is an awaitable, run it.\\n                try:\\n                    await_meth = res.__await__\\n                except AttributeError:\\n                    pass\\n                else:\\n                    if isinstance(res, collections.abc.Awaitable):\\n                        res = yield from await_meth()\\n            return res\\n\\n    coro = types.coroutine(coro)\\n    if not _DEBUG:\\n        wrapper = coro\\n    else:\\n        @functools.wraps(func)\\n        def wrapper(*args, **kwds):\\n            w = CoroWrapper(coro(*args, **kwds), func=func)\\n            if w._source_traceback:\\n                del w._source_traceback[-1]\\n            # Python < 3.5 does not implement __qualname__\\n            # on generator objects, so we set it manually.\\n            # We use getattr as some callables (such as\\n            # functools.partial may lack __qualname__).\\n            w.__name__ = getattr(func, '__name__', None)\\n            w.__qualname__ = getattr(func, '__qualname__', None)\\n            return w\\n\\n    wrapper._is_coroutine = _is_coroutine  # For iscoroutinefunction().\\n    return wrapper\\n\\n\\n# A marker for iscoroutinefunction.\\n_is_coroutine = object()\\n\\n\\ndef iscoroutinefunction(func):\\n    \\\"\\\"\\\"Return True if func is a decorated coroutine function.\\\"\\\"\\\"\\n    return (inspect.iscoroutinefunction(func) or\\n            getattr(func, '_is_coroutine', None) is _is_coroutine)\\n\\n\\n# Prioritize native coroutine check to speed-up\\n# asyncio.iscoroutine.\\n_COROUTINE_TYPES = (types.CoroutineType, types.GeneratorType,\\n                    collections.abc.Coroutine, CoroWrapper)\\n_iscoroutine_typecache = set()\\n\\n\\ndef iscoroutine(obj):\\n    \\\"\\\"\\\"Return True if obj is a coroutine object.\\\"\\\"\\\"\\n    if type(obj) in _iscoroutine_typecache:\\n        return True\\n\\n    if isinstance(obj, _COROUTINE_TYPES):\\n        # Just in case we don't want to cache more than 100\\n        # positive types.  That shouldn't ever happen, unless\\n        # someone stressing the system on purpose.\\n        if len(_iscoroutine_typecache) < 100:\\n            _iscoroutine_typecache.add(type(obj))\\n        return True\\n    else:\\n        return False\\n\\n\\ndef _format_coroutine(coro):\\n    assert iscoroutine(coro)\\n\\n    is_corowrapper = isinstance(coro, CoroWrapper)\\n\\n    def get_name(coro):\\n        # Coroutines compiled with Cython sometimes don't have\\n        # proper __qualname__ or __name__.  While that is a bug\\n        # in Cython, asyncio shouldn't crash with an AttributeError\\n        # in its __repr__ functions.\\n        if is_corowrapper:\\n            return format_helpers._format_callback(coro.func, (), {})\\n\\n        if hasattr(coro, '__qualname__') and coro.__qualname__:\\n            coro_name = coro.__qualname__\\n        elif hasattr(coro, '__name__') and coro.__name__:\\n            coro_name = coro.__name__\\n        else:\\n            # Stop masking Cython bugs, expose them in a friendly way.\\n            coro_name = f'<{type(coro).__name__} without __name__>'\\n        return f'{coro_name}()'\\n\\n    def is_running(coro):\\n        try:\\n            return coro.cr_running\\n        except AttributeError:\\n            try:\\n                return coro.gi_running\\n            except AttributeError:\\n                return False\\n\\n    coro_code = None\\n    if hasattr(coro, 'cr_code') and coro.cr_code:\\n        coro_code = coro.cr_code\\n    elif hasattr(coro, 'gi_code') and coro.gi_code:\\n        coro_code = coro.gi_code\\n\\n    coro_name = get_name(coro)\\n\\n    if not coro_code:\\n        # Built-in types might not have __qualname__ or __name__.\\n        if is_running(coro):\\n            return f'{coro_name} running'\\n        else:\\n            return coro_name\\n\\n    coro_frame = None\\n    if hasattr(coro, 'gi_frame') and coro.gi_frame:\\n        coro_frame = coro.gi_frame\\n    elif hasattr(coro, 'cr_frame') and coro.cr_frame:\\n        coro_frame = coro.cr_frame\\n\\n    # If Cython's coroutine has a fake code object without proper\\n    # co_filename -- expose that.\\n    filename = coro_code.co_filename or '<empty co_filename>'\\n\\n    lineno = 0\\n    if (is_corowrapper and\\n            coro.func is not None and\\n            not inspect.isgeneratorfunction(coro.func)):\\n        source = format_helpers._get_function_source(coro.func)\\n        if source is not None:\\n            filename, lineno = source\\n        if coro_frame is None:\\n            coro_repr = f'{coro_name} done, defined at {filename}:{lineno}'\\n        else:\\n            coro_repr = f'{coro_name} running, defined at {filename}:{lineno}'\\n\\n    elif coro_frame is not None:\\n        lineno = coro_frame.f_lineno\\n        coro_repr = f'{coro_name} running at {filename}:{lineno}'\\n\\n    else:\\n        lineno = coro_code.co_firstlineno\\n        coro_repr = f'{coro_name} done, defined at {filename}:{lineno}'\\n\\n    return coro_repr\\n\", 269], \"/Users/sangcho/anaconda3/envs/core/lib/python3.9/asyncio/futures.py\": [\"\\\"\\\"\\\"A Future class similar to the one in PEP 3148.\\\"\\\"\\\"\\n\\n__all__ = (\\n    'Future', 'wrap_future', 'isfuture',\\n)\\n\\nimport concurrent.futures\\nimport contextvars\\nimport logging\\nimport sys\\nfrom types import GenericAlias\\n\\nfrom . import base_futures\\nfrom . import events\\nfrom . import exceptions\\nfrom . import format_helpers\\n\\n\\nisfuture = base_futures.isfuture\\n\\n\\n_PENDING = base_futures._PENDING\\n_CANCELLED = base_futures._CANCELLED\\n_FINISHED = base_futures._FINISHED\\n\\n\\nSTACK_DEBUG = logging.DEBUG - 1  # heavy-duty debugging\\n\\n\\nclass Future:\\n    \\\"\\\"\\\"This class is *almost* compatible with concurrent.futures.Future.\\n\\n    Differences:\\n\\n    - This class is not thread-safe.\\n\\n    - result() and exception() do not take a timeout argument and\\n      raise an exception when the future isn't done yet.\\n\\n    - Callbacks registered with add_done_callback() are always called\\n      via the event loop's call_soon().\\n\\n    - This class is not compatible with the wait() and as_completed()\\n      methods in the concurrent.futures package.\\n\\n    (In Python 3.4 or later we may be able to unify the implementations.)\\n    \\\"\\\"\\\"\\n\\n    # Class variables serving as defaults for instance variables.\\n    _state = _PENDING\\n    _result = None\\n    _exception = None\\n    _loop = None\\n    _source_traceback = None\\n    _cancel_message = None\\n    # A saved CancelledError for later chaining as an exception context.\\n    _cancelled_exc = None\\n\\n    # This field is used for a dual purpose:\\n    # - Its presence is a marker to declare that a class implements\\n    #   the Future protocol (i.e. is intended to be duck-type compatible).\\n    #   The value must also be not-None, to enable a subclass to declare\\n    #   that it is not compatible by setting this to None.\\n    # - It is set by __iter__() below so that Task._step() can tell\\n    #   the difference between\\n    #   `await Future()` or`yield from Future()` (correct) vs.\\n    #   `yield Future()` (incorrect).\\n    _asyncio_future_blocking = False\\n\\n    __log_traceback = False\\n\\n    def __init__(self, *, loop=None):\\n        \\\"\\\"\\\"Initialize the future.\\n\\n        The optional event_loop argument allows explicitly setting the event\\n        loop object used by the future. If it's not provided, the future uses\\n        the default event loop.\\n        \\\"\\\"\\\"\\n        if loop is None:\\n            self._loop = events.get_event_loop()\\n        else:\\n            self._loop = loop\\n        self._callbacks = []\\n        if self._loop.get_debug():\\n            self._source_traceback = format_helpers.extract_stack(\\n                sys._getframe(1))\\n\\n    _repr_info = base_futures._future_repr_info\\n\\n    def __repr__(self):\\n        return '<{} {}>'.format(self.__class__.__name__,\\n                                ' '.join(self._repr_info()))\\n\\n    def __del__(self):\\n        if not self.__log_traceback:\\n            # set_exception() was not called, or result() or exception()\\n            # has consumed the exception\\n            return\\n        exc = self._exception\\n        context = {\\n            'message':\\n                f'{self.__class__.__name__} exception was never retrieved',\\n            'exception': exc,\\n            'future': self,\\n        }\\n        if self._source_traceback:\\n            context['source_traceback'] = self._source_traceback\\n        self._loop.call_exception_handler(context)\\n\\n    __class_getitem__ = classmethod(GenericAlias)\\n\\n    @property\\n    def _log_traceback(self):\\n        return self.__log_traceback\\n\\n    @_log_traceback.setter\\n    def _log_traceback(self, val):\\n        if bool(val):\\n            raise ValueError('_log_traceback can only be set to False')\\n        self.__log_traceback = False\\n\\n    def get_loop(self):\\n        \\\"\\\"\\\"Return the event loop the Future is bound to.\\\"\\\"\\\"\\n        loop = self._loop\\n        if loop is None:\\n            raise RuntimeError(\\\"Future object is not initialized.\\\")\\n        return loop\\n\\n    def _make_cancelled_error(self):\\n        \\\"\\\"\\\"Create the CancelledError to raise if the Future is cancelled.\\n\\n        This should only be called once when handling a cancellation since\\n        it erases the saved context exception value.\\n        \\\"\\\"\\\"\\n        if self._cancel_message is None:\\n            exc = exceptions.CancelledError()\\n        else:\\n            exc = exceptions.CancelledError(self._cancel_message)\\n        exc.__context__ = self._cancelled_exc\\n        # Remove the reference since we don't need this anymore.\\n        self._cancelled_exc = None\\n        return exc\\n\\n    def cancel(self, msg=None):\\n        \\\"\\\"\\\"Cancel the future and schedule callbacks.\\n\\n        If the future is already done or cancelled, return False.  Otherwise,\\n        change the future's state to cancelled, schedule the callbacks and\\n        return True.\\n        \\\"\\\"\\\"\\n        self.__log_traceback = False\\n        if self._state != _PENDING:\\n            return False\\n        self._state = _CANCELLED\\n        self._cancel_message = msg\\n        self.__schedule_callbacks()\\n        return True\\n\\n    def __schedule_callbacks(self):\\n        \\\"\\\"\\\"Internal: Ask the event loop to call all callbacks.\\n\\n        The callbacks are scheduled to be called as soon as possible. Also\\n        clears the callback list.\\n        \\\"\\\"\\\"\\n        callbacks = self._callbacks[:]\\n        if not callbacks:\\n            return\\n\\n        self._callbacks[:] = []\\n        for callback, ctx in callbacks:\\n            self._loop.call_soon(callback, self, context=ctx)\\n\\n    def cancelled(self):\\n        \\\"\\\"\\\"Return True if the future was cancelled.\\\"\\\"\\\"\\n        return self._state == _CANCELLED\\n\\n    # Don't implement running(); see http://bugs.python.org/issue18699\\n\\n    def done(self):\\n        \\\"\\\"\\\"Return True if the future is done.\\n\\n        Done means either that a result / exception are available, or that the\\n        future was cancelled.\\n        \\\"\\\"\\\"\\n        return self._state != _PENDING\\n\\n    def result(self):\\n        \\\"\\\"\\\"Return the result this future represents.\\n\\n        If the future has been cancelled, raises CancelledError.  If the\\n        future's result isn't yet available, raises InvalidStateError.  If\\n        the future is done and has an exception set, this exception is raised.\\n        \\\"\\\"\\\"\\n        if self._state == _CANCELLED:\\n            exc = self._make_cancelled_error()\\n            raise exc\\n        if self._state != _FINISHED:\\n            raise exceptions.InvalidStateError('Result is not ready.')\\n        self.__log_traceback = False\\n        if self._exception is not None:\\n            raise self._exception\\n        return self._result\\n\\n    def exception(self):\\n        \\\"\\\"\\\"Return the exception that was set on this future.\\n\\n        The exception (or None if no exception was set) is returned only if\\n        the future is done.  If the future has been cancelled, raises\\n        CancelledError.  If the future isn't done yet, raises\\n        InvalidStateError.\\n        \\\"\\\"\\\"\\n        if self._state == _CANCELLED:\\n            exc = self._make_cancelled_error()\\n            raise exc\\n        if self._state != _FINISHED:\\n            raise exceptions.InvalidStateError('Exception is not set.')\\n        self.__log_traceback = False\\n        return self._exception\\n\\n    def add_done_callback(self, fn, *, context=None):\\n        \\\"\\\"\\\"Add a callback to be run when the future becomes done.\\n\\n        The callback is called with a single argument - the future object. If\\n        the future is already done when this is called, the callback is\\n        scheduled with call_soon.\\n        \\\"\\\"\\\"\\n        if self._state != _PENDING:\\n            self._loop.call_soon(fn, self, context=context)\\n        else:\\n            if context is None:\\n                context = contextvars.copy_context()\\n            self._callbacks.append((fn, context))\\n\\n    # New method not in PEP 3148.\\n\\n    def remove_done_callback(self, fn):\\n        \\\"\\\"\\\"Remove all instances of a callback from the \\\"call when done\\\" list.\\n\\n        Returns the number of callbacks removed.\\n        \\\"\\\"\\\"\\n        filtered_callbacks = [(f, ctx)\\n                              for (f, ctx) in self._callbacks\\n                              if f != fn]\\n        removed_count = len(self._callbacks) - len(filtered_callbacks)\\n        if removed_count:\\n            self._callbacks[:] = filtered_callbacks\\n        return removed_count\\n\\n    # So-called internal methods (note: no set_running_or_notify_cancel()).\\n\\n    def set_result(self, result):\\n        \\\"\\\"\\\"Mark the future done and set its result.\\n\\n        If the future is already done when this method is called, raises\\n        InvalidStateError.\\n        \\\"\\\"\\\"\\n        if self._state != _PENDING:\\n            raise exceptions.InvalidStateError(f'{self._state}: {self!r}')\\n        self._result = result\\n        self._state = _FINISHED\\n        self.__schedule_callbacks()\\n\\n    def set_exception(self, exception):\\n        \\\"\\\"\\\"Mark the future done and set an exception.\\n\\n        If the future is already done when this method is called, raises\\n        InvalidStateError.\\n        \\\"\\\"\\\"\\n        if self._state != _PENDING:\\n            raise exceptions.InvalidStateError(f'{self._state}: {self!r}')\\n        if isinstance(exception, type):\\n            exception = exception()\\n        if type(exception) is StopIteration:\\n            raise TypeError(\\\"StopIteration interacts badly with generators \\\"\\n                            \\\"and cannot be raised into a Future\\\")\\n        self._exception = exception\\n        self._state = _FINISHED\\n        self.__schedule_callbacks()\\n        self.__log_traceback = True\\n\\n    def __await__(self):\\n        if not self.done():\\n            self._asyncio_future_blocking = True\\n            yield self  # This tells Task to wait for completion.\\n        if not self.done():\\n            raise RuntimeError(\\\"await wasn't used with future\\\")\\n        return self.result()  # May raise too.\\n\\n    __iter__ = __await__  # make compatible with 'yield from'.\\n\\n\\n# Needed for testing purposes.\\n_PyFuture = Future\\n\\n\\ndef _get_loop(fut):\\n    # Tries to call Future.get_loop() if it's available.\\n    # Otherwise fallbacks to using the old '_loop' property.\\n    try:\\n        get_loop = fut.get_loop\\n    except AttributeError:\\n        pass\\n    else:\\n        return get_loop()\\n    return fut._loop\\n\\n\\ndef _set_result_unless_cancelled(fut, result):\\n    \\\"\\\"\\\"Helper setting the result only if the future was not cancelled.\\\"\\\"\\\"\\n    if fut.cancelled():\\n        return\\n    fut.set_result(result)\\n\\n\\ndef _convert_future_exc(exc):\\n    exc_class = type(exc)\\n    if exc_class is concurrent.futures.CancelledError:\\n        return exceptions.CancelledError(*exc.args)\\n    elif exc_class is concurrent.futures.TimeoutError:\\n        return exceptions.TimeoutError(*exc.args)\\n    elif exc_class is concurrent.futures.InvalidStateError:\\n        return exceptions.InvalidStateError(*exc.args)\\n    else:\\n        return exc\\n\\n\\ndef _set_concurrent_future_state(concurrent, source):\\n    \\\"\\\"\\\"Copy state from a future to a concurrent.futures.Future.\\\"\\\"\\\"\\n    assert source.done()\\n    if source.cancelled():\\n        concurrent.cancel()\\n    if not concurrent.set_running_or_notify_cancel():\\n        return\\n    exception = source.exception()\\n    if exception is not None:\\n        concurrent.set_exception(_convert_future_exc(exception))\\n    else:\\n        result = source.result()\\n        concurrent.set_result(result)\\n\\n\\ndef _copy_future_state(source, dest):\\n    \\\"\\\"\\\"Internal helper to copy state from another Future.\\n\\n    The other Future may be a concurrent.futures.Future.\\n    \\\"\\\"\\\"\\n    assert source.done()\\n    if dest.cancelled():\\n        return\\n    assert not dest.done()\\n    if source.cancelled():\\n        dest.cancel()\\n    else:\\n        exception = source.exception()\\n        if exception is not None:\\n            dest.set_exception(_convert_future_exc(exception))\\n        else:\\n            result = source.result()\\n            dest.set_result(result)\\n\\n\\ndef _chain_future(source, destination):\\n    \\\"\\\"\\\"Chain two futures so that when one completes, so does the other.\\n\\n    The result (or exception) of source will be copied to destination.\\n    If destination is cancelled, source gets cancelled too.\\n    Compatible with both asyncio.Future and concurrent.futures.Future.\\n    \\\"\\\"\\\"\\n    if not isfuture(source) and not isinstance(source,\\n                                               concurrent.futures.Future):\\n        raise TypeError('A future is required for source argument')\\n    if not isfuture(destination) and not isinstance(destination,\\n                                                    concurrent.futures.Future):\\n        raise TypeError('A future is required for destination argument')\\n    source_loop = _get_loop(source) if isfuture(source) else None\\n    dest_loop = _get_loop(destination) if isfuture(destination) else None\\n\\n    def _set_state(future, other):\\n        if isfuture(future):\\n            _copy_future_state(other, future)\\n        else:\\n            _set_concurrent_future_state(future, other)\\n\\n    def _call_check_cancel(destination):\\n        if destination.cancelled():\\n            if source_loop is None or source_loop is dest_loop:\\n                source.cancel()\\n            else:\\n                source_loop.call_soon_threadsafe(source.cancel)\\n\\n    def _call_set_state(source):\\n        if (destination.cancelled() and\\n                dest_loop is not None and dest_loop.is_closed()):\\n            return\\n        if dest_loop is None or dest_loop is source_loop:\\n            _set_state(destination, source)\\n        else:\\n            dest_loop.call_soon_threadsafe(_set_state, destination, source)\\n\\n    destination.add_done_callback(_call_check_cancel)\\n    source.add_done_callback(_call_set_state)\\n\\n\\ndef wrap_future(future, *, loop=None):\\n    \\\"\\\"\\\"Wrap concurrent.futures.Future object.\\\"\\\"\\\"\\n    if isfuture(future):\\n        return future\\n    assert isinstance(future, concurrent.futures.Future), \\\\\\n        f'concurrent.futures.Future is expected, got {future!r}'\\n    if loop is None:\\n        loop = events.get_event_loop()\\n    new_future = loop.create_future()\\n    _chain_future(future, new_future)\\n    return new_future\\n\\n\\ntry:\\n    import _asyncio\\nexcept ImportError:\\n    pass\\nelse:\\n    # _CFuture is needed for tests.\\n    Future = _CFuture = _asyncio.Future\\n\", 423], \"/Users/sangcho/anaconda3/envs/core/lib/python3.9/threading.py\": [\"\\\"\\\"\\\"Thread module emulating a subset of Java's threading model.\\\"\\\"\\\"\\n\\nimport os as _os\\nimport sys as _sys\\nimport _thread\\nimport functools\\n\\nfrom time import monotonic as _time\\nfrom _weakrefset import WeakSet\\nfrom itertools import islice as _islice, count as _count\\ntry:\\n    from _collections import deque as _deque\\nexcept ImportError:\\n    from collections import deque as _deque\\n\\n# Note regarding PEP 8 compliant names\\n#  This threading model was originally inspired by Java, and inherited\\n# the convention of camelCase function and method names from that\\n# language. Those original names are not in any imminent danger of\\n# being deprecated (even for Py3k),so this module provides them as an\\n# alias for the PEP 8 compliant names\\n# Note that using the new PEP 8 compliant names facilitates substitution\\n# with the multiprocessing module, which doesn't provide the old\\n# Java inspired names.\\n\\n__all__ = ['get_ident', 'active_count', 'Condition', 'current_thread',\\n           'enumerate', 'main_thread', 'TIMEOUT_MAX',\\n           'Event', 'Lock', 'RLock', 'Semaphore', 'BoundedSemaphore', 'Thread',\\n           'Barrier', 'BrokenBarrierError', 'Timer', 'ThreadError',\\n           'setprofile', 'settrace', 'local', 'stack_size',\\n           'excepthook', 'ExceptHookArgs']\\n\\n# Rename some stuff so \\\"from threading import *\\\" is safe\\n_start_new_thread = _thread.start_new_thread\\n_allocate_lock = _thread.allocate_lock\\n_set_sentinel = _thread._set_sentinel\\nget_ident = _thread.get_ident\\ntry:\\n    get_native_id = _thread.get_native_id\\n    _HAVE_THREAD_NATIVE_ID = True\\n    __all__.append('get_native_id')\\nexcept AttributeError:\\n    _HAVE_THREAD_NATIVE_ID = False\\nThreadError = _thread.error\\ntry:\\n    _CRLock = _thread.RLock\\nexcept AttributeError:\\n    _CRLock = None\\nTIMEOUT_MAX = _thread.TIMEOUT_MAX\\ndel _thread\\n\\n\\n# Support for profile and trace hooks\\n\\n_profile_hook = None\\n_trace_hook = None\\n\\ndef setprofile(func):\\n    \\\"\\\"\\\"Set a profile function for all threads started from the threading module.\\n\\n    The func will be passed to sys.setprofile() for each thread, before its\\n    run() method is called.\\n\\n    \\\"\\\"\\\"\\n    global _profile_hook\\n    _profile_hook = func\\n\\ndef settrace(func):\\n    \\\"\\\"\\\"Set a trace function for all threads started from the threading module.\\n\\n    The func will be passed to sys.settrace() for each thread, before its run()\\n    method is called.\\n\\n    \\\"\\\"\\\"\\n    global _trace_hook\\n    _trace_hook = func\\n\\n# Synchronization classes\\n\\nLock = _allocate_lock\\n\\ndef RLock(*args, **kwargs):\\n    \\\"\\\"\\\"Factory function that returns a new reentrant lock.\\n\\n    A reentrant lock must be released by the thread that acquired it. Once a\\n    thread has acquired a reentrant lock, the same thread may acquire it again\\n    without blocking; the thread must release it once for each time it has\\n    acquired it.\\n\\n    \\\"\\\"\\\"\\n    if _CRLock is None:\\n        return _PyRLock(*args, **kwargs)\\n    return _CRLock(*args, **kwargs)\\n\\nclass _RLock:\\n    \\\"\\\"\\\"This class implements reentrant lock objects.\\n\\n    A reentrant lock must be released by the thread that acquired it. Once a\\n    thread has acquired a reentrant lock, the same thread may acquire it\\n    again without blocking; the thread must release it once for each time it\\n    has acquired it.\\n\\n    \\\"\\\"\\\"\\n\\n    def __init__(self):\\n        self._block = _allocate_lock()\\n        self._owner = None\\n        self._count = 0\\n\\n    def __repr__(self):\\n        owner = self._owner\\n        try:\\n            owner = _active[owner].name\\n        except KeyError:\\n            pass\\n        return \\\"<%s %s.%s object owner=%r count=%d at %s>\\\" % (\\n            \\\"locked\\\" if self._block.locked() else \\\"unlocked\\\",\\n            self.__class__.__module__,\\n            self.__class__.__qualname__,\\n            owner,\\n            self._count,\\n            hex(id(self))\\n        )\\n\\n    def _at_fork_reinit(self):\\n        self._block._at_fork_reinit()\\n        self._owner = None\\n        self._count = 0\\n\\n    def acquire(self, blocking=True, timeout=-1):\\n        \\\"\\\"\\\"Acquire a lock, blocking or non-blocking.\\n\\n        When invoked without arguments: if this thread already owns the lock,\\n        increment the recursion level by one, and return immediately. Otherwise,\\n        if another thread owns the lock, block until the lock is unlocked. Once\\n        the lock is unlocked (not owned by any thread), then grab ownership, set\\n        the recursion level to one, and return. If more than one thread is\\n        blocked waiting until the lock is unlocked, only one at a time will be\\n        able to grab ownership of the lock. There is no return value in this\\n        case.\\n\\n        When invoked with the blocking argument set to true, do the same thing\\n        as when called without arguments, and return true.\\n\\n        When invoked with the blocking argument set to false, do not block. If a\\n        call without an argument would block, return false immediately;\\n        otherwise, do the same thing as when called without arguments, and\\n        return true.\\n\\n        When invoked with the floating-point timeout argument set to a positive\\n        value, block for at most the number of seconds specified by timeout\\n        and as long as the lock cannot be acquired.  Return true if the lock has\\n        been acquired, false if the timeout has elapsed.\\n\\n        \\\"\\\"\\\"\\n        me = get_ident()\\n        if self._owner == me:\\n            self._count += 1\\n            return 1\\n        rc = self._block.acquire(blocking, timeout)\\n        if rc:\\n            self._owner = me\\n            self._count = 1\\n        return rc\\n\\n    __enter__ = acquire\\n\\n    def release(self):\\n        \\\"\\\"\\\"Release a lock, decrementing the recursion level.\\n\\n        If after the decrement it is zero, reset the lock to unlocked (not owned\\n        by any thread), and if any other threads are blocked waiting for the\\n        lock to become unlocked, allow exactly one of them to proceed. If after\\n        the decrement the recursion level is still nonzero, the lock remains\\n        locked and owned by the calling thread.\\n\\n        Only call this method when the calling thread owns the lock. A\\n        RuntimeError is raised if this method is called when the lock is\\n        unlocked.\\n\\n        There is no return value.\\n\\n        \\\"\\\"\\\"\\n        if self._owner != get_ident():\\n            raise RuntimeError(\\\"cannot release un-acquired lock\\\")\\n        self._count = count = self._count - 1\\n        if not count:\\n            self._owner = None\\n            self._block.release()\\n\\n    def __exit__(self, t, v, tb):\\n        self.release()\\n\\n    # Internal methods used by condition variables\\n\\n    def _acquire_restore(self, state):\\n        self._block.acquire()\\n        self._count, self._owner = state\\n\\n    def _release_save(self):\\n        if self._count == 0:\\n            raise RuntimeError(\\\"cannot release un-acquired lock\\\")\\n        count = self._count\\n        self._count = 0\\n        owner = self._owner\\n        self._owner = None\\n        self._block.release()\\n        return (count, owner)\\n\\n    def _is_owned(self):\\n        return self._owner == get_ident()\\n\\n_PyRLock = _RLock\\n\\n\\nclass Condition:\\n    \\\"\\\"\\\"Class that implements a condition variable.\\n\\n    A condition variable allows one or more threads to wait until they are\\n    notified by another thread.\\n\\n    If the lock argument is given and not None, it must be a Lock or RLock\\n    object, and it is used as the underlying lock. Otherwise, a new RLock object\\n    is created and used as the underlying lock.\\n\\n    \\\"\\\"\\\"\\n\\n    def __init__(self, lock=None):\\n        if lock is None:\\n            lock = RLock()\\n        self._lock = lock\\n        # Export the lock's acquire() and release() methods\\n        self.acquire = lock.acquire\\n        self.release = lock.release\\n        # If the lock defines _release_save() and/or _acquire_restore(),\\n        # these override the default implementations (which just call\\n        # release() and acquire() on the lock).  Ditto for _is_owned().\\n        try:\\n            self._release_save = lock._release_save\\n        except AttributeError:\\n            pass\\n        try:\\n            self._acquire_restore = lock._acquire_restore\\n        except AttributeError:\\n            pass\\n        try:\\n            self._is_owned = lock._is_owned\\n        except AttributeError:\\n            pass\\n        self._waiters = _deque()\\n\\n    def _at_fork_reinit(self):\\n        self._lock._at_fork_reinit()\\n        self._waiters.clear()\\n\\n    def __enter__(self):\\n        return self._lock.__enter__()\\n\\n    def __exit__(self, *args):\\n        return self._lock.__exit__(*args)\\n\\n    def __repr__(self):\\n        return \\\"<Condition(%s, %d)>\\\" % (self._lock, len(self._waiters))\\n\\n    def _release_save(self):\\n        self._lock.release()           # No state to save\\n\\n    def _acquire_restore(self, x):\\n        self._lock.acquire()           # Ignore saved state\\n\\n    def _is_owned(self):\\n        # Return True if lock is owned by current_thread.\\n        # This method is called only if _lock doesn't have _is_owned().\\n        if self._lock.acquire(False):\\n            self._lock.release()\\n            return False\\n        else:\\n            return True\\n\\n    def wait(self, timeout=None):\\n        \\\"\\\"\\\"Wait until notified or until a timeout occurs.\\n\\n        If the calling thread has not acquired the lock when this method is\\n        called, a RuntimeError is raised.\\n\\n        This method releases the underlying lock, and then blocks until it is\\n        awakened by a notify() or notify_all() call for the same condition\\n        variable in another thread, or until the optional timeout occurs. Once\\n        awakened or timed out, it re-acquires the lock and returns.\\n\\n        When the timeout argument is present and not None, it should be a\\n        floating point number specifying a timeout for the operation in seconds\\n        (or fractions thereof).\\n\\n        When the underlying lock is an RLock, it is not released using its\\n        release() method, since this may not actually unlock the lock when it\\n        was acquired multiple times recursively. Instead, an internal interface\\n        of the RLock class is used, which really unlocks it even when it has\\n        been recursively acquired several times. Another internal interface is\\n        then used to restore the recursion level when the lock is reacquired.\\n\\n        \\\"\\\"\\\"\\n        if not self._is_owned():\\n            raise RuntimeError(\\\"cannot wait on un-acquired lock\\\")\\n        waiter = _allocate_lock()\\n        waiter.acquire()\\n        self._waiters.append(waiter)\\n        saved_state = self._release_save()\\n        gotit = False\\n        try:    # restore state no matter what (e.g., KeyboardInterrupt)\\n            if timeout is None:\\n                waiter.acquire()\\n                gotit = True\\n            else:\\n                if timeout > 0:\\n                    gotit = waiter.acquire(True, timeout)\\n                else:\\n                    gotit = waiter.acquire(False)\\n            return gotit\\n        finally:\\n            self._acquire_restore(saved_state)\\n            if not gotit:\\n                try:\\n                    self._waiters.remove(waiter)\\n                except ValueError:\\n                    pass\\n\\n    def wait_for(self, predicate, timeout=None):\\n        \\\"\\\"\\\"Wait until a condition evaluates to True.\\n\\n        predicate should be a callable which result will be interpreted as a\\n        boolean value.  A timeout may be provided giving the maximum time to\\n        wait.\\n\\n        \\\"\\\"\\\"\\n        endtime = None\\n        waittime = timeout\\n        result = predicate()\\n        while not result:\\n            if waittime is not None:\\n                if endtime is None:\\n                    endtime = _time() + waittime\\n                else:\\n                    waittime = endtime - _time()\\n                    if waittime <= 0:\\n                        break\\n            self.wait(waittime)\\n            result = predicate()\\n        return result\\n\\n    def notify(self, n=1):\\n        \\\"\\\"\\\"Wake up one or more threads waiting on this condition, if any.\\n\\n        If the calling thread has not acquired the lock when this method is\\n        called, a RuntimeError is raised.\\n\\n        This method wakes up at most n of the threads waiting for the condition\\n        variable; it is a no-op if no threads are waiting.\\n\\n        \\\"\\\"\\\"\\n        if not self._is_owned():\\n            raise RuntimeError(\\\"cannot notify on un-acquired lock\\\")\\n        waiters = self._waiters\\n        while waiters and n > 0:\\n            waiter = waiters[0]\\n            try:\\n                waiter.release()\\n            except RuntimeError:\\n                # gh-92530: The previous call of notify() released the lock,\\n                # but was interrupted before removing it from the queue.\\n                # It can happen if a signal handler raises an exception,\\n                # like CTRL+C which raises KeyboardInterrupt.\\n                pass\\n            else:\\n                n -= 1\\n            try:\\n                waiters.remove(waiter)\\n            except ValueError:\\n                pass\\n\\n    def notify_all(self):\\n        \\\"\\\"\\\"Wake up all threads waiting on this condition.\\n\\n        If the calling thread has not acquired the lock when this method\\n        is called, a RuntimeError is raised.\\n\\n        \\\"\\\"\\\"\\n        self.notify(len(self._waiters))\\n\\n    notifyAll = notify_all\\n\\n\\nclass Semaphore:\\n    \\\"\\\"\\\"This class implements semaphore objects.\\n\\n    Semaphores manage a counter representing the number of release() calls minus\\n    the number of acquire() calls, plus an initial value. The acquire() method\\n    blocks if necessary until it can return without making the counter\\n    negative. If not given, value defaults to 1.\\n\\n    \\\"\\\"\\\"\\n\\n    # After Tim Peters' semaphore class, but not quite the same (no maximum)\\n\\n    def __init__(self, value=1):\\n        if value < 0:\\n            raise ValueError(\\\"semaphore initial value must be >= 0\\\")\\n        self._cond = Condition(Lock())\\n        self._value = value\\n\\n    def acquire(self, blocking=True, timeout=None):\\n        \\\"\\\"\\\"Acquire a semaphore, decrementing the internal counter by one.\\n\\n        When invoked without arguments: if the internal counter is larger than\\n        zero on entry, decrement it by one and return immediately. If it is zero\\n        on entry, block, waiting until some other thread has called release() to\\n        make it larger than zero. This is done with proper interlocking so that\\n        if multiple acquire() calls are blocked, release() will wake exactly one\\n        of them up. The implementation may pick one at random, so the order in\\n        which blocked threads are awakened should not be relied on. There is no\\n        return value in this case.\\n\\n        When invoked with blocking set to true, do the same thing as when called\\n        without arguments, and return true.\\n\\n        When invoked with blocking set to false, do not block. If a call without\\n        an argument would block, return false immediately; otherwise, do the\\n        same thing as when called without arguments, and return true.\\n\\n        When invoked with a timeout other than None, it will block for at\\n        most timeout seconds.  If acquire does not complete successfully in\\n        that interval, return false.  Return true otherwise.\\n\\n        \\\"\\\"\\\"\\n        if not blocking and timeout is not None:\\n            raise ValueError(\\\"can't specify timeout for non-blocking acquire\\\")\\n        rc = False\\n        endtime = None\\n        with self._cond:\\n            while self._value == 0:\\n                if not blocking:\\n                    break\\n                if timeout is not None:\\n                    if endtime is None:\\n                        endtime = _time() + timeout\\n                    else:\\n                        timeout = endtime - _time()\\n                        if timeout <= 0:\\n                            break\\n                self._cond.wait(timeout)\\n            else:\\n                self._value -= 1\\n                rc = True\\n        return rc\\n\\n    __enter__ = acquire\\n\\n    def release(self, n=1):\\n        \\\"\\\"\\\"Release a semaphore, incrementing the internal counter by one or more.\\n\\n        When the counter is zero on entry and another thread is waiting for it\\n        to become larger than zero again, wake up that thread.\\n\\n        \\\"\\\"\\\"\\n        if n < 1:\\n            raise ValueError('n must be one or more')\\n        with self._cond:\\n            self._value += n\\n            for i in range(n):\\n                self._cond.notify()\\n\\n    def __exit__(self, t, v, tb):\\n        self.release()\\n\\n\\nclass BoundedSemaphore(Semaphore):\\n    \\\"\\\"\\\"Implements a bounded semaphore.\\n\\n    A bounded semaphore checks to make sure its current value doesn't exceed its\\n    initial value. If it does, ValueError is raised. In most situations\\n    semaphores are used to guard resources with limited capacity.\\n\\n    If the semaphore is released too many times it's a sign of a bug. If not\\n    given, value defaults to 1.\\n\\n    Like regular semaphores, bounded semaphores manage a counter representing\\n    the number of release() calls minus the number of acquire() calls, plus an\\n    initial value. The acquire() method blocks if necessary until it can return\\n    without making the counter negative. If not given, value defaults to 1.\\n\\n    \\\"\\\"\\\"\\n\\n    def __init__(self, value=1):\\n        Semaphore.__init__(self, value)\\n        self._initial_value = value\\n\\n    def release(self, n=1):\\n        \\\"\\\"\\\"Release a semaphore, incrementing the internal counter by one or more.\\n\\n        When the counter is zero on entry and another thread is waiting for it\\n        to become larger than zero again, wake up that thread.\\n\\n        If the number of releases exceeds the number of acquires,\\n        raise a ValueError.\\n\\n        \\\"\\\"\\\"\\n        if n < 1:\\n            raise ValueError('n must be one or more')\\n        with self._cond:\\n            if self._value + n > self._initial_value:\\n                raise ValueError(\\\"Semaphore released too many times\\\")\\n            self._value += n\\n            for i in range(n):\\n                self._cond.notify()\\n\\n\\nclass Event:\\n    \\\"\\\"\\\"Class implementing event objects.\\n\\n    Events manage a flag that can be set to true with the set() method and reset\\n    to false with the clear() method. The wait() method blocks until the flag is\\n    true.  The flag is initially false.\\n\\n    \\\"\\\"\\\"\\n\\n    # After Tim Peters' event class (without is_posted())\\n\\n    def __init__(self):\\n        self._cond = Condition(Lock())\\n        self._flag = False\\n\\n    def _at_fork_reinit(self):\\n        # Private method called by Thread._reset_internal_locks()\\n        self._cond._at_fork_reinit()\\n\\n    def is_set(self):\\n        \\\"\\\"\\\"Return true if and only if the internal flag is true.\\\"\\\"\\\"\\n        return self._flag\\n\\n    isSet = is_set\\n\\n    def set(self):\\n        \\\"\\\"\\\"Set the internal flag to true.\\n\\n        All threads waiting for it to become true are awakened. Threads\\n        that call wait() once the flag is true will not block at all.\\n\\n        \\\"\\\"\\\"\\n        with self._cond:\\n            self._flag = True\\n            self._cond.notify_all()\\n\\n    def clear(self):\\n        \\\"\\\"\\\"Reset the internal flag to false.\\n\\n        Subsequently, threads calling wait() will block until set() is called to\\n        set the internal flag to true again.\\n\\n        \\\"\\\"\\\"\\n        with self._cond:\\n            self._flag = False\\n\\n    def wait(self, timeout=None):\\n        \\\"\\\"\\\"Block until the internal flag is true.\\n\\n        If the internal flag is true on entry, return immediately. Otherwise,\\n        block until another thread calls set() to set the flag to true, or until\\n        the optional timeout occurs.\\n\\n        When the timeout argument is present and not None, it should be a\\n        floating point number specifying a timeout for the operation in seconds\\n        (or fractions thereof).\\n\\n        This method returns the internal flag on exit, so it will always return\\n        True except if a timeout is given and the operation times out.\\n\\n        \\\"\\\"\\\"\\n        with self._cond:\\n            signaled = self._flag\\n            if not signaled:\\n                signaled = self._cond.wait(timeout)\\n            return signaled\\n\\n\\n# A barrier class.  Inspired in part by the pthread_barrier_* api and\\n# the CyclicBarrier class from Java.  See\\n# http://sourceware.org/pthreads-win32/manual/pthread_barrier_init.html and\\n# http://java.sun.com/j2se/1.5.0/docs/api/java/util/concurrent/\\n#        CyclicBarrier.html\\n# for information.\\n# We maintain two main states, 'filling' and 'draining' enabling the barrier\\n# to be cyclic.  Threads are not allowed into it until it has fully drained\\n# since the previous cycle.  In addition, a 'resetting' state exists which is\\n# similar to 'draining' except that threads leave with a BrokenBarrierError,\\n# and a 'broken' state in which all threads get the exception.\\nclass Barrier:\\n    \\\"\\\"\\\"Implements a Barrier.\\n\\n    Useful for synchronizing a fixed number of threads at known synchronization\\n    points.  Threads block on 'wait()' and are simultaneously awoken once they\\n    have all made that call.\\n\\n    \\\"\\\"\\\"\\n\\n    def __init__(self, parties, action=None, timeout=None):\\n        \\\"\\\"\\\"Create a barrier, initialised to 'parties' threads.\\n\\n        'action' is a callable which, when supplied, will be called by one of\\n        the threads after they have all entered the barrier and just prior to\\n        releasing them all. If a 'timeout' is provided, it is used as the\\n        default for all subsequent 'wait()' calls.\\n\\n        \\\"\\\"\\\"\\n        self._cond = Condition(Lock())\\n        self._action = action\\n        self._timeout = timeout\\n        self._parties = parties\\n        self._state = 0  # 0 filling, 1 draining, -1 resetting, -2 broken\\n        self._count = 0\\n\\n    def wait(self, timeout=None):\\n        \\\"\\\"\\\"Wait for the barrier.\\n\\n        When the specified number of threads have started waiting, they are all\\n        simultaneously awoken. If an 'action' was provided for the barrier, one\\n        of the threads will have executed that callback prior to returning.\\n        Returns an individual index number from 0 to 'parties-1'.\\n\\n        \\\"\\\"\\\"\\n        if timeout is None:\\n            timeout = self._timeout\\n        with self._cond:\\n            self._enter() # Block while the barrier drains.\\n            index = self._count\\n            self._count += 1\\n            try:\\n                if index + 1 == self._parties:\\n                    # We release the barrier\\n                    self._release()\\n                else:\\n                    # We wait until someone releases us\\n                    self._wait(timeout)\\n                return index\\n            finally:\\n                self._count -= 1\\n                # Wake up any threads waiting for barrier to drain.\\n                self._exit()\\n\\n    # Block until the barrier is ready for us, or raise an exception\\n    # if it is broken.\\n    def _enter(self):\\n        while self._state in (-1, 1):\\n            # It is draining or resetting, wait until done\\n            self._cond.wait()\\n        #see if the barrier is in a broken state\\n        if self._state < 0:\\n            raise BrokenBarrierError\\n        assert self._state == 0\\n\\n    # Optionally run the 'action' and release the threads waiting\\n    # in the barrier.\\n    def _release(self):\\n        try:\\n            if self._action:\\n                self._action()\\n            # enter draining state\\n            self._state = 1\\n            self._cond.notify_all()\\n        except:\\n            #an exception during the _action handler.  Break and reraise\\n            self._break()\\n            raise\\n\\n    # Wait in the barrier until we are released.  Raise an exception\\n    # if the barrier is reset or broken.\\n    def _wait(self, timeout):\\n        if not self._cond.wait_for(lambda : self._state != 0, timeout):\\n            #timed out.  Break the barrier\\n            self._break()\\n            raise BrokenBarrierError\\n        if self._state < 0:\\n            raise BrokenBarrierError\\n        assert self._state == 1\\n\\n    # If we are the last thread to exit the barrier, signal any threads\\n    # waiting for the barrier to drain.\\n    def _exit(self):\\n        if self._count == 0:\\n            if self._state in (-1, 1):\\n                #resetting or draining\\n                self._state = 0\\n                self._cond.notify_all()\\n\\n    def reset(self):\\n        \\\"\\\"\\\"Reset the barrier to the initial state.\\n\\n        Any threads currently waiting will get the BrokenBarrier exception\\n        raised.\\n\\n        \\\"\\\"\\\"\\n        with self._cond:\\n            if self._count > 0:\\n                if self._state == 0:\\n                    #reset the barrier, waking up threads\\n                    self._state = -1\\n                elif self._state == -2:\\n                    #was broken, set it to reset state\\n                    #which clears when the last thread exits\\n                    self._state = -1\\n            else:\\n                self._state = 0\\n            self._cond.notify_all()\\n\\n    def abort(self):\\n        \\\"\\\"\\\"Place the barrier into a 'broken' state.\\n\\n        Useful in case of error.  Any currently waiting threads and threads\\n        attempting to 'wait()' will have BrokenBarrierError raised.\\n\\n        \\\"\\\"\\\"\\n        with self._cond:\\n            self._break()\\n\\n    def _break(self):\\n        # An internal error was detected.  The barrier is set to\\n        # a broken state all parties awakened.\\n        self._state = -2\\n        self._cond.notify_all()\\n\\n    @property\\n    def parties(self):\\n        \\\"\\\"\\\"Return the number of threads required to trip the barrier.\\\"\\\"\\\"\\n        return self._parties\\n\\n    @property\\n    def n_waiting(self):\\n        \\\"\\\"\\\"Return the number of threads currently waiting at the barrier.\\\"\\\"\\\"\\n        # We don't need synchronization here since this is an ephemeral result\\n        # anyway.  It returns the correct value in the steady state.\\n        if self._state == 0:\\n            return self._count\\n        return 0\\n\\n    @property\\n    def broken(self):\\n        \\\"\\\"\\\"Return True if the barrier is in a broken state.\\\"\\\"\\\"\\n        return self._state == -2\\n\\n# exception raised by the Barrier class\\nclass BrokenBarrierError(RuntimeError):\\n    pass\\n\\n\\n# Helper to generate new thread names\\n_counter = _count().__next__\\n_counter() # Consume 0 so first non-main thread has id 1.\\ndef _newname(template=\\\"Thread-%d\\\"):\\n    return template % _counter()\\n\\n# Active thread administration.\\n#\\n# bpo-44422: Use a reentrant lock to allow reentrant calls to functions like\\n# threading.enumerate().\\n_active_limbo_lock = RLock()\\n_active = {}    # maps thread id to Thread object\\n_limbo = {}\\n_dangling = WeakSet()\\n\\n# Set of Thread._tstate_lock locks of non-daemon threads used by _shutdown()\\n# to wait until all Python thread states get deleted:\\n# see Thread._set_tstate_lock().\\n_shutdown_locks_lock = _allocate_lock()\\n_shutdown_locks = set()\\n\\ndef _maintain_shutdown_locks():\\n    \\\"\\\"\\\"\\n    Drop any shutdown locks that don't correspond to running threads anymore.\\n\\n    Calling this from time to time avoids an ever-growing _shutdown_locks\\n    set when Thread objects are not joined explicitly. See bpo-37788.\\n\\n    This must be called with _shutdown_locks_lock acquired.\\n    \\\"\\\"\\\"\\n    # If a lock was released, the corresponding thread has exited\\n    to_remove = [lock for lock in _shutdown_locks if not lock.locked()]\\n    _shutdown_locks.difference_update(to_remove)\\n\\n\\n# Main class for threads\\n\\nclass Thread:\\n    \\\"\\\"\\\"A class that represents a thread of control.\\n\\n    This class can be safely subclassed in a limited fashion. There are two ways\\n    to specify the activity: by passing a callable object to the constructor, or\\n    by overriding the run() method in a subclass.\\n\\n    \\\"\\\"\\\"\\n\\n    _initialized = False\\n\\n    def __init__(self, group=None, target=None, name=None,\\n                 args=(), kwargs=None, *, daemon=None):\\n        \\\"\\\"\\\"This constructor should always be called with keyword arguments. Arguments are:\\n\\n        *group* should be None; reserved for future extension when a ThreadGroup\\n        class is implemented.\\n\\n        *target* is the callable object to be invoked by the run()\\n        method. Defaults to None, meaning nothing is called.\\n\\n        *name* is the thread name. By default, a unique name is constructed of\\n        the form \\\"Thread-N\\\" where N is a small decimal number.\\n\\n        *args* is the argument tuple for the target invocation. Defaults to ().\\n\\n        *kwargs* is a dictionary of keyword arguments for the target\\n        invocation. Defaults to {}.\\n\\n        If a subclass overrides the constructor, it must make sure to invoke\\n        the base class constructor (Thread.__init__()) before doing anything\\n        else to the thread.\\n\\n        \\\"\\\"\\\"\\n        assert group is None, \\\"group argument must be None for now\\\"\\n        if kwargs is None:\\n            kwargs = {}\\n        self._target = target\\n        self._name = str(name or _newname())\\n        self._args = args\\n        self._kwargs = kwargs\\n        if daemon is not None:\\n            self._daemonic = daemon\\n        else:\\n            self._daemonic = current_thread().daemon\\n        self._ident = None\\n        if _HAVE_THREAD_NATIVE_ID:\\n            self._native_id = None\\n        self._tstate_lock = None\\n        self._started = Event()\\n        self._is_stopped = False\\n        self._initialized = True\\n        # Copy of sys.stderr used by self._invoke_excepthook()\\n        self._stderr = _sys.stderr\\n        self._invoke_excepthook = _make_invoke_excepthook()\\n        # For debugging and _after_fork()\\n        _dangling.add(self)\\n\\n    def _reset_internal_locks(self, is_alive):\\n        # private!  Called by _after_fork() to reset our internal locks as\\n        # they may be in an invalid state leading to a deadlock or crash.\\n        self._started._at_fork_reinit()\\n        if is_alive:\\n            # bpo-42350: If the fork happens when the thread is already stopped\\n            # (ex: after threading._shutdown() has been called), _tstate_lock\\n            # is None. Do nothing in this case.\\n            if self._tstate_lock is not None:\\n                self._tstate_lock._at_fork_reinit()\\n                self._tstate_lock.acquire()\\n        else:\\n            # The thread isn't alive after fork: it doesn't have a tstate\\n            # anymore.\\n            self._is_stopped = True\\n            self._tstate_lock = None\\n\\n    def __repr__(self):\\n        assert self._initialized, \\\"Thread.__init__() was not called\\\"\\n        status = \\\"initial\\\"\\n        if self._started.is_set():\\n            status = \\\"started\\\"\\n        self.is_alive() # easy way to get ._is_stopped set when appropriate\\n        if self._is_stopped:\\n            status = \\\"stopped\\\"\\n        if self._daemonic:\\n            status += \\\" daemon\\\"\\n        if self._ident is not None:\\n            status += \\\" %s\\\" % self._ident\\n        return \\\"<%s(%s, %s)>\\\" % (self.__class__.__name__, self._name, status)\\n\\n    def start(self):\\n        \\\"\\\"\\\"Start the thread's activity.\\n\\n        It must be called at most once per thread object. It arranges for the\\n        object's run() method to be invoked in a separate thread of control.\\n\\n        This method will raise a RuntimeError if called more than once on the\\n        same thread object.\\n\\n        \\\"\\\"\\\"\\n        if not self._initialized:\\n            raise RuntimeError(\\\"thread.__init__() not called\\\")\\n\\n        if self._started.is_set():\\n            raise RuntimeError(\\\"threads can only be started once\\\")\\n\\n        with _active_limbo_lock:\\n            _limbo[self] = self\\n        try:\\n            _start_new_thread(self._bootstrap, ())\\n        except Exception:\\n            with _active_limbo_lock:\\n                del _limbo[self]\\n            raise\\n        self._started.wait()\\n\\n    def run(self):\\n        \\\"\\\"\\\"Method representing the thread's activity.\\n\\n        You may override this method in a subclass. The standard run() method\\n        invokes the callable object passed to the object's constructor as the\\n        target argument, if any, with sequential and keyword arguments taken\\n        from the args and kwargs arguments, respectively.\\n\\n        \\\"\\\"\\\"\\n        try:\\n            if self._target:\\n                self._target(*self._args, **self._kwargs)\\n        finally:\\n            # Avoid a refcycle if the thread is running a function with\\n            # an argument that has a member that points to the thread.\\n            del self._target, self._args, self._kwargs\\n\\n    def _bootstrap(self):\\n        # Wrapper around the real bootstrap code that ignores\\n        # exceptions during interpreter cleanup.  Those typically\\n        # happen when a daemon thread wakes up at an unfortunate\\n        # moment, finds the world around it destroyed, and raises some\\n        # random exception *** while trying to report the exception in\\n        # _bootstrap_inner() below ***.  Those random exceptions\\n        # don't help anybody, and they confuse users, so we suppress\\n        # them.  We suppress them only when it appears that the world\\n        # indeed has already been destroyed, so that exceptions in\\n        # _bootstrap_inner() during normal business hours are properly\\n        # reported.  Also, we only suppress them for daemonic threads;\\n        # if a non-daemonic encounters this, something else is wrong.\\n        try:\\n            self._bootstrap_inner()\\n        except:\\n            if self._daemonic and _sys is None:\\n                return\\n            raise\\n\\n    def _set_ident(self):\\n        self._ident = get_ident()\\n\\n    if _HAVE_THREAD_NATIVE_ID:\\n        def _set_native_id(self):\\n            self._native_id = get_native_id()\\n\\n    def _set_tstate_lock(self):\\n        \\\"\\\"\\\"\\n        Set a lock object which will be released by the interpreter when\\n        the underlying thread state (see pystate.h) gets deleted.\\n        \\\"\\\"\\\"\\n        self._tstate_lock = _set_sentinel()\\n        self._tstate_lock.acquire()\\n\\n        if not self.daemon:\\n            with _shutdown_locks_lock:\\n                _maintain_shutdown_locks()\\n                _shutdown_locks.add(self._tstate_lock)\\n\\n    def _bootstrap_inner(self):\\n        try:\\n            self._set_ident()\\n            self._set_tstate_lock()\\n            if _HAVE_THREAD_NATIVE_ID:\\n                self._set_native_id()\\n            self._started.set()\\n            with _active_limbo_lock:\\n                _active[self._ident] = self\\n                del _limbo[self]\\n\\n            if _trace_hook:\\n                _sys.settrace(_trace_hook)\\n            if _profile_hook:\\n                _sys.setprofile(_profile_hook)\\n\\n            try:\\n                self.run()\\n            except:\\n                self._invoke_excepthook(self)\\n        finally:\\n            with _active_limbo_lock:\\n                try:\\n                    # We don't call self._delete() because it also\\n                    # grabs _active_limbo_lock.\\n                    del _active[get_ident()]\\n                except:\\n                    pass\\n\\n    def _stop(self):\\n        # After calling ._stop(), .is_alive() returns False and .join() returns\\n        # immediately.  ._tstate_lock must be released before calling ._stop().\\n        #\\n        # Normal case:  C code at the end of the thread's life\\n        # (release_sentinel in _threadmodule.c) releases ._tstate_lock, and\\n        # that's detected by our ._wait_for_tstate_lock(), called by .join()\\n        # and .is_alive().  Any number of threads _may_ call ._stop()\\n        # simultaneously (for example, if multiple threads are blocked in\\n        # .join() calls), and they're not serialized.  That's harmless -\\n        # they'll just make redundant rebindings of ._is_stopped and\\n        # ._tstate_lock.  Obscure:  we rebind ._tstate_lock last so that the\\n        # \\\"assert self._is_stopped\\\" in ._wait_for_tstate_lock() always works\\n        # (the assert is executed only if ._tstate_lock is None).\\n        #\\n        # Special case:  _main_thread releases ._tstate_lock via this\\n        # module's _shutdown() function.\\n        lock = self._tstate_lock\\n        if lock is not None:\\n            assert not lock.locked()\\n        self._is_stopped = True\\n        self._tstate_lock = None\\n        if not self.daemon:\\n            with _shutdown_locks_lock:\\n                # Remove our lock and other released locks from _shutdown_locks\\n                _maintain_shutdown_locks()\\n\\n    def _delete(self):\\n        \\\"Remove current thread from the dict of currently running threads.\\\"\\n        with _active_limbo_lock:\\n            del _active[get_ident()]\\n            # There must not be any python code between the previous line\\n            # and after the lock is released.  Otherwise a tracing function\\n            # could try to acquire the lock again in the same thread, (in\\n            # current_thread()), and would block.\\n\\n    def join(self, timeout=None):\\n        \\\"\\\"\\\"Wait until the thread terminates.\\n\\n        This blocks the calling thread until the thread whose join() method is\\n        called terminates -- either normally or through an unhandled exception\\n        or until the optional timeout occurs.\\n\\n        When the timeout argument is present and not None, it should be a\\n        floating point number specifying a timeout for the operation in seconds\\n        (or fractions thereof). As join() always returns None, you must call\\n        is_alive() after join() to decide whether a timeout happened -- if the\\n        thread is still alive, the join() call timed out.\\n\\n        When the timeout argument is not present or None, the operation will\\n        block until the thread terminates.\\n\\n        A thread can be join()ed many times.\\n\\n        join() raises a RuntimeError if an attempt is made to join the current\\n        thread as that would cause a deadlock. It is also an error to join() a\\n        thread before it has been started and attempts to do so raises the same\\n        exception.\\n\\n        \\\"\\\"\\\"\\n        if not self._initialized:\\n            raise RuntimeError(\\\"Thread.__init__() not called\\\")\\n        if not self._started.is_set():\\n            raise RuntimeError(\\\"cannot join thread before it is started\\\")\\n        if self is current_thread():\\n            raise RuntimeError(\\\"cannot join current thread\\\")\\n\\n        if timeout is None:\\n            self._wait_for_tstate_lock()\\n        else:\\n            # the behavior of a negative timeout isn't documented, but\\n            # historically .join(timeout=x) for x<0 has acted as if timeout=0\\n            self._wait_for_tstate_lock(timeout=max(timeout, 0))\\n\\n    def _wait_for_tstate_lock(self, block=True, timeout=-1):\\n        # Issue #18808: wait for the thread state to be gone.\\n        # At the end of the thread's life, after all knowledge of the thread\\n        # is removed from C data structures, C code releases our _tstate_lock.\\n        # This method passes its arguments to _tstate_lock.acquire().\\n        # If the lock is acquired, the C code is done, and self._stop() is\\n        # called.  That sets ._is_stopped to True, and ._tstate_lock to None.\\n        lock = self._tstate_lock\\n        if lock is None:\\n            # already determined that the C code is done\\n            assert self._is_stopped\\n            return\\n\\n        try:\\n            if lock.acquire(block, timeout):\\n                lock.release()\\n                self._stop()\\n        except:\\n            if lock.locked():\\n                # bpo-45274: lock.acquire() acquired the lock, but the function\\n                # was interrupted with an exception before reaching the\\n                # lock.release(). It can happen if a signal handler raises an\\n                # exception, like CTRL+C which raises KeyboardInterrupt.\\n                lock.release()\\n                self._stop()\\n            raise\\n\\n    @property\\n    def name(self):\\n        \\\"\\\"\\\"A string used for identification purposes only.\\n\\n        It has no semantics. Multiple threads may be given the same name. The\\n        initial name is set by the constructor.\\n\\n        \\\"\\\"\\\"\\n        assert self._initialized, \\\"Thread.__init__() not called\\\"\\n        return self._name\\n\\n    @name.setter\\n    def name(self, name):\\n        assert self._initialized, \\\"Thread.__init__() not called\\\"\\n        self._name = str(name)\\n\\n    @property\\n    def ident(self):\\n        \\\"\\\"\\\"Thread identifier of this thread or None if it has not been started.\\n\\n        This is a nonzero integer. See the get_ident() function. Thread\\n        identifiers may be recycled when a thread exits and another thread is\\n        created. The identifier is available even after the thread has exited.\\n\\n        \\\"\\\"\\\"\\n        assert self._initialized, \\\"Thread.__init__() not called\\\"\\n        return self._ident\\n\\n    if _HAVE_THREAD_NATIVE_ID:\\n        @property\\n        def native_id(self):\\n            \\\"\\\"\\\"Native integral thread ID of this thread, or None if it has not been started.\\n\\n            This is a non-negative integer. See the get_native_id() function.\\n            This represents the Thread ID as reported by the kernel.\\n\\n            \\\"\\\"\\\"\\n            assert self._initialized, \\\"Thread.__init__() not called\\\"\\n            return self._native_id\\n\\n    def is_alive(self):\\n        \\\"\\\"\\\"Return whether the thread is alive.\\n\\n        This method returns True just before the run() method starts until just\\n        after the run() method terminates. See also the module function\\n        enumerate().\\n\\n        \\\"\\\"\\\"\\n        assert self._initialized, \\\"Thread.__init__() not called\\\"\\n        if self._is_stopped or not self._started.is_set():\\n            return False\\n        self._wait_for_tstate_lock(False)\\n        return not self._is_stopped\\n\\n    @property\\n    def daemon(self):\\n        \\\"\\\"\\\"A boolean value indicating whether this thread is a daemon thread.\\n\\n        This must be set before start() is called, otherwise RuntimeError is\\n        raised. Its initial value is inherited from the creating thread; the\\n        main thread is not a daemon thread and therefore all threads created in\\n        the main thread default to daemon = False.\\n\\n        The entire Python program exits when only daemon threads are left.\\n\\n        \\\"\\\"\\\"\\n        assert self._initialized, \\\"Thread.__init__() not called\\\"\\n        return self._daemonic\\n\\n    @daemon.setter\\n    def daemon(self, daemonic):\\n        if not self._initialized:\\n            raise RuntimeError(\\\"Thread.__init__() not called\\\")\\n        if self._started.is_set():\\n            raise RuntimeError(\\\"cannot set daemon status of active thread\\\")\\n        self._daemonic = daemonic\\n\\n    def isDaemon(self):\\n        return self.daemon\\n\\n    def setDaemon(self, daemonic):\\n        self.daemon = daemonic\\n\\n    def getName(self):\\n        return self.name\\n\\n    def setName(self, name):\\n        self.name = name\\n\\n\\ntry:\\n    from _thread import (_excepthook as excepthook,\\n                         _ExceptHookArgs as ExceptHookArgs)\\nexcept ImportError:\\n    # Simple Python implementation if _thread._excepthook() is not available\\n    from traceback import print_exception as _print_exception\\n    from collections import namedtuple\\n\\n    _ExceptHookArgs = namedtuple(\\n        'ExceptHookArgs',\\n        'exc_type exc_value exc_traceback thread')\\n\\n    def ExceptHookArgs(args):\\n        return _ExceptHookArgs(*args)\\n\\n    def excepthook(args, /):\\n        \\\"\\\"\\\"\\n        Handle uncaught Thread.run() exception.\\n        \\\"\\\"\\\"\\n        if args.exc_type == SystemExit:\\n            # silently ignore SystemExit\\n            return\\n\\n        if _sys is not None and _sys.stderr is not None:\\n            stderr = _sys.stderr\\n        elif args.thread is not None:\\n            stderr = args.thread._stderr\\n            if stderr is None:\\n                # do nothing if sys.stderr is None and sys.stderr was None\\n                # when the thread was created\\n                return\\n        else:\\n            # do nothing if sys.stderr is None and args.thread is None\\n            return\\n\\n        if args.thread is not None:\\n            name = args.thread.name\\n        else:\\n            name = get_ident()\\n        print(f\\\"Exception in thread {name}:\\\",\\n              file=stderr, flush=True)\\n        _print_exception(args.exc_type, args.exc_value, args.exc_traceback,\\n                         file=stderr)\\n        stderr.flush()\\n\\n\\ndef _make_invoke_excepthook():\\n    # Create a local namespace to ensure that variables remain alive\\n    # when _invoke_excepthook() is called, even if it is called late during\\n    # Python shutdown. It is mostly needed for daemon threads.\\n\\n    old_excepthook = excepthook\\n    old_sys_excepthook = _sys.excepthook\\n    if old_excepthook is None:\\n        raise RuntimeError(\\\"threading.excepthook is None\\\")\\n    if old_sys_excepthook is None:\\n        raise RuntimeError(\\\"sys.excepthook is None\\\")\\n\\n    sys_exc_info = _sys.exc_info\\n    local_print = print\\n    local_sys = _sys\\n\\n    def invoke_excepthook(thread):\\n        global excepthook\\n        try:\\n            hook = excepthook\\n            if hook is None:\\n                hook = old_excepthook\\n\\n            args = ExceptHookArgs([*sys_exc_info(), thread])\\n\\n            hook(args)\\n        except Exception as exc:\\n            exc.__suppress_context__ = True\\n            del exc\\n\\n            if local_sys is not None and local_sys.stderr is not None:\\n                stderr = local_sys.stderr\\n            else:\\n                stderr = thread._stderr\\n\\n            local_print(\\\"Exception in threading.excepthook:\\\",\\n                        file=stderr, flush=True)\\n\\n            if local_sys is not None and local_sys.excepthook is not None:\\n                sys_excepthook = local_sys.excepthook\\n            else:\\n                sys_excepthook = old_sys_excepthook\\n\\n            sys_excepthook(*sys_exc_info())\\n        finally:\\n            # Break reference cycle (exception stored in a variable)\\n            args = None\\n\\n    return invoke_excepthook\\n\\n\\n# The timer class was contributed by Itamar Shtull-Trauring\\n\\nclass Timer(Thread):\\n    \\\"\\\"\\\"Call a function after a specified number of seconds:\\n\\n            t = Timer(30.0, f, args=None, kwargs=None)\\n            t.start()\\n            t.cancel()     # stop the timer's action if it's still waiting\\n\\n    \\\"\\\"\\\"\\n\\n    def __init__(self, interval, function, args=None, kwargs=None):\\n        Thread.__init__(self)\\n        self.interval = interval\\n        self.function = function\\n        self.args = args if args is not None else []\\n        self.kwargs = kwargs if kwargs is not None else {}\\n        self.finished = Event()\\n\\n    def cancel(self):\\n        \\\"\\\"\\\"Stop the timer if it hasn't finished yet.\\\"\\\"\\\"\\n        self.finished.set()\\n\\n    def run(self):\\n        self.finished.wait(self.interval)\\n        if not self.finished.is_set():\\n            self.function(*self.args, **self.kwargs)\\n        self.finished.set()\\n\\n\\n# Special thread class to represent the main thread\\n\\nclass _MainThread(Thread):\\n\\n    def __init__(self):\\n        Thread.__init__(self, name=\\\"MainThread\\\", daemon=False)\\n        self._set_tstate_lock()\\n        self._started.set()\\n        self._set_ident()\\n        if _HAVE_THREAD_NATIVE_ID:\\n            self._set_native_id()\\n        with _active_limbo_lock:\\n            _active[self._ident] = self\\n\\n\\n# Dummy thread class to represent threads not started here.\\n# These aren't garbage collected when they die, nor can they be waited for.\\n# If they invoke anything in threading.py that calls current_thread(), they\\n# leave an entry in the _active dict forever after.\\n# Their purpose is to return *something* from current_thread().\\n# They are marked as daemon threads so we won't wait for them\\n# when we exit (conform previous semantics).\\n\\nclass _DummyThread(Thread):\\n\\n    def __init__(self):\\n        Thread.__init__(self, name=_newname(\\\"Dummy-%d\\\"), daemon=True)\\n\\n        self._started.set()\\n        self._set_ident()\\n        if _HAVE_THREAD_NATIVE_ID:\\n            self._set_native_id()\\n        with _active_limbo_lock:\\n            _active[self._ident] = self\\n\\n    def _stop(self):\\n        pass\\n\\n    def is_alive(self):\\n        assert not self._is_stopped and self._started.is_set()\\n        return True\\n\\n    def join(self, timeout=None):\\n        assert False, \\\"cannot join a dummy thread\\\"\\n\\n\\n# Global API functions\\n\\ndef current_thread():\\n    \\\"\\\"\\\"Return the current Thread object, corresponding to the caller's thread of control.\\n\\n    If the caller's thread of control was not created through the threading\\n    module, a dummy thread object with limited functionality is returned.\\n\\n    \\\"\\\"\\\"\\n    try:\\n        return _active[get_ident()]\\n    except KeyError:\\n        return _DummyThread()\\n\\ncurrentThread = current_thread\\n\\ndef active_count():\\n    \\\"\\\"\\\"Return the number of Thread objects currently alive.\\n\\n    The returned count is equal to the length of the list returned by\\n    enumerate().\\n\\n    \\\"\\\"\\\"\\n    with _active_limbo_lock:\\n        return len(_active) + len(_limbo)\\n\\nactiveCount = active_count\\n\\ndef _enumerate():\\n    # Same as enumerate(), but without the lock. Internal use only.\\n    return list(_active.values()) + list(_limbo.values())\\n\\ndef enumerate():\\n    \\\"\\\"\\\"Return a list of all Thread objects currently alive.\\n\\n    The list includes daemonic threads, dummy thread objects created by\\n    current_thread(), and the main thread. It excludes terminated threads and\\n    threads that have not yet been started.\\n\\n    \\\"\\\"\\\"\\n    with _active_limbo_lock:\\n        return list(_active.values()) + list(_limbo.values())\\n\\n\\n_threading_atexits = []\\n_SHUTTING_DOWN = False\\n\\ndef _register_atexit(func, *arg, **kwargs):\\n    \\\"\\\"\\\"CPython internal: register *func* to be called before joining threads.\\n\\n    The registered *func* is called with its arguments just before all\\n    non-daemon threads are joined in `_shutdown()`. It provides a similar\\n    purpose to `atexit.register()`, but its functions are called prior to\\n    threading shutdown instead of interpreter shutdown.\\n\\n    For similarity to atexit, the registered functions are called in reverse.\\n    \\\"\\\"\\\"\\n    if _SHUTTING_DOWN:\\n        raise RuntimeError(\\\"can't register atexit after shutdown\\\")\\n\\n    call = functools.partial(func, *arg, **kwargs)\\n    _threading_atexits.append(call)\\n\\n\\nfrom _thread import stack_size\\n\\n# Create the main thread object,\\n# and make it available for the interpreter\\n# (Py_Main) as threading._shutdown.\\n\\n_main_thread = _MainThread()\\n\\ndef _shutdown():\\n    \\\"\\\"\\\"\\n    Wait until the Python thread state of all non-daemon threads get deleted.\\n    \\\"\\\"\\\"\\n    # Obscure:  other threads may be waiting to join _main_thread.  That's\\n    # dubious, but some code does it.  We can't wait for C code to release\\n    # the main thread's tstate_lock - that won't happen until the interpreter\\n    # is nearly dead.  So we release it here.  Note that just calling _stop()\\n    # isn't enough:  other threads may already be waiting on _tstate_lock.\\n    if _main_thread._is_stopped:\\n        # _shutdown() was already called\\n        return\\n\\n    global _SHUTTING_DOWN\\n    _SHUTTING_DOWN = True\\n\\n    # Call registered threading atexit functions before threads are joined.\\n    # Order is reversed, similar to atexit.\\n    for atexit_call in reversed(_threading_atexits):\\n        atexit_call()\\n\\n    # Main thread\\n    if _main_thread.ident == get_ident():\\n        tlock = _main_thread._tstate_lock\\n        # The main thread isn't finished yet, so its thread state lock can't\\n        # have been released.\\n        assert tlock is not None\\n        assert tlock.locked()\\n        tlock.release()\\n        _main_thread._stop()\\n    else:\\n        # bpo-1596321: _shutdown() must be called in the main thread.\\n        # If the threading module was not imported by the main thread,\\n        # _main_thread is the thread which imported the threading module.\\n        # In this case, ignore _main_thread, similar behavior than for threads\\n        # spawned by C libraries or using _thread.start_new_thread().\\n        pass\\n\\n    # Join all non-deamon threads\\n    while True:\\n        with _shutdown_locks_lock:\\n            locks = list(_shutdown_locks)\\n            _shutdown_locks.clear()\\n\\n        if not locks:\\n            break\\n\\n        for lock in locks:\\n            # mimic Thread.join()\\n            lock.acquire()\\n            lock.release()\\n\\n        # new threads can be spawned while we were waiting for the other\\n        # threads to complete\\n\\n\\ndef main_thread():\\n    \\\"\\\"\\\"Return the main thread object.\\n\\n    In normal conditions, the main thread is the thread from which the\\n    Python interpreter was started.\\n    \\\"\\\"\\\"\\n    return _main_thread\\n\\n# get thread-local implementation, either from the thread\\n# module, or from the python fallback\\n\\ntry:\\n    from _thread import _local as local\\nexcept ImportError:\\n    from _threading_local import local\\n\\n\\ndef _after_fork():\\n    \\\"\\\"\\\"\\n    Cleanup threading module state that should not exist after a fork.\\n    \\\"\\\"\\\"\\n    # Reset _active_limbo_lock, in case we forked while the lock was held\\n    # by another (non-forked) thread.  http://bugs.python.org/issue874900\\n    global _active_limbo_lock, _main_thread\\n    global _shutdown_locks_lock, _shutdown_locks\\n    _active_limbo_lock = RLock()\\n\\n    # fork() only copied the current thread; clear references to others.\\n    new_active = {}\\n\\n    try:\\n        current = _active[get_ident()]\\n    except KeyError:\\n        # fork() was called in a thread which was not spawned\\n        # by threading.Thread. For example, a thread spawned\\n        # by thread.start_new_thread().\\n        current = _MainThread()\\n\\n    _main_thread = current\\n\\n    # reset _shutdown() locks: threads re-register their _tstate_lock below\\n    _shutdown_locks_lock = _allocate_lock()\\n    _shutdown_locks = set()\\n\\n    with _active_limbo_lock:\\n        # Dangling thread instances must still have their locks reset,\\n        # because someone may join() them.\\n        threads = set(_enumerate())\\n        threads.update(_dangling)\\n        for thread in threads:\\n            # Any lock/condition variable may be currently locked or in an\\n            # invalid state, so we reinitialize them.\\n            if thread is current:\\n                # There is only one active thread. We reset the ident to\\n                # its new value since it can have changed.\\n                thread._reset_internal_locks(True)\\n                ident = get_ident()\\n                thread._ident = ident\\n                new_active[ident] = thread\\n            else:\\n                # All the others are already stopped.\\n                thread._reset_internal_locks(False)\\n                thread._stop()\\n\\n        _limbo.clear()\\n        _active.clear()\\n        _active.update(new_active)\\n        assert len(_active) == 1\\n\\n\\nif hasattr(_os, \\\"register_at_fork\\\"):\\n    _os.register_at_fork(after_in_child=_after_fork)\\n\", 1555], \"/Users/sangcho/anaconda3/envs/core/lib/python3.9/concurrent/futures/_base.py\": [\"# Copyright 2009 Brian Quinlan. All Rights Reserved.\\n# Licensed to PSF under a Contributor Agreement.\\n\\n__author__ = 'Brian Quinlan (brian@sweetapp.com)'\\n\\nimport collections\\nimport logging\\nimport threading\\nimport time\\nimport types\\n\\nFIRST_COMPLETED = 'FIRST_COMPLETED'\\nFIRST_EXCEPTION = 'FIRST_EXCEPTION'\\nALL_COMPLETED = 'ALL_COMPLETED'\\n_AS_COMPLETED = '_AS_COMPLETED'\\n\\n# Possible future states (for internal use by the futures package).\\nPENDING = 'PENDING'\\nRUNNING = 'RUNNING'\\n# The future was cancelled by the user...\\nCANCELLED = 'CANCELLED'\\n# ...and _Waiter.add_cancelled() was called by a worker.\\nCANCELLED_AND_NOTIFIED = 'CANCELLED_AND_NOTIFIED'\\nFINISHED = 'FINISHED'\\n\\n_FUTURE_STATES = [\\n    PENDING,\\n    RUNNING,\\n    CANCELLED,\\n    CANCELLED_AND_NOTIFIED,\\n    FINISHED\\n]\\n\\n_STATE_TO_DESCRIPTION_MAP = {\\n    PENDING: \\\"pending\\\",\\n    RUNNING: \\\"running\\\",\\n    CANCELLED: \\\"cancelled\\\",\\n    CANCELLED_AND_NOTIFIED: \\\"cancelled\\\",\\n    FINISHED: \\\"finished\\\"\\n}\\n\\n# Logger for internal use by the futures package.\\nLOGGER = logging.getLogger(\\\"concurrent.futures\\\")\\n\\nclass Error(Exception):\\n    \\\"\\\"\\\"Base class for all future-related exceptions.\\\"\\\"\\\"\\n    pass\\n\\nclass CancelledError(Error):\\n    \\\"\\\"\\\"The Future was cancelled.\\\"\\\"\\\"\\n    pass\\n\\nclass TimeoutError(Error):\\n    \\\"\\\"\\\"The operation exceeded the given deadline.\\\"\\\"\\\"\\n    pass\\n\\nclass InvalidStateError(Error):\\n    \\\"\\\"\\\"The operation is not allowed in this state.\\\"\\\"\\\"\\n    pass\\n\\nclass _Waiter(object):\\n    \\\"\\\"\\\"Provides the event that wait() and as_completed() block on.\\\"\\\"\\\"\\n    def __init__(self):\\n        self.event = threading.Event()\\n        self.finished_futures = []\\n\\n    def add_result(self, future):\\n        self.finished_futures.append(future)\\n\\n    def add_exception(self, future):\\n        self.finished_futures.append(future)\\n\\n    def add_cancelled(self, future):\\n        self.finished_futures.append(future)\\n\\nclass _AsCompletedWaiter(_Waiter):\\n    \\\"\\\"\\\"Used by as_completed().\\\"\\\"\\\"\\n\\n    def __init__(self):\\n        super(_AsCompletedWaiter, self).__init__()\\n        self.lock = threading.Lock()\\n\\n    def add_result(self, future):\\n        with self.lock:\\n            super(_AsCompletedWaiter, self).add_result(future)\\n            self.event.set()\\n\\n    def add_exception(self, future):\\n        with self.lock:\\n            super(_AsCompletedWaiter, self).add_exception(future)\\n            self.event.set()\\n\\n    def add_cancelled(self, future):\\n        with self.lock:\\n            super(_AsCompletedWaiter, self).add_cancelled(future)\\n            self.event.set()\\n\\nclass _FirstCompletedWaiter(_Waiter):\\n    \\\"\\\"\\\"Used by wait(return_when=FIRST_COMPLETED).\\\"\\\"\\\"\\n\\n    def add_result(self, future):\\n        super().add_result(future)\\n        self.event.set()\\n\\n    def add_exception(self, future):\\n        super().add_exception(future)\\n        self.event.set()\\n\\n    def add_cancelled(self, future):\\n        super().add_cancelled(future)\\n        self.event.set()\\n\\nclass _AllCompletedWaiter(_Waiter):\\n    \\\"\\\"\\\"Used by wait(return_when=FIRST_EXCEPTION and ALL_COMPLETED).\\\"\\\"\\\"\\n\\n    def __init__(self, num_pending_calls, stop_on_exception):\\n        self.num_pending_calls = num_pending_calls\\n        self.stop_on_exception = stop_on_exception\\n        self.lock = threading.Lock()\\n        super().__init__()\\n\\n    def _decrement_pending_calls(self):\\n        with self.lock:\\n            self.num_pending_calls -= 1\\n            if not self.num_pending_calls:\\n                self.event.set()\\n\\n    def add_result(self, future):\\n        super().add_result(future)\\n        self._decrement_pending_calls()\\n\\n    def add_exception(self, future):\\n        super().add_exception(future)\\n        if self.stop_on_exception:\\n            self.event.set()\\n        else:\\n            self._decrement_pending_calls()\\n\\n    def add_cancelled(self, future):\\n        super().add_cancelled(future)\\n        self._decrement_pending_calls()\\n\\nclass _AcquireFutures(object):\\n    \\\"\\\"\\\"A context manager that does an ordered acquire of Future conditions.\\\"\\\"\\\"\\n\\n    def __init__(self, futures):\\n        self.futures = sorted(futures, key=id)\\n\\n    def __enter__(self):\\n        for future in self.futures:\\n            future._condition.acquire()\\n\\n    def __exit__(self, *args):\\n        for future in self.futures:\\n            future._condition.release()\\n\\ndef _create_and_install_waiters(fs, return_when):\\n    if return_when == _AS_COMPLETED:\\n        waiter = _AsCompletedWaiter()\\n    elif return_when == FIRST_COMPLETED:\\n        waiter = _FirstCompletedWaiter()\\n    else:\\n        pending_count = sum(\\n                f._state not in [CANCELLED_AND_NOTIFIED, FINISHED] for f in fs)\\n\\n        if return_when == FIRST_EXCEPTION:\\n            waiter = _AllCompletedWaiter(pending_count, stop_on_exception=True)\\n        elif return_when == ALL_COMPLETED:\\n            waiter = _AllCompletedWaiter(pending_count, stop_on_exception=False)\\n        else:\\n            raise ValueError(\\\"Invalid return condition: %r\\\" % return_when)\\n\\n    for f in fs:\\n        f._waiters.append(waiter)\\n\\n    return waiter\\n\\n\\ndef _yield_finished_futures(fs, waiter, ref_collect):\\n    \\\"\\\"\\\"\\n    Iterate on the list *fs*, yielding finished futures one by one in\\n    reverse order.\\n    Before yielding a future, *waiter* is removed from its waiters\\n    and the future is removed from each set in the collection of sets\\n    *ref_collect*.\\n\\n    The aim of this function is to avoid keeping stale references after\\n    the future is yielded and before the iterator resumes.\\n    \\\"\\\"\\\"\\n    while fs:\\n        f = fs[-1]\\n        for futures_set in ref_collect:\\n            futures_set.remove(f)\\n        with f._condition:\\n            f._waiters.remove(waiter)\\n        del f\\n        # Careful not to keep a reference to the popped value\\n        yield fs.pop()\\n\\n\\ndef as_completed(fs, timeout=None):\\n    \\\"\\\"\\\"An iterator over the given futures that yields each as it completes.\\n\\n    Args:\\n        fs: The sequence of Futures (possibly created by different Executors) to\\n            iterate over.\\n        timeout: The maximum number of seconds to wait. If None, then there\\n            is no limit on the wait time.\\n\\n    Returns:\\n        An iterator that yields the given Futures as they complete (finished or\\n        cancelled). If any given Futures are duplicated, they will be returned\\n        once.\\n\\n    Raises:\\n        TimeoutError: If the entire result iterator could not be generated\\n            before the given timeout.\\n    \\\"\\\"\\\"\\n    if timeout is not None:\\n        end_time = timeout + time.monotonic()\\n\\n    fs = set(fs)\\n    total_futures = len(fs)\\n    with _AcquireFutures(fs):\\n        finished = set(\\n                f for f in fs\\n                if f._state in [CANCELLED_AND_NOTIFIED, FINISHED])\\n        pending = fs - finished\\n        waiter = _create_and_install_waiters(fs, _AS_COMPLETED)\\n    finished = list(finished)\\n    try:\\n        yield from _yield_finished_futures(finished, waiter,\\n                                           ref_collect=(fs,))\\n\\n        while pending:\\n            if timeout is None:\\n                wait_timeout = None\\n            else:\\n                wait_timeout = end_time - time.monotonic()\\n                if wait_timeout < 0:\\n                    raise TimeoutError(\\n                            '%d (of %d) futures unfinished' % (\\n                            len(pending), total_futures))\\n\\n            waiter.event.wait(wait_timeout)\\n\\n            with waiter.lock:\\n                finished = waiter.finished_futures\\n                waiter.finished_futures = []\\n                waiter.event.clear()\\n\\n            # reverse to keep finishing order\\n            finished.reverse()\\n            yield from _yield_finished_futures(finished, waiter,\\n                                               ref_collect=(fs, pending))\\n\\n    finally:\\n        # Remove waiter from unfinished futures\\n        for f in fs:\\n            with f._condition:\\n                f._waiters.remove(waiter)\\n\\nDoneAndNotDoneFutures = collections.namedtuple(\\n        'DoneAndNotDoneFutures', 'done not_done')\\ndef wait(fs, timeout=None, return_when=ALL_COMPLETED):\\n    \\\"\\\"\\\"Wait for the futures in the given sequence to complete.\\n\\n    Args:\\n        fs: The sequence of Futures (possibly created by different Executors) to\\n            wait upon.\\n        timeout: The maximum number of seconds to wait. If None, then there\\n            is no limit on the wait time.\\n        return_when: Indicates when this function should return. The options\\n            are:\\n\\n            FIRST_COMPLETED - Return when any future finishes or is\\n                              cancelled.\\n            FIRST_EXCEPTION - Return when any future finishes by raising an\\n                              exception. If no future raises an exception\\n                              then it is equivalent to ALL_COMPLETED.\\n            ALL_COMPLETED -   Return when all futures finish or are cancelled.\\n\\n    Returns:\\n        A named 2-tuple of sets. The first set, named 'done', contains the\\n        futures that completed (is finished or cancelled) before the wait\\n        completed. The second set, named 'not_done', contains uncompleted\\n        futures. Duplicate futures given to *fs* are removed and will be \\n        returned only once.\\n    \\\"\\\"\\\"\\n    fs = set(fs)\\n    with _AcquireFutures(fs):\\n        done = {f for f in fs\\n                   if f._state in [CANCELLED_AND_NOTIFIED, FINISHED]}\\n        not_done = fs - done\\n        if (return_when == FIRST_COMPLETED) and done:\\n            return DoneAndNotDoneFutures(done, not_done)\\n        elif (return_when == FIRST_EXCEPTION) and done:\\n            if any(f for f in done\\n                   if not f.cancelled() and f.exception() is not None):\\n                return DoneAndNotDoneFutures(done, not_done)\\n\\n        if len(done) == len(fs):\\n            return DoneAndNotDoneFutures(done, not_done)\\n\\n        waiter = _create_and_install_waiters(fs, return_when)\\n\\n    waiter.event.wait(timeout)\\n    for f in fs:\\n        with f._condition:\\n            f._waiters.remove(waiter)\\n\\n    done.update(waiter.finished_futures)\\n    return DoneAndNotDoneFutures(done, fs - done)\\n\\nclass Future(object):\\n    \\\"\\\"\\\"Represents the result of an asynchronous computation.\\\"\\\"\\\"\\n\\n    def __init__(self):\\n        \\\"\\\"\\\"Initializes the future. Should not be called by clients.\\\"\\\"\\\"\\n        self._condition = threading.Condition()\\n        self._state = PENDING\\n        self._result = None\\n        self._exception = None\\n        self._waiters = []\\n        self._done_callbacks = []\\n\\n    def _invoke_callbacks(self):\\n        for callback in self._done_callbacks:\\n            try:\\n                callback(self)\\n            except Exception:\\n                LOGGER.exception('exception calling callback for %r', self)\\n\\n    def __repr__(self):\\n        with self._condition:\\n            if self._state == FINISHED:\\n                if self._exception:\\n                    return '<%s at %#x state=%s raised %s>' % (\\n                        self.__class__.__name__,\\n                        id(self),\\n                        _STATE_TO_DESCRIPTION_MAP[self._state],\\n                        self._exception.__class__.__name__)\\n                else:\\n                    return '<%s at %#x state=%s returned %s>' % (\\n                        self.__class__.__name__,\\n                        id(self),\\n                        _STATE_TO_DESCRIPTION_MAP[self._state],\\n                        self._result.__class__.__name__)\\n            return '<%s at %#x state=%s>' % (\\n                    self.__class__.__name__,\\n                    id(self),\\n                   _STATE_TO_DESCRIPTION_MAP[self._state])\\n\\n    def cancel(self):\\n        \\\"\\\"\\\"Cancel the future if possible.\\n\\n        Returns True if the future was cancelled, False otherwise. A future\\n        cannot be cancelled if it is running or has already completed.\\n        \\\"\\\"\\\"\\n        with self._condition:\\n            if self._state in [RUNNING, FINISHED]:\\n                return False\\n\\n            if self._state in [CANCELLED, CANCELLED_AND_NOTIFIED]:\\n                return True\\n\\n            self._state = CANCELLED\\n            self._condition.notify_all()\\n\\n        self._invoke_callbacks()\\n        return True\\n\\n    def cancelled(self):\\n        \\\"\\\"\\\"Return True if the future was cancelled.\\\"\\\"\\\"\\n        with self._condition:\\n            return self._state in [CANCELLED, CANCELLED_AND_NOTIFIED]\\n\\n    def running(self):\\n        \\\"\\\"\\\"Return True if the future is currently executing.\\\"\\\"\\\"\\n        with self._condition:\\n            return self._state == RUNNING\\n\\n    def done(self):\\n        \\\"\\\"\\\"Return True if the future was cancelled or finished executing.\\\"\\\"\\\"\\n        with self._condition:\\n            return self._state in [CANCELLED, CANCELLED_AND_NOTIFIED, FINISHED]\\n\\n    def __get_result(self):\\n        if self._exception:\\n            try:\\n                raise self._exception\\n            finally:\\n                # Break a reference cycle with the exception in self._exception\\n                self = None\\n        else:\\n            return self._result\\n\\n    def add_done_callback(self, fn):\\n        \\\"\\\"\\\"Attaches a callable that will be called when the future finishes.\\n\\n        Args:\\n            fn: A callable that will be called with this future as its only\\n                argument when the future completes or is cancelled. The callable\\n                will always be called by a thread in the same process in which\\n                it was added. If the future has already completed or been\\n                cancelled then the callable will be called immediately. These\\n                callables are called in the order that they were added.\\n        \\\"\\\"\\\"\\n        with self._condition:\\n            if self._state not in [CANCELLED, CANCELLED_AND_NOTIFIED, FINISHED]:\\n                self._done_callbacks.append(fn)\\n                return\\n        try:\\n            fn(self)\\n        except Exception:\\n            LOGGER.exception('exception calling callback for %r', self)\\n\\n    def result(self, timeout=None):\\n        \\\"\\\"\\\"Return the result of the call that the future represents.\\n\\n        Args:\\n            timeout: The number of seconds to wait for the result if the future\\n                isn't done. If None, then there is no limit on the wait time.\\n\\n        Returns:\\n            The result of the call that the future represents.\\n\\n        Raises:\\n            CancelledError: If the future was cancelled.\\n            TimeoutError: If the future didn't finish executing before the given\\n                timeout.\\n            Exception: If the call raised then that exception will be raised.\\n        \\\"\\\"\\\"\\n        try:\\n            with self._condition:\\n                if self._state in [CANCELLED, CANCELLED_AND_NOTIFIED]:\\n                    raise CancelledError()\\n                elif self._state == FINISHED:\\n                    return self.__get_result()\\n\\n                self._condition.wait(timeout)\\n\\n                if self._state in [CANCELLED, CANCELLED_AND_NOTIFIED]:\\n                    raise CancelledError()\\n                elif self._state == FINISHED:\\n                    return self.__get_result()\\n                else:\\n                    raise TimeoutError()\\n        finally:\\n            # Break a reference cycle with the exception in self._exception\\n            self = None\\n\\n    def exception(self, timeout=None):\\n        \\\"\\\"\\\"Return the exception raised by the call that the future represents.\\n\\n        Args:\\n            timeout: The number of seconds to wait for the exception if the\\n                future isn't done. If None, then there is no limit on the wait\\n                time.\\n\\n        Returns:\\n            The exception raised by the call that the future represents or None\\n            if the call completed without raising.\\n\\n        Raises:\\n            CancelledError: If the future was cancelled.\\n            TimeoutError: If the future didn't finish executing before the given\\n                timeout.\\n        \\\"\\\"\\\"\\n\\n        with self._condition:\\n            if self._state in [CANCELLED, CANCELLED_AND_NOTIFIED]:\\n                raise CancelledError()\\n            elif self._state == FINISHED:\\n                return self._exception\\n\\n            self._condition.wait(timeout)\\n\\n            if self._state in [CANCELLED, CANCELLED_AND_NOTIFIED]:\\n                raise CancelledError()\\n            elif self._state == FINISHED:\\n                return self._exception\\n            else:\\n                raise TimeoutError()\\n\\n    # The following methods should only be used by Executors and in tests.\\n    def set_running_or_notify_cancel(self):\\n        \\\"\\\"\\\"Mark the future as running or process any cancel notifications.\\n\\n        Should only be used by Executor implementations and unit tests.\\n\\n        If the future has been cancelled (cancel() was called and returned\\n        True) then any threads waiting on the future completing (though calls\\n        to as_completed() or wait()) are notified and False is returned.\\n\\n        If the future was not cancelled then it is put in the running state\\n        (future calls to running() will return True) and True is returned.\\n\\n        This method should be called by Executor implementations before\\n        executing the work associated with this future. If this method returns\\n        False then the work should not be executed.\\n\\n        Returns:\\n            False if the Future was cancelled, True otherwise.\\n\\n        Raises:\\n            RuntimeError: if this method was already called or if set_result()\\n                or set_exception() was called.\\n        \\\"\\\"\\\"\\n        with self._condition:\\n            if self._state == CANCELLED:\\n                self._state = CANCELLED_AND_NOTIFIED\\n                for waiter in self._waiters:\\n                    waiter.add_cancelled(self)\\n                # self._condition.notify_all() is not necessary because\\n                # self.cancel() triggers a notification.\\n                return False\\n            elif self._state == PENDING:\\n                self._state = RUNNING\\n                return True\\n            else:\\n                LOGGER.critical('Future %s in unexpected state: %s',\\n                                id(self),\\n                                self._state)\\n                raise RuntimeError('Future in unexpected state')\\n\\n    def set_result(self, result):\\n        \\\"\\\"\\\"Sets the return value of work associated with the future.\\n\\n        Should only be used by Executor implementations and unit tests.\\n        \\\"\\\"\\\"\\n        with self._condition:\\n            if self._state in {CANCELLED, CANCELLED_AND_NOTIFIED, FINISHED}:\\n                raise InvalidStateError('{}: {!r}'.format(self._state, self))\\n            self._result = result\\n            self._state = FINISHED\\n            for waiter in self._waiters:\\n                waiter.add_result(self)\\n            self._condition.notify_all()\\n        self._invoke_callbacks()\\n\\n    def set_exception(self, exception):\\n        \\\"\\\"\\\"Sets the result of the future as being the given exception.\\n\\n        Should only be used by Executor implementations and unit tests.\\n        \\\"\\\"\\\"\\n        with self._condition:\\n            if self._state in {CANCELLED, CANCELLED_AND_NOTIFIED, FINISHED}:\\n                raise InvalidStateError('{}: {!r}'.format(self._state, self))\\n            self._exception = exception\\n            self._state = FINISHED\\n            for waiter in self._waiters:\\n                waiter.add_exception(self)\\n            self._condition.notify_all()\\n        self._invoke_callbacks()\\n\\n    __class_getitem__ = classmethod(types.GenericAlias)\\n\\nclass Executor(object):\\n    \\\"\\\"\\\"This is an abstract base class for concrete asynchronous executors.\\\"\\\"\\\"\\n\\n    def submit(self, fn, /, *args, **kwargs):\\n        \\\"\\\"\\\"Submits a callable to be executed with the given arguments.\\n\\n        Schedules the callable to be executed as fn(*args, **kwargs) and returns\\n        a Future instance representing the execution of the callable.\\n\\n        Returns:\\n            A Future representing the given call.\\n        \\\"\\\"\\\"\\n        raise NotImplementedError()\\n\\n    def map(self, fn, *iterables, timeout=None, chunksize=1):\\n        \\\"\\\"\\\"Returns an iterator equivalent to map(fn, iter).\\n\\n        Args:\\n            fn: A callable that will take as many arguments as there are\\n                passed iterables.\\n            timeout: The maximum number of seconds to wait. If None, then there\\n                is no limit on the wait time.\\n            chunksize: The size of the chunks the iterable will be broken into\\n                before being passed to a child process. This argument is only\\n                used by ProcessPoolExecutor; it is ignored by\\n                ThreadPoolExecutor.\\n\\n        Returns:\\n            An iterator equivalent to: map(func, *iterables) but the calls may\\n            be evaluated out-of-order.\\n\\n        Raises:\\n            TimeoutError: If the entire result iterator could not be generated\\n                before the given timeout.\\n            Exception: If fn(*args) raises for any values.\\n        \\\"\\\"\\\"\\n        if timeout is not None:\\n            end_time = timeout + time.monotonic()\\n\\n        fs = [self.submit(fn, *args) for args in zip(*iterables)]\\n\\n        # Yield must be hidden in closure so that the futures are submitted\\n        # before the first iterator value is required.\\n        def result_iterator():\\n            try:\\n                # reverse to keep finishing order\\n                fs.reverse()\\n                while fs:\\n                    # Careful not to keep a reference to the popped future\\n                    if timeout is None:\\n                        yield fs.pop().result()\\n                    else:\\n                        yield fs.pop().result(end_time - time.monotonic())\\n            finally:\\n                for future in fs:\\n                    future.cancel()\\n        return result_iterator()\\n\\n    def shutdown(self, wait=True, *, cancel_futures=False):\\n        \\\"\\\"\\\"Clean-up the resources associated with the Executor.\\n\\n        It is safe to call this method several times. Otherwise, no other\\n        methods can be called after this one.\\n\\n        Args:\\n            wait: If True then shutdown will not return until all running\\n                futures have finished executing and the resources used by the\\n                executor have been reclaimed.\\n            cancel_futures: If True then shutdown will cancel all pending\\n                futures. Futures that are completed or running will not be\\n                cancelled.\\n        \\\"\\\"\\\"\\n        pass\\n\\n    def __enter__(self):\\n        return self\\n\\n    def __exit__(self, exc_type, exc_val, exc_tb):\\n        self.shutdown(wait=True)\\n        return False\\n\\n\\nclass BrokenExecutor(RuntimeError):\\n    \\\"\\\"\\\"\\n    Raised when a executor has become non-functional after a severe failure.\\n    \\\"\\\"\\\"\\n\", 644], \"/Users/sangcho/work/ray/a.py\": [\"import time\\nimport asyncio\\nimport logging\\nfrom typing import Tuple\\nimport ray\\n\\nimport aiohttp\\nimport click\\nfrom starlette.responses import StreamingResponse\\n\\nfrom ray import serve\\nfrom ray.serve._private.benchmarks.common import run_throughput_benchmark\\nfrom ray.serve.handle import DeploymentHandle, RayServeHandle\\n\\nBATCH_SIZE = 1\\n\\n\\n@ray.remote(num_cpus=0)\\nclass Downstream:\\n    def __init__(self, tokens_per_request: int):\\n        logging.getLogger(\\\"ray.serve\\\").setLevel(logging.WARNING)\\n\\n        self._tokens_per_request = tokens_per_request\\n\\n    def stream(self, i):\\n        batches = []\\n        s = time.time()\\n        for i in range(self._tokens_per_request):\\n            batches.append(\\\"hi\\\")\\n            if i % BATCH_SIZE == 0:\\n                ss = time.time()\\n                yield batches\\n                print(f\\\"yield takes {(time.time() - ss) * 1000} ms\\\")\\n                batches = []\\n        e = (time.time() - s)\\n        print(f\\\"data generation takes {e * 1000} ms. throughput: {self._tokens_per_request / e} / s\\\")\\n\\n\\n@ray.remote(num_cpus=0)\\nclass Intermediate:\\n    def __init__(self, downstream):\\n        self._h = downstream\\n\\n    async def stream(self, i):\\n        gen = self._h.stream.options(\\n            num_returns=\\\"streaming\\\"\\n        ).remote(i)\\n        print(\\\"wait until downstream is finished\\\")\\n        # await asyncio.sleep(10)\\n        s = time.time()\\n        total_elapsed = 0\\n        total_tokens = 0\\n        while True:\\n            try:\\n                anext_time = time.time()\\n                from viztracer import VizTracer\\n                with VizTracer(output_file=\\\"/tmp/a.json\\\") as tracer:\\n                    tokens = await gen.__anext__()\\n                    print(f\\\"id {i} anext took {(time.time() - anext_time) * 1000} ms\\\")\\n                    token_time = time.time()\\n                    tokens = await tokens\\n                    print(f\\\"id {i} token_time took {(time.time() - token_time) * 1000} ms\\\")\\n                # await gen._obj_ref_gen._generator_ref\\n                ss = time.time()\\n                for token in tokens:\\n                    total_tokens += 1\\n                    yield token\\n                total_elapsed += (time.time() - ss) * 1000\\n            except StopAsyncIteration:\\n                break\\n        e = (time.time() - s)\\n        print(f\\\"id {i} yield takes {e * 1000} ms, inner elapse: {total_elapsed}, throughput: {total_tokens / e} / s\\\")\\n\\n\\nasync def _consume_single_stream(h, i):\\n    # async for line in h.stream.options(num_returns=\\\"streaming\\\").remote(i):\\n    #     # print(ray.get(line))\\n    #     pass\\n    gen = h.stream.options(num_returns=\\\"streaming\\\").remote(i)\\n    await gen._generator_ref\\n\\n\\nasync def run_benchmark(\\n    h,\\n    tokens_per_request: int,\\n    batch_size: int,\\n    num_trials: int,\\n    trial_runtime: float,\\n) -> Tuple[float, float]:\\n    async def _do_single_batch():\\n        s = time.time()\\n        await asyncio.gather(*[_consume_single_stream(h, i) for i in range(batch_size)])\\n        print(f\\\"Took {(batch_size * 1000) / ((time.time() - s))} tokens/s to iterate at the driver.\\\")\\n\\n    return await run_throughput_benchmark(\\n        fn=_do_single_batch,\\n        multiplier=batch_size * tokens_per_request,\\n        num_trials=num_trials,\\n        trial_runtime=trial_runtime,\\n    )\\n\\n\\n@click.command(help=\\\"Benchmark streaming HTTP throughput.\\\")\\n@click.option(\\n    \\\"--tokens-per-request\\\",\\n    type=int,\\n    default=1000,\\n    help=\\\"Number of requests to send to downstream deployment in each trial.\\\",\\n)\\n@click.option(\\n    \\\"--batch-size\\\",\\n    type=int,\\n    default=10,\\n    help=\\\"Number of requests to send to downstream deployment in each trial.\\\",\\n)\\n@click.option(\\n    \\\"--num-replicas\\\",\\n    type=int,\\n    default=1,\\n    help=\\\"Number of replicas in the downstream deployment.\\\",\\n)\\n@click.option(\\n    \\\"--num-trials\\\",\\n    type=int,\\n    default=5,\\n    help=\\\"Number of trials of the benchmark to run.\\\",\\n)\\n@click.option(\\n    \\\"--trial-runtime\\\",\\n    type=int,\\n    default=1,\\n    help=\\\"Duration to run each trial of the benchmark for (seconds).\\\",\\n)\\n@click.option(\\n    \\\"--use-intermediate-deployment\\\",\\n    is_flag=True,\\n    default=False,\\n    help=\\\"Whether to run an intermediate deployment proxying the requests.\\\",\\n)\\ndef main(\\n    tokens_per_request: int,\\n    batch_size: int,\\n    num_replicas: int,\\n    num_trials: int,\\n    trial_runtime: float,\\n    use_intermediate_deployment: bool,\\n):\\n    app = Downstream.remote(tokens_per_request)\\n    if use_intermediate_deployment:\\n        app = Intermediate.remote(app)\\n\\n    mean, stddev = asyncio.new_event_loop().run_until_complete(\\n        run_benchmark(\\n            app,\\n            tokens_per_request,\\n            batch_size,\\n            num_trials,\\n            trial_runtime,\\n        )\\n    )\\n    print(\\n        \\\"HTTP streaming throughput {}: {} +- {} tokens/s\\\".format(\\n            f\\\"(num_replicas={num_replicas}, \\\"\\n            f\\\"tokens_per_request={tokens_per_request}, \\\"\\n            f\\\"batch_size={batch_size}, \\\"\\n            f\\\"use_intermediate_deployment={use_intermediate_deployment})\\\",\\n            mean,\\n            stddev,\\n        )\\n    )\\n\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\n\", 174]}, \"functions\": {\"connected (/Users/sangcho/work/ray/python/ray/_private/worker.py:471)\": [\"/Users/sangcho/work/ray/python/ray/_private/worker.py\", 471], \"check_connected (/Users/sangcho/work/ray/python/ray/_private/worker.py:631)\": [\"/Users/sangcho/work/ray/python/ray/_private/worker.py\", 631], \"add (/Users/sangcho/anaconda3/envs/core/lib/python3.9/_weakrefset.py:86)\": [\"/Users/sangcho/anaconda3/envs/core/lib/python3.9/_weakrefset.py\", 86], \"_set_task_name (/Users/sangcho/anaconda3/envs/core/lib/python3.9/asyncio/tasks.py:89)\": [\"/Users/sangcho/anaconda3/envs/core/lib/python3.9/asyncio/tasks.py\", 89], \"create_task (/Users/sangcho/anaconda3/envs/core/lib/python3.9/asyncio/tasks.py:355)\": [\"/Users/sangcho/anaconda3/envs/core/lib/python3.9/asyncio/tasks.py\", 355], \"write (/Users/sangcho/work/ray/python/ray/_private/utils.py:405)\": [\"/Users/sangcho/work/ray/python/ray/_private/utils.py\", 405], \"isfuture (/Users/sangcho/anaconda3/envs/core/lib/python3.9/asyncio/base_futures.py:14)\": [\"/Users/sangcho/anaconda3/envs/core/lib/python3.9/asyncio/base_futures.py\", 14], \"__instancecheck__ (/Users/sangcho/anaconda3/envs/core/lib/python3.9/abc.py:117)\": [\"/Users/sangcho/anaconda3/envs/core/lib/python3.9/abc.py\", 117], \"iscoroutine (/Users/sangcho/anaconda3/envs/core/lib/python3.9/asyncio/coroutines.py:177)\": [\"/Users/sangcho/anaconda3/envs/core/lib/python3.9/asyncio/coroutines.py\", 177], \"<genexpr> (/Users/sangcho/anaconda3/envs/core/lib/python3.9/asyncio/tasks.py:405)\": [\"/Users/sangcho/anaconda3/envs/core/lib/python3.9/asyncio/tasks.py\", 405], \"_get_loop (/Users/sangcho/anaconda3/envs/core/lib/python3.9/asyncio/futures.py:296)\": [\"/Users/sangcho/anaconda3/envs/core/lib/python3.9/asyncio/futures.py\", 296], \"ensure_future (/Users/sangcho/anaconda3/envs/core/lib/python3.9/asyncio/tasks.py:657)\": [\"/Users/sangcho/anaconda3/envs/core/lib/python3.9/asyncio/tasks.py\", 657], \"<setcomp> (/Users/sangcho/anaconda3/envs/core/lib/python3.9/asyncio/tasks.py:411)\": [\"/Users/sangcho/anaconda3/envs/core/lib/python3.9/asyncio/tasks.py\", 411], \"_wait (/Users/sangcho/anaconda3/envs/core/lib/python3.9/asyncio/tasks.py:497)\": [\"/Users/sangcho/anaconda3/envs/core/lib/python3.9/asyncio/tasks.py\", 497], \"wait (/Users/sangcho/anaconda3/envs/core/lib/python3.9/asyncio/tasks.py:373)\": [\"/Users/sangcho/anaconda3/envs/core/lib/python3.9/asyncio/tasks.py\", 373], \"RLock (/Users/sangcho/anaconda3/envs/core/lib/python3.9/threading.py:82)\": [\"/Users/sangcho/anaconda3/envs/core/lib/python3.9/threading.py\", 82], \"__init__ (/Users/sangcho/anaconda3/envs/core/lib/python3.9/threading.py:228)\": [\"/Users/sangcho/anaconda3/envs/core/lib/python3.9/threading.py\", 228], \"__init__ (/Users/sangcho/anaconda3/envs/core/lib/python3.9/concurrent/futures/_base.py:318)\": [\"/Users/sangcho/anaconda3/envs/core/lib/python3.9/concurrent/futures/_base.py\", 318], \"__enter__ (/Users/sangcho/anaconda3/envs/core/lib/python3.9/threading.py:256)\": [\"/Users/sangcho/anaconda3/envs/core/lib/python3.9/threading.py\", 256], \"__exit__ (/Users/sangcho/anaconda3/envs/core/lib/python3.9/threading.py:259)\": [\"/Users/sangcho/anaconda3/envs/core/lib/python3.9/threading.py\", 259], \"add_done_callback (/Users/sangcho/anaconda3/envs/core/lib/python3.9/concurrent/futures/_base.py:398)\": [\"/Users/sangcho/anaconda3/envs/core/lib/python3.9/concurrent/futures/_base.py\", 398], \"_chain_future (/Users/sangcho/anaconda3/envs/core/lib/python3.9/asyncio/futures.py:362)\": [\"/Users/sangcho/anaconda3/envs/core/lib/python3.9/asyncio/futures.py\", 362], \"wrap_future (/Users/sangcho/anaconda3/envs/core/lib/python3.9/asyncio/futures.py:404)\": [\"/Users/sangcho/anaconda3/envs/core/lib/python3.9/asyncio/futures.py\", 404], \"done (/Users/sangcho/anaconda3/envs/core/lib/python3.9/concurrent/futures/_base.py:383)\": [\"/Users/sangcho/anaconda3/envs/core/lib/python3.9/concurrent/futures/_base.py\", 383], \"cancelled (/Users/sangcho/anaconda3/envs/core/lib/python3.9/concurrent/futures/_base.py:373)\": [\"/Users/sangcho/anaconda3/envs/core/lib/python3.9/concurrent/futures/_base.py\", 373], \"exception (/Users/sangcho/anaconda3/envs/core/lib/python3.9/concurrent/futures/_base.py:453)\": [\"/Users/sangcho/anaconda3/envs/core/lib/python3.9/concurrent/futures/_base.py\", 453], \"__get_result (/Users/sangcho/anaconda3/envs/core/lib/python3.9/concurrent/futures/_base.py:388)\": [\"/Users/sangcho/anaconda3/envs/core/lib/python3.9/concurrent/futures/_base.py\", 388], \"result (/Users/sangcho/anaconda3/envs/core/lib/python3.9/concurrent/futures/_base.py:418)\": [\"/Users/sangcho/anaconda3/envs/core/lib/python3.9/concurrent/futures/_base.py\", 418], \"_copy_future_state (/Users/sangcho/anaconda3/envs/core/lib/python3.9/asyncio/futures.py:342)\": [\"/Users/sangcho/anaconda3/envs/core/lib/python3.9/asyncio/futures.py\", 342], \"_set_state (/Users/sangcho/anaconda3/envs/core/lib/python3.9/asyncio/futures.py:378)\": [\"/Users/sangcho/anaconda3/envs/core/lib/python3.9/asyncio/futures.py\", 378], \"_call_check_cancel (/Users/sangcho/anaconda3/envs/core/lib/python3.9/asyncio/futures.py:384)\": [\"/Users/sangcho/anaconda3/envs/core/lib/python3.9/asyncio/futures.py\", 384], \"_on_completion (/Users/sangcho/anaconda3/envs/core/lib/python3.9/asyncio/tasks.py:509)\": [\"/Users/sangcho/anaconda3/envs/core/lib/python3.9/asyncio/tasks.py\", 509], \"_remove (/Users/sangcho/anaconda3/envs/core/lib/python3.9/_weakrefset.py:39)\": [\"/Users/sangcho/anaconda3/envs/core/lib/python3.9/_weakrefset.py\", 39], \"stream (/Users/sangcho/work/ray/a.py:44)\": [\"/Users/sangcho/work/ray/a.py\", 44]}}}", 0], "/Users/sangcho/anaconda3/envs/core/lib/python3.9/multiprocessing/util.py": ["#\n# Module providing various facilities to other parts of the package\n#\n# multiprocessing/util.py\n#\n# Copyright (c) 2006-2008, R Oudkerk\n# Licensed to PSF under a Contributor Agreement.\n#\n\nimport os\nimport itertools\nimport sys\nimport weakref\nimport atexit\nimport threading        # we want threading to install it's\n                        # cleanup function before multiprocessing does\nfrom subprocess import _args_from_interpreter_flags\n\nfrom . import process\n\n__all__ = [\n    'sub_debug', 'debug', 'info', 'sub_warning', 'get_logger',\n    'log_to_stderr', 'get_temp_dir', 'register_after_fork',\n    'is_exiting', 'Finalize', 'ForkAwareThreadLock', 'ForkAwareLocal',\n    'close_all_fds_except', 'SUBDEBUG', 'SUBWARNING',\n    ]\n\n#\n# Logging\n#\n\nNOTSET = 0\nSUBDEBUG = 5\nDEBUG = 10\nINFO = 20\nSUBWARNING = 25\n\nLOGGER_NAME = 'multiprocessing'\nDEFAULT_LOGGING_FORMAT = '[%(levelname)s/%(processName)s] %(message)s'\n\n_logger = None\n_log_to_stderr = False\n\ndef sub_debug(msg, *args):\n    if _logger:\n        _logger.log(SUBDEBUG, msg, *args)\n\ndef debug(msg, *args):\n    if _logger:\n        _logger.log(DEBUG, msg, *args)\n\ndef info(msg, *args):\n    if _logger:\n        _logger.log(INFO, msg, *args)\n\ndef sub_warning(msg, *args):\n    if _logger:\n        _logger.log(SUBWARNING, msg, *args)\n\ndef get_logger():\n    '''\n    Returns logger used by multiprocessing\n    '''\n    global _logger\n    import logging\n\n    logging._acquireLock()\n    try:\n        if not _logger:\n\n            _logger = logging.getLogger(LOGGER_NAME)\n            _logger.propagate = 0\n\n            # XXX multiprocessing should cleanup before logging\n            if hasattr(atexit, 'unregister'):\n                atexit.unregister(_exit_function)\n                atexit.register(_exit_function)\n            else:\n                atexit._exithandlers.remove((_exit_function, (), {}))\n                atexit._exithandlers.append((_exit_function, (), {}))\n\n    finally:\n        logging._releaseLock()\n\n    return _logger\n\ndef log_to_stderr(level=None):\n    '''\n    Turn on logging and add a handler which prints to stderr\n    '''\n    global _log_to_stderr\n    import logging\n\n    logger = get_logger()\n    formatter = logging.Formatter(DEFAULT_LOGGING_FORMAT)\n    handler = logging.StreamHandler()\n    handler.setFormatter(formatter)\n    logger.addHandler(handler)\n\n    if level:\n        logger.setLevel(level)\n    _log_to_stderr = True\n    return _logger\n\n\n# Abstract socket support\n\ndef _platform_supports_abstract_sockets():\n    if sys.platform == \"linux\":\n        return True\n    if hasattr(sys, 'getandroidapilevel'):\n        return True\n    return False\n\n\ndef is_abstract_socket_namespace(address):\n    if not address:\n        return False\n    if isinstance(address, bytes):\n        return address[0] == 0\n    elif isinstance(address, str):\n        return address[0] == \"\\0\"\n    raise TypeError(f'address type of {address!r} unrecognized')\n\n\nabstract_sockets_supported = _platform_supports_abstract_sockets()\n\n#\n# Function returning a temp directory which will be removed on exit\n#\n\ndef _remove_temp_dir(rmtree, tempdir):\n    rmtree(tempdir)\n\n    current_process = process.current_process()\n    # current_process() can be None if the finalizer is called\n    # late during Python finalization\n    if current_process is not None:\n        current_process._config['tempdir'] = None\n\ndef get_temp_dir():\n    # get name of a temp directory which will be automatically cleaned up\n    tempdir = process.current_process()._config.get('tempdir')\n    if tempdir is None:\n        import shutil, tempfile\n        tempdir = tempfile.mkdtemp(prefix='pymp-')\n        info('created temp directory %s', tempdir)\n        # keep a strong reference to shutil.rmtree(), since the finalizer\n        # can be called late during Python shutdown\n        Finalize(None, _remove_temp_dir, args=(shutil.rmtree, tempdir),\n                 exitpriority=-100)\n        process.current_process()._config['tempdir'] = tempdir\n    return tempdir\n\n#\n# Support for reinitialization of objects when bootstrapping a child process\n#\n\n_afterfork_registry = weakref.WeakValueDictionary()\n_afterfork_counter = itertools.count()\n\ndef _run_after_forkers():\n    items = list(_afterfork_registry.items())\n    items.sort()\n    for (index, ident, func), obj in items:\n        try:\n            func(obj)\n        except Exception as e:\n            info('after forker raised exception %s', e)\n\ndef register_after_fork(obj, func):\n    _afterfork_registry[(next(_afterfork_counter), id(obj), func)] = obj\n\n#\n# Finalization using weakrefs\n#\n\n_finalizer_registry = {}\n_finalizer_counter = itertools.count()\n\n\nclass Finalize(object):\n    '''\n    Class which supports object finalization using weakrefs\n    '''\n    def __init__(self, obj, callback, args=(), kwargs=None, exitpriority=None):\n        if (exitpriority is not None) and not isinstance(exitpriority,int):\n            raise TypeError(\n                \"Exitpriority ({0!r}) must be None or int, not {1!s}\".format(\n                    exitpriority, type(exitpriority)))\n\n        if obj is not None:\n            self._weakref = weakref.ref(obj, self)\n        elif exitpriority is None:\n            raise ValueError(\"Without object, exitpriority cannot be None\")\n\n        self._callback = callback\n        self._args = args\n        self._kwargs = kwargs or {}\n        self._key = (exitpriority, next(_finalizer_counter))\n        self._pid = os.getpid()\n\n        _finalizer_registry[self._key] = self\n\n    def __call__(self, wr=None,\n                 # Need to bind these locally because the globals can have\n                 # been cleared at shutdown\n                 _finalizer_registry=_finalizer_registry,\n                 sub_debug=sub_debug, getpid=os.getpid):\n        '''\n        Run the callback unless it has already been called or cancelled\n        '''\n        try:\n            del _finalizer_registry[self._key]\n        except KeyError:\n            sub_debug('finalizer no longer registered')\n        else:\n            if self._pid != getpid():\n                sub_debug('finalizer ignored because different process')\n                res = None\n            else:\n                sub_debug('finalizer calling %s with args %s and kwargs %s',\n                          self._callback, self._args, self._kwargs)\n                res = self._callback(*self._args, **self._kwargs)\n            self._weakref = self._callback = self._args = \\\n                            self._kwargs = self._key = None\n            return res\n\n    def cancel(self):\n        '''\n        Cancel finalization of the object\n        '''\n        try:\n            del _finalizer_registry[self._key]\n        except KeyError:\n            pass\n        else:\n            self._weakref = self._callback = self._args = \\\n                            self._kwargs = self._key = None\n\n    def still_active(self):\n        '''\n        Return whether this finalizer is still waiting to invoke callback\n        '''\n        return self._key in _finalizer_registry\n\n    def __repr__(self):\n        try:\n            obj = self._weakref()\n        except (AttributeError, TypeError):\n            obj = None\n\n        if obj is None:\n            return '<%s object, dead>' % self.__class__.__name__\n\n        x = '<%s object, callback=%s' % (\n                self.__class__.__name__,\n                getattr(self._callback, '__name__', self._callback))\n        if self._args:\n            x += ', args=' + str(self._args)\n        if self._kwargs:\n            x += ', kwargs=' + str(self._kwargs)\n        if self._key[0] is not None:\n            x += ', exitpriority=' + str(self._key[0])\n        return x + '>'\n\n\ndef _run_finalizers(minpriority=None):\n    '''\n    Run all finalizers whose exit priority is not None and at least minpriority\n\n    Finalizers with highest priority are called first; finalizers with\n    the same priority will be called in reverse order of creation.\n    '''\n    if _finalizer_registry is None:\n        # This function may be called after this module's globals are\n        # destroyed.  See the _exit_function function in this module for more\n        # notes.\n        return\n\n    if minpriority is None:\n        f = lambda p : p[0] is not None\n    else:\n        f = lambda p : p[0] is not None and p[0] >= minpriority\n\n    # Careful: _finalizer_registry may be mutated while this function\n    # is running (either by a GC run or by another thread).\n\n    # list(_finalizer_registry) should be atomic, while\n    # list(_finalizer_registry.items()) is not.\n    keys = [key for key in list(_finalizer_registry) if f(key)]\n    keys.sort(reverse=True)\n\n    for key in keys:\n        finalizer = _finalizer_registry.get(key)\n        # key may have been removed from the registry\n        if finalizer is not None:\n            sub_debug('calling %s', finalizer)\n            try:\n                finalizer()\n            except Exception:\n                import traceback\n                traceback.print_exc()\n\n    if minpriority is None:\n        _finalizer_registry.clear()\n\n#\n# Clean up on exit\n#\n\ndef is_exiting():\n    '''\n    Returns true if the process is shutting down\n    '''\n    return _exiting or _exiting is None\n\n_exiting = False\n\ndef _exit_function(info=info, debug=debug, _run_finalizers=_run_finalizers,\n                   active_children=process.active_children,\n                   current_process=process.current_process):\n    # We hold on to references to functions in the arglist due to the\n    # situation described below, where this function is called after this\n    # module's globals are destroyed.\n\n    global _exiting\n\n    if not _exiting:\n        _exiting = True\n\n        info('process shutting down')\n        debug('running all \"atexit\" finalizers with priority >= 0')\n        _run_finalizers(0)\n\n        if current_process() is not None:\n            # We check if the current process is None here because if\n            # it's None, any call to ``active_children()`` will raise\n            # an AttributeError (active_children winds up trying to\n            # get attributes from util._current_process).  One\n            # situation where this can happen is if someone has\n            # manipulated sys.modules, causing this module to be\n            # garbage collected.  The destructor for the module type\n            # then replaces all values in the module dict with None.\n            # For instance, after setuptools runs a test it replaces\n            # sys.modules with a copy created earlier.  See issues\n            # #9775 and #15881.  Also related: #4106, #9205, and\n            # #9207.\n\n            for p in active_children():\n                if p.daemon:\n                    info('calling terminate() for daemon %s', p.name)\n                    p._popen.terminate()\n\n            for p in active_children():\n                info('calling join() for process %s', p.name)\n                p.join()\n\n        debug('running the remaining \"atexit\" finalizers')\n        _run_finalizers()\n\natexit.register(_exit_function)\n\n#\n# Some fork aware types\n#\n\nclass ForkAwareThreadLock(object):\n    def __init__(self):\n        self._lock = threading.Lock()\n        self.acquire = self._lock.acquire\n        self.release = self._lock.release\n        register_after_fork(self, ForkAwareThreadLock._at_fork_reinit)\n\n    def _at_fork_reinit(self):\n        self._lock._at_fork_reinit()\n\n    def __enter__(self):\n        return self._lock.__enter__()\n\n    def __exit__(self, *args):\n        return self._lock.__exit__(*args)\n\n\nclass ForkAwareLocal(threading.local):\n    def __init__(self):\n        register_after_fork(self, lambda obj : obj.__dict__.clear())\n    def __reduce__(self):\n        return type(self), ()\n\n#\n# Close fds except those specified\n#\n\ntry:\n    MAXFD = os.sysconf(\"SC_OPEN_MAX\")\nexcept Exception:\n    MAXFD = 256\n\ndef close_all_fds_except(fds):\n    fds = list(fds) + [-1, MAXFD]\n    fds.sort()\n    assert fds[-1] == MAXFD, 'fd too large'\n    for i in range(len(fds) - 1):\n        os.closerange(fds[i]+1, fds[i+1])\n#\n# Close sys.stdin and replace stdin with os.devnull\n#\n\ndef _close_stdin():\n    if sys.stdin is None:\n        return\n\n    try:\n        sys.stdin.close()\n    except (OSError, ValueError):\n        pass\n\n    try:\n        fd = os.open(os.devnull, os.O_RDONLY)\n        try:\n            sys.stdin = open(fd, closefd=False)\n        except:\n            os.close(fd)\n            raise\n    except (OSError, ValueError):\n        pass\n\n#\n# Flush standard streams, if any\n#\n\ndef _flush_std_streams():\n    try:\n        sys.stdout.flush()\n    except (AttributeError, ValueError):\n        pass\n    try:\n        sys.stderr.flush()\n    except (AttributeError, ValueError):\n        pass\n\n#\n# Start a program with only specified fds kept open\n#\n\ndef spawnv_passfds(path, args, passfds):\n    import _posixsubprocess\n    passfds = tuple(sorted(map(int, passfds)))\n    errpipe_read, errpipe_write = os.pipe()\n    try:\n        return _posixsubprocess.fork_exec(\n            args, [os.fsencode(path)], True, passfds, None, None,\n            -1, -1, -1, -1, -1, -1, errpipe_read, errpipe_write,\n            False, False, None, None, None, -1, None)\n    finally:\n        os.close(errpipe_read)\n        os.close(errpipe_write)\n\n\ndef close_fds(*fds):\n    \"\"\"Close each file descriptor given as an argument\"\"\"\n    for fd in fds:\n        os.close(fd)\n\n\ndef _cleanup_tests():\n    \"\"\"Cleanup multiprocessing resources when multiprocessing tests\n    completed.\"\"\"\n\n    from test import support\n\n    # cleanup multiprocessing\n    process._cleanup()\n\n    # Stop the ForkServer process if it's running\n    from multiprocessing import forkserver\n    forkserver._forkserver._stop()\n\n    # Stop the ResourceTracker process if it's running\n    from multiprocessing import resource_tracker\n    resource_tracker._resource_tracker._stop()\n\n    # bpo-37421: Explicitly call _run_finalizers() to remove immediately\n    # temporary directories created by multiprocessing.util.get_temp_dir().\n    _run_finalizers()\n    support.gc_collect()\n\n    support.reap_children()\n", 489], "/Users/sangcho/anaconda3/envs/core/lib/python3.9/multiprocessing/process.py": ["#\n# Module providing the `Process` class which emulates `threading.Thread`\n#\n# multiprocessing/process.py\n#\n# Copyright (c) 2006-2008, R Oudkerk\n# Licensed to PSF under a Contributor Agreement.\n#\n\n__all__ = ['BaseProcess', 'current_process', 'active_children',\n           'parent_process']\n\n#\n# Imports\n#\n\nimport os\nimport sys\nimport signal\nimport itertools\nimport threading\nfrom _weakrefset import WeakSet\n\n#\n#\n#\n\ntry:\n    ORIGINAL_DIR = os.path.abspath(os.getcwd())\nexcept OSError:\n    ORIGINAL_DIR = None\n\n#\n# Public functions\n#\n\ndef current_process():\n    '''\n    Return process object representing the current process\n    '''\n    return _current_process\n\ndef active_children():\n    '''\n    Return list of process objects corresponding to live child processes\n    '''\n    _cleanup()\n    return list(_children)\n\n\ndef parent_process():\n    '''\n    Return process object representing the parent process\n    '''\n    return _parent_process\n\n#\n#\n#\n\ndef _cleanup():\n    # check for processes which have finished\n    for p in list(_children):\n        if p._popen.poll() is not None:\n            _children.discard(p)\n\n#\n# The `Process` class\n#\n\nclass BaseProcess(object):\n    '''\n    Process objects represent activity that is run in a separate process\n\n    The class is analogous to `threading.Thread`\n    '''\n    def _Popen(self):\n        raise NotImplementedError\n\n    def __init__(self, group=None, target=None, name=None, args=(), kwargs={},\n                 *, daemon=None):\n        assert group is None, 'group argument must be None for now'\n        count = next(_process_counter)\n        self._identity = _current_process._identity + (count,)\n        self._config = _current_process._config.copy()\n        self._parent_pid = os.getpid()\n        self._parent_name = _current_process.name\n        self._popen = None\n        self._closed = False\n        self._target = target\n        self._args = tuple(args)\n        self._kwargs = dict(kwargs)\n        self._name = name or type(self).__name__ + '-' + \\\n                     ':'.join(str(i) for i in self._identity)\n        if daemon is not None:\n            self.daemon = daemon\n        _dangling.add(self)\n\n    def _check_closed(self):\n        if self._closed:\n            raise ValueError(\"process object is closed\")\n\n    def run(self):\n        '''\n        Method to be run in sub-process; can be overridden in sub-class\n        '''\n        if self._target:\n            self._target(*self._args, **self._kwargs)\n\n    def start(self):\n        '''\n        Start child process\n        '''\n        self._check_closed()\n        assert self._popen is None, 'cannot start a process twice'\n        assert self._parent_pid == os.getpid(), \\\n               'can only start a process object created by current process'\n        assert not _current_process._config.get('daemon'), \\\n               'daemonic processes are not allowed to have children'\n        _cleanup()\n        self._popen = self._Popen(self)\n        self._sentinel = self._popen.sentinel\n        # Avoid a refcycle if the target function holds an indirect\n        # reference to the process object (see bpo-30775)\n        del self._target, self._args, self._kwargs\n        _children.add(self)\n\n    def terminate(self):\n        '''\n        Terminate process; sends SIGTERM signal or uses TerminateProcess()\n        '''\n        self._check_closed()\n        self._popen.terminate()\n\n    def kill(self):\n        '''\n        Terminate process; sends SIGKILL signal or uses TerminateProcess()\n        '''\n        self._check_closed()\n        self._popen.kill()\n\n    def join(self, timeout=None):\n        '''\n        Wait until child process terminates\n        '''\n        self._check_closed()\n        assert self._parent_pid == os.getpid(), 'can only join a child process'\n        assert self._popen is not None, 'can only join a started process'\n        res = self._popen.wait(timeout)\n        if res is not None:\n            _children.discard(self)\n\n    def is_alive(self):\n        '''\n        Return whether process is alive\n        '''\n        self._check_closed()\n        if self is _current_process:\n            return True\n        assert self._parent_pid == os.getpid(), 'can only test a child process'\n\n        if self._popen is None:\n            return False\n\n        returncode = self._popen.poll()\n        if returncode is None:\n            return True\n        else:\n            _children.discard(self)\n            return False\n\n    def close(self):\n        '''\n        Close the Process object.\n\n        This method releases resources held by the Process object.  It is\n        an error to call this method if the child process is still running.\n        '''\n        if self._popen is not None:\n            if self._popen.poll() is None:\n                raise ValueError(\"Cannot close a process while it is still running. \"\n                                 \"You should first call join() or terminate().\")\n            self._popen.close()\n            self._popen = None\n            del self._sentinel\n            _children.discard(self)\n        self._closed = True\n\n    @property\n    def name(self):\n        return self._name\n\n    @name.setter\n    def name(self, name):\n        assert isinstance(name, str), 'name must be a string'\n        self._name = name\n\n    @property\n    def daemon(self):\n        '''\n        Return whether process is a daemon\n        '''\n        return self._config.get('daemon', False)\n\n    @daemon.setter\n    def daemon(self, daemonic):\n        '''\n        Set whether process is a daemon\n        '''\n        assert self._popen is None, 'process has already started'\n        self._config['daemon'] = daemonic\n\n    @property\n    def authkey(self):\n        return self._config['authkey']\n\n    @authkey.setter\n    def authkey(self, authkey):\n        '''\n        Set authorization key of process\n        '''\n        self._config['authkey'] = AuthenticationString(authkey)\n\n    @property\n    def exitcode(self):\n        '''\n        Return exit code of process or `None` if it has yet to stop\n        '''\n        self._check_closed()\n        if self._popen is None:\n            return self._popen\n        return self._popen.poll()\n\n    @property\n    def ident(self):\n        '''\n        Return identifier (PID) of process or `None` if it has yet to start\n        '''\n        self._check_closed()\n        if self is _current_process:\n            return os.getpid()\n        else:\n            return self._popen and self._popen.pid\n\n    pid = ident\n\n    @property\n    def sentinel(self):\n        '''\n        Return a file descriptor (Unix) or handle (Windows) suitable for\n        waiting for process termination.\n        '''\n        self._check_closed()\n        try:\n            return self._sentinel\n        except AttributeError:\n            raise ValueError(\"process not started\") from None\n\n    def __repr__(self):\n        exitcode = None\n        if self is _current_process:\n            status = 'started'\n        elif self._closed:\n            status = 'closed'\n        elif self._parent_pid != os.getpid():\n            status = 'unknown'\n        elif self._popen is None:\n            status = 'initial'\n        else:\n            exitcode = self._popen.poll()\n            if exitcode is not None:\n                status = 'stopped'\n            else:\n                status = 'started'\n\n        info = [type(self).__name__, 'name=%r' % self._name]\n        if self._popen is not None:\n            info.append('pid=%s' % self._popen.pid)\n        info.append('parent=%s' % self._parent_pid)\n        info.append(status)\n        if exitcode is not None:\n            exitcode = _exitcode_to_name.get(exitcode, exitcode)\n            info.append('exitcode=%s' % exitcode)\n        if self.daemon:\n            info.append('daemon')\n        return '<%s>' % ' '.join(info)\n\n    ##\n\n    def _bootstrap(self, parent_sentinel=None):\n        from . import util, context\n        global _current_process, _parent_process, _process_counter, _children\n\n        try:\n            if self._start_method is not None:\n                context._force_start_method(self._start_method)\n            _process_counter = itertools.count(1)\n            _children = set()\n            util._close_stdin()\n            old_process = _current_process\n            _current_process = self\n            _parent_process = _ParentProcess(\n                self._parent_name, self._parent_pid, parent_sentinel)\n            if threading._HAVE_THREAD_NATIVE_ID:\n                threading.main_thread()._set_native_id()\n            try:\n                util._finalizer_registry.clear()\n                util._run_after_forkers()\n            finally:\n                # delay finalization of the old process object until after\n                # _run_after_forkers() is executed\n                del old_process\n            util.info('child process calling self.run()')\n            try:\n                self.run()\n                exitcode = 0\n            finally:\n                util._exit_function()\n        except SystemExit as e:\n            if e.code is None:\n                exitcode = 0\n            elif isinstance(e.code, int):\n                exitcode = e.code\n            else:\n                sys.stderr.write(str(e.code) + '\\n')\n                exitcode = 1\n        except:\n            exitcode = 1\n            import traceback\n            sys.stderr.write('Process %s:\\n' % self.name)\n            traceback.print_exc()\n        finally:\n            threading._shutdown()\n            util.info('process exiting with exitcode %d' % exitcode)\n            util._flush_std_streams()\n\n        return exitcode\n\n#\n# We subclass bytes to avoid accidental transmission of auth keys over network\n#\n\nclass AuthenticationString(bytes):\n    def __reduce__(self):\n        from .context import get_spawning_popen\n        if get_spawning_popen() is None:\n            raise TypeError(\n                'Pickling an AuthenticationString object is '\n                'disallowed for security reasons'\n                )\n        return AuthenticationString, (bytes(self),)\n\n\n#\n# Create object representing the parent process\n#\n\nclass _ParentProcess(BaseProcess):\n\n    def __init__(self, name, pid, sentinel):\n        self._identity = ()\n        self._name = name\n        self._pid = pid\n        self._parent_pid = None\n        self._popen = None\n        self._closed = False\n        self._sentinel = sentinel\n        self._config = {}\n\n    def is_alive(self):\n        from multiprocessing.connection import wait\n        return not wait([self._sentinel], timeout=0)\n\n    @property\n    def ident(self):\n        return self._pid\n\n    def join(self, timeout=None):\n        '''\n        Wait until parent process terminates\n        '''\n        from multiprocessing.connection import wait\n        wait([self._sentinel], timeout=timeout)\n\n    pid = ident\n\n#\n# Create object representing the main process\n#\n\nclass _MainProcess(BaseProcess):\n\n    def __init__(self):\n        self._identity = ()\n        self._name = 'MainProcess'\n        self._parent_pid = None\n        self._popen = None\n        self._closed = False\n        self._config = {'authkey': AuthenticationString(os.urandom(32)),\n                        'semprefix': '/mp'}\n        # Note that some versions of FreeBSD only allow named\n        # semaphores to have names of up to 14 characters.  Therefore\n        # we choose a short prefix.\n        #\n        # On MacOSX in a sandbox it may be necessary to use a\n        # different prefix -- see #19478.\n        #\n        # Everything in self._config will be inherited by descendant\n        # processes.\n\n    def close(self):\n        pass\n\n\n_parent_process = None\n_current_process = _MainProcess()\n_process_counter = itertools.count(1)\n_children = set()\ndel _MainProcess\n\n#\n# Give names to some return codes\n#\n\n_exitcode_to_name = {}\n\nfor name, signum in list(signal.__dict__.items()):\n    if name[:3]=='SIG' and '_' not in name:\n        _exitcode_to_name[-signum] = f'-{name}'\n\n# For debug and leak testing\n_dangling = WeakSet()\n", 432]}, "functions": {"<module> (/tmp/a.json:1)": ["/tmp/a.json", 1], "info (/Users/sangcho/anaconda3/envs/core/lib/python3.9/multiprocessing/util.py:52)": ["/Users/sangcho/anaconda3/envs/core/lib/python3.9/multiprocessing/util.py", 52], "debug (/Users/sangcho/anaconda3/envs/core/lib/python3.9/multiprocessing/util.py:48)": ["/Users/sangcho/anaconda3/envs/core/lib/python3.9/multiprocessing/util.py", 48], "<lambda> (/Users/sangcho/anaconda3/envs/core/lib/python3.9/multiprocessing/util.py:284)": ["/Users/sangcho/anaconda3/envs/core/lib/python3.9/multiprocessing/util.py", 284], "<listcomp> (/Users/sangcho/anaconda3/envs/core/lib/python3.9/multiprocessing/util.py:291)": ["/Users/sangcho/anaconda3/envs/core/lib/python3.9/multiprocessing/util.py", 291], "_run_finalizers (/Users/sangcho/anaconda3/envs/core/lib/python3.9/multiprocessing/util.py:268)": ["/Users/sangcho/anaconda3/envs/core/lib/python3.9/multiprocessing/util.py", 268], "current_process (/Users/sangcho/anaconda3/envs/core/lib/python3.9/multiprocessing/process.py:37)": ["/Users/sangcho/anaconda3/envs/core/lib/python3.9/multiprocessing/process.py", 37], "_cleanup (/Users/sangcho/anaconda3/envs/core/lib/python3.9/multiprocessing/process.py:61)": ["/Users/sangcho/anaconda3/envs/core/lib/python3.9/multiprocessing/process.py", 61], "active_children (/Users/sangcho/anaconda3/envs/core/lib/python3.9/multiprocessing/process.py:43)": ["/Users/sangcho/anaconda3/envs/core/lib/python3.9/multiprocessing/process.py", 43], "<lambda> (/Users/sangcho/anaconda3/envs/core/lib/python3.9/multiprocessing/util.py:282)": ["/Users/sangcho/anaconda3/envs/core/lib/python3.9/multiprocessing/util.py", 282], "sub_debug (/Users/sangcho/anaconda3/envs/core/lib/python3.9/multiprocessing/util.py:44)": ["/Users/sangcho/anaconda3/envs/core/lib/python3.9/multiprocessing/util.py", 44]}}}