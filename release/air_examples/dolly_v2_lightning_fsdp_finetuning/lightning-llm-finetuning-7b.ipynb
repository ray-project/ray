{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tune Dolly-v2-7b with Ray AIR LightningTrainer and FSDP"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up ray cluster \n",
    "In this example, we are using a ray cluster with 1 g4dn.8xlarge instance (head node) and 16 g4dn.4xlarge instances (worker nodes). Each instance has one Tesla T4 GPU (16GiB Memory). \n",
    "\n",
    "We define a `runtime_env` to install the necessary Python libraries on each node. You can skip this step if you have already installed all the required packages in your workers' base image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ray/anaconda3/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "find: ‘.git’: No such file or directory\n",
      "2023-05-03 01:22:08,570\tINFO worker.py:1432 -- Connecting to existing Ray cluster at address: 10.0.108.10:6379...\n",
      "2023-05-03 01:22:08,586\tINFO worker.py:1607 -- Connected to Ray cluster. View the dashboard at https://console.anyscale-staging.com/api/v2/sessions/ses_m411tiqu8eluvt1k5ivfqj4q5r/services?redirect_to=dashboard \n",
      "2023-05-03 01:22:09,161\tINFO packaging.py:520 -- Creating a file package for local directory '/tmp/ray_tmp_module/ray'.\n",
      "2023-05-03 01:22:09,262\tWARNING packaging.py:394 -- File /tmp/ray_tmp_module/ray/jars/ray_dist.jar is very large (30.48MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/tmp/ray_tmp_module/ray/jars/ray_dist.jar']})`\n",
      "2023-05-03 01:22:09,317\tWARNING packaging.py:394 -- File /tmp/ray_tmp_module/ray/_raylet.so is very large (25.37MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/tmp/ray_tmp_module/ray/_raylet.so']})`\n",
      "2023-05-03 01:22:09,363\tWARNING packaging.py:394 -- File /tmp/ray_tmp_module/ray/core/src/ray/gcs/gcs_server is very large (21.25MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/tmp/ray_tmp_module/ray/core/src/ray/gcs/gcs_server']})`\n",
      "2023-05-03 01:22:09,401\tWARNING packaging.py:394 -- File /tmp/ray_tmp_module/ray/core/src/ray/raylet/raylet is very large (20.56MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/tmp/ray_tmp_module/ray/core/src/ray/raylet/raylet']})`\n",
      "2023-05-03 01:22:10,019\tINFO packaging.py:347 -- Pushing file package 'gcs://_ray_pkg_f34236f1aec697e6.zip' (152.99MiB) to Ray cluster...\n",
      "2023-05-03 01:22:10,568\tINFO packaging.py:360 -- Successfully pushed file package 'gcs://_ray_pkg_f34236f1aec697e6.zip'.\n",
      "2023-05-03 01:22:10,596\tINFO packaging.py:347 -- Pushing file package 'gcs://_ray_pkg_15f0985dd965ef454042c1796aa119b5.zip' (0.16MiB) to Ray cluster...\n",
      "2023-05-03 01:22:10,597\tINFO packaging.py:360 -- Successfully pushed file package 'gcs://_ray_pkg_15f0985dd965ef454042c1796aa119b5.zip'.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "    <div style=\"margin-left: 50px;display: flex;flex-direction: row;align-items: center\">\n",
       "        <h3 style=\"color: var(--jp-ui-font-color0)\">Ray</h3>\n",
       "        <svg version=\"1.1\" id=\"ray\" width=\"3em\" viewBox=\"0 0 144.5 144.6\" style=\"margin-left: 3em;margin-right: 3em\">\n",
       "            <g id=\"layer-1\">\n",
       "                <path fill=\"#00a2e9\" class=\"st0\" d=\"M97.3,77.2c-3.8-1.1-6.2,0.9-8.3,5.1c-3.5,6.8-9.9,9.9-17.4,9.6S58,88.1,54.8,81.2c-1.4-3-3-4-6.3-4.1\n",
       "                    c-5.6-0.1-9.9,0.1-13.1,6.4c-3.8,7.6-13.6,10.2-21.8,7.6C5.2,88.4-0.4,80.5,0,71.7c0.1-8.4,5.7-15.8,13.8-18.2\n",
       "                    c8.4-2.6,17.5,0.7,22.3,8c1.3,1.9,1.3,5.2,3.6,5.6c3.9,0.6,8,0.2,12,0.2c1.8,0,1.9-1.6,2.4-2.8c3.5-7.8,9.7-11.8,18-11.9\n",
       "                    c8.2-0.1,14.4,3.9,17.8,11.4c1.3,2.8,2.9,3.6,5.7,3.3c1-0.1,2,0.1,3,0c2.8-0.5,6.4,1.7,8.1-2.7s-2.3-5.5-4.1-7.5\n",
       "                    c-5.1-5.7-10.9-10.8-16.1-16.3C84,38,81.9,37.1,78,38.3C66.7,42,56.2,35.7,53,24.1C50.3,14,57.3,2.8,67.7,0.5\n",
       "                    C78.4-2,89,4.7,91.5,15.3c0.1,0.3,0.1,0.5,0.2,0.8c0.7,3.4,0.7,6.9-0.8,9.8c-1.7,3.2-0.8,5,1.5,7.2c6.7,6.5,13.3,13,19.8,19.7\n",
       "                    c1.8,1.8,3,2.1,5.5,1.2c9.1-3.4,17.9-0.6,23.4,7c4.8,6.9,4.6,16.1-0.4,22.9c-5.4,7.2-14.2,9.9-23.1,6.5c-2.3-0.9-3.5-0.6-5.1,1.1\n",
       "                    c-6.7,6.9-13.6,13.7-20.5,20.4c-1.8,1.8-2.5,3.2-1.4,5.9c3.5,8.7,0.3,18.6-7.7,23.6c-7.9,5-18.2,3.8-24.8-2.9\n",
       "                    c-6.4-6.4-7.4-16.2-2.5-24.3c4.9-7.8,14.5-11,23.1-7.8c3,1.1,4.7,0.5,6.9-1.7C91.7,98.4,98,92.3,104.2,86c1.6-1.6,4.1-2.7,2.6-6.2\n",
       "                    c-1.4-3.3-3.8-2.5-6.2-2.6C99.8,77.2,98.9,77.2,97.3,77.2z M72.1,29.7c5.5,0.1,9.9-4.3,10-9.8c0-0.1,0-0.2,0-0.3\n",
       "                    C81.8,14,77,9.8,71.5,10.2c-5,0.3-9,4.2-9.3,9.2c-0.2,5.5,4,10.1,9.5,10.3C71.8,29.7,72,29.7,72.1,29.7z M72.3,62.3\n",
       "                    c-5.4-0.1-9.9,4.2-10.1,9.7c0,0.2,0,0.3,0,0.5c0.2,5.4,4.5,9.7,9.9,10c5.1,0.1,9.9-4.7,10.1-9.8c0.2-5.5-4-10-9.5-10.3\n",
       "                    C72.6,62.3,72.4,62.3,72.3,62.3z M115,72.5c0.1,5.4,4.5,9.7,9.8,9.9c5.6-0.2,10-4.8,10-10.4c-0.2-5.4-4.6-9.7-10-9.7\n",
       "                    c-5.3-0.1-9.8,4.2-9.9,9.5C115,72.1,115,72.3,115,72.5z M19.5,62.3c-5.4,0.1-9.8,4.4-10,9.8c-0.1,5.1,5.2,10.4,10.2,10.3\n",
       "                    c5.6-0.2,10-4.9,9.8-10.5c-0.1-5.4-4.5-9.7-9.9-9.6C19.6,62.3,19.5,62.3,19.5,62.3z M71.8,134.6c5.9,0.2,10.3-3.9,10.4-9.6\n",
       "                    c0.5-5.5-3.6-10.4-9.1-10.8c-5.5-0.5-10.4,3.6-10.8,9.1c0,0.5,0,0.9,0,1.4c-0.2,5.3,4,9.8,9.3,10\n",
       "                    C71.6,134.6,71.7,134.6,71.8,134.6z\"/>\n",
       "            </g>\n",
       "        </svg>\n",
       "        <table>\n",
       "            <tr>\n",
       "                <td style=\"text-align: left\"><b>Python version:</b></td>\n",
       "                <td style=\"text-align: left\"><b>3.8.13</b></td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                <td style=\"text-align: left\"><b>Ray version:</b></td>\n",
       "                <td style=\"text-align: left\"><b> 3.0.0.dev0</b></td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "    <td style=\"text-align: left\"><b>Dashboard:</b></td>\n",
       "    <td style=\"text-align: left\"><b><a href=\"http://console.anyscale-staging.com/api/v2/sessions/ses_m411tiqu8eluvt1k5ivfqj4q5r/services?redirect_to=dashboard\" target=\"_blank\">http://console.anyscale-staging.com/api/v2/sessions/ses_m411tiqu8eluvt1k5ivfqj4q5r/services?redirect_to=dashboard</a></b></td>\n",
       "</tr>\n",
       "\n",
       "        </table>\n",
       "    </div>\n",
       "</div>\n"
      ],
      "text/plain": [
       "RayContext(dashboard_url='console.anyscale-staging.com/api/v2/sessions/ses_m411tiqu8eluvt1k5ivfqj4q5r/services?redirect_to=dashboard', python_version='3.8.13', ray_version='3.0.0.dev0', ray_commit='b5e5bd7beb1d725dbc7689ee1e7fa3c9a8bef771', address_info={'node_ip_address': '10.0.108.10', 'raylet_ip_address': '10.0.108.10', 'redis_address': None, 'object_store_address': '/tmp/ray/session_2023-05-03_01-20-23_835246_2838/sockets/plasma_store', 'raylet_socket_name': '/tmp/ray/session_2023-05-03_01-20-23_835246_2838/sockets/raylet', 'webui_url': 'console.anyscale-staging.com/api/v2/sessions/ses_m411tiqu8eluvt1k5ivfqj4q5r/services?redirect_to=dashboard', 'session_dir': '/tmp/ray/session_2023-05-03_01-20-23_835246_2838', 'metrics_export_port': 8085, 'gcs_address': '10.0.108.10:6379', 'address': '10.0.108.10:6379', 'dashboard_agent_listen_port': 6822, 'node_id': '2aad11373a5acad46caf1a2813487e71fdb77c129e91a3adbc67c235'})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ray\n",
    "\n",
    "ray.init(\n",
    "    runtime_env={\n",
    "        \"pip\": [\n",
    "            \"datasets\",\n",
    "            \"evaluate\",\n",
    "            \"accelerate>=0.18.0\",\n",
    "            \"transformers>=4.28.0\",\n",
    "            \"torch>=2.0.0\",\n",
    "            \"pytorch_lightning>=2.0\",\n",
    "        ]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers = 16\n",
    "batch_size_per_worker = 10\n",
    "MODEL_NAME = \"databricks/dolly-v2-7b\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare your data \n",
    "We are using tiny_shakespeare for fine-tuning, which contains 40,000 lines of Shakespeare from a variety of Shakespeare's plays. Featured in Andrej Karpathy's blog post ['The Unreasonable Effectiveness of Recurrent Neural Networks'](http://karpathy.github.io/2015/05/21/rnn-effectiveness/). \n",
    "\n",
    "Dataset samples:\n",
    "```\n",
    "BAPTISTA:\n",
    "I know him well: you are welcome for his sake.\n",
    "\n",
    "GREMIO:\n",
    "Saving your tale, Petruchio, I pray,\n",
    "Let us, that are poor petitioners, speak too:\n",
    "Baccare! you are marvellous forward.\n",
    "\n",
    "PETRUCHIO:\n",
    "O, pardon me, Signior Gremio; I would fain be doing.\n",
    "```\n",
    "\n",
    "Here, we have adopted similar pre-processing logic from another demo: {ref}`GPT-J-6B Fine-Tuning with Ray AIR and DeepSpeed <gpt-j-6b-finetune-deepspeed>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 100%|██████████| 3.73k/3.73k [00:00<00:00, 4.33MB/s]\n",
      "Downloading metadata: 100%|██████████| 1.90k/1.90k [00:00<00:00, 1.86MB/s]\n",
      "Downloading readme: 100%|██████████| 6.10k/6.10k [00:00<00:00, 6.94MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset tiny_shakespeare/default to /home/ray/.cache/huggingface/datasets/tiny_shakespeare/default/1.0.0/b5b13969f09fe8707337f6cb296314fbe06960bd9a868dca39e713e163d27b5e...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 1.12MB [00:00, 15.3MB/s]                  \n",
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset tiny_shakespeare downloaded and prepared to /home/ray/.cache/huggingface/datasets/tiny_shakespeare/default/1.0.0/b5b13969f09fe8707337f6cb296314fbe06960bd9a868dca39e713e163d27b5e. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 1164.33it/s]\n"
     ]
    }
   ],
   "source": [
    "import ray\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from ray.data.preprocessors import BatchMapper, Chain\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "def split_text(batch: pd.DataFrame) -> pd.DataFrame:\n",
    "    text = list(batch[\"text\"])\n",
    "    flat_text = \"\".join(text)\n",
    "    split_text = [\n",
    "        x.strip()\n",
    "        for x in flat_text.split(\"\\n\")\n",
    "        if x.strip() and not x.strip()[-1] == \":\"\n",
    "    ]\n",
    "    return pd.DataFrame(split_text, columns=[\"text\"])\n",
    "\n",
    "\n",
    "def tokenize(batch: pd.DataFrame) -> dict:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, padding_side=\"left\")\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    ret = tokenizer(\n",
    "        list(batch[\"text\"]),\n",
    "        truncation=True,\n",
    "        max_length=256,\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=\"np\",\n",
    "    )\n",
    "    ret[\"labels\"] = ret[\"input_ids\"].copy()\n",
    "    return dict(ret)\n",
    "\n",
    "splitter = BatchMapper(split_text, batch_format=\"pandas\")\n",
    "tokenizer = BatchMapper(tokenize, batch_format=\"pandas\")\n",
    "preprocessor = Chain(splitter, tokenizer)\n",
    "\n",
    "hf_dataset = load_dataset(\"tiny_shakespeare\")\n",
    "ray_datasets = ray.data.from_huggingface(hf_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first split the original paragraphs into multiple sentences, then tokenize them. Here are some samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-03 01:22:20,623\tINFO datastream.py:2271 -- Tip: Use `take_batch()` instead of `take() / show()` to return records in pandas or numpy batch format.\n",
      "2023-05-03 01:22:20,626\tINFO streaming_executor.py:87 -- Executing DAG InputDataBuffer[Input] -> TaskPoolMapOperator[BatchMapper]\n",
      "2023-05-03 01:22:20,629\tINFO streaming_executor.py:88 -- Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=None), locality_with_output=False, preserve_order=False, actor_locality_enabled=True, verbose_progress=False)\n",
      "2023-05-03 01:22:20,629\tINFO streaming_executor.py:90 -- Tip: To enable per-operator progress reporting, set RAY_DATA_VERBOSE_PROGRESS=1.\n",
      "                                                                                                                   \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'text': 'Before we proceed any further, hear me speak.'},\n",
       " {'text': 'Speak, speak.'},\n",
       " {'text': 'You are all resolved rather to die than to famish?'},\n",
       " {'text': 'Resolved. resolved.'},\n",
       " {'text': 'First, you know Caius Marcius is chief enemy to the people.'},\n",
       " {'text': \"We know't, we know't.\"},\n",
       " {'text': \"Let us kill him, and we'll have corn at our own price.\"},\n",
       " {'text': \"Is't a verdict?\"},\n",
       " {'text': \"No more talking on't; let it be done: away, away!\"},\n",
       " {'text': 'One word, good citizens.'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = ray_datasets[\"train\"]\n",
    "splitter.fit_transform(ds).take(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define your lightning model\n",
    "\n",
    "In this example, we use the [Dolly-v2-7b](https://huggingface.co/databricks/dolly-v2-7b) model for finetuning. It is an instruction-following large language model trained on the Databricks machine learning platform that is licensed for commercial use. We load the model weights from Huggingface Model Hub and encapsulate it into a `pl.LightningModule`.\n",
    "\n",
    ":::{note}\n",
    "Make sure you pass the FSDP wrapped model parameters `self.trainer.model.parameters()` into the optimizer, instead of `self.model.parameters()`. \n",
    ":::\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "class DollyV2Model(pl.LightningModule):\n",
    "    def __init__(self, lr=2e-5, eps=1e-8):\n",
    "        super().__init__()\n",
    "        self.lr = lr\n",
    "        self.eps = eps\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n",
    "        self.predictions = []\n",
    "        self.references = []\n",
    "\n",
    "    def forward(self, batch):\n",
    "        outputs = self.model(\n",
    "            batch[\"input_ids\"], \n",
    "            attention_mask=batch[\"attention_mask\"], \n",
    "            labels=batch[\"labels\"]\n",
    "        )\n",
    "        return outputs.loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss = self.forward(batch)\n",
    "        self.log(\"train_loss\", loss, prog_bar=True, on_step=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        if self.global_rank == 0:\n",
    "            print(self.trainer.model)\n",
    "        return torch.optim.AdamW(self.trainer.model.parameters(), lr=self.lr, eps=self.eps)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure your FSDP strategy\n",
    "As Dolly-v2-7b is a relatively large model, it cannot be properly fit into a single commercial GPU. In this example, we use the FSDP strategy to shard model parameters across multiple workers. This allows us to avoid GPU out-of-memory issues and support a larger global batch size.\n",
    "\n",
    ":::{note}\n",
    "FSDP is a type of data parallelism that shards model parameters, optimizer states and gradients across DDP ranks. This was inspired by Xu et al. as well as the ZeRO Stage 3 from DeepSpeed. You may refer to these blogs for more information:\n",
    "\n",
    "- [Getting Started with Fully Sharded Data Parallel(FSDP)](https://pytorch.org/tutorials/intermediate/FSDP_tutorial.html#:~:text=FSDP%20is%20a%20type%20of,sizes%20for%20our%20training%20job.)\n",
    "- [Fully Sharded Data Parallel: faster AI training with fewer GPUs](https://engineering.fb.com/2021/07/15/open-source/fsdp/)\n",
    "- [PyTorch FSDP Tutorial](https://www.youtube.com/watch?v=8_k76AHu__s&list=PL_lsbAsL_o2BT6aerEKgIoufVD_fodnuT)\n",
    ":::\n",
    "\n",
    "To start trainig with Lightning's [FSDPStrategy](https://lightning.ai/docs/pytorch/stable/api/lightning.pytorch.strategies.FSDPStrategy.html#lightning.pytorch.strategies.FSDPStrategy), you only need to provide the initialization arguments in `LightningConfigBuilder.strategy()`. Behind the scenes, LightningTrainer handles the cluster environment settings and job launching.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{tips}\n",
    "Some tips for FSDP configutarion:\n",
    "- `sharding_strategy`:\n",
    "    - `ShardingStrategy.NO_SHARD`: Parameters, gradients, and optimizer states are not sharded. Similar to DDP.\n",
    "    - `ShardingStrategy.SHARD_GRAD_OP`: Gradients and optimizer states are sharded during computation, while parameters are sharded outside computation. Similar to ZeRO stage 2.\n",
    "    - `ShardingStrategy.FULL_SHARD`: Parameters, gradients, and optimizer states are sharded. It has minimal GRAM usage among the 3 options. Similar to ZeRO stage 3.\n",
    "- `auto_wrap_policy`:\n",
    "    - Model layers are often wrapped with FSDP in a layered fashion. This means that only the layers in a single FSDP instance are required to aggregate all parameters to a single device during forwarding or backward calculations.\n",
    "    - Use `transformer_auto_wrap_policy` to automatically wrap each Transformer Block into a single FSDP instance. \n",
    "- `backward_prefetch` and `forward_prefetch`:\n",
    "    - Overlap the upcoming all-gather while executing the current forward/backward pass. It can improve throughput but may slightly increase peak memory usage.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "from ray.train.lightning import LightningTrainer, LightningConfigBuilder\n",
    "from ray.air.config import RunConfig, ScalingConfig, CheckpointConfig\n",
    "from torch.distributed.fsdp.wrap import transformer_auto_wrap_policy\n",
    "from torch.distributed.fsdp import ShardingStrategy, BackwardPrefetch\n",
    "from transformers.models.gpt_neox.modeling_gpt_neox import GPTNeoXLayer\n",
    "\n",
    "# Define the model sharding policy:\n",
    "# Wrap every GPTNeoXLayer as its own FSDP instance\n",
    "auto_wrap_policy = functools.partial(\n",
    "    transformer_auto_wrap_policy,\n",
    "    transformer_layer_cls = {GPTNeoXLayer}\n",
    ")\n",
    "\n",
    "# Aggregate all arguments for LightningTrainer\n",
    "lightning_config = (\n",
    "    LightningConfigBuilder()\n",
    "    .module(cls=DollyV2Model, lr=2e-5, eps=1e-8)\n",
    "    .trainer(\n",
    "        max_epochs=1, \n",
    "        accelerator=\"gpu\", \n",
    "        precision=\"16-mixed\",\n",
    "        max_steps=40, # Accelerate the release test\n",
    "    )\n",
    "    .strategy(\n",
    "        name=\"fsdp\",\n",
    "        sharding_strategy=ShardingStrategy.FULL_SHARD,\n",
    "        backward_prefetch=BackwardPrefetch.BACKWARD_PRE,\n",
    "        forward_prefetch=True,\n",
    "        auto_wrap_policy=auto_wrap_policy,\n",
    "        limit_all_gathers=True,\n",
    "        activation_checkpointing=[GPTNeoXLayer],\n",
    "    )\n",
    "    .checkpointing(save_top_k=0, save_weights_only=True, save_last=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import TQDMProgressBar\n",
    "\n",
    "# Create a customized progress bar for LightningTrainer\n",
    "class DollyV2ProgressBar(TQDMProgressBar):\n",
    "    def __init__(self, num_iters_per_epoch, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.num_iters_per_epoch = num_iters_per_epoch\n",
    "    \n",
    "    def on_train_epoch_start(self, trainer, *_):\n",
    "        super().on_train_epoch_start(trainer, *_)\n",
    "        self.train_progress_bar.reset(self.num_iters_per_epoch)\n",
    "\n",
    "total_batches = splitter.fit_transform(ray_datasets[\"train\"]).count()\n",
    "num_iters_per_epoch = total_batches // (num_workers * batch_size_per_worker)\n",
    "lightning_config.trainer(callbacks=[DollyV2ProgressBar(num_iters_per_epoch)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tune with LightningTrainer\n",
    "\n",
    "```{note}\n",
    "Here we upload the checkpoints to cloud storage by setting S3 bucket URI to {class}`air.RunConfig(storage_path) <ray.air.RunConfig>`. You can also write to your local file system. See {ref}`train-run-config` for an example.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2023-05-03 02:22:06</td></tr>\n",
       "<tr><td>Running for: </td><td>00:59:42.80        </td></tr>\n",
       "<tr><td>Memory:      </td><td>7.4/124.4 GiB      </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using FIFO scheduling algorithm.<br>Logical resource usage: 0/272 CPUs, 0/16 GPUs (0.0/16.0 accelerator_type:T4)\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status    </th><th>loc             </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  train_loss</th><th style=\"text-align: right;\">  epoch</th><th style=\"text-align: right;\">  step</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>LightningTrainer_a1a3d_00000</td><td>TERMINATED</td><td>10.0.108.10:8284</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3024.09</td><td style=\"text-align: right;\">    0.176025</td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">   135</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-03 01:22:24,188\tWARNING trial_runner.py:1607 -- The maximum number of pending trials has been automatically set to the number of available cluster CPUs, which is high (299 CPUs/pending trials). If you're running an experiment with a large number of trials, this could lead to scheduling overhead. In this case, consider setting the `TUNE_MAX_PENDING_TRIALS_PG` environment variable to the desired maximum number of concurrent trials.\n",
      "(LightningTrainer pid=8284) 2023-05-03 01:22:31,584\tINFO backend_executor.py:128 -- Starting distributed worker processes: ['8425 (10.0.108.10)', '3544 (10.0.102.225)', '3576 (10.0.80.223)', '3541 (10.0.102.21)', '3563 (10.0.108.25)', '3541 (10.0.114.187)', '3508 (10.0.67.62)', '3424 (10.0.86.122)', '3532 (10.0.113.13)', '3488 (10.0.96.142)', '3474 (10.0.122.128)', '3572 (10.0.112.171)', '3504 (10.0.78.238)', '3554 (10.0.79.247)', '3521 (10.0.107.4)', '3570 (10.0.104.19)']\n",
      "(RayTrainWorker pid=8425) 2023-05-03 01:22:33,824\tINFO config.py:86 -- Setting up process group for: env:// [rank=0, world_size=16]\n",
      "(LightningTrainer pid=8284) 2023-05-03 01:22:34,427\tINFO streaming_executor.py:87 -- Executing DAG InputDataBuffer[Input] -> TaskPoolMapOperator[BatchMapper->BatchMapper] -> AllToAllOperator[RandomizeBlockOrder]\n",
      "(LightningTrainer pid=8284) 2023-05-03 01:22:34,427\tINFO streaming_executor.py:88 -- Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=None), locality_with_output=False, preserve_order=False, actor_locality_enabled=True, verbose_progress=False)\n",
      "(LightningTrainer pid=8284) 2023-05-03 01:22:34,427\tINFO streaming_executor.py:90 -- Tip: To enable per-operator progress reporting, set RAY_DATA_VERBOSE_PROGRESS=1.\n",
      "\n",
      "(pid=8284) Running: 0.0/272.0 CPU, 0.0/16.0 GPU, 0.0 MiB/73.21 GiB object_store_memory:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "(pid=8284) Running: 1.0/272.0 CPU, 0.0/16.0 GPU, 0.96 MiB/73.21 GiB object_store_memory:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 450/450 [00:00<00:00, 68.4kB/s]                                          \n",
      "\n",
      "(pid=8284) Running: 1.0/272.0 CPU, 0.0/16.0 GPU, 0.96 MiB/73.21 GiB object_store_memory:   0%|          | 0/1 [00:02<?, ?it/s]\n",
      "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]                                               \n",
      "\n",
      "(pid=8284) Running: 1.0/272.0 CPU, 0.0/16.0 GPU, 0.96 MiB/73.21 GiB object_store_memory:   0%|          | 0/1 [00:02<?, ?it/s]\n",
      "Downloading (…)/main/tokenizer.json: 100%|██████████| 2.11M/2.11M [00:00<00:00, 15.9MB/s]                                      \n",
      "\n",
      "(pid=8284) Running: 1.0/272.0 CPU, 0.0/16.0 GPU, 0.96 MiB/73.21 GiB object_store_memory:   0%|          | 0/1 [00:02<?, ?it/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 228/228 [00:00<00:00, 140kB/s]                                           \n",
      "\n",
      "(pid=8284) Running: 0.0/272.0 CPU, 0.0/16.0 GPU, 0.0 MiB/73.21 GiB object_store_memory:   0%|          | 0/1 [00:05<?, ?it/s]   \n",
      "(pid=8284) - RandomizeBlockOrder: 0 active, 0 queued, 0.0 MiB objects, 1 output:   0%|          | 0/1 [00:05<?, ?it/s]\n",
      "(pid=8284) Running: 0.0/272.0 CPU, 0.0/16.0 GPU, 126.69 MiB/73.21 GiB object_store_memory:   0%|          | 0/1 [00:05<?, ?it/s]\n",
      "                                                                                                                                        \n",
      "Downloading (…)lve/main/config.json:   0%|          | 0.00/819 [00:00<?, ?B/s]                                                (RayTrainWorker pid=3504, ip=10.0.78.238) \n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 819/819 [00:00<00:00, 126kB/s]\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 819/819 [00:00<00:00, 126kB/s]\n",
      "Downloading pytorch_model.bin:   0%|          | 0.00/13.8G [00:00<?, ?B/s]\n",
      "Downloading pytorch_model.bin:   0%|          | 21.0M/13.8G [00:00<01:36, 143MB/s]\n",
      "Downloading pytorch_model.bin:   7%|▋         | 1.04G/13.8G [00:04<00:55, 229MB/s]\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 819/819 [00:00<00:00, 126kB/s] [repeated 13x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)\n",
      "Downloading pytorch_model.bin:   8%|▊         | 1.07G/13.8G [00:04<00:55, 229MB/s]\n",
      "Downloading pytorch_model.bin:   0%|          | 0.00/13.8G [00:00<?, ?B/s] [repeated 15x across cluster]\n",
      "Downloading pytorch_model.bin:   8%|▊         | 1.17G/13.8G [00:05<00:53, 236MB/s] [repeated 579x across cluster]\n",
      "Downloading pytorch_model.bin:   7%|▋         | 1.01G/13.8G [00:09<01:40, 128MB/s] [repeated 583x across cluster]\n",
      "Downloading pytorch_model.bin:  23%|██▎       | 3.18G/13.8G [00:14<00:50, 211MB/s] [repeated 588x across cluster]\n",
      "Downloading pytorch_model.bin:  33%|███▎      | 4.56G/13.8G [00:20<00:40, 227MB/s] [repeated 595x across cluster]\n",
      "Downloading pytorch_model.bin:  38%|███▊      | 5.28G/13.8G [00:25<00:40, 210MB/s] [repeated 590x across cluster]\n",
      "Downloading pytorch_model.bin:  45%|████▌     | 6.26G/13.8G [00:30<00:35, 212MB/s] [repeated 599x across cluster]\n",
      "Downloading pytorch_model.bin:  50%|█████     | 6.97G/13.8G [00:34<00:34, 198MB/s] [repeated 575x across cluster]\n",
      "Downloading pytorch_model.bin:  61%|██████    | 8.40G/13.8G [00:39<00:26, 207MB/s] [repeated 581x across cluster]\n",
      "Downloading pytorch_model.bin:  69%|██████▊   | 9.50G/13.8G [00:45<00:20, 211MB/s] [repeated 599x across cluster]\n",
      "Downloading pytorch_model.bin:  76%|███████▌  | 10.5G/13.8G [00:50<00:16, 209MB/s] [repeated 588x across cluster]\n",
      "Downloading pytorch_model.bin:  40%|████      | 5.58G/13.8G [00:51<01:21, 101MB/s] \n",
      "Downloading pytorch_model.bin:  91%|█████████▏| 12.7G/13.8G [00:53<00:04, 256MB/s]\n",
      "Downloading pytorch_model.bin:  92%|█████████▏| 12.7G/13.8G [00:54<00:04, 255MB/s]\n",
      "Downloading pytorch_model.bin:  92%|█████████▏| 12.7G/13.8G [00:54<00:04, 253MB/s]\n",
      "Downloading pytorch_model.bin:  92%|█████████▏| 12.8G/13.8G [00:54<00:04, 252MB/s]\n",
      "Downloading pytorch_model.bin:  92%|█████████▏| 12.8G/13.8G [00:54<00:04, 251MB/s]\n",
      "Downloading pytorch_model.bin:  93%|█████████▎| 12.8G/13.8G [00:54<00:04, 250MB/s]\n",
      "Downloading pytorch_model.bin:  93%|█████████▎| 12.8G/13.8G [00:54<00:04, 250MB/s]\n",
      "Downloading pytorch_model.bin:  84%|████████▍ | 11.6G/13.8G [00:55<00:10, 207MB/s] [repeated 572x across cluster]\n",
      "Downloading pytorch_model.bin: 100%|██████████| 13.8G/13.8G [00:58<00:00, 236MB/s]\n",
      "Downloading pytorch_model.bin:  97%|█████████▋| 13.4G/13.8G [00:58<00:01, 241MB/s] [repeated 90x across cluster]\n",
      "Downloading pytorch_model.bin:  91%|█████████ | 12.6G/13.8G [01:00<00:05, 236MB/s] [repeated 451x across cluster]\n",
      "Downloading pytorch_model.bin: 100%|██████████| 13.8G/13.8G [01:00<00:00, 228MB/s] [repeated 2x across cluster]\n",
      "Downloading pytorch_model.bin:  98%|█████████▊| 13.6G/13.8G [01:03<00:01, 225MB/s] [repeated 348x across cluster]\n",
      "Downloading pytorch_model.bin:  51%|█████     | 7.07G/13.8G [01:04<00:51, 132MB/s] [repeated 51x across cluster]\n",
      "Downloading pytorch_model.bin: 100%|██████████| 13.8G/13.8G [01:05<00:00, 210MB/s] [repeated 5x across cluster]\n",
      "Downloading pytorch_model.bin: 100%|█████████▉| 13.8G/13.8G [01:08<00:00, 209MB/s] [repeated 148x across cluster]\n",
      "Downloading pytorch_model.bin:  56%|█████▌    | 7.72G/13.8G [01:10<00:47, 128MB/s] [repeated 31x across cluster]\n",
      "Downloading pytorch_model.bin:  59%|█████▉    | 8.16G/13.8G [01:13<00:50, 113MB/s]\n",
      "Downloading pytorch_model.bin:  60%|██████    | 8.33G/13.8G [01:15<00:58, 94.4MB/s] [repeated 29x across cluster]\n",
      "Downloading pytorch_model.bin:  64%|██████▎   | 8.82G/13.8G [01:20<00:48, 103MB/s]  [repeated 39x across cluster]\n",
      "Downloading pytorch_model.bin:  67%|██████▋   | 9.31G/13.8G [01:25<00:41, 109MB/s] [repeated 32x across cluster]\n",
      "Downloading pytorch_model.bin:  71%|███████   | 9.85G/13.8G [01:30<00:38, 104MB/s] [repeated 33x across cluster]\n",
      "Downloading pytorch_model.bin:  75%|███████▍  | 10.3G/13.8G [01:35<00:30, 115MB/s] [repeated 34x across cluster]\n",
      "Downloading pytorch_model.bin:  79%|███████▉  | 11.0G/13.8G [01:40<00:21, 133MB/s] [repeated 30x across cluster]\n",
      "Downloading pytorch_model.bin:  83%|████████▎ | 11.5G/13.8G [01:45<00:27, 85.5MB/s] [repeated 37x across cluster]\n",
      "Downloading pytorch_model.bin:  87%|████████▋ | 12.0G/13.8G [01:50<00:14, 126MB/s] [repeated 34x across cluster]\n",
      "Downloading pytorch_model.bin:  91%|█████████ | 12.6G/13.8G [01:56<00:09, 129MB/s] [repeated 30x across cluster]\n",
      "Downloading pytorch_model.bin:  91%|█████████▏| 12.6G/13.8G [01:56<00:09, 130MB/s]\n",
      "Downloading pytorch_model.bin:  91%|█████████▏| 12.7G/13.8G [01:56<00:09, 130MB/s]\n",
      "Downloading pytorch_model.bin:  92%|█████████▏| 12.7G/13.8G [01:56<00:10, 108MB/s]\n",
      "Downloading pytorch_model.bin:  92%|█████████▏| 12.7G/13.8G [01:56<00:09, 115MB/s]\n",
      "Downloading pytorch_model.bin:  92%|█████████▏| 12.7G/13.8G [01:57<00:09, 120MB/s]\n",
      "Downloading pytorch_model.bin:  92%|█████████▏| 12.8G/13.8G [01:57<00:08, 125MB/s]\n",
      "Downloading pytorch_model.bin:  92%|█████████▏| 12.8G/13.8G [01:57<00:08, 128MB/s]\n",
      "Downloading pytorch_model.bin:  92%|█████████▏| 12.8G/13.8G [01:57<00:08, 131MB/s]\n",
      "Downloading pytorch_model.bin:  93%|█████████▎| 12.8G/13.8G [01:57<00:07, 133MB/s]\n",
      "Downloading pytorch_model.bin:  93%|█████████▎| 12.8G/13.8G [01:57<00:07, 133MB/s]\n",
      "Downloading pytorch_model.bin:  93%|█████████▎| 12.9G/13.8G [01:58<00:08, 111MB/s]\n",
      "Downloading pytorch_model.bin:  93%|█████████▎| 12.9G/13.8G [01:58<00:09, 102MB/s]\n",
      "Downloading pytorch_model.bin:  93%|█████████▎| 12.9G/13.8G [01:58<00:09, 95.7MB/s]\n",
      "Downloading pytorch_model.bin:  93%|█████████▎| 12.9G/13.8G [01:58<00:09, 94.8MB/s]\n",
      "Downloading pytorch_model.bin:  93%|█████████▎| 12.9G/13.8G [01:58<00:09, 94.1MB/s]\n",
      "Downloading pytorch_model.bin:  93%|█████████▎| 12.9G/13.8G [01:58<00:09, 94.3MB/s]\n",
      "Downloading pytorch_model.bin:  93%|█████████▎| 12.9G/13.8G [01:59<00:09, 94.1MB/s]\n",
      "Downloading pytorch_model.bin:  94%|█████████▎| 12.9G/13.8G [01:59<00:09, 93.9MB/s]\n",
      "Downloading pytorch_model.bin:  94%|█████████▎| 13.0G/13.8G [01:59<00:09, 94.0MB/s]\n",
      "Downloading pytorch_model.bin:  94%|█████████▎| 13.0G/13.8G [01:59<00:09, 95.9MB/s]\n",
      "Downloading pytorch_model.bin:  94%|█████████▍| 13.0G/13.8G [01:59<00:09, 89.6MB/s]\n",
      "Downloading pytorch_model.bin:  94%|█████████▍| 13.0G/13.8G [01:59<00:09, 92.8MB/s]\n",
      "Downloading pytorch_model.bin:  94%|█████████▍| 13.0G/13.8G [01:59<00:08, 94.6MB/s]\n",
      "Downloading pytorch_model.bin:  94%|█████████▍| 13.0G/13.8G [01:59<00:08, 96.8MB/s]\n",
      "Downloading pytorch_model.bin:  94%|█████████▍| 13.0G/13.8G [01:59<00:08, 98.5MB/s]\n",
      "Downloading pytorch_model.bin:  94%|█████████▍| 13.0G/13.8G [02:00<00:08, 100MB/s] \n",
      "Downloading pytorch_model.bin:  94%|█████████▍| 13.1G/13.8G [02:00<00:07, 103MB/s]\n",
      "Downloading pytorch_model.bin:  94%|█████████▍| 13.1G/13.8G [02:00<00:07, 104MB/s]\n",
      "Downloading pytorch_model.bin:  95%|█████████▍| 13.1G/13.8G [02:00<00:07, 106MB/s]\n",
      "Downloading pytorch_model.bin:  95%|█████████▍| 13.1G/13.8G [02:00<00:08, 87.4MB/s]\n",
      "Downloading pytorch_model.bin:  95%|█████████▍| 13.1G/13.8G [02:01<00:07, 94.0MB/s]\n",
      "Downloading pytorch_model.bin:  95%|█████████▌| 13.2G/13.8G [02:01<00:06, 99.2MB/s]\n",
      "Downloading pytorch_model.bin:  95%|█████████▌| 13.2G/13.8G [02:01<00:06, 103MB/s] \n",
      "Downloading pytorch_model.bin:  95%|█████████▌| 13.2G/13.8G [02:01<00:06, 106MB/s]\n",
      "Downloading pytorch_model.bin:  95%|█████████▌| 13.2G/13.8G [02:01<00:05, 108MB/s]\n",
      "(RayTrainWorker pid=3570, ip=10.0.104.19) Using 16bit Automatic Mixed Precision (AMP)\n",
      "(RayTrainWorker pid=3570, ip=10.0.104.19) Missing logger folder: /home/ray/ray_results/finetune_dolly-v2-7b/LightningTrainer_a1a3d_00000_0_2023-05-03_01-22-24/rank_15/lightning_logs\n",
      "Downloading pytorch_model.bin:  96%|█████████▌| 13.2G/13.8G [02:02<00:06, 93.1MB/s]\n",
      "Downloading pytorch_model.bin:  96%|█████████▌| 13.3G/13.8G [02:02<00:06, 90.0MB/s]\n",
      "Downloading pytorch_model.bin:  96%|█████████▌| 13.3G/13.8G [02:02<00:05, 97.2MB/s]\n",
      "Downloading pytorch_model.bin:  96%|█████████▌| 13.3G/13.8G [02:02<00:05, 103MB/s] \n",
      "Downloading pytorch_model.bin:  96%|█████████▌| 13.3G/13.8G [02:02<00:04, 107MB/s]\n",
      "Downloading pytorch_model.bin:  96%|█████████▋| 13.3G/13.8G [02:02<00:04, 112MB/s]\n",
      "Downloading pytorch_model.bin:  96%|█████████▋| 13.4G/13.8G [02:03<00:04, 115MB/s]\n",
      "Downloading pytorch_model.bin:  97%|█████████▋| 13.4G/13.8G [02:03<00:03, 118MB/s]\n",
      "Downloading pytorch_model.bin:  97%|█████████▋| 13.4G/13.8G [02:03<00:05, 87.3MB/s]\n",
      "Downloading pytorch_model.bin:  97%|█████████▋| 13.4G/13.8G [02:03<00:04, 96.6MB/s]\n",
      "Downloading pytorch_model.bin:  97%|█████████▋| 13.4G/13.8G [02:04<00:03, 105MB/s] \n",
      "Downloading pytorch_model.bin:  97%|█████████▋| 13.5G/13.8G [02:04<00:03, 111MB/s]\n",
      "Downloading pytorch_model.bin:  97%|█████████▋| 13.5G/13.8G [02:04<00:03, 117MB/s]\n",
      "Downloading pytorch_model.bin:  98%|█████████▊| 13.5G/13.8G [02:04<00:02, 121MB/s]\n",
      "Downloading pytorch_model.bin:  98%|█████████▊| 13.5G/13.8G [02:04<00:02, 124MB/s]\n",
      "Downloading pytorch_model.bin:  98%|█████████▊| 13.5G/13.8G [02:04<00:02, 111MB/s]\n",
      "Downloading pytorch_model.bin:  98%|█████████▊| 13.6G/13.8G [02:05<00:02, 117MB/s]\n",
      "Downloading pytorch_model.bin:  98%|█████████▊| 13.6G/13.8G [02:05<00:02, 116MB/s]\n",
      "Downloading pytorch_model.bin:  98%|█████████▊| 13.6G/13.8G [02:05<00:01, 121MB/s]\n",
      "Downloading pytorch_model.bin:  98%|█████████▊| 13.6G/13.8G [02:05<00:01, 124MB/s]\n",
      "Downloading pytorch_model.bin:  99%|█████████▊| 13.7G/13.8G [02:05<00:01, 127MB/s]\n",
      "Downloading pytorch_model.bin:  99%|█████████▉| 13.7G/13.8G [02:05<00:01, 129MB/s]\n",
      "Downloading pytorch_model.bin:  99%|█████████▉| 13.7G/13.8G [02:06<00:01, 131MB/s]\n",
      "Downloading pytorch_model.bin:  99%|█████████▉| 13.7G/13.8G [02:06<00:00, 132MB/s]\n",
      "Downloading pytorch_model.bin:  99%|█████████▉| 13.7G/13.8G [02:06<00:01, 88.0MB/s]\n",
      "Downloading pytorch_model.bin:  99%|█████████▉| 13.8G/13.8G [02:06<00:00, 96.4MB/s]\n",
      "Downloading pytorch_model.bin: 100%|█████████▉| 13.8G/13.8G [02:06<00:00, 105MB/s] \n",
      "Downloading pytorch_model.bin: 100%|█████████▉| 13.8G/13.8G [02:07<00:00, 81.9MB/s]\n",
      "(RayTrainWorker pid=3508, ip=10.0.67.62) Using 16bit Automatic Mixed Precision (AMP) [repeated 2x across cluster]\n",
      "(RayTrainWorker pid=3508, ip=10.0.67.62) Missing logger folder: /home/ray/ray_results/finetune_dolly-v2-7b/LightningTrainer_a1a3d_00000_0_2023-05-03_01-22-24/rank_6/lightning_logs [repeated 2x across cluster]\n",
      "Downloading pytorch_model.bin: 100%|█████████▉| 13.8G/13.8G [02:07<00:00, 92.8MB/s]\n",
      "Downloading pytorch_model.bin: 100%|██████████| 13.8G/13.8G [02:07<00:00, 108MB/s] \n",
      "(RayTrainWorker pid=8425) GPU available: True (cuda), used: True\n",
      "(RayTrainWorker pid=8425) TPU available: False, using: 0 TPU cores\n",
      "(RayTrainWorker pid=8425) IPU available: False, using: 0 IPUs\n",
      "(RayTrainWorker pid=8425) HPU available: False, using: 0 HPUs\n",
      "(RayTrainWorker pid=3488, ip=10.0.96.142) Using 16bit Automatic Mixed Precision (AMP) [repeated 12x across cluster]\n",
      "(RayTrainWorker pid=3554, ip=10.0.79.247) Missing logger folder: /home/ray/ray_results/finetune_dolly-v2-7b/LightningTrainer_a1a3d_00000_0_2023-05-03_01-22-24/rank_13/lightning_logs [repeated 11x across cluster]\n",
      "(RayTrainWorker pid=3521, ip=10.0.107.4) Using 16bit Automatic Mixed Precision (AMP)\n",
      "(RayTrainWorker pid=3488, ip=10.0.96.142) Using 16bit Automatic Mixed Precision (AMP)\n",
      "(RayTrainWorker pid=3521, ip=10.0.107.4) Missing logger folder: /home/ray/ray_results/finetune_dolly-v2-7b/LightningTrainer_a1a3d_00000_0_2023-05-03_01-22-24/rank_14/lightning_logs\n",
      "(RayTrainWorker pid=3474, ip=10.0.122.128) LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=3474, ip=10.0.122.128) FullyShardedDataParallel(\n",
      "(RayTrainWorker pid=3474, ip=10.0.122.128)   (_fsdp_wrapped_module): _LightningModuleWrapperBase(\n",
      "(RayTrainWorker pid=3474, ip=10.0.122.128)     (_forward_module): DollyV2Model(\n",
      "(RayTrainWorker pid=3474, ip=10.0.122.128)       (model): GPTNeoXForCausalLM(\n",
      "(RayTrainWorker pid=3474, ip=10.0.122.128)         (gpt_neox): GPTNeoXModel(\n",
      "(RayTrainWorker pid=3474, ip=10.0.122.128)           (embed_in): Embedding(50280, 4096)\n",
      "(RayTrainWorker pid=3474, ip=10.0.122.128)           (layers): ModuleList(\n",
      "(RayTrainWorker pid=3474, ip=10.0.122.128)             (0-31): 32 x FullyShardedDataParallel(\n",
      "(RayTrainWorker pid=3474, ip=10.0.122.128)               (_fsdp_wrapped_module): CheckpointWrapper(\n",
      "(RayTrainWorker pid=3474, ip=10.0.122.128)                 (_checkpoint_wrapped_module): GPTNeoXLayer(\n",
      "(RayTrainWorker pid=3474, ip=10.0.122.128)                   (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "(RayTrainWorker pid=3474, ip=10.0.122.128)                   (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "(RayTrainWorker pid=3474, ip=10.0.122.128)                   (attention): GPTNeoXAttention(\n",
      "(RayTrainWorker pid=3474, ip=10.0.122.128)                     (rotary_emb): RotaryEmbedding()\n",
      "(RayTrainWorker pid=3474, ip=10.0.122.128)                     (query_key_value): Linear(in_features=4096, out_features=12288, bias=True)\n",
      "(RayTrainWorker pid=3474, ip=10.0.122.128)                     (dense): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "(RayTrainWorker pid=3474, ip=10.0.122.128)                   )\n",
      "(RayTrainWorker pid=3474, ip=10.0.122.128)                   (mlp): GPTNeoXMLP(\n",
      "(RayTrainWorker pid=3474, ip=10.0.122.128)                     (dense_h_to_4h): Linear(in_features=4096, out_features=16384, bias=True)\n",
      "(RayTrainWorker pid=3474, ip=10.0.122.128)                     (dense_4h_to_h): Linear(in_features=16384, out_features=4096, bias=True)\n",
      "(RayTrainWorker pid=3474, ip=10.0.122.128)                     (act): GELUActivation()\n",
      "(RayTrainWorker pid=3474, ip=10.0.122.128)                   )\n",
      "(RayTrainWorker pid=3474, ip=10.0.122.128)                 )\n",
      "(RayTrainWorker pid=3474, ip=10.0.122.128)               )\n",
      "(RayTrainWorker pid=3474, ip=10.0.122.128)             )\n",
      "(RayTrainWorker pid=3474, ip=10.0.122.128)           )\n",
      "(RayTrainWorker pid=3474, ip=10.0.122.128)           (final_layer_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "(RayTrainWorker pid=3474, ip=10.0.122.128)         )\n",
      "(RayTrainWorker pid=3474, ip=10.0.122.128)         (embed_out): Linear(in_features=4096, out_features=50280, bias=False)\n",
      "(RayTrainWorker pid=3474, ip=10.0.122.128)       )\n",
      "(RayTrainWorker pid=3474, ip=10.0.122.128)     )\n",
      "(RayTrainWorker pid=3474, ip=10.0.122.128)   )\n",
      "(RayTrainWorker pid=3474, ip=10.0.122.128) )\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=8425) \n",
      "(RayTrainWorker pid=8425)   | Name  | Type               | Params\n",
      "(RayTrainWorker pid=8425) ---------------------------------------------\n",
      "(RayTrainWorker pid=8425) 0 | model | GPTNeoXForCausalLM | 402 M \n",
      "(RayTrainWorker pid=8425) ---------------------------------------------\n",
      "(RayTrainWorker pid=8425) 402 M     Trainable params\n",
      "(RayTrainWorker pid=8425) 0         Non-trainable params\n",
      "(RayTrainWorker pid=8425) 402 M     Total params\n",
      "(RayTrainWorker pid=8425) 1,611.039 Total estimated model params size (MB)\n",
      "(RayTrainWorker pid=8425) /home/ray/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "(RayTrainWorker pid=8425)   rank_zero_warn(\n",
      "(RayTrainWorker pid=3532, ip=10.0.113.13) LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0] [repeated 15x across cluster]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/134 [00:00<?, ?it/s]\n",
      "Epoch 0:   1%|          | 1/134 [00:20<45:23, 20.48s/it, v_num=0, train_loss=12.90]\n",
      "(RayTrainWorker pid=8425) FullyShardedDataParallel( [repeated 15x across cluster]\n",
      "(RayTrainWorker pid=8425)   (_fsdp_wrapped_module): _LightningModuleWrapperBase( [repeated 15x across cluster]\n",
      "(RayTrainWorker pid=8425)     (_forward_module): DollyV2Model( [repeated 15x across cluster]\n",
      "(RayTrainWorker pid=8425)       (model): GPTNeoXForCausalLM( [repeated 15x across cluster]\n",
      "(RayTrainWorker pid=8425)         (gpt_neox): GPTNeoXModel( [repeated 15x across cluster]\n",
      "(RayTrainWorker pid=8425)           (embed_in): Embedding(50280, 4096) [repeated 15x across cluster]\n",
      "(RayTrainWorker pid=8425)           (layers): ModuleList( [repeated 15x across cluster]\n",
      "(RayTrainWorker pid=8425)             (0-31): 32 x FullyShardedDataParallel( [repeated 15x across cluster]\n",
      "(RayTrainWorker pid=8425)               (_fsdp_wrapped_module): CheckpointWrapper( [repeated 15x across cluster]\n",
      "(RayTrainWorker pid=8425)                 (_checkpoint_wrapped_module): GPTNeoXLayer( [repeated 15x across cluster]\n",
      "(RayTrainWorker pid=8425)                   (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True) [repeated 15x across cluster]\n",
      "(RayTrainWorker pid=8425)                   (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True) [repeated 15x across cluster]\n",
      "(RayTrainWorker pid=8425)                   (attention): GPTNeoXAttention( [repeated 15x across cluster]\n",
      "(RayTrainWorker pid=8425)                     (rotary_emb): RotaryEmbedding() [repeated 15x across cluster]\n",
      "(RayTrainWorker pid=8425)                     (query_key_value): Linear(in_features=4096, out_features=12288, bias=True) [repeated 15x across cluster]\n",
      "(RayTrainWorker pid=8425)                     (dense): Linear(in_features=4096, out_features=4096, bias=True) [repeated 15x across cluster]\n",
      "(RayTrainWorker pid=8425) ) [repeated 165x across cluster]\n",
      "(RayTrainWorker pid=8425)                   (mlp): GPTNeoXMLP( [repeated 15x across cluster]\n",
      "(RayTrainWorker pid=8425)                     (dense_4h_to_h): Linear(in_features=16384, out_features=4096, bias=True) [repeated 30x across cluster]\n",
      "(RayTrainWorker pid=8425)                     (act): GELUActivation() [repeated 15x across cluster]\n",
      "(RayTrainWorker pid=8425)           (final_layer_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True) [repeated 15x across cluster]\n",
      "(RayTrainWorker pid=8425)         (embed_out): Linear(in_features=4096, out_features=50280, bias=False) [repeated 15x across cluster]\n",
      "Epoch 0:   1%|▏         | 2/134 [00:38<42:25, 19.29s/it, v_num=0, train_loss=12.50]\n",
      "Epoch 0:   2%|▏         | 3/134 [00:56<41:04, 18.81s/it, v_num=0, train_loss=12.50]\n",
      "Epoch 0:   3%|▎         | 4/134 [01:14<40:23, 18.64s/it, v_num=0, train_loss=12.50]\n",
      "Epoch 0:   4%|▎         | 5/134 [01:32<39:50, 18.53s/it, v_num=0, train_loss=12.50]\n",
      "Epoch 0:   4%|▍         | 6/134 [01:50<39:26, 18.49s/it, v_num=0, train_loss=12.50]\n",
      "Epoch 0:   5%|▌         | 7/134 [02:08<38:59, 18.42s/it, v_num=0, train_loss=12.50]\n",
      "Epoch 0:   6%|▌         | 8/134 [02:27<38:36, 18.38s/it, v_num=0, train_loss=12.50]\n",
      "Epoch 0:   7%|▋         | 9/134 [02:45<38:11, 18.33s/it, v_num=0, train_loss=12.50]\n",
      "Epoch 0:   7%|▋         | 9/134 [02:45<38:14, 18.35s/it, v_num=0, train_loss=12.50]\n",
      "Epoch 0:   7%|▋         | 10/134 [03:03<37:52, 18.33s/it, v_num=0, train_loss=12.50]\n",
      "Epoch 0:   7%|▋         | 10/134 [03:03<37:54, 18.34s/it, v_num=0, train_loss=0.586]\n",
      "Epoch 0:   8%|▊         | 11/134 [03:21<37:32, 18.32s/it, v_num=0, train_loss=0.586]\n",
      "Epoch 0:   8%|▊         | 11/134 [03:21<37:34, 18.33s/it, v_num=0, train_loss=0.600]\n",
      "Epoch 0:   9%|▉         | 12/134 [03:39<37:14, 18.31s/it, v_num=0, train_loss=0.600]\n",
      "Epoch 0:   9%|▉         | 12/134 [03:39<37:15, 18.32s/it, v_num=0, train_loss=0.590]\n",
      "Epoch 0:  10%|▉         | 13/134 [03:58<36:55, 18.31s/it, v_num=0, train_loss=0.590]\n",
      "Epoch 0:  10%|▉         | 13/134 [03:58<36:57, 18.32s/it, v_num=0, train_loss=0.592]\n",
      "Epoch 0:  10%|█         | 14/134 [04:16<36:39, 18.33s/it, v_num=0, train_loss=0.592]\n",
      "Epoch 0:  10%|█         | 14/134 [04:16<36:40, 18.34s/it, v_num=0, train_loss=0.590]\n",
      "Epoch 0:  11%|█         | 15/134 [04:35<36:22, 18.34s/it, v_num=0, train_loss=0.590]\n",
      "Epoch 0:  11%|█         | 15/134 [04:35<36:23, 18.35s/it, v_num=0, train_loss=0.551]\n",
      "Epoch 0:  12%|█▏        | 16/134 [04:53<36:04, 18.34s/it, v_num=0, train_loss=0.551]\n",
      "Epoch 0:  12%|█▏        | 16/134 [04:53<36:05, 18.35s/it, v_num=0, train_loss=0.521]\n",
      "Epoch 0:  13%|█▎        | 17/134 [05:11<35:46, 18.34s/it, v_num=0, train_loss=0.521]\n",
      "Epoch 0:  13%|█▎        | 17/134 [05:11<35:47, 18.35s/it, v_num=0, train_loss=0.522]\n",
      "Epoch 0:  13%|█▎        | 18/134 [05:30<35:28, 18.35s/it, v_num=0, train_loss=0.522]\n",
      "Epoch 0:  13%|█▎        | 18/134 [05:30<35:29, 18.35s/it, v_num=0, train_loss=0.517]\n",
      "Epoch 0:  14%|█▍        | 19/134 [05:48<35:10, 18.35s/it, v_num=0, train_loss=0.517]\n",
      "Epoch 0:  14%|█▍        | 19/134 [05:48<35:11, 18.36s/it, v_num=0, train_loss=0.476]\n",
      "Epoch 0:  15%|█▍        | 20/134 [06:07<34:52, 18.35s/it, v_num=0, train_loss=0.476]\n",
      "Epoch 0:  15%|█▍        | 20/134 [06:07<34:53, 18.36s/it, v_num=0, train_loss=0.457]\n",
      "Epoch 0:  16%|█▌        | 21/134 [06:25<34:32, 18.34s/it, v_num=0, train_loss=0.457]\n",
      "Epoch 0:  16%|█▌        | 21/134 [06:25<34:33, 18.35s/it, v_num=0, train_loss=0.476]\n",
      "Epoch 0:  16%|█▋        | 22/134 [06:43<34:14, 18.35s/it, v_num=0, train_loss=0.476]\n",
      "Epoch 0:  16%|█▋        | 22/134 [06:43<34:15, 18.35s/it, v_num=0, train_loss=0.447]\n",
      "Epoch 0:  17%|█▋        | 23/134 [07:01<33:56, 18.35s/it, v_num=0, train_loss=0.447]\n",
      "Epoch 0:  17%|█▋        | 23/134 [07:02<33:57, 18.35s/it, v_num=0, train_loss=0.412]\n",
      "Epoch 0:  18%|█▊        | 24/134 [07:20<33:37, 18.34s/it, v_num=0, train_loss=0.412]\n",
      "Epoch 0:  18%|█▊        | 24/134 [07:20<33:38, 18.35s/it, v_num=0, train_loss=0.385]\n",
      "Epoch 0:  19%|█▊        | 25/134 [07:38<33:20, 18.36s/it, v_num=0, train_loss=0.385]\n",
      "Epoch 0:  19%|█▊        | 25/134 [07:39<33:21, 18.36s/it, v_num=0, train_loss=0.385]\n",
      "Epoch 0:  19%|█▉        | 26/134 [07:57<33:03, 18.36s/it, v_num=0, train_loss=0.385]\n",
      "Epoch 0:  19%|█▉        | 26/134 [07:57<33:03, 18.37s/it, v_num=0, train_loss=0.407]\n",
      "Epoch 0:  20%|██        | 27/134 [08:15<32:44, 18.36s/it, v_num=0, train_loss=0.407]\n",
      "Epoch 0:  20%|██        | 27/134 [08:15<32:45, 18.37s/it, v_num=0, train_loss=0.379]\n",
      "Epoch 0:  21%|██        | 28/134 [08:33<32:25, 18.35s/it, v_num=0, train_loss=0.379]\n",
      "Epoch 0:  21%|██        | 28/134 [08:34<32:26, 18.36s/it, v_num=0, train_loss=0.405]\n",
      "Epoch 0:  22%|██▏       | 29/134 [08:52<32:06, 18.35s/it, v_num=0, train_loss=0.405]\n",
      "Epoch 0:  22%|██▏       | 29/134 [08:52<32:06, 18.35s/it, v_num=0, train_loss=0.355]\n",
      "Epoch 0:  22%|██▏       | 30/134 [09:10<31:47, 18.34s/it, v_num=0, train_loss=0.355]\n",
      "Epoch 0:  22%|██▏       | 30/134 [09:10<31:47, 18.35s/it, v_num=0, train_loss=0.375]\n",
      "Epoch 0:  23%|██▎       | 31/134 [09:28<31:28, 18.33s/it, v_num=0, train_loss=0.375]\n",
      "Epoch 0:  23%|██▎       | 31/134 [09:28<31:28, 18.34s/it, v_num=0, train_loss=0.331]\n",
      "Epoch 0:  24%|██▍       | 32/134 [09:46<31:09, 18.33s/it, v_num=0, train_loss=0.331]\n",
      "Epoch 0:  24%|██▍       | 32/134 [09:46<31:09, 18.33s/it, v_num=0, train_loss=0.360]\n",
      "Epoch 0:  25%|██▍       | 33/134 [10:04<30:49, 18.32s/it, v_num=0, train_loss=0.360]\n",
      "Epoch 0:  25%|██▍       | 33/134 [10:04<30:50, 18.32s/it, v_num=0, train_loss=0.319]\n",
      "Epoch 0:  25%|██▌       | 34/134 [10:22<30:31, 18.32s/it, v_num=0, train_loss=0.319]\n",
      "Epoch 0:  25%|██▌       | 34/134 [10:22<30:32, 18.32s/it, v_num=0, train_loss=0.360]\n",
      "Epoch 0:  26%|██▌       | 35/134 [10:41<30:13, 18.32s/it, v_num=0, train_loss=0.360]\n",
      "Epoch 0:  26%|██▌       | 35/134 [10:41<30:13, 18.32s/it, v_num=0, train_loss=0.405]\n",
      "Epoch 0:  27%|██▋       | 36/134 [10:59<29:54, 18.31s/it, v_num=0, train_loss=0.405]\n",
      "Epoch 0:  27%|██▋       | 36/134 [10:59<29:54, 18.31s/it, v_num=0, train_loss=0.362]\n",
      "Epoch 0:  28%|██▊       | 37/134 [11:17<29:36, 18.31s/it, v_num=0, train_loss=0.362]\n",
      "Epoch 0:  28%|██▊       | 37/134 [11:17<29:36, 18.32s/it, v_num=0, train_loss=0.343]\n",
      "Epoch 0:  28%|██▊       | 38/134 [11:35<29:17, 18.31s/it, v_num=0, train_loss=0.343]\n",
      "Epoch 0:  28%|██▊       | 38/134 [11:35<29:18, 18.31s/it, v_num=0, train_loss=0.335]\n",
      "Epoch 0:  29%|██▉       | 39/134 [11:54<28:59, 18.31s/it, v_num=0, train_loss=0.335]\n",
      "Epoch 0:  29%|██▉       | 39/134 [11:54<28:59, 18.31s/it, v_num=0, train_loss=0.326]\n",
      "Epoch 0:  30%|██▉       | 40/134 [12:12<28:40, 18.30s/it, v_num=0, train_loss=0.326]\n",
      "Epoch 0:  30%|██▉       | 40/134 [12:12<28:40, 18.31s/it, v_num=0, train_loss=0.344]\n",
      "Epoch 0:  31%|███       | 41/134 [12:30<28:22, 18.30s/it, v_num=0, train_loss=0.344]\n",
      "Epoch 0:  31%|███       | 41/134 [12:30<28:22, 18.31s/it, v_num=0, train_loss=0.312]\n",
      "Epoch 0:  31%|███▏      | 42/134 [12:48<28:03, 18.30s/it, v_num=0, train_loss=0.312]\n",
      "Epoch 0:  31%|███▏      | 42/134 [12:48<28:03, 18.30s/it, v_num=0, train_loss=0.338]\n",
      "Epoch 0:  32%|███▏      | 43/134 [13:06<27:44, 18.29s/it, v_num=0, train_loss=0.338]\n",
      "Epoch 0:  32%|███▏      | 43/134 [13:06<27:44, 18.29s/it, v_num=0, train_loss=0.316]\n",
      "Epoch 0:  33%|███▎      | 44/134 [13:24<27:25, 18.28s/it, v_num=0, train_loss=0.316]\n",
      "Epoch 0:  33%|███▎      | 44/134 [13:24<27:25, 18.29s/it, v_num=0, train_loss=0.329]\n",
      "Epoch 0:  34%|███▎      | 45/134 [13:42<27:06, 18.28s/it, v_num=0, train_loss=0.329]\n",
      "Epoch 0:  34%|███▎      | 45/134 [13:42<27:07, 18.28s/it, v_num=0, train_loss=0.254]\n",
      "Epoch 0:  34%|███▍      | 46/134 [14:00<26:48, 18.28s/it, v_num=0, train_loss=0.254]\n",
      "Epoch 0:  34%|███▍      | 46/134 [14:00<26:48, 18.28s/it, v_num=0, train_loss=0.310]\n",
      "Epoch 0:  35%|███▌      | 47/134 [14:19<26:31, 18.29s/it, v_num=0, train_loss=0.310]\n",
      "Epoch 0:  35%|███▌      | 47/134 [14:19<26:31, 18.29s/it, v_num=0, train_loss=0.294]\n",
      "Epoch 0:  36%|███▌      | 48/134 [14:37<26:12, 18.29s/it, v_num=0, train_loss=0.294]\n",
      "Epoch 0:  36%|███▌      | 48/134 [14:37<26:13, 18.29s/it, v_num=0, train_loss=0.302]\n",
      "Epoch 0:  37%|███▋      | 49/134 [14:55<25:53, 18.28s/it, v_num=0, train_loss=0.302]\n",
      "Epoch 0:  37%|███▋      | 49/134 [14:55<25:54, 18.28s/it, v_num=0, train_loss=0.327]\n",
      "Epoch 0:  37%|███▋      | 50/134 [15:13<25:35, 18.27s/it, v_num=0, train_loss=0.327]\n",
      "Epoch 0:  37%|███▋      | 50/134 [15:13<25:35, 18.28s/it, v_num=0, train_loss=0.251]\n",
      "Epoch 0:  38%|███▊      | 51/134 [15:31<25:16, 18.27s/it, v_num=0, train_loss=0.251]\n",
      "Epoch 0:  38%|███▊      | 51/134 [15:32<25:16, 18.28s/it, v_num=0, train_loss=0.291]\n",
      "Epoch 0:  39%|███▉      | 52/134 [15:49<24:57, 18.26s/it, v_num=0, train_loss=0.291]\n",
      "Epoch 0:  39%|███▉      | 52/134 [15:49<24:57, 18.27s/it, v_num=0, train_loss=0.263]\n",
      "Epoch 0:  40%|███▉      | 53/134 [16:07<24:39, 18.26s/it, v_num=0, train_loss=0.263]\n",
      "Epoch 0:  40%|███▉      | 53/134 [16:08<24:39, 18.27s/it, v_num=0, train_loss=0.292]\n",
      "Epoch 0:  40%|████      | 54/134 [16:26<24:20, 18.26s/it, v_num=0, train_loss=0.292]\n",
      "Epoch 0:  40%|████      | 54/134 [16:26<24:21, 18.26s/it, v_num=0, train_loss=0.245]\n",
      "Epoch 0:  41%|████      | 55/134 [16:44<24:02, 18.26s/it, v_num=0, train_loss=0.245]\n",
      "Epoch 0:  41%|████      | 55/134 [16:44<24:02, 18.26s/it, v_num=0, train_loss=0.265]\n",
      "Epoch 0:  42%|████▏     | 56/134 [17:02<23:44, 18.26s/it, v_num=0, train_loss=0.265]\n",
      "Epoch 0:  42%|████▏     | 56/134 [17:02<23:44, 18.26s/it, v_num=0, train_loss=0.233]\n",
      "Epoch 0:  43%|████▎     | 57/134 [17:20<23:25, 18.26s/it, v_num=0, train_loss=0.233]\n",
      "Epoch 0:  43%|████▎     | 57/134 [17:20<23:26, 18.26s/it, v_num=0, train_loss=0.228]\n",
      "Epoch 0:  43%|████▎     | 58/134 [17:39<23:07, 18.26s/it, v_num=0, train_loss=0.228]\n",
      "Epoch 0:  43%|████▎     | 58/134 [17:39<23:07, 18.26s/it, v_num=0, train_loss=0.222]\n",
      "Epoch 0:  44%|████▍     | 59/134 [17:57<22:49, 18.25s/it, v_num=0, train_loss=0.222]\n",
      "Epoch 0:  44%|████▍     | 59/134 [17:57<22:49, 18.26s/it, v_num=0, train_loss=0.242]\n",
      "Epoch 0:  45%|████▍     | 60/134 [18:15<22:30, 18.26s/it, v_num=0, train_loss=0.242]\n",
      "Epoch 0:  45%|████▍     | 60/134 [18:15<22:31, 18.26s/it, v_num=0, train_loss=0.220]\n",
      "Epoch 0:  46%|████▌     | 61/134 [18:33<22:12, 18.26s/it, v_num=0, train_loss=0.220]\n",
      "Epoch 0:  46%|████▌     | 61/134 [18:33<22:12, 18.26s/it, v_num=0, train_loss=0.236]\n",
      "Epoch 0:  46%|████▋     | 62/134 [18:51<21:54, 18.25s/it, v_num=0, train_loss=0.236]\n",
      "Epoch 0:  46%|████▋     | 62/134 [18:51<21:54, 18.25s/it, v_num=0, train_loss=0.230]\n",
      "Epoch 0:  47%|████▋     | 63/134 [19:09<21:35, 18.25s/it, v_num=0, train_loss=0.230]\n",
      "Epoch 0:  47%|████▋     | 63/134 [19:09<21:35, 18.25s/it, v_num=0, train_loss=0.247]\n",
      "Epoch 0:  48%|████▊     | 64/134 [19:27<21:17, 18.25s/it, v_num=0, train_loss=0.247]\n",
      "Epoch 0:  48%|████▊     | 64/134 [19:27<21:17, 18.25s/it, v_num=0, train_loss=0.243]\n",
      "Epoch 0:  49%|████▊     | 65/134 [19:45<20:58, 18.24s/it, v_num=0, train_loss=0.243]\n",
      "Epoch 0:  49%|████▊     | 65/134 [19:45<20:58, 18.25s/it, v_num=0, train_loss=0.233]\n",
      "Epoch 0:  49%|████▉     | 66/134 [20:07<20:44, 18.30s/it, v_num=0, train_loss=0.233]\n",
      "Epoch 0:  49%|████▉     | 66/134 [20:07<20:44, 18.30s/it, v_num=0, train_loss=0.253]\n",
      "Epoch 0:  50%|█████     | 67/134 [20:25<20:25, 18.29s/it, v_num=0, train_loss=0.253]\n",
      "Epoch 0:  50%|█████     | 67/134 [20:25<20:25, 18.29s/it, v_num=0, train_loss=0.235]\n",
      "Epoch 0:  51%|█████     | 68/134 [20:43<20:06, 18.29s/it, v_num=0, train_loss=0.235]\n",
      "Epoch 0:  51%|█████     | 68/134 [20:43<20:07, 18.29s/it, v_num=0, train_loss=0.270]\n",
      "Epoch 0:  51%|█████▏    | 69/134 [21:01<19:48, 18.29s/it, v_num=0, train_loss=0.270]\n",
      "Epoch 0:  51%|█████▏    | 69/134 [21:01<19:48, 18.29s/it, v_num=0, train_loss=0.219]\n",
      "Epoch 0:  52%|█████▏    | 70/134 [21:19<19:30, 18.29s/it, v_num=0, train_loss=0.219]\n",
      "Epoch 0:  52%|█████▏    | 70/134 [21:20<19:30, 18.29s/it, v_num=0, train_loss=0.249]\n",
      "Epoch 0:  53%|█████▎    | 71/134 [21:38<19:11, 18.28s/it, v_num=0, train_loss=0.249]\n",
      "Epoch 0:  53%|█████▎    | 71/134 [21:38<19:11, 18.28s/it, v_num=0, train_loss=0.231]\n",
      "Epoch 0:  54%|█████▎    | 72/134 [21:56<18:53, 18.28s/it, v_num=0, train_loss=0.231]\n",
      "Epoch 0:  54%|█████▎    | 72/134 [21:56<18:53, 18.28s/it, v_num=0, train_loss=0.206]\n",
      "Epoch 0:  54%|█████▍    | 73/134 [22:13<18:34, 18.27s/it, v_num=0, train_loss=0.206]\n",
      "Epoch 0:  54%|█████▍    | 73/134 [22:14<18:34, 18.28s/it, v_num=0, train_loss=0.266]\n",
      "Epoch 0:  55%|█████▌    | 74/134 [22:32<18:16, 18.27s/it, v_num=0, train_loss=0.266]\n",
      "Epoch 0:  55%|█████▌    | 74/134 [22:32<18:16, 18.27s/it, v_num=0, train_loss=0.251]\n",
      "Epoch 0:  56%|█████▌    | 75/134 [22:50<17:57, 18.27s/it, v_num=0, train_loss=0.251]\n",
      "Epoch 0:  56%|█████▌    | 75/134 [22:50<17:58, 18.27s/it, v_num=0, train_loss=0.219]\n",
      "Epoch 0:  57%|█████▋    | 76/134 [23:07<17:39, 18.26s/it, v_num=0, train_loss=0.219]\n",
      "Epoch 0:  57%|█████▋    | 76/134 [23:08<17:39, 18.27s/it, v_num=0, train_loss=0.195]\n",
      "Epoch 0:  57%|█████▋    | 77/134 [23:25<17:20, 18.26s/it, v_num=0, train_loss=0.195]\n",
      "Epoch 0:  57%|█████▋    | 77/134 [23:26<17:20, 18.26s/it, v_num=0, train_loss=0.211]\n",
      "Epoch 0:  58%|█████▊    | 78/134 [23:44<17:02, 18.26s/it, v_num=0, train_loss=0.211]\n",
      "Epoch 0:  58%|█████▊    | 78/134 [23:44<17:02, 18.26s/it, v_num=0, train_loss=0.197]\n",
      "Epoch 0:  59%|█████▉    | 79/134 [24:02<16:44, 18.26s/it, v_num=0, train_loss=0.197]\n",
      "Epoch 0:  59%|█████▉    | 79/134 [24:02<16:44, 18.26s/it, v_num=0, train_loss=0.233]\n",
      "Epoch 0:  60%|█████▉    | 80/134 [24:21<16:26, 18.27s/it, v_num=0, train_loss=0.233]\n",
      "Epoch 0:  60%|█████▉    | 80/134 [24:21<16:26, 18.27s/it, v_num=0, train_loss=0.265]\n",
      "Epoch 0:  60%|██████    | 81/134 [24:39<16:08, 18.27s/it, v_num=0, train_loss=0.265]\n",
      "Epoch 0:  60%|██████    | 81/134 [24:39<16:08, 18.27s/it, v_num=0, train_loss=0.244]\n",
      "Epoch 0:  61%|██████    | 82/134 [24:57<15:49, 18.26s/it, v_num=0, train_loss=0.244]\n",
      "Epoch 0:  61%|██████    | 82/134 [24:57<15:49, 18.26s/it, v_num=0, train_loss=0.173]\n",
      "Epoch 0:  62%|██████▏   | 83/134 [25:15<15:31, 18.26s/it, v_num=0, train_loss=0.173]\n",
      "Epoch 0:  62%|██████▏   | 83/134 [25:15<15:31, 18.26s/it, v_num=0, train_loss=0.224]\n",
      "Epoch 0:  63%|██████▎   | 84/134 [25:33<15:12, 18.25s/it, v_num=0, train_loss=0.224]\n",
      "Epoch 0:  63%|██████▎   | 84/134 [25:33<15:12, 18.26s/it, v_num=0, train_loss=0.231]\n",
      "Epoch 0:  63%|██████▎   | 85/134 [25:51<14:54, 18.25s/it, v_num=0, train_loss=0.231]\n",
      "Epoch 0:  63%|██████▎   | 85/134 [25:51<14:54, 18.25s/it, v_num=0, train_loss=0.234]\n",
      "Epoch 0:  64%|██████▍   | 86/134 [26:09<14:35, 18.25s/it, v_num=0, train_loss=0.234]\n",
      "Epoch 0:  64%|██████▍   | 86/134 [26:09<14:36, 18.25s/it, v_num=0, train_loss=0.257]\n",
      "Epoch 0:  65%|██████▍   | 87/134 [26:27<14:17, 18.25s/it, v_num=0, train_loss=0.257]\n",
      "Epoch 0:  65%|██████▍   | 87/134 [26:27<14:17, 18.25s/it, v_num=0, train_loss=0.296]\n",
      "Epoch 0:  66%|██████▌   | 88/134 [26:44<13:58, 18.24s/it, v_num=0, train_loss=0.296]\n",
      "Epoch 0:  66%|██████▌   | 88/134 [26:45<13:59, 18.24s/it, v_num=0, train_loss=0.275]\n",
      "Epoch 0:  66%|██████▋   | 89/134 [27:02<13:40, 18.23s/it, v_num=0, train_loss=0.275]\n",
      "Epoch 0:  66%|██████▋   | 89/134 [27:02<13:40, 18.23s/it, v_num=0, train_loss=0.263]\n",
      "Epoch 0:  67%|██████▋   | 90/134 [27:20<13:22, 18.23s/it, v_num=0, train_loss=0.263]\n",
      "Epoch 0:  67%|██████▋   | 90/134 [27:20<13:22, 18.23s/it, v_num=0, train_loss=0.268]\n",
      "Epoch 0:  68%|██████▊   | 91/134 [27:38<13:03, 18.23s/it, v_num=0, train_loss=0.268]\n",
      "Epoch 0:  68%|██████▊   | 91/134 [27:38<13:03, 18.23s/it, v_num=0, train_loss=0.270]\n",
      "Epoch 0:  69%|██████▊   | 92/134 [27:56<12:45, 18.22s/it, v_num=0, train_loss=0.270]\n",
      "Epoch 0:  69%|██████▊   | 92/134 [27:56<12:45, 18.22s/it, v_num=0, train_loss=0.199]\n",
      "Epoch 0:  69%|██████▉   | 93/134 [28:14<12:26, 18.22s/it, v_num=0, train_loss=0.199]\n",
      "Epoch 0:  69%|██████▉   | 93/134 [28:14<12:26, 18.22s/it, v_num=0, train_loss=0.229]\n",
      "Epoch 0:  70%|███████   | 94/134 [28:31<12:08, 18.21s/it, v_num=0, train_loss=0.229]\n",
      "Epoch 0:  70%|███████   | 94/134 [28:32<12:08, 18.21s/it, v_num=0, train_loss=0.247]\n",
      "Epoch 0:  71%|███████   | 95/134 [28:50<11:50, 18.21s/it, v_num=0, train_loss=0.247]\n",
      "Epoch 0:  71%|███████   | 95/134 [28:50<11:50, 18.21s/it, v_num=0, train_loss=0.217]\n",
      "Epoch 0:  72%|███████▏  | 96/134 [29:08<11:31, 18.21s/it, v_num=0, train_loss=0.217]\n",
      "Epoch 0:  72%|███████▏  | 96/134 [29:08<11:32, 18.21s/it, v_num=0, train_loss=0.215]\n",
      "Epoch 0:  72%|███████▏  | 97/134 [29:25<11:13, 18.21s/it, v_num=0, train_loss=0.215]\n",
      "Epoch 0:  72%|███████▏  | 97/134 [29:26<11:13, 18.21s/it, v_num=0, train_loss=0.238]\n",
      "Epoch 0:  73%|███████▎  | 98/134 [29:43<10:55, 18.20s/it, v_num=0, train_loss=0.238]\n",
      "Epoch 0:  73%|███████▎  | 98/134 [29:43<10:55, 18.20s/it, v_num=0, train_loss=0.167]\n",
      "Epoch 0:  74%|███████▍  | 99/134 [30:01<10:37, 18.20s/it, v_num=0, train_loss=0.167]\n",
      "Epoch 0:  74%|███████▍  | 99/134 [30:02<10:37, 18.20s/it, v_num=0, train_loss=0.197]\n",
      "Epoch 0:  75%|███████▍  | 100/134 [30:19<10:18, 18.20s/it, v_num=0, train_loss=0.197]\n",
      "Epoch 0:  75%|███████▍  | 100/134 [30:20<10:18, 18.20s/it, v_num=0, train_loss=0.205]\n",
      "Epoch 0:  75%|███████▌  | 101/134 [30:38<10:00, 18.20s/it, v_num=0, train_loss=0.205]\n",
      "Epoch 0:  75%|███████▌  | 101/134 [30:38<10:00, 18.20s/it, v_num=0, train_loss=0.165]\n",
      "Epoch 0:  76%|███████▌  | 102/134 [30:56<09:42, 18.20s/it, v_num=0, train_loss=0.165]\n",
      "Epoch 0:  76%|███████▌  | 102/134 [30:56<09:42, 18.20s/it, v_num=0, train_loss=0.262]\n",
      "Epoch 0:  77%|███████▋  | 103/134 [31:14<09:24, 18.20s/it, v_num=0, train_loss=0.262]\n",
      "Epoch 0:  77%|███████▋  | 103/134 [31:14<09:24, 18.20s/it, v_num=0, train_loss=0.250]\n",
      "Epoch 0:  78%|███████▊  | 104/134 [31:32<09:05, 18.20s/it, v_num=0, train_loss=0.250]\n",
      "Epoch 0:  78%|███████▊  | 104/134 [31:32<09:05, 18.20s/it, v_num=0, train_loss=0.161]\n",
      "Epoch 0:  78%|███████▊  | 105/134 [31:50<08:47, 18.20s/it, v_num=0, train_loss=0.161]\n",
      "Epoch 0:  78%|███████▊  | 105/134 [31:50<08:47, 18.20s/it, v_num=0, train_loss=0.201]\n",
      "Epoch 0:  79%|███████▉  | 106/134 [32:08<08:29, 18.20s/it, v_num=0, train_loss=0.201]\n",
      "Epoch 0:  79%|███████▉  | 106/134 [32:09<08:29, 18.20s/it, v_num=0, train_loss=0.176]\n",
      "Epoch 0:  80%|███████▉  | 107/134 [32:27<08:11, 18.20s/it, v_num=0, train_loss=0.176]\n",
      "Epoch 0:  80%|███████▉  | 107/134 [32:27<08:11, 18.20s/it, v_num=0, train_loss=0.225]\n",
      "Epoch 0:  81%|████████  | 108/134 [32:45<07:53, 18.20s/it, v_num=0, train_loss=0.225]\n",
      "Epoch 0:  81%|████████  | 108/134 [32:45<07:53, 18.20s/it, v_num=0, train_loss=0.187]\n",
      "Epoch 0:  81%|████████▏ | 109/134 [33:03<07:34, 18.20s/it, v_num=0, train_loss=0.187]\n",
      "Epoch 0:  81%|████████▏ | 109/134 [33:03<07:34, 18.20s/it, v_num=0, train_loss=0.205]\n",
      "Epoch 0:  82%|████████▏ | 110/134 [33:21<07:16, 18.20s/it, v_num=0, train_loss=0.205]\n",
      "Epoch 0:  82%|████████▏ | 110/134 [33:21<07:16, 18.20s/it, v_num=0, train_loss=0.217]\n",
      "Epoch 0:  83%|████████▎ | 111/134 [33:39<06:58, 18.19s/it, v_num=0, train_loss=0.217]\n",
      "Epoch 0:  83%|████████▎ | 111/134 [33:39<06:58, 18.20s/it, v_num=0, train_loss=0.259]\n",
      "Epoch 0:  84%|████████▎ | 112/134 [33:57<06:40, 18.20s/it, v_num=0, train_loss=0.259]\n",
      "Epoch 0:  84%|████████▎ | 112/134 [33:57<06:40, 18.20s/it, v_num=0, train_loss=0.254]\n",
      "Epoch 0:  84%|████████▍ | 113/134 [34:16<06:22, 18.20s/it, v_num=0, train_loss=0.254]\n",
      "Epoch 0:  84%|████████▍ | 113/134 [34:16<06:22, 18.20s/it, v_num=0, train_loss=0.220]\n",
      "Epoch 0:  85%|████████▌ | 114/134 [34:34<06:03, 18.20s/it, v_num=0, train_loss=0.220]\n",
      "Epoch 0:  85%|████████▌ | 114/134 [34:34<06:03, 18.20s/it, v_num=0, train_loss=0.185]\n",
      "Epoch 0:  86%|████████▌ | 115/134 [34:52<05:45, 18.19s/it, v_num=0, train_loss=0.185]\n",
      "Epoch 0:  86%|████████▌ | 115/134 [34:52<05:45, 18.20s/it, v_num=0, train_loss=0.188]\n",
      "Epoch 0:  87%|████████▋ | 116/134 [35:10<05:27, 18.19s/it, v_num=0, train_loss=0.188]\n",
      "Epoch 0:  87%|████████▋ | 116/134 [35:10<05:27, 18.19s/it, v_num=0, train_loss=0.169]\n",
      "Epoch 0:  87%|████████▋ | 117/134 [35:28<05:09, 18.19s/it, v_num=0, train_loss=0.169]\n",
      "Epoch 0:  87%|████████▋ | 117/134 [35:28<05:09, 18.19s/it, v_num=0, train_loss=0.167]\n",
      "Epoch 0:  88%|████████▊ | 118/134 [35:46<04:51, 18.19s/it, v_num=0, train_loss=0.167]\n",
      "Epoch 0:  88%|████████▊ | 118/134 [35:46<04:51, 18.19s/it, v_num=0, train_loss=0.185]\n",
      "Epoch 0:  89%|████████▉ | 119/134 [36:04<04:32, 18.19s/it, v_num=0, train_loss=0.185]\n",
      "Epoch 0:  89%|████████▉ | 119/134 [36:04<04:32, 18.19s/it, v_num=0, train_loss=0.229]\n",
      "Epoch 0:  90%|████████▉ | 120/134 [36:22<04:14, 18.19s/it, v_num=0, train_loss=0.229]\n",
      "Epoch 0:  90%|████████▉ | 120/134 [36:22<04:14, 18.19s/it, v_num=0, train_loss=0.251]\n",
      "Epoch 0:  90%|█████████ | 121/134 [36:40<03:56, 18.19s/it, v_num=0, train_loss=0.251]\n",
      "Epoch 0:  90%|█████████ | 121/134 [36:40<03:56, 18.19s/it, v_num=0, train_loss=0.244]\n",
      "Epoch 0:  91%|█████████ | 122/134 [36:58<03:38, 18.19s/it, v_num=0, train_loss=0.244]\n",
      "Epoch 0:  91%|█████████ | 122/134 [36:58<03:38, 18.19s/it, v_num=0, train_loss=0.231]\n",
      "Epoch 0:  92%|█████████▏| 123/134 [37:16<03:20, 18.19s/it, v_num=0, train_loss=0.231]\n",
      "Epoch 0:  92%|█████████▏| 123/134 [37:16<03:20, 18.19s/it, v_num=0, train_loss=0.201]\n",
      "Epoch 0:  93%|█████████▎| 124/134 [37:34<03:01, 18.18s/it, v_num=0, train_loss=0.201]\n",
      "Epoch 0:  93%|█████████▎| 124/134 [37:34<03:01, 18.19s/it, v_num=0, train_loss=0.152]\n",
      "Epoch 0:  93%|█████████▎| 125/134 [37:52<02:43, 18.18s/it, v_num=0, train_loss=0.152]\n",
      "Epoch 0:  93%|█████████▎| 125/134 [37:52<02:43, 18.18s/it, v_num=0, train_loss=0.162]\n",
      "Epoch 0:  94%|█████████▍| 126/134 [38:10<02:25, 18.18s/it, v_num=0, train_loss=0.162]\n",
      "Epoch 0:  94%|█████████▍| 126/134 [38:10<02:25, 18.18s/it, v_num=0, train_loss=0.183]\n",
      "Epoch 0:  95%|█████████▍| 127/134 [38:28<02:07, 18.18s/it, v_num=0, train_loss=0.183]\n",
      "Epoch 0:  95%|█████████▍| 127/134 [38:29<02:07, 18.18s/it, v_num=0, train_loss=0.187]\n",
      "Epoch 0:  96%|█████████▌| 128/134 [38:46<01:49, 18.18s/it, v_num=0, train_loss=0.187]\n",
      "Epoch 0:  96%|█████████▌| 128/134 [38:46<01:49, 18.18s/it, v_num=0, train_loss=0.209]\n",
      "Epoch 0:  96%|█████████▋| 129/134 [39:04<01:30, 18.18s/it, v_num=0, train_loss=0.209]\n",
      "Epoch 0:  96%|█████████▋| 129/134 [39:05<01:30, 18.18s/it, v_num=0, train_loss=0.243]\n",
      "Epoch 0:  97%|█████████▋| 130/134 [39:23<01:12, 18.18s/it, v_num=0, train_loss=0.243]\n",
      "Epoch 0:  97%|█████████▋| 130/134 [39:23<01:12, 18.18s/it, v_num=0, train_loss=0.243]\n",
      "Epoch 0:  98%|█████████▊| 131/134 [39:41<00:54, 18.18s/it, v_num=0, train_loss=0.243]\n",
      "Epoch 0:  98%|█████████▊| 131/134 [39:41<00:54, 18.18s/it, v_num=0, train_loss=0.214]\n",
      "Epoch 0:  99%|█████████▊| 132/134 [39:59<00:36, 18.18s/it, v_num=0, train_loss=0.214]\n",
      "Epoch 0:  99%|█████████▊| 132/134 [39:59<00:36, 18.18s/it, v_num=0, train_loss=0.214]\n",
      "Epoch 0:  99%|█████████▉| 133/134 [40:17<00:18, 18.18s/it, v_num=0, train_loss=0.214]\n",
      "Epoch 0:  99%|█████████▉| 133/134 [40:17<00:18, 18.18s/it, v_num=0, train_loss=0.183]\n",
      "Epoch 0: 100%|██████████| 134/134 [40:35<00:00, 18.18s/it, v_num=0, train_loss=0.183]\n",
      "Epoch 0: 100%|██████████| 134/134 [40:35<00:00, 18.18s/it, v_num=0, train_loss=0.174]\n",
      "Epoch 0: : 135it [40:53, 18.18s/it, v_num=0, train_loss=0.174]                       \n",
      "Epoch 0: : 135it [40:53, 18.18s/it, v_num=0, train_loss=0.176]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"trialProgress\">\n",
       "  <h3>Trial Progress</h3>\n",
       "  <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>_report_on     </th><th>date               </th><th>done  </th><th style=\"text-align: right;\">  epoch</th><th style=\"text-align: right;\">  experiment_tag</th><th>hostname      </th><th style=\"text-align: right;\">  iterations_since_restore</th><th>node_ip    </th><th style=\"text-align: right;\">  pid</th><th>should_checkpoint  </th><th style=\"text-align: right;\">  step</th><th style=\"text-align: right;\">  time_since_restore</th><th style=\"text-align: right;\">  time_this_iter_s</th><th style=\"text-align: right;\">  time_total_s</th><th style=\"text-align: right;\">  timestamp</th><th style=\"text-align: right;\">  train_loss</th><th style=\"text-align: right;\">  training_iteration</th><th>trial_id   </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>LightningTrainer_a1a3d_00000</td><td>train_epoch_end</td><td>2023-05-03_02-12-53</td><td>True  </td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">               0</td><td>ip-10-0-108-10</td><td style=\"text-align: right;\">                         1</td><td>10.0.108.10</td><td style=\"text-align: right;\"> 8284</td><td>True               </td><td style=\"text-align: right;\">   135</td><td style=\"text-align: right;\">             3024.09</td><td style=\"text-align: right;\">           3024.09</td><td style=\"text-align: right;\">       3024.09</td><td style=\"text-align: right;\"> 1683105172</td><td style=\"text-align: right;\">    0.176025</td><td style=\"text-align: right;\">                   1</td><td>a1a3d_00000</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".trialProgress {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".trialProgress h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".trialProgress td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(RayTrainWorker pid=8425) `Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "(RayTrainWorker pid=8425) RayFSDPStrategy: tearing down strategy...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: : 135it [46:32, 20.68s/it, v_num=0, train_loss=0.176]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(LightningTrainer pid=8284) 2023-05-03 02:16:06,564\tWARNING util.py:315 -- Uploading trial artifacts took 152.554 s, which may be a performance bottleneck. Consider saving fewer/smaller artifacts to the trial log directory, or disable artifact syncing with `SyncConfig(sync_artifacts=False)`.\n",
      "2023-05-03 02:22:06,992\tWARNING experiment_state.py:306 -- Syncing the experiment checkpoint to cloud took a long time with 358.18 seconds. This can be due to a large number of trials, large logfiles, or throttling from the remote storage provider for too frequent syncs. If your `CheckpointConfig.num_to_keep` is a low number, this can trigger frequent syncing, in which case you should increase it. \n",
      "2023-05-03 02:22:06,997\tINFO tune.py:1010 -- Total run time: 3583.09 seconds (3224.62 seconds for the tuning loop).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Result(\n",
       "  metrics={'_report_on': 'train_epoch_end', 'train_loss': 0.176025390625, 'epoch': 0, 'step': 135, 'should_checkpoint': True, 'done': True, 'trial_id': 'a1a3d_00000', 'experiment_tag': '0'},\n",
       "  path='s3://anyscale-staging-data-cld-kvedzwag2qa8i5bjxuevf5i7/ray-lightning-results-7b/finetune_dolly-v2-7b/LightningTrainer_a1a3d_00000_0_2023-05-03_01-22-24',\n",
       "  checkpoint=LightningCheckpoint(uri=s3://anyscale-staging-data-cld-kvedzwag2qa8i5bjxuevf5i7/ray-lightning-results-7b/finetune_dolly-v2-7b/LightningTrainer_a1a3d_00000_0_2023-05-03_01-22-24/checkpoint_000000)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ray.tune.syncer import SyncConfig\n",
    "# Save AIR checkpoints according to the performance on validation set\n",
    "run_config = RunConfig(\n",
    "    name=\"finetune_dolly-v2-7b\",\n",
    "    # storage_path=\"s3://anyscale-staging-data-cld-kvedzwag2qa8i5bjxuevf5i7/ray-lightning-results-7b/\",\n",
    "    checkpoint_config=CheckpointConfig(),\n",
    "    sync_config=SyncConfig(sync_artifacts=False)\n",
    ")\n",
    "\n",
    "# Scale the DDP training workload across 16 GPUs\n",
    "# You can change this config based on your compute resources.\n",
    "scaling_config = ScalingConfig(\n",
    "    num_workers=num_workers, use_gpu=True, resources_per_worker={\"CPU\": 12, \"GPU\": 1}\n",
    ")\n",
    "\n",
    "trainer = LightningTrainer(\n",
    "    lightning_config=lightning_config.build(),\n",
    "    run_config=run_config,\n",
    "    scaling_config=scaling_config,\n",
    "    datasets={\"train\": ray_datasets[\"train\"]},\n",
    "    datasets_iter_config={\"batch_size\": batch_size_per_worker},\n",
    "    preprocessor=preprocessor,\n",
    ")\n",
    "result = trainer.fit()\n",
    "\n",
    "result\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We finished training in 3024s. The price for an on-demand g4dn.4xlarge instance is `$1.204/hour`, while a g4dn.4xlarge instance costs `$2.176/hour`. The total cost would be `($1.204 * 15 + $2.176) * 3024 / 3600 = $17`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text-generation with HuggingFace Pipeline\n",
    "\n",
    "Next, we can use the [HuggingFace Pipeline](https://huggingface.co/docs/transformers/main_classes/pipelines) to generate predictions from our fine-tuned model. Let's input some prompts and see if our tuned Dolly can speak like Shakespeare:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, padding_side=\"right\")\n",
    "\n",
    "# 7B model cannot fit in one T4 GPU (15GiB). Load it to CPU first.\n",
    "dolly = result.checkpoint.get_model(model_class=DollyV2Model, map_location=torch.device(\"cpu\"))\n",
    "\n",
    "# Using device_map=\"auto\", 🤗 Accelerate automatically put layers to different devices based on the available resources.\n",
    "nlp_pipeline = pipeline(task=\"text-generation\", model=dolly.model, tokenizer=tokenizer, device_map=\"auto\")\n",
    "\n",
    "for prompt in [\"This is\", \"I am\", \"Once more\"]:\n",
    "    print(nlp_pipeline(prompt, max_new_tokens=15, do_sample=True, pad_token_id=tokenizer.eos_token_id))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
