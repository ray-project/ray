ppo-breakoutnoframeskip-v5:
    env: ALE/Breakout-v5
    run: PPO
    # Minimum reward and total ts (in given time_total_s) to pass this test.
    pass_criteria:
        sampler_results/episode_reward_mean: 100.0
        timesteps_total: 10000000
    stop:
        time_total_s: 3600
    config:
        # Make analogous to old v4 + NoFrameskip.
        env_config:
            frameskip: 1
            full_action_space: false
            repeat_action_probability: 0.0
        lambda: 0.95
        kl_coeff: 0.5
        clip_rewards: true
        clip_param: 0.1
        vf_clip_param: 10.0
        entropy_coeff: 0.01
        train_batch_size: 16000
        rollout_fragment_length: auto
        sgd_minibatch_size: 2000
        num_sgd_iter: 10
        num_workers: 59
        num_envs_per_worker: 1
        model:
            vf_share_layers: true
            conv_filters: [[16, 4, 2], [32, 4, 2], [64, 4, 2], [128, 4, 2]]
            conv_activation: relu
            post_fcnet_hiddens: [256]

        min_time_s_per_iteration: 30
        lr: 0.0001

        grad_clip: 100.0
        grad_clip_by: global_norm

        # Run with Learner- and RLModule API (new stack).
        _enable_learner_api: true
        _enable_rl_module_api: true
        # Use N Learner worker on the GPU
        num_learner_workers: 4
        num_gpus_per_learner_worker: 1
        num_gpus: 0  # No GPU needed for driver.
        # Since we are using learner workers, the driver process does not need
        # a CPU in particular.
        num_cpus_for_local_worker: 1
        # Need to unset this b/c we are using the RLModule API, which
        # provides exploration control via the RLModule's `forward_exploration` method.
        exploration_config: {}
