applications:
  - args:
      llm_configs:
        - model_loading_config:
            model_id: my_llama
            model_source:
              bucket_uri: s3://anonymous@air-example-data/rayllm-ossci/meta-Llama-3.2-1B-Instruct
          accelerator_type: A10G
          engine_kwargs:
            max_model_len: 8192
          runtime_env:
            env_vars:
              VLLM_USE_V1: "0"
    import_path: ray.serve.llm:build_openai_app
    name: llm-endpoint
    route_prefix: /
