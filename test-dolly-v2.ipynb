{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"RAY_ML_DEV\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ray/anaconda3/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Found cached dataset tiny_shakespeare (/home/ray/.cache/huggingface/datasets/tiny_shakespeare/default/1.0.0/b5b13969f09fe8707337f6cb296314fbe06960bd9a868dca39e713e163d27b5e)\n",
      "100%|██████████| 3/3 [00:00<00:00, 1043.88it/s]\n",
      "2023-04-29 01:48:06,010\tINFO worker.py:1432 -- Connecting to existing Ray cluster at address: 10.0.121.51:6379...\n",
      "2023-04-29 01:48:06,019\tINFO worker.py:1607 -- Connected to Ray cluster. View the dashboard at https://console.anyscale-staging.com/api/v2/sessions/ses_m411tiqu8eluvt1k5ivfqj4q5r/services?redirect_to=dashboard \n",
      "2023-04-29 01:48:06,615\tINFO packaging.py:520 -- Creating a file package for local directory '/tmp/ray_tmp_module/ray'.\n",
      "2023-04-29 01:48:06,743\tWARNING packaging.py:394 -- File /tmp/ray_tmp_module/ray/jars/ray_dist.jar is very large (30.48MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/tmp/ray_tmp_module/ray/jars/ray_dist.jar']})`\n",
      "2023-04-29 01:48:06,798\tWARNING packaging.py:394 -- File /tmp/ray_tmp_module/ray/_raylet.so is very large (25.37MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/tmp/ray_tmp_module/ray/_raylet.so']})`\n",
      "2023-04-29 01:48:06,844\tWARNING packaging.py:394 -- File /tmp/ray_tmp_module/ray/core/src/ray/gcs/gcs_server is very large (21.24MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/tmp/ray_tmp_module/ray/core/src/ray/gcs/gcs_server']})`\n",
      "2023-04-29 01:48:06,885\tWARNING packaging.py:394 -- File /tmp/ray_tmp_module/ray/core/src/ray/raylet/raylet is very large (20.56MiB). Consider adding this file to the 'excludes' list to skip uploading it: `ray.init(..., runtime_env={'excludes': ['/tmp/ray_tmp_module/ray/core/src/ray/raylet/raylet']})`\n",
      "2023-04-29 01:48:07,526\tINFO packaging.py:347 -- Pushing file package 'gcs://_ray_pkg_06d4befa94ba66a2.zip' (155.57MiB) to Ray cluster...\n",
      "2023-04-29 01:48:08,072\tINFO packaging.py:360 -- Successfully pushed file package 'gcs://_ray_pkg_06d4befa94ba66a2.zip'.\n",
      "2023-04-29 01:48:08,580\tINFO packaging.py:347 -- Pushing file package 'gcs://_ray_pkg_b251e5aa15b6f1da1ee47f252b2efb08.zip' (165.60MiB) to Ray cluster...\n",
      "2023-04-29 01:48:09,138\tINFO packaging.py:360 -- Successfully pushed file package 'gcs://_ray_pkg_b251e5aa15b6f1da1ee47f252b2efb08.zip'.\n",
      "2023-04-29 01:48:14,517\tINFO streaming_executor.py:87 -- Executing DAG InputDataBuffer[Input] -> TaskPoolMapOperator[BatchMapper]\n",
      "2023-04-29 01:48:14,519\tINFO streaming_executor.py:88 -- Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=None), locality_with_output=False, preserve_order=False, actor_locality_enabled=True, verbose_progress=False)\n",
      "2023-04-29 01:48:14,519\tINFO streaming_executor.py:90 -- Tip: To enable per-operator progress reporting, set RAY_DATA_VERBOSE_PROGRESS=1.\n",
      "                                                                                                                   \r"
     ]
    }
   ],
   "source": [
    "from typing import Any\n",
    "import ray\n",
    "from ray.tune.syncer import SyncConfig\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import load_dataset, load_metric\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ray.data.preprocessors import Chain\n",
    "import evaluate\n",
    "\n",
    "MODEL_NAME = \"databricks/dolly-v2-3b\"\n",
    "\n",
    "current_dataset = load_dataset(\"tiny_shakespeare\")\n",
    "\n",
    "from ray.data.preprocessors import BatchMapper\n",
    "\n",
    "\n",
    "def split_text(batch: pd.DataFrame) -> pd.DataFrame:\n",
    "    text = list(batch[\"text\"])\n",
    "    flat_text = \"\".join(text)\n",
    "    split_text = [\n",
    "        x.strip()\n",
    "        for x in flat_text.split(\"\\n\")\n",
    "        if x.strip() and not x.strip()[-1] == \":\"\n",
    "    ]\n",
    "    return pd.DataFrame(split_text, columns=[\"text\"])\n",
    "\n",
    "\n",
    "def tokenize(batch: pd.DataFrame) -> dict:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, padding_side=\"left\")\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    ret = tokenizer(\n",
    "        list(batch[\"text\"]),\n",
    "        truncation=True,\n",
    "        max_length=256,\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=\"np\",\n",
    "    )\n",
    "    ret[\"labels\"] = ret[\"input_ids\"].copy()\n",
    "    return dict(ret)\n",
    "\n",
    "\n",
    "splitter = BatchMapper(split_text, batch_format=\"pandas\")\n",
    "tokenizer = BatchMapper(tokenize, batch_format=\"pandas\")\n",
    "preprocessor = Chain(splitter, tokenizer)\n",
    "\n",
    "ray_datasets = ray.data.from_huggingface(current_dataset)\n",
    "\n",
    "\n",
    "total_train_batches = splitter.fit_transform(ray_datasets[\"train\"]).count()\n",
    "\n",
    "from transformers.models.gpt_neox.modeling_gpt_neox import GPTNeoXLayer\n",
    "\n",
    "class DollyV2Model(pl.LightningModule):\n",
    "    def __init__(self, lr=2e-5, eps=1e-8):\n",
    "        super().__init__()\n",
    "        self.lr = lr\n",
    "        self.eps = eps\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n",
    "\n",
    "        self.metric = evaluate.load(\"accuracy\")\n",
    "        self.predictions = []\n",
    "        self.references = []\n",
    "\n",
    "    def forward(self, batch):\n",
    "        labels = batch[\"labels\"]\n",
    "        input_ids, attention_mask = batch[\"input_ids\"], batch[\"attention_mask\"]\n",
    "        outputs = self.model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs[0]\n",
    "        if self.global_rank == 0:\n",
    "            print(\"loss = \", loss.item())\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss = self.forward(batch)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.AdamW(self.trainer.model.parameters(), lr=self.lr, eps=self.eps)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from ray.train.lightning import LightningTrainer, LightningConfigBuilder\n",
    "from ray.air.config import RunConfig, ScalingConfig, CheckpointConfig\n",
    "from pytorch_lightning.callbacks import TQDMProgressBar\n",
    "\n",
    "from torch.distributed.fsdp.wrap import size_based_auto_wrap_policy, transformer_auto_wrap_policy\n",
    "from torch.distributed.fsdp import ShardingStrategy, MixedPrecision, CPUOffload\n",
    "from pytorch_lightning.callbacks.progress import TQDMProgressBar\n",
    "\n",
    "import functools\n",
    "wrap_policy = functools.partial(\n",
    "    transformer_auto_wrap_policy,\n",
    "    transformer_layer_cls = {GPTNeoXLayer}\n",
    ")\n",
    "\n",
    "mixed_precision_policy = MixedPrecision(\n",
    "    param_dtype=torch.float16,\n",
    "    reduce_dtype=torch.float16,\n",
    "    buffer_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "cpu_offload = CPUOffload(\n",
    "    offload_params=True\n",
    ")\n",
    "\n",
    "class DollyV2Progressbar(TQDMProgressBar):\n",
    "    def __init__(self, num_iters_per_epoch, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.num_iters_per_epoch = num_iters_per_epoch\n",
    "    \n",
    "    def on_train_epoch_start(self, trainer, *_):\n",
    "        super().on_train_epoch_start(trainer, *_)\n",
    "        self.train_progress_bar.reset(self.num_iters_per_epoch)\n",
    "    \n",
    "num_workers = 16\n",
    "batch_size_per_worker = 8\n",
    "num_iters_per_epoch = total_train_batches // (num_workers * batch_size_per_worker)\n",
    "progress_bar = DollyV2Progressbar(num_iters_per_epoch)\n",
    "\n",
    "# Define the configs for LightningTrainer\n",
    "lightning_config = (\n",
    "    LightningConfigBuilder()\n",
    "    .module(cls=DollyV2Model, lr=1e-5, eps=1e-8)\n",
    "    .trainer(\n",
    "        max_epochs=1, \n",
    "        accelerator=\"gpu\", \n",
    "        log_every_n_steps=1,\n",
    "        precision=\"16-mixed\",\n",
    "        limit_train_batches=5,\n",
    "        callbacks=[progress_bar],\n",
    "        # plugins=[mixed_precision_plugin],\n",
    "    )\n",
    "    .checkpointing(monitor=\"train_loss\", mode=\"min\", save_top_k = 1, save_last=True)\n",
    "    .strategy(\n",
    "        name=\"fsdp\",\n",
    "        sharding_strategy=ShardingStrategy.FULL_SHARD,\n",
    "        auto_wrap_policy=wrap_policy,\n",
    "        # cpu_offload=cpu_offload\n",
    "    )\n",
    "    .build()\n",
    ")\n",
    "\n",
    "from ray.tune.syncer import SyncConfig\n",
    "\n",
    "# Save AIR checkpoints according to the performance on validation set\n",
    "run_config = RunConfig(\n",
    "    name=\"ptl-finetune-dolly-v2\",\n",
    "    storage_path=\"s3://large-dl-models-mirror/models--dolly-v2-3b-fp16/model-checkpoint\",\n",
    "    checkpoint_config=CheckpointConfig(),\n",
    ")\n",
    "\n",
    "# Scale the DDP training workload across 4 GPUs\n",
    "# You can change this config based on your compute resources.\n",
    "scaling_config = ScalingConfig(\n",
    "    num_workers=num_workers, use_gpu=True, resources_per_worker={\"CPU\": 8, \"GPU\": 1}\n",
    ")\n",
    "\n",
    "\n",
    "trainer = LightningTrainer(\n",
    "    lightning_config=lightning_config,\n",
    "    run_config=run_config,\n",
    "    scaling_config=scaling_config,\n",
    "    datasets={\"train\": ray_datasets[\"train\"]},\n",
    "    datasets_iter_config={\"batch_size\": batch_size_per_worker},\n",
    "    preprocessor=preprocessor,\n",
    ")\n",
    "result = trainer.fit()\n",
    "\n",
    "result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.train.lightning import LightningCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt = LightningCheckpoint.from_uri(\"s3://large-dl-models-mirror/models--dolly-v2-3b-fp16/model-checkpoint/ptl-finetune-dolly-v2/LightningTrainer_ede1d_00000_0_2023-04-28_17-29-40/checkpoint_000000/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ckpt.get_model(DollyV2Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://large-dl-models-mirror/models--dolly-v2-3b-fp16/model-checkpoint/ptl-finetune-dolly-v2/LightningTrainer_ede1d_00000_0_2023-04-28_17-29-40/checkpoint_000000/_preprocessor to ../s3/ckpt/_preprocessor\n",
      "download: s3://large-dl-models-mirror/models--dolly-v2-3b-fp16/model-checkpoint/ptl-finetune-dolly-v2/LightningTrainer_ede1d_00000_0_2023-04-28_17-29-40/checkpoint_000000/.metadata.pkl to ../s3/ckpt/.metadata.pkl\n",
      "download: s3://large-dl-models-mirror/models--dolly-v2-3b-fp16/model-checkpoint/ptl-finetune-dolly-v2/LightningTrainer_ede1d_00000_0_2023-04-28_17-29-40/checkpoint_000000/.tune_metadata to ../s3/ckpt/.tune_metadata\n",
      "download: s3://large-dl-models-mirror/models--dolly-v2-3b-fp16/model-checkpoint/ptl-finetune-dolly-v2/LightningTrainer_ede1d_00000_0_2023-04-28_17-29-40/checkpoint_000000/.is_checkpoint to ../s3/ckpt/.is_checkpoint\n",
      "download: s3://large-dl-models-mirror/models--dolly-v2-3b-fp16/model-checkpoint/ptl-finetune-dolly-v2/LightningTrainer_ede1d_00000_0_2023-04-28_17-29-40/checkpoint_000000/model to ../s3/ckpt/model\n"
     ]
    }
   ],
   "source": [
    "!aws s3 sync s3://large-dl-models-mirror/models--dolly-v2-3b-fp16/model-checkpoint/ptl-finetune-dolly-v2/LightningTrainer_ede1d_00000_0_2023-04-28_17-29-40/checkpoint_000000/ /home/ray/s3/ckpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)lve/main/config.json: 100%|██████████| 819/819 [00:00<00:00, 185kB/s]\n",
      "Downloading pytorch_model.bin: 100%|██████████| 5.68G/5.68G [00:28<00:00, 201MB/s] \n",
      "Downloading builder script: 100%|██████████| 4.20k/4.20k [00:00<00:00, 6.62MB/s]\n"
     ]
    }
   ],
   "source": [
    "model = DollyV2Model.load_from_checkpoint(\"/home/ray/s3/ckpt/model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)okenizer_config.json: 100%|██████████| 450/450 [00:00<00:00, 279kB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100%|██████████| 2.11M/2.11M [00:00<00:00, 21.2MB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 228/228 [00:00<00:00, 158kB/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"databricks/dolly-v2-3b\", padding_side=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "dolly = model.model.cuda()\n",
    "nlp_pipeline = pipeline(task=\"text-generation\", model=dolly, tokenizer=tokenizer, device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.: Determine whether this is a positive or negative comment: The movie is super gooood! #Response: Positive.: Super gooood is a positive word.: Super gooood is a positive word.\\n\\nSuper gooood is a positive word.\\n\\nSuper gooood is a positive word.\\n\\nSuper gooood is a positive word.\\n\\nSuper gooood is a positive word.\\n\\nSuper gooood is a positive word.\\n\\nSuper gooood is a positive word.\\n\\nSuper gooood is a positive word.\\n\\nSuper'}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "nlp_pipeline(\"Below is an instruction that describes a task. Write a response that appropriately completes the request.: Determine whether this is a positive or negative comment: The movie is super gooood! #Response:\", max_new_tokens = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from instruct_pipeline import InstructionTextGenerationPipeline\n",
    "generate_text = InstructionTextGenerationPipeline(model=dolly, tokenizer=tokenizer, device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = generate_text(\"中国队勇夺世界杯\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"While China's squad was not particularly strong going into the 2008 AFC Youth Championship, they completely turned the tournament around to win the tournament, becoming the first Chinese team to win the title. Coach Guochuan Lai made several shrewd substitutions to match his opponents' style of play, like replacing Cheng Tiantian with Chen Bo and Liu Tao with Yu Haixin. The Chinese also used a 4-3-3 formation as a formation of choice during the tournament, and would also switch back and forth between 4-2-4 and 4-3-3 formations during the tournament.\\nIn the quarterfinals, China played Uzbekistan. The Chinese kept a man-oriented formation, and it seemed to pay off as they opened the scoring through Wang Yong in the 7th minute. However, Uzbekistan turned the game around in the 25th minute, when they capitalized on a Chinese error to score the first goal of the game. From that point onwards, China's offensive performance tailed off, and the Uzbekistan player's kept control of the game. Uzbekistan eventually won 3-1, and in the semis, China faced Tunisia. China adopted a man-oriented formation again, but Tunisia again\""
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[0][\"generated_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
