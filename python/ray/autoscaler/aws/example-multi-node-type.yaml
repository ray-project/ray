# Experimental: an example of configuring a mixed-node-type cluster.
cluster_name: multi_node_type
min_workers: 1
max_workers: 40

# Cloud-provider specific configuration.
provider:
    type: aws
    region: us-west-2
    availability_zone: us-west-2a

# Tell the autoscaler the allowed node types and the resources they provide.
# This only has an effect if you use the experimental request_resources() call.
available_instance_types:
    m4.xlarge:
        resources: {"CPU": 4}
        max_workers: 10
    m4.4xlarge:
        resources: {"CPU": 16, "Custom1": 1}
        max_workers: 10
        node_config:
          # For more documentation on available fields, see:
          # http://boto3.readthedocs.io/en/latest/reference/services/ec2.html#EC2.ServiceResource.create_instances
          InstanceType: m5.large
          ImageId: ami-0a2363a9cff180a64 # Deep Learning AMI (Ubuntu) Version 30

          # You can provision additional disk space with a conf as follows
          BlockDeviceMappings:
              - DeviceName: /dev/sda1
                Ebs:
                    VolumeSize: 120
    p2.xlarge:
        resources: {"CPU": 4, "GPU": 1, "Custom2": 2}
        max_workers: 4
    p2.8xlarge:
        resources: {"CPU": 32, "GPU": 8}
        max_workers: 2

# Specify the node type of the head node (as configured above).
head_node_type: cpu_4_ondemand

# Specify the default type of the worker node (as configured above).
worker_default_node_type: cpu_4_spot

# The default settings for the head node. This will be merged with the per-node
# type configs given above.
head_node:
    ImageId: latest_dlami

# The default settings for worker nodes. This will be merged with the per-node
# type configs given above.
worker_nodes:
    ImageId: latest_dlami

# Configure the cluster for very conservative auto-scaling otherwise.
target_utilization_fraction: 1.0
idle_timeout_minutes: 2

# How Ray will authenticate with newly launched nodes.
auth:
    ssh_user: ubuntu
