apiVersion: cluster.ray.io/v1
kind: RayCluster
metadata:
  name: test-cluster
spec:
  targetUtilizationFraction: 0.8
  idleTimeoutMinutes: 5
  headPodType: head-node
  workerDefaultPodType: worker-nodes
  podTypes:
  - name: head-node
    podConfig:
      apiVersion: v1
      kind: Pod
      metadata:
        generateName: test-cluster-ray-head-
        labels:
          component: ray-head
      spec:
        restartPolicy: Never

        # This volume allocates shared memory for Ray to use for its plasma
        # object store. If you do not provide this, Ray will fall back to
        # /tmp which cause slowdowns if is not a shared memory volume.
        volumes:
        - name: dshm
          emptyDir:
            medium: Memory
        containers:
        - name: ray-node
          imagePullPolicy: Always
          image: rayproject/ray:nightly
          # Do not change this command - it keeps the pod alive until it is
          # explicitly killed.
          command: ["/bin/bash", "-c", "--"]
          args: ['trap : TERM INT; sleep infinity & wait;']
          ports:
          - containerPort: 6379 # Redis port.
          - containerPort: 6380 # Redis port.
          - containerPort: 6381 # Redis port.
          - containerPort: 12345 # Ray internal communication.
          - containerPort: 12346 # Ray internal communication.

          # This volume allocates shared memory for Ray to use for its plasma
          # object store. If you do not provide this, Ray will fall back to
          # /tmp which cause slowdowns if is not a shared memory volume.
          volumeMounts:
          - mountPath: /dev/shm
            name: dshm
          resources:
            requests:
              cpu: 1000m
              memory: 512Mi
            limits:
              # The maximum memory that this pod is allowed to use. The
              # limit will be detected by ray and split to use 10% for
              # redis, 30% for the shared memory object store, and the
              # rest for application memory. If this limit is not set and
              # the object store size is not set manually, ray will
              # allocate a very large object store in each pod that may
              # cause problems for other pods.
              memory: 2Gi
          env:
          # The pod uses this as an argument to 'ray start' command so that
          # Ray can spawn the correct number of processes. Omitting this
          # may lead to degraded performance.
          - name: MYCPUREQUEST
            valueFrom:
              resourceFieldRef:
                resource: requests.cpu
  - name: worker-nodes
    minWorkers: 1
    maxWorkers: 2
    podConfig:
      apiVersion: v1
      kind: Pod
      metadata:
        generateName: test-cluster-ray-worker-
        labels:
          component: ray-worker
      spec:
        restartPolicy: Never
        volumes:
        - name: dshm
          emptyDir:
            medium: Memory
        containers:
        - name: ray-node
          imagePullPolicy: Always
          image: rayproject/ray:nightly
          command: ["/bin/bash", "-c", "--"]
          args: ["trap : TERM INT; sleep infinity & wait;"]
          ports:
          - containerPort: 12345
          - containerPort: 12346
          volumeMounts:
          - mountPath: /dev/shm
            name: dshm
          resources:
            requests:
              cpu: 100m
              memory: 512Mi
            limits:
              memory: 2Gi
          env:
          - name: MYCPUREQUEST
            valueFrom:
              resourceFieldRef:
                resource: requests.cpu
