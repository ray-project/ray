# An unique identifier for the head node and workers of this cluster.
cluster_name: default

# The autoscaler will scale up the cluster to this target fraction of resource
# usage. For example, if a cluster of 10 nodes is 100% busy and
# target_utilization is 0.8, it would resize the cluster to 13. This fraction
# can be decreased to increase the aggressiveness of upscaling.
# This value must be less than 1.0 for scaling to happen.
target_utilization_fraction: 0.8

# If a node is idle for this many minutes, it will be removed.
idle_timeout_minutes: 5

# Kubernetes resources that need to be configured for the autoscaler to be
# able to manage the Ray cluster. If any of the provided resources don't
# exist, the autoscaler will attempt to create them. If this fails, you may
# not have the required permissions and will have to request them to be
# created by your cluster administrator.
provider:
    type: kubernetes

    # Exposing external IP addresses for ray pods isn't currently supported.
    use_internal_ips: true

    # Namespace to use for all resources created.
    namespace: ray

    services:
      # Service that maps to the head node of the Ray cluster.
      - apiVersion: v1
        kind: Service
        metadata:
            # NOTE: If you're running multiple Ray clusters with services
            # on one Kubernetes cluster, they must have unique service
            # names.
            name: ray-head
        spec:
            # This selector must match the head node pod's selector below.
            selector:
                component: ray-head
            ports:
                - protocol: TCP
                  port: 8000
                  targetPort: 8000

      # Service that maps to the worker nodes of the Ray cluster.
      - apiVersion: v1
        kind: Service
        metadata:
            # NOTE: If you're running multiple Ray clusters with services
            # on one Kubernetes cluster, they must have unique service
            # names.
            name: ray-workers
        spec:
            # This selector must match the worker node pods' selector below.
            selector:
                component: ray-worker
            ports:
                - protocol: TCP
                  port: 8000
                  targetPort: 8000

# Kubernetes pod config for the head node pod.
available_node_types:
  head_node:
      resources: {}
      node_config:
        apiVersion: v1
        kind: Pod
        metadata:
            # Automatically generates a name for the pod with this prefix.
            generateName: ray-head-

            # Must match the head node service selector above if a head node
            # service is required.
            labels:
                component: ray-head
        spec:
            # Restarting the head node automatically is not currently supported.
            # If the head node goes down, `ray up` must be run again.
            restartPolicy: Never

            # This volume allocates shared memory for Ray to use for its plasma
            # object store. If you do not provide this, Ray will fall back to
            # /tmp which cause slowdowns if is not a shared memory volume.
            volumes:
            - name: dshm
              emptyDir:
                  medium: Memory

            containers:
            - name: ray-node
              imagePullPolicy: Always
              # You are free (and encouraged) to use your own container image,
              # but it should have the following installed:
              #   - rsync (used for `ray rsync` commands and file mounts)
              #   - screen (used for `ray attach`)
              #   - kubectl (used by the autoscaler to manage worker pods)
              image: rayproject/ray:nightly
              # Do not change this command - it keeps the pod alive until it is
              # explicitly killed.
              command: ["/bin/bash", "-c", "--"]
              args: ["trap : TERM INT; sleep infinity & wait;"]
              ports:
                  - containerPort: 6379 # Redis port.
                  - containerPort: 6380 # Redis port.
                  - containerPort: 6381 # Redis port.
                  - containerPort: 12345 # Ray internal communication.
                  - containerPort: 12346 # Ray internal communication.

              # This volume allocates shared memory for Ray to use for its plasma
              # object store. If you do not provide this, Ray will fall back to
              # /tmp which cause slowdowns if is not a shared memory volume.
              volumeMounts:
                  - mountPath: /dev/shm
                    name: dshm
              resources:
                  requests:
                      cpu: 1000m
                      memory: 512Mi
                  limits:
                      # The maximum memory that this pod is allowed to use. The
                      # limit will be detected by ray and split to use 10% for
                      # redis, 30% for the shared memory object store, and the
                      # rest for application memory. If this limit is not set and
                      # the object store size is not set manually, ray will
                      # allocate a very large object store in each pod that may
                      # cause problems for other pods.
                      memory: 2Gi
              env:
                  # This is used in the head_start_ray_commands below so that
                  # Ray can spawn the correct number of processes. Omitting this
                  # may lead to degraded performance.
                  - name: MY_CPU_REQUEST
                    valueFrom:
                        resourceFieldRef:
                            resource: requests.cpu

  worker_nodes:
      resources: {}
      min_workers: 1
      max_workers: 2
      node_config:
        apiVersion: v1
        kind: Pod
        metadata:
            # Automatically generates a name for the pod with this prefix.
            generateName: ray-worker-

            # Must match the worker node service selector above if a worker node
            # service is required.
            labels:
                component: ray-worker
        spec:
            serviceAccountName: default

            # Worker nodes will be managed automatically by the head node, so
            # do not change the restart policy.
            restartPolicy: Never

            # This volume allocates shared memory for Ray to use for its plasma
            # object store. If you do not provide this, Ray will fall back to
            # /tmp which cause slowdowns if is not a shared memory volume.
            volumes:
            - name: dshm
              emptyDir:
                  medium: Memory

            containers:
            - name: ray-node
              imagePullPolicy: Always
              # You are free (and encouraged) to use your own container image,
              # but it should have the following installed:
              #   - rsync (used for `ray rsync` commands and file mounts)
              image: rayproject/ray:nightly
              # Do not change this command - it keeps the pod alive until it is
              # explicitly killed.
              command: ["/bin/bash", "-c", "--"]
              args: ["trap : TERM INT; sleep infinity & wait;"]
              ports:
                  - containerPort: 12345 # Ray internal communication.
                  - containerPort: 12346 # Ray internal communication.

              # This volume allocates shared memory for Ray to use for its plasma
              # object store. If you do not provide this, Ray will fall back to
              # /tmp which cause slowdowns if is not a shared memory volume.
              volumeMounts:
                  - mountPath: /dev/shm
                    name: dshm
              resources:
                  requests:
                      cpu: 100m
                      memory: 512Mi
                  limits:
                      # This memory limit will be detected by ray and split into
                      # 30% for plasma, and 70% for workers.
                      memory: 2Gi
              env:
                  # This is used in the head_start_ray_commands below so that
                  # Ray can spawn the correct number of processes. Omitting this
                  # may lead to degraded performance.
                  - name: MY_CPU_REQUEST
                    valueFrom:
                        resourceFieldRef:
                            resource: requests.cpu

head_node_type:
  head_node

worker_default_node_type:
  worker_nodes
# Files or directories to copy to the head and worker nodes. The format is a
# dictionary from REMOTE_PATH: LOCAL_PATH, e.g.
file_mounts: {
}

# Files or directories to copy from the head node to the worker nodes. The format is a
# list of paths. The same path on the head node will be copied to the worker node.
# This behavior is a subset of the file_mounts behavior. In the vast majority of cases
# you should just use file_mounts. Only use this if you know what you're doing!
cluster_synced_files: []

# Whether changes to directories in file_mounts or cluster_synced_files in the head node
# should sync to the worker node continuously
file_mounts_sync_continuously: False

# Patterns for files to exclude when running rsync up or rsync down.
# This is not supported on kubernetes.
rsync_exclude: []

# Pattern files to use for filtering out files when running rsync up or rsync down. The file is searched for
# in the source directory and recursively through all subdirectories. For example, if .gitignore is provided
# as a value, the behavior will match git's behavior for finding and using .gitignore files.
# This is not supported on kubernetes.
rsync_filter: []

# List of commands that will be run before `setup_commands`. If docker is
# enabled, these commands will run outside the container and before docker
# is setup.
initialization_commands: []

# List of shell commands to run to set up nodes.
setup_commands: []

# Custom commands that will be run on the head node after common setup.
head_setup_commands: []

# Custom commands that will be run on worker nodes after common setup.
worker_setup_commands: []

# Command to start ray on the head node. You don't need to change this.
# Note webui-host is set to 0.0.0.0 so that kubernetes can port forward.
head_start_ray_commands:
    - ray stop
    - ulimit -n 65536; ray start --head --num-cpus=$MY_CPU_REQUEST --object-manager-port=8076 --dashboard-host 0.0.0.0

# Command to start ray on worker nodes. You don't need to change this.
worker_start_ray_commands:
    - ray stop
    - ulimit -n 65536; ray start --num-cpus=$MY_CPU_REQUEST --address=$RAY_HEAD_IP:6379 --object-manager-port=8076
