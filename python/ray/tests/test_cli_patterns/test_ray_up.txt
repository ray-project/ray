Commands running under a login shell can produce more output than special processing can handle\.
Thus, the output from subcommands will be logged as is\.
Consider using --use-normal-shells, if you tested your workflow and it is compatible\.

Cluster configuration valid

Cluster: test-cli

Bootstraping AWS config
AWS config
  IAM Profile: .+ \[default\]
  EC2 Key pair \(head & workers\): .+ \[default\]
  VPC Subnets \(head & workers\): subnet-.+ \[default\]
  EC2 Security groups \(head & workers\): sg-.+ \[default\]
  EC2 AMI \(head & workers\): ami-.+ \[dlami\]

No head node found\. Launching a new cluster\. Confirm \[y/N\]: y \[automatic, due to --yes\]

Acquiring an up-to-date head node
  Launched 1 nodes \[subnet_id=subnet-.+\]
    Launched instance i-.+ \[state=pending, info=pending\]
  Launched a new head node
  Fetching the new head node

<1/1> Setting up head node
  Prepared bootstrap config
  New status: waiting-for-ssh
  \[1/6\] Waiting for SSH to become available
    Running `uptime` as a test\.
    Fetched IP: .+
    Success\.
  Updating cluster configuration\. \[hash=.+\]
  New status: syncing-files
  \[3/6\] Processing file mounts
    ~/tests/ from ./
  \[4/6\] No worker file mounts to sync
  New status: setting-up
  \[3/5\] Running initialization commands
  \[4/6\] Running setup commands
    \(0/4\) echo a
    \(1/4\) echo b
    \(2/4\) echo \${echo hi}
    \(3/4\) echo head
  \[6/6\] Starting the Ray runtime
  New status: up-to-date

Useful commands
  Monitor autoscaling with
    ray exec .+ 'tail -n 100 -f /tmp/ray/session_\*/logs/monitor\*'
  Connect to a terminal on the cluster head
    ray attach .+
