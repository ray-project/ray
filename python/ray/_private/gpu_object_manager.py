from collections import namedtuple
from typing import TYPE_CHECKING, Any, Dict, List, Optional, Tuple

from ray._raylet import ObjectRef
from ray.actor import ActorHandle

if TYPE_CHECKING:
    import torch

# GPUObjectMeta is a named tuple containing the source actor and tensor metadata.
# The tensor metadata is a list of tuples, each containing the shape and dtype
# of a tensor in the GPU object store.
GPUObjectMeta = namedtuple("GPUObjectMeta", ["src_actor", "tensor_meta"])


class GPUObjectManager:
    def __init__(self):
        # A dictionary that maps from an object ID to a list of tensors.
        #
        # Note: Currently, `gpu_object_store` is only supported for Ray Actors.
        self.gpu_object_store: Dict[str, List["torch.Tensor"]] = {}
        # A dictionary that maps from owned object ref to a metadata tuple: (actor handle, object ref).
        # The actual data of the object is stored at GPU object store of the actor referenced by the ActorHandle.
        # The object ref in the tuple contains a list of tuples, each containing the shape
        # and dtype of a tensor.
        # The entries in this dictionary are 1:1 with ObjectRefs created by this process with a tensor_transport hint and that are currently in scope.
        self.gpu_object_refs: Dict[ObjectRef, GPUObjectMeta] = {}

    def has_gpu_object(self, obj_id: str) -> bool:
        return obj_id in self.gpu_object_store

    def get_gpu_object(self, obj_id: str) -> Optional[List["torch.Tensor"]]:
        return self.gpu_object_store[obj_id]

    def add_gpu_object(self, obj_id: str, gpu_object: List["torch.Tensor"]):
        self.gpu_object_store[obj_id] = gpu_object

    def remove_gpu_object(self, obj_id: str):
        del self.gpu_object_store[obj_id]

    def _get_tensor_meta(self, src_actor: ActorHandle, obj_id: str) -> ObjectRef:
        # Submit a Ray actor task to the source actor to get the tensor metadata.
        # The metadata is a list of tuples, where each tuple contains the shape and dtype
        # of a tensor in the GPU object store. This function returns an ObjectRef that
        # points to the tensor metadata.
        def __ray_get_tensor_meta__(self, obj_id: str):
            from ray._private.worker import global_worker

            gpu_object_manager = global_worker.gpu_object_manager
            assert gpu_object_manager.has_gpu_object(
                obj_id
            ), f"obj_id={obj_id} not found in GPU object store"
            tensors = gpu_object_manager.get_gpu_object(obj_id)
            return [(t.shape, t.dtype) for t in tensors]

        return src_actor.__ray_call__.remote(__ray_get_tensor_meta__, obj_id)

    def add_gpu_object_ref(self, obj_ref: ObjectRef, src_actor: ActorHandle):
        # `obj_ref` is an ObjectRef generated by the `src_actor`'s actor task
        # that is annotated with `@ray.method(tensor_transport=...)`. This function
        # adds the `obj_ref` to the `gpu_object_refs` dictionary so that the coordinator
        # process can determine whether the `obj_ref` is a GPU object reference or not.
        tensor_meta = self._get_tensor_meta(src_actor, obj_ref.hex())
        self.gpu_object_refs[obj_ref] = GPUObjectMeta(
            src_actor=src_actor, tensor_meta=tensor_meta
        )

    # TODO(kevin85421): Call this function to remove the `obj_ref` from the `gpu_object_refs` dictionary
    # to allow garbage collection of the object.
    def remove_gpu_object_ref(self, obj_ref: ObjectRef):
        del self.gpu_object_refs[obj_ref]

    def _get_gpu_object_ref(self, obj_ref: ObjectRef) -> Optional[GPUObjectMeta]:
        return self.gpu_object_refs[obj_ref]

    def _is_gpu_object_ref(self, obj_ref: ObjectRef) -> bool:
        return obj_ref in self.gpu_object_refs

    def _send_gpu_object(self, src_actor: ActorHandle, obj_id: str, dst_rank: int):
        # Send tensors stored in the `src_actor`'s GPU object store to the
        # destination rank `dst_rank`.
        def __ray_send__(self, obj_id: str, dst_rank: int):
            import torch.distributed as dist

            from ray._private.worker import global_worker

            gpu_object_manager = global_worker.gpu_object_manager
            assert gpu_object_manager.has_gpu_object(
                obj_id
            ), f"obj_id={obj_id} not found in GPU object store"
            tensors = gpu_object_manager.get_gpu_object(obj_id)
            for tensor in tensors:
                dist.send(tensor, dst_rank)
            # TODO(kevin85421): The current garbage collection implementation for the
            # in-actor object store is naive. We garbage collect each object after it
            # is consumed once.
            gpu_object_manager.remove_gpu_object(obj_id)

        src_actor.__ray_call__.remote(__ray_send__, obj_id, dst_rank)

    def _recv_gpu_object(
        self,
        dst_actor: ActorHandle,
        obj_id: str,
        src_rank: int,
        tensor_meta: List[Tuple["torch.Size", "torch.dtype"]],
    ):
        # Receive tensors from the source rank and store them in the
        # `dst_actor`'s GPU object store.
        def __ray_recv__(
            self,
            obj_id: str,
            src_rank: int,
            tensor_meta: List[Tuple["torch.Size", "torch.dtype"]],
        ):
            import torch
            import torch.distributed as dist

            from ray._private.worker import global_worker

            gpu_object_manager = global_worker.gpu_object_manager
            tensors = []
            for meta in tensor_meta:
                shape, dtype = meta
                tensor = torch.zeros(shape, dtype=dtype)
                dist.recv(tensor, src_rank)
                tensors.append(tensor)
            gpu_object_manager.add_gpu_object(obj_id, tensors)

        dst_actor.__ray_call__.remote(__ray_recv__, obj_id, src_rank, tensor_meta)

    def trigger_out_of_band_tensor_transfer(
        self, dst_actor: ActorHandle, task_args: Tuple[Any, ...]
    ):
        """
        Triggers tensor communication operations between actors. When an ObjectRef containing
        in-actor tensors (i.e. ObjectRef exists in `gpu_object_refs`) is passed to another
        actor task, CPU data will still be passed through the object store, but the in-actor
        tensors will be passed out-of-band.

        This function triggers the out-of-band tensor transfer by submitting Ray actor
        tasks `__ray_send__` to the sender actor and `__ray_recv__` to the receiver actor to initiate
        tensor communication using protocols like NCCL or GLOO.

        Before the receiver actor executes the actor task, the deserializer combines the
        CPU data with the tensors from the sender actor to reconstruct the original task output
        generated by the sender actor.

        Args:
            dst_actor: The target actor to receive tensors
            task_args: List of arguments for the target actor task that may contain ObjectRefs.
        """
        from ray.experimental.channel import ChannelContext

        ctx = ChannelContext.get_current()

        actor_id_to_rank = {}
        for arg in task_args:
            # If an ObjectRef exists in `gpu_object_refs`, it means the ObjectRef
            # is in-actor tensors. Therefore, this function will trigger a tensor
            # communication operation between the sender and receiver actors.
            if not isinstance(arg, ObjectRef):
                continue

            if not self._is_gpu_object_ref(arg):
                continue
            gpu_object_meta = self._get_gpu_object_ref(arg)

            src_actor = gpu_object_meta.src_actor
            tensor_meta = gpu_object_meta.tensor_meta
            if not actor_id_to_rank:
                # TODO(kevin85421): Support multiple communicators.
                if len(ctx.communicators) != 1:
                    raise ValueError(
                        f"There are {len(ctx.communicators)} communicators in the current context. "
                        "Currently, GPU objects only support 1 communicator. Please make sure only "
                        "one communicator exists."
                    )
                actor_id_to_rank = {
                    a._ray_actor_id: i for i, a in enumerate(ctx.communicators[0])
                }
            if src_actor._ray_actor_id not in actor_id_to_rank:
                raise ValueError(
                    f"Sender actor {src_actor._ray_actor_id} not found in communicator. "
                    "Please make sure the sender and receiver are in the same communicator."
                )
            if dst_actor._ray_actor_id not in actor_id_to_rank:
                raise ValueError(
                    f"Receiver actor {dst_actor._ray_actor_id} not found in communicator. "
                    "Please make sure the sender and receiver are in the same communicator."
                )
            src_rank = actor_id_to_rank[src_actor._ray_actor_id]
            dst_rank = actor_id_to_rank[dst_actor._ray_actor_id]
            if src_rank == dst_rank:
                # If the source and destination ranks are the same, the tensors can
                # be transferred intra-process, so we skip the out-of-band tensor
                # transfer.
                continue
            self._send_gpu_object(src_actor, arg.hex(), dst_rank)
            self._recv_gpu_object(dst_actor, arg.hex(), src_rank, tensor_meta)
