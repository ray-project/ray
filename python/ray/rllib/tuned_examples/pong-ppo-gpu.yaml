# On a Tesla K80 GPU, this achieves the maximum reward in about 1-1.5 hours.
#
# $ python train.py -f tuned_examples/pong-ppo.yaml --num-gpus=1
#
# - PPO_PongDeterministic-v4_0:  TERMINATED [pid=16387], 4984 s, 1117981 ts, 21 rew
# - PPO_PongDeterministic-v4_0:  TERMINATED [pid=83606], 4592 s, 1068671 ts, 21 rew
#
pong-deterministic-ppo:
    env: PongDeterministic-v4
    run: PPO
    repeat: 50
    resources:
        cpu: 6
        gpu: 1
        driver_gpu_limit: 1
    stop:
        episode_reward_mean: 21
        timesteps_total: 800000
    config:
        gamma: 0.99
        horizon: 5000
        devices: ["/gpu:0"]
        observation_filter: NoFilter
        sgd_stepsize:
            eval: 10 ** (np.random.uniform(-5, -2))
        sgd_batchsize:
            eval: random.choice([2 ** v for v in range(3, 10)])
        entropy_coeff:
            eval: np.random.uniform(0, 1)
        vf_loss_coeff:
            eval: np.random.uniform(0, 1)
        lambda:
            eval: np.random.uniform(0, 1)


