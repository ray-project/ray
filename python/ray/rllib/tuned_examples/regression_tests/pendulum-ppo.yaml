pendulum-ppo:
    env: Pendulum-v0
    run: PPO
    stop:
        episode_reward_mean: -160
        # expect -140 within 300-500k steps
        timesteps_total: 600000
    config:
        train_batch_size: 2048
        num_workers: 4
        lambda: 0.1
        gamma: 0.95
        lr: 0.0003
        sgd_batchsize: 64
        num_sgd_iter: 10
        model:
            fcnet_hiddens: [64, 64]
            squash_to_range: True
