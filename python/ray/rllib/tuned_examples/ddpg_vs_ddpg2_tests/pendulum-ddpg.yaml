# This configuration can expect to reach -160 reward in 10k-20k timesteps
pendulum-ddpg:
    env: Pendulum-v0
    run: DDPG
    stop:
        episode_reward_mean: -160
        time_total_s: 600 # 10 minutes
    config:
        timesteps_per_iteration: 600
        tau: 0.001
        buffer_size: 10000
        learning_starts: 500
        train_batch_size: 64
        use_huber: True
        clip_rewards: False
        exploration_fraction: 0.1
        random_starts: False 
        smoothing_num_episodes: 10
        