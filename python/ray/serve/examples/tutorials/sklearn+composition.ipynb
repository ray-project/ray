{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Serve Tutorial: scikit-learn and Model Compositions\n",
    "\n",
    "In this tutorial, we will show you:\n",
    "- how to deploy scikit-learn models with Ray Serve\n",
    "- how to compose multiple models together using RayServeHandle\n",
    "\n",
    "[scikit-learn](https://scikit-learn.org/) is a popular library with easy to use classification and regression models. In this tutorial, we will train several models to tackle a simple classification task with the iris dataset and compose them together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install scikit-learn numpy ray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Training Classifers\n",
    "\n",
    "We will be using the classical `iris` dataset, consisting of 150 samples and 3 categories.\n",
    "\n",
    "Let's train a models and validate its accuracy. We will use gradient boosted trees to start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 1.38\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "data, target, target_names, description, feature_names, _ = load_iris().values()\n",
    "\n",
    "# Instantiate model\n",
    "model = GradientBoostingClassifier()\n",
    "\n",
    "# Training and validation split\n",
    "np.random.shuffle(data), np.random.shuffle(target)\n",
    "train_x, train_y = data[:100], target[:100]\n",
    "val_x, val_y = data[100:], target[100:]\n",
    "\n",
    "# Train and evaluate models\n",
    "model.fit(train_x, train_y)\n",
    "print(\"MSE:\", mean_squared_error(model.predict(val_x), val_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Save the model and deploy to RayServe\n",
    "In this part, we will deploy the model to RayServe and issue queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-08 18:59:16,795\tINFO resource_spec.py:212 -- Starting Ray with 33.06 GiB memory available for workers and up to 0.09 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).\n",
      "2020-04-08 18:59:17,201\tINFO services.py:1148 -- View the Ray dashboard at \u001b[1m\u001b[32mlocalhost:8265\u001b[39m\u001b[22m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=37037)\u001b[0m INFO:     Started server process [37037]\n",
      "\u001b[2m\u001b[36m(pid=37037)\u001b[0m INFO:     Waiting for application startup.\n",
      "\u001b[2m\u001b[36m(pid=37037)\u001b[0m INFO:     Application startup complete.\n"
     ]
    }
   ],
   "source": [
    "from ray import serve\n",
    "\n",
    "serve.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model and label to file\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "with open(\"/tmp/iris_model_logistic_regression.pkl\", \"wb\") as f:\n",
    "    pickle.dump(model, f)\n",
    "with open(\"/tmp/iris_labels.json\", \"w\") as f:\n",
    "    json.dump(target_names.tolist(), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our models are now available as files in temp folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/iris_model_logistic_regression.pkl\n"
     ]
    }
   ],
   "source": [
    "!ls /tmp/iris_model_logistic_regression.pkl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first deploy the logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@serve.route(\"/logistic_regressor\")\n",
    "class LogisticRegressionModel:\n",
    "    def __init__(self):\n",
    "        with open(\"/tmp/iris_model_logistic_regression.pkl\", \"rb\") as f:\n",
    "            self.model = pickle.load(f)\n",
    "        with open(\"/tmp/iris_labels.json\") as f:\n",
    "            self.label_list = json.load(f)\n",
    "\n",
    "    def __call__(self, flask_request):\n",
    "        payload = flask_request.json\n",
    "        print(\"Worker: received flask request with data\", payload)\n",
    "\n",
    "        input_vector = [\n",
    "            payload[\"sepal length\"],\n",
    "            payload[\"sepal width\"],\n",
    "            payload[\"petal length\"],\n",
    "            payload[\"petal width\"],\n",
    "        ]\n",
    "        prediction = self.model.predict([input_vector])[0]\n",
    "        human_name = self.label_list[prediction]\n",
    "        return human_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's query it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'setosa'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=37038)\u001b[0m Worker: received flask request with data {'sepal length': 1.2, 'sepal width': 1.0, 'petal length': 1.1, 'petal width': 0.9}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "sample_request_input = {\n",
    "    \"sepal length\": 1.2,\n",
    "    \"sepal width\": 1.0,\n",
    "    \"petal length\": 1.1,\n",
    "    \"petal width\": 0.9,\n",
    "}\n",
    "response = requests.get(\n",
    "    \"http://localhost:8000/logistic_regressor\", json=sample_request_input\n",
    ")\n",
    "response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Deploy multiple models as a pipeline\n",
    "Often time we need to deploy multiple model to form a pipeline. In this tutorial, we will deploy an _ensemble_ pipeline that compose two models and aggregate their responses. \n",
    "\n",
    "```\n",
    "          -> Logistic Regression -->\n",
    "         /                          \\\n",
    "Request ->                           -> Ensemble -> Response\n",
    "         \\                          /\n",
    "          ->   SVM Classifier   -->\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin, let's train both classifiers and save them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "models = {\"logistic_regression\": LogisticRegression(), \"svm_classifier\": SVC()}\n",
    "for name, model in models.items():\n",
    "    model.fit(train_x, train_y)\n",
    "    with open(\"/tmp/iris_{}.pkl\".format(name), \"wb\") as f:\n",
    "        pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the models saved to disk, let's define our deployment class and deploy them with Serve's low level deployment API. We will enable the sklearn model to accept *Python* input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SKLearnModel:\n",
    "    def __init__(self, model_path, label_path=\"/tmp/iris_labels.json\"):\n",
    "        with open(model_path, \"rb\") as f:\n",
    "            self.model = pickle.load(f)\n",
    "        with open(label_path) as f:\n",
    "            self.label_list = json.load(f)\n",
    "\n",
    "    def __call__(self, flask_request, *, payload=None):\n",
    "        if serve.context.web:\n",
    "            payload = flask_request.json\n",
    "        else:\n",
    "            payload = payload\n",
    "\n",
    "        input_vector = [\n",
    "            payload[\"sepal length\"],\n",
    "            payload[\"sepal width\"],\n",
    "            payload[\"petal length\"],\n",
    "            payload[\"petal width\"],\n",
    "        ]\n",
    "        prediction = self.model.predict([input_vector])[0]\n",
    "        human_name = self.label_list[prediction]\n",
    "        return human_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Serve's endpoints are logical names for the service. It can be associated with several backends. Each backend correspond to a function or a class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "serve.create_endpoint(endpoint_name=\"logistic\", route=\"/logistic\")\n",
    "serve.create_endpoint(endpoint_name=\"svm\", route=\"/smv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "serve.create_backend(SKLearnModel, \"logistic:v1\", \"/tmp/iris_logistic_regression.pkl\")\n",
    "serve.create_backend(SKLearnModel, \"svm:v1\", \"/tmp/iris_svm_classifier.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "serve.link(\"logistic\", \"logistic:v1\")\n",
    "serve.link(\"svm\", \"svm:v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can we implement the ensemble function? Each endpoints in Serve is reachable via RayServeHandle. You can directly call the it via python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "RayServeHandle(\n",
       "    Endpoint=\"svm\",\n",
       "    URL=\"http://127.0.0.1:8000/svm\",\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "handle = serve.get_handle(\"svm\")\n",
    "handle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_input = {\n",
    "    \"sepal length\": 1.2,\n",
    "    \"sepal width\": 1.0,\n",
    "    \"petal length\": 1.1,\n",
    "    \"petal width\": 0.9,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you invoke the handle, an `ObjectID` is returned immediately. The `ObjectID` contains the future result from prediction. You can retrieve the result via `ray.get`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectID(0bead116322a6c2b93415054010000c801000000)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "object_id = handle.remote(payload=sample_input)\n",
    "object_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'versicolor'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ray\n",
    "ray.get(object_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can implement our ensemble function this way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "@serve.route(\"/ensemble\")\n",
    "def ensemble(flask_request):\n",
    "    payload = flask_request.json\n",
    "\n",
    "    # Both requests are fired at the same time\n",
    "    logistic_out_object_id = serve.get_handle(\"logistic\").remote(payload=payload)\n",
    "    svm_out_object_id = serve.get_handle(\"svm\").remote(payload=payload)\n",
    "\n",
    "    # We retrieve from both models together\n",
    "    logistic_out, svm_out = ray.get([logistic_out_object_id, svm_out_object_id])\n",
    "\n",
    "    # We will return both output\n",
    "    return {\"logistic\": logistic_out, \"svm\": svm_out}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"logistic\": \"virginica\",\n",
      "  \"svm\": \"versicolor\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(requests.get(\"http://127.0.0.1:8000/ensemble\", json=sample_input).text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What just happened?\n",
    "\n",
    "There is only a single HTTP request fired to ensemble. The ensemble worker queries logistic regression and SVM _directly_ inside the Serve system without going through HTTP again. This is how Serve enable fast and scalable model composition. Keep in mind that both logistic regression and SVM models are evaluated _in parallel_!\n",
    "\n",
    "![serve_sklearn_ensemble.png](serve_sklearn_ensemble.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
