import abc
import math
from typing import TYPE_CHECKING, Any, Callable, List, Optional

import numpy as np

from ray.data._internal.util import is_null
from ray.data.block import AggType, Block, BlockAccessor, KeyType, T, U
from ray.util.annotations import Deprecated, PublicAPI

if TYPE_CHECKING:
    from ray.data import Schema


@Deprecated(message="AggregateFn is deprecated, please use AggregateFnV2")
@PublicAPI
class AggregateFn:
    """NOTE: THIS IS DEPRECATED, PLEASE USE AggregateFnV2 INSTEAD

    Defines how to perform a custom aggregation in Ray Data.

    `AggregateFn` instances are passed to a Dataset's ``.aggregate(...)`` method to
    specify the steps required to transform and combine rows sharing the same key.
    This enables implementing custom aggregators beyond the standard
    built-in options like Sum, Min, Max, Mean, etc.

    Args:
        init: Function that creates an initial aggregator for each group. Receives a key
            (the group key) and returns the initial accumulator state (commonly 0,
            an empty list, or an empty dictionary).
        merge: Function that merges two accumulators generated by different workers
            into one accumulator.
        name: An optional display name for the aggregator. Useful for debugging.
        accumulate_row: Function that processes an individual row. It receives the current
            accumulator and a row, then returns an updated accumulator. Cannot be
            used if `accumulate_block` is provided.
        accumulate_block: Function that processes an entire block of rows at once. It receives the
            current accumulator and a block of rows, then returns an updated accumulator.
            This allows for vectorized operations. Cannot be used if `accumulate_row`
            is provided.
        finalize: Function that finishes the aggregation by transforming the final
            accumulator state into the desired output. For example, if your
            accumulator is a list of items, you may want to compute a statistic
            from the list. If not provided, the final accumulator state is returned
            as-is.

    Example:
        .. testcode::

            import ray
            from ray.data.aggregate import AggregateFn

            # A simple aggregator that counts how many rows there are per group
            count_agg = AggregateFn(
                init=lambda k: 0,
                accumulate_row=lambda counter, row: counter + 1,
                merge=lambda c1, c2: c1 + c2,
                name="custom_count"
            )
            ds = ray.data.from_items([{"group": "A"}, {"group": "B"}, {"group": "A"}])
            result = ds.groupby("group").aggregate(count_agg).take_all()
            # result: [{'group': 'A', 'custom_count': 2}, {'group': 'B', 'custom_count': 1}]
    """

    def __init__(
        self,
        init: Callable[[KeyType], AggType],
        merge: Callable[[AggType, AggType], AggType],
        name: str,
        accumulate_row: Callable[[AggType, T], AggType] = None,
        accumulate_block: Callable[[AggType, Block], AggType] = None,
        finalize: Optional[Callable[[AggType], U]] = None,
    ):
        if (accumulate_row is None and accumulate_block is None) or (
            accumulate_row is not None and accumulate_block is not None
        ):
            raise ValueError(
                "Exactly one of accumulate_row or accumulate_block must be provided."
            )

        if accumulate_block is None:

            def accumulate_block(a: AggType, block: Block) -> AggType:
                block_acc = BlockAccessor.for_block(block)
                for r in block_acc.iter_rows(public_row_format=False):
                    a = accumulate_row(a, r)
                return a

        if not isinstance(name, str):
            raise TypeError("`name` must be provided.")

        if finalize is None:
            finalize = lambda a: a  # noqa: E731

        self.name = name
        self.init = init
        self.merge = merge
        self.accumulate_block = accumulate_block
        self.finalize = finalize

    def _validate(self, schema: Optional["Schema"]) -> None:
        """Raise an error if this cannot be applied to the given schema."""
        pass


@PublicAPI(stability="alpha")
class AggregateFnV2(AggregateFn, abc.ABC):
    """Provides an interface to implement efficient aggregations to be applied
    to the dataset.

    `AggregateFnV2` instances are passed to a Dataset's ``.aggregate(...)`` method to
    perform distributed aggregations. To create a custom aggregation, you should subclass
    `AggregateFnV2` and implement the `aggregate_block` and `combine` methods.
    The `_finalize` method can also be overridden if the final accumulated state
    needs further transformation.

    Aggregation follows these steps:
        1. **Initialization**: For each group (if grouping) or for the entire dataset,
           an initial accumulator is created using `zero_factory`.
        2. **Block Aggregation**: The `aggregate_block` method is applied to
           each block independently, producing a partial aggregation result for that block.
        3. **Combination**: The `combine` method is used to merge these partial
           results (or an existing accumulated result with a new partial result)
           into a single, combined accumulator.
        4. **Finalization**: Optionally, the `_finalize` method transforms the
           final combined accumulator into the desired output format.

    See built-in aggregations like `Count`, `Sum`, `Min`, `Max`, and `Mean` for 
    examples of how to implement custom aggregations.

    Args:
        name: The name of the aggregation. This will be used as the column name
            in the output, e.g., "sum(my_col)".
        zero_factory: A callable that returns the initial "zero" value for the
            accumulator. For example, for a sum, this would be `lambda: 0`; for
            finding a minimum, `lambda: float("inf")`, for finding a maximum,
            `lambda: float("-inf")`.
        on: The name of the column to perform the aggregation on. If `None`,
            the aggregation is performed over the entire row (e.g., for `Count()`).
        ignore_nulls: Whether to ignore null values during aggregation.
            If `True`, nulls are skipped.
            If `False`, the presence of a null value might result in a null output,
            depending on the aggregation logic.
    """

    def __init__(
        self,
        name: str,
        zero_factory: Callable[[], AggType],
        *,
        on: Optional[str],
        ignore_nulls: bool,
    ):
        if not name:
            raise ValueError(
                f"Non-empty string has to be provided as name (got {name})"
            )

        self._target_col_name = on
        self._ignore_nulls = ignore_nulls

        _safe_combine = _null_safe_combine(self.combine, ignore_nulls)
        _safe_aggregate = _null_safe_aggregate(self.aggregate_block, ignore_nulls)
        _safe_finalize = _null_safe_finalize(self._finalize)

        _safe_zero_factory = _null_safe_zero_factory(zero_factory, ignore_nulls)

        super().__init__(
            name=name,
            init=_safe_zero_factory,
            merge=_safe_combine,
            accumulate_block=lambda _, block: _safe_aggregate(block),
            finalize=_safe_finalize,
        )

    def get_target_column(self) -> Optional[str]:
        return self._target_col_name

    @abc.abstractmethod
    def combine(self, current_accumulator: AggType, new: AggType) -> AggType:
        """Combines a new partial aggregation result with the current accumulator.

        This method defines how two intermediate aggregation states are merged.
        For example, if `aggregate_block` produces partial sums `s1` and `s2` from
        two different blocks, `combine(s1, s2)` should return `s1 + s2`.

        Args:
            current_accumulator: The current accumulated state (e.g., the result of
                previous `combine` calls or an initial value from `zero_factory`).
            new: A new partially aggregated value, typically the output of
                `aggregate_block` from a new block of data, or another accumulator
                from a parallel task.

        Returns:
            The updated accumulator after combining it with the new value.
        """
        ...

    @abc.abstractmethod
    def aggregate_block(self, block: Block) -> AggType:
        """Aggregates data within a single block.

        This method processes all rows in a given `Block` and returns a partial
        aggregation result for that block. For instance, if implementing a sum,
        this method would sum all relevant values within the block.

        Args:
            block: A `Block` of data to be aggregated.

        Returns:
            A partial aggregation result for the input block. The type of this
            result (`AggType`) should be consistent with the `current_accumulator`
            and `new` arguments of the `combine` method, and the `accumulator`
            argument of the `_finalize` method.
        """
        ...

    def _finalize(self, accumulator: AggType) -> Optional[U]:
        """Transforms the final accumulated state into the desired output.

        This method is called once per group after all blocks have been processed
        and all partial results have been combined. It provides an opportunity
        to perform a final transformation on the accumulated data.

        For many aggregations (e.g., Sum, Count, Min, Max), the accumulated state
        is already the final result, so this method can simply return the
        accumulator as is (which is the default behavior).

        For other aggregations, like Mean, this method is crucial.
        A Mean aggregation might accumulate `[sum, count]`. The `_finalize`
        method would then compute `sum / count` to get the final mean.

        Args:
            accumulator: The final accumulated state for a group, after all
                `aggregate_block` and `combine` operations.

        Returns:
            The final result of the aggregation for the group.
        """
        return accumulator

    def _validate(self, schema: Optional["Schema"]) -> None:
        if self._target_col_name:
            from ray.data._internal.planner.exchange.sort_task_spec import SortKey

            SortKey(self._target_col_name).validate_schema(schema)


@PublicAPI
class Count(AggregateFnV2):
    """Defines count aggregation.

    Counts the number of items in each group. If an `on` column is specified,
    it counts non-null values in that column (or all values if `ignore_nulls=False`).
    If `on` is not specified, it counts the total number of rows in the group.

    Usage:
        Counting all rows per group:
        `ds.groupby("group_key").aggregate(Count())`

        Counting non-null values in "value_col" per group (default `ignore_nulls=True`):
        `ds.groupby("group_key").aggregate(Count(on="value_col"))`

        Counting all values (including nulls) in "value_col" per group:
        `ds.groupby("group_key").aggregate(Count(on="value_col", ignore_nulls=False))`

    Args:
        on: Optional name of the column to count values on. If None, counts rows.
        ignore_nulls: Whether to ignore null values when counting. Only applies if
                      `on` is specified. Default is `False` which means `Count()` on a column
                      will count nulls by default. To match pandas default behavior of not counting nulls,
                      set `ignore_nulls=True`.
        alias_name: Optional name for the resulting column.
    """

    def __init__(
        self,
        on: Optional[str] = None,
        ignore_nulls: bool = False, # Note: Default is False for Count to align with SQL COUNT(*)
        alias_name: Optional[str] = None,
    ):
        super().__init__(
            # Name for the output column, e.g., "count()" or "count(col_name)"
            alias_name if alias_name else f"count({on or ''})",
            on=on, # Target column, if any
            ignore_nulls=ignore_nulls,
            # zero_factory provides the initial value for the count, which is 0.
            zero_factory=lambda: 0,
        )

    def aggregate_block(self, block: Block) -> AggType:
        block_accessor = BlockAccessor.for_block(block)

        if self._target_col_name is None:
            # If no target column ('on' was not specified), count all rows in the block.
            return block_accessor.num_rows()

        # If a target column is specified, count values in that column.
        # The behavior regarding nulls is controlled by self._ignore_nulls.
        return block_accessor.count(
            self._target_col_name, ignore_nulls=self._ignore_nulls
        )

    def combine(self, current_accumulator: AggType, new: AggType) -> AggType:
        # To combine two partial counts, simply add them together.
        # current_accumulator is the count from previous blocks/combines.
        # new is the count from a new block.
        return current_accumulator + new
    
    # _finalize is not needed for Count, as the accumulated sum (the count)
    # is already the final desired value.


@PublicAPI
class Sum(AggregateFnV2):
    """Defines sum aggregation.

    Calculates the sum of values in the specified `on` column for each group.
    The `on` column must be specified and should contain numerical data.

    Usage:
        `ds.groupby("group_key").aggregate(Sum(on="value_col"))`

    Args:
        on: The name of the numerical column to sum. Must be provided.
        ignore_nulls: Whether to ignore null values during summation. If `True` (default),
                      nulls are skipped. If `False`, the sum will be null if any
                      value in the group is null.
        alias_name: Optional name for the resulting column.
    """

    def __init__(
        self,
        on: Optional[str] = None,
        ignore_nulls: bool = True, # Default is True, common for sum operations
        alias_name: Optional[str] = None,
    ):
        super().__init__(
            # Name for the output column, e.g., "sum(col_name)"
            alias_name if alias_name else f"sum({str(on)})",
            on=on, # Target numerical column to sum
            ignore_nulls=ignore_nulls,
            # zero_factory provides the initial value for the sum, which is 0.
            zero_factory=lambda: 0,
        )

    def aggregate_block(self, block: Block) -> AggType:
        # For each block, calculate the sum of values in the 'on' column.
        # BlockAccessor.sum() handles nulls based on self._ignore_nulls.
        return BlockAccessor.for_block(block).sum(
            self._target_col_name, self._ignore_nulls
        )

    def combine(self, current_accumulator: AggType, new: AggType) -> AggType:
        # To combine two partial sums, add them together.
        # current_accumulator is the sum from previous blocks/combines.
        # new is the sum from a new block.
        return current_accumulator + new
    
    # _finalize is not needed for Sum, as the accumulated sum
    # is already the final desired value.


@PublicAPI
class Min(AggregateFnV2):
    """Defines min aggregation.

    Finds the minimum value in the specified `on` column for each group.
    The `on` column must be specified and should contain comparable data types
    (e.g., numerical, string, datetime).

    Usage:
        `ds.groupby("group_key").aggregate(Min(on="value_col"))`

    Args:
        on: The name of the column to find the minimum value from. Must be provided.
        ignore_nulls: Whether to ignore null values. If `True` (default), nulls are
                      skipped. If `False`, the minimum will be null if any value in
                      the group is null (for most data types, or follow type-specific
                      comparison rules with nulls).
        alias_name: Optional name for the resulting column.
    """

    def __init__(
        self,
        on: Optional[str] = None,
        ignore_nulls: bool = True,
        alias_name: Optional[str] = None,
    ):
        super().__init__(
            # Name for the output column, e.g., "min(col_name)"
            alias_name if alias_name else f"min({str(on)})",
            on=on, # Target column to find the minimum value from
            ignore_nulls=ignore_nulls,
            # zero_factory provides the initial value for finding a minimum.
            # Positive infinity is used so any actual value will be smaller.
            zero_factory=lambda: float("+inf"),
        )

    def aggregate_block(self, block: Block) -> AggType:
        # For each block, find the minimum value in the 'on' column.
        # BlockAccessor.min() handles nulls based on self._ignore_nulls.
        return BlockAccessor.for_block(block).min(
            self._target_col_name, self._ignore_nulls
        )

    def combine(self, current_accumulator: AggType, new: AggType) -> AggType:
        # To combine two partial minimums, return the smaller of the two.
        # current_accumulator is the minimum from previous blocks/combines.
        # new is the minimum from a new block.
        return min(current_accumulator, new)
    
    # _finalize is not needed for Min, as the accumulated minimum
    # is already the final desired value.


@PublicAPI
class Max(AggregateFnV2):
    """Defines max aggregation.

    Finds the maximum value in the specified `on` column for each group.
    The `on` column must be specified and should contain comparable data types
    (e.g., numerical, string, datetime).

    Usage:
        `ds.groupby("group_key").aggregate(Max(on="value_col"))`

    Args:
        on: The name of the column to find the maximum value from. Must be provided.
        ignore_nulls: Whether to ignore null values. If `True` (default), nulls are
                      skipped. If `False`, the maximum will be null if any value in
                      the group is null (for most data types, or follow type-specific
                      comparison rules with nulls).
        alias_name: Optional name for the resulting column.
    """

    def __init__(
        self,
        on: Optional[str] = None,
        ignore_nulls: bool = True, # Default is True, common for max operations
        alias_name: Optional[str] = None,
    ):
        super().__init__(
            # Name for the output column, e.g., "max(col_name)"
            alias_name if alias_name else f"max({str(on)})",
            on=on, # Target column to find the maximum value from
            ignore_nulls=ignore_nulls,
            # zero_factory provides the initial value for finding a maximum.
            # Negative infinity is used so any actual value will be larger.
            zero_factory=lambda: float("-inf"),
        )

    def aggregate_block(self, block: Block) -> AggType:
        # For each block, find the maximum value in the 'on' column.
        # BlockAccessor.max() handles nulls based on self._ignore_nulls.
        return BlockAccessor.for_block(block).max(
            self._target_col_name, self._ignore_nulls
        )

    def combine(self, current_accumulator: AggType, new: AggType) -> AggType:
        # To combine two partial maximums, return the larger of the two.
        # current_accumulator is the maximum from previous blocks/combines.
        # new is the maximum from a new block.
        return max(current_accumulator, new)
    
    # _finalize is not needed for Max, as the accumulated maximum
    # is already the final desired value.


@PublicAPI
class Mean(AggregateFnV2):
    """Defines mean (average) aggregation.

    Calculates the mean of values in the specified `on` column for each group.
    The `on` column must be specified and should contain numerical data.

    Usage:
        `ds.groupby("group_key").aggregate(Mean(on="value_col"))`

    Args:
        on: The name of the numerical column to calculate the mean on. Must be provided.
        ignore_nulls: Whether to ignore null values. If `True` (default), nulls are
                      skipped. If `False`, the mean will be null if any value in the
                      group is null.
        alias_name: Optional name for the resulting column.
    """

    def __init__(
        self,
        on: Optional[str] = None,
        ignore_nulls: bool = True, # Default is True, common for mean operations
        alias_name: Optional[str] = None,
    ):
        super().__init__(
            # Name for the output column, e.g., "mean(col_name)"
            alias_name if alias_name else f"mean({str(on)})",
            on=on, # Target numerical column for mean calculation
            ignore_nulls=ignore_nulls,
            # The accumulator is a list: [current_sum, current_count].
            # zero_factory initializes this to [0, 0].
            # NOTE: We copy the returned list `list([0,0])` as some internal mechanisms
            # might modify accumulators in-place.
            zero_factory=lambda: list([0, 0]),  # noqa: C410
        )

    def aggregate_block(self, block: Block) -> AggType:
        block_acc = BlockAccessor.for_block(block)
        # Calculate count of non-null values in the target column for this block.
        count = block_acc.count(self._target_col_name, self._ignore_nulls)

        if count == 0 or count is None:
            # If block is empty or all values are null (and ignore_nulls=True),
            # the partial sum/count is undefined for this block.
            # This will be handled by null-safe wrappers (returns None).
            return None

        # Calculate sum of values in the target column for this block.
        sum_ = block_acc.sum(self._target_col_name, self._ignore_nulls)

        if is_null(sum_):
            # If sum is null (e.g., ignore_nulls=False and a null was encountered),
            # return the null sum directly.
            return sum_

        # Return the partial accumulator for this block: [block_sum, block_count]
        return [sum_, count]

    def combine(self, current_accumulator: AggType, new: AggType) -> AggType:
        # current_accumulator is [sum_A, count_A] from previous combines/blocks.
        # new is [sum_B, count_B] from a new block.
        # Combine by adding respective sums and counts.
        return [current_accumulator[0] + new[0], current_accumulator[1] + new[1]]

    def _finalize(self, accumulator: AggType) -> Optional[U]:
        # The final accumulator for a group is [total_sum, total_count].
        if accumulator[1] == 0:
            # If total_count is 0 (e.g., group was empty or all nulls ignored),
            # the mean is undefined. Return NaN
            return np.nan

        # Calculate mean: total_sum / total_count.
        return accumulator[0] / accumulator[1]


@PublicAPI
class Std(AggregateFnV2):
    """Defines standard deviation aggregation.

    Calculates the standard deviation of values in the specified `on` column for
    each group. If the `on` column is not specified, it will result in an error.

    Uses Welford's online algorithm for numerical stability. This method computes
    the standard deviation in a single pass. Results may differ slightly from
    libraries like NumPy or Pandas that use a two-pass algorithm but are generally
    more accurate.

    See: https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#Welford's_online_algorithm

    Example:
        `ds.groupby("group_key").aggregate(Std(on="value_col"))`

    Args:
        on: The name of the column to calculate standard deviation on.
        ddof: Delta Degrees of Freedom. The divisor used in calculations is `N - ddof`,
            where `N` is the number of elements. Default is 1.
        ignore_nulls: Whether to ignore null values. Default is True.
        alias_name: Optional name for the resulting column.
    """

    def __init__(
        self,
        on: Optional[str] = None,
        ddof: int = 1,
        ignore_nulls: bool = True,
        alias_name: Optional[str] = None,
    ):
        super().__init__(
            alias_name if alias_name else f"std({str(on)})",
            on=on, # Target column for standard deviation
            ignore_nulls=ignore_nulls,
            # Accumulator: [M2, mean, count]
            # M2: sum of squares of differences from the current mean
            # mean: current mean
            # count: current count of non-null elements
            # We need to copy the list as it might be modified in-place by some aggregations.
            zero_factory=lambda: list([0, 0, 0]),  # noqa: C410
        )

        self._ddof = ddof

    def aggregate_block(self, block: Block) -> AggType:
        block_acc = BlockAccessor.for_block(block)
        # Count non-null elements in the target column for this block
        count = block_acc.count(self._target_col_name, ignore_nulls=self._ignore_nulls)
        if count == 0 or count is None:
            # If block is empty or all values are null (and ignore_nulls=True),
            # return None. This will be handled by null-safe wrappers.
            return None
        # Calculate sum for the block
        sum_ = block_acc.sum(self._target_col_name, self._ignore_nulls)
        if is_null(sum_):
            # If sum is null (e.g., ignore_nulls=False and a null was encountered),
            # return the null value directly.
            return sum_
        # Calculate mean for the block
        mean = sum_ / count
        # Calculate M2 (sum of squares of differences from the mean) for the block
        M2 = block_acc.sum_of_squared_diffs_from_mean(
            self._target_col_name, self._ignore_nulls, mean
        )
        # Return the partial accumulator: [M2, mean, count]
        return [M2, mean, count]

    def combine(self, current_accumulator: List[float], new: List[float]) -> AggType:
        # Merges two accumulators [M2, mean, count] using a parallel algorithm.
        # See: https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#Parallel_algorithm
        M2_a, mean_a, count_a = current_accumulator
        M2_b, mean_b, count_b = new
        delta = mean_b - mean_a
        count = count_a + count_b

        if count == 0:
            # Avoid division by zero if combined count is zero (e.g. both empty)
            return [0, 0, 0] # Return a zero state

        # Numerically stable way to calculate the combined mean
        mean = (mean_a * count_a + mean_b * count_b) / count
        # Update M2 (sum of squared differences)
        M2 = M2_a + M2_b + (delta**2) * count_a * count_b / count
        return [M2, mean, count]

    def _finalize(self, accumulator: List[float]) -> Optional[U]:
        # Final accumulator: [M2, mean, count]
        M2, mean, count = accumulator
        # Denominator for variance calculation is count - ddof
        if count - self._ddof <= 0:
            # If count - ddof is not positive, variance/std is undefined (or zero).
            # Return NaN, consistent with pandas/numpy.
            return np.nan
        # Standard deviation is the square root of variance (M2 / (count - ddof))
        return math.sqrt(M2 / (count - self._ddof))


@PublicAPI
class AbsMax(AggregateFnV2):
    """Defines absolute max aggregation.

    Calculates the maximum of the absolute values in the specified `on` column
    for each group. The `on` column must be specified and should contain
    numerical data.

    Example:
        `ds.groupby("group_key").aggregate(AbsMax(on="value_col"))`

    Args:
        on: The name of the column to calculate absolute maximum on. Must be provided.
        ignore_nulls: Whether to ignore null values. Default is True.
        alias_name: Optional name for the resulting column.
    """

    def __init__(
        self,
        on: Optional[str] = None,
        ignore_nulls: bool = True,
        alias_name: Optional[str] = None,
    ):
        if on is None or not isinstance(on, str):
            raise ValueError(f"Column to aggregate on has to be provided (got {on})")

        super().__init__(
            alias_name if alias_name else f"abs_max({str(on)})",
            on=on, # Target column for absolute max
            ignore_nulls=ignore_nulls,
            # Initial accumulator for absolute max is 0.
            # Any absolute value will be >= 0.
            zero_factory=lambda: 0,
        )

    def aggregate_block(self, block: Block) -> AggType:
        block_accessor = BlockAccessor.for_block(block)

        # Find the maximum value in the block for the target column
        max_ = block_accessor.max(self._target_col_name, self._ignore_nulls)
        # Find the minimum value in the block for the target column
        min_ = block_accessor.min(self._target_col_name, self._ignore_nulls)

        if is_null(max_) or is_null(min_):
            # If either max or min is null (e.g., block is empty or all nulls
            # and ignore_nulls=True), the abs_max is undefined for this block.
            # This will be handled by null-safe wrappers.
            return None

        # The absolute max for the block is the greater of abs(max_) and abs(min_)
        return max(
            abs(max_),
            abs(min_),
        )

    def combine(self, current_accumulator: AggType, new: AggType) -> AggType:
        # Given two absolute maximums (current_accumulator from previous combines/blocks,
        # and new from a new block), return the larger of the two.
        return max(current_accumulator, new)
    
    # _finalize is not needed here, as the accumulated absolute maximum 
    # is already the final desired value.


@PublicAPI
class Quantile(AggregateFnV2):
    """Defines Quantile aggregation.

    Calculates the specified quantile `q` for the values in the `on` column
    for each group. The `on` column must be specified.

    Example:
        To calculate the median (0.5 quantile):
        `ds.groupby("group_key").aggregate(Quantile(on="value_col", q=0.5))`

        To calculate the 75th percentile (0.75 quantile):
        `ds.groupby("group_key").aggregate(Quantile(on="value_col", q=0.75))`

    Args:
        on: The name of the column to calculate the quantile on. Must be provided.
        q: The quantile to compute, which must be between 0 and 1 inclusive.
           For example, q=0.5 computes the median.
        ignore_nulls: Whether to ignore null values. Default is True.
        alias_name: Optional name for the resulting column.
    """

    def __init__(
        self,
        on: Optional[str] = None,
        q: float = 0.5,
        ignore_nulls: bool = True,
        alias_name: Optional[str] = None,
    ):
        self._q = q

        super().__init__(
            alias_name if alias_name else f"quantile({str(on)})",
            on=on, # Target column
            ignore_nulls=ignore_nulls,
            # The accumulator is a list of all values seen so far for the group.
            # zero_factory returns an empty list.
            zero_factory=list,
        )

    def combine(self, current_accumulator: List[Any], new: List[Any]) -> List[Any]:
        # Combines two lists of values (accumulators) into one.
        # Handles cases where one or both inputs might not be lists initially
        # (e.g. single items from blocks, or initial None from null_safe_zero_factory).

        if isinstance(current_accumulator, List) and isinstance(new, List):
            # Both are lists, extend current with new.
            current_accumulator.extend(new)
            return current_accumulator

        if isinstance(current_accumulator, List) and (not isinstance(new, List)):
            # Current is a list, new is a single item (or None).
            if new is not None and new != "": # Append if new is a valid item.
                current_accumulator.append(new)
            return current_accumulator

        if isinstance(new, List) and (not isinstance(current_accumulator, List)):
            # New is a list, current is a single item (or None).
            if current_accumulator is not None and current_accumulator != "": # Append if current is valid.
                new.append(current_accumulator)
            return new

        # Neither is a list; create a new list with valid items.
        ls = []
        if current_accumulator is not None and current_accumulator != "":
            ls.append(current_accumulator)
        if new is not None and new != "":
            ls.append(new)
        return ls

    def aggregate_block(self, block: Block) -> AggType:
        block_acc = BlockAccessor.for_block(block)
        ls = []
        # Iterate over rows in the block and extract values from the target column.
        for row in block_acc.iter_rows(public_row_format=False):
            ls.append(row.get(self._target_col_name))
        # Returns a list of all values from the target column in this block.
        return ls

    def _finalize(self, accumulator: List[Any]) -> Optional[U]:
        # Accumulator is the list of all values for the group.
        if self._ignore_nulls:
            # Filter out nulls if ignore_nulls is True.
            accumulator = [v for v in accumulator if not is_null(v)]
        else:
            # If not ignoring nulls, check if any null exists.
            nulls = [v for v in accumulator if is_null(v)]
            if len(nulls) > 0:
                # If nulls are present and not ignored, the quantile is undefined (null).
                # Return the first null encountered to preserve column type if possible.
                return nulls[0]

        if not accumulator:
            # If the list is empty (e.g., all values were null and ignored, or no values),
            # quantile is undefined.
            return None

        # Sort the values to prepare for quantile calculation.
        # The key is identity function, assuming comparable elements.
        key = lambda x: x  # noqa: E731
        input_values = sorted(accumulator)
        
        # Linear interpolation for quantile calculation, similar to numpy.quantile.
        k = (len(input_values) - 1) * self._q
        f = math.floor(k)
        c = math.ceil(k)

        if f == c:
            # If k is an integer, the quantile is the element at that index.
            return key(input_values[int(k)])

        # Interpolate between the elements at floor and ceil indices.
        d0 = key(input_values[int(f)]) * (c - k)
        d1 = key(input_values[int(c)]) * (k - f)

        return round(d0 + d1, 5) # Round to 5 decimal places for consistency.


@PublicAPI
class Unique(AggregateFnV2):
    """Defines unique aggregation.

    Collects all unique values from the specified `on` column for each group.
    If the `on` column is not specified, it may lead to errors or unexpected
    behavior as it typically operates on a specific column's values.

    The result for each group will be a list of unique items. The order of items
    in the list is not guaranteed.

    Usage:
        `ds.groupby("group_key").aggregate(Unique(on="value_col"))`

    Args:
        on: The name of the column from which to collect unique values.
        ignore_nulls: Whether to ignore null values when collecting unique items.
                      Default is True (nulls are excluded).
        alias_name: Optional name for the resulting column.
    """

    def __init__(
        self,
        on: Optional[str] = None,
        ignore_nulls: bool = True,
        alias_name: Optional[str] = None,
    ):
        super().__init__(
            alias_name if alias_name else f"unique({str(on)})",
            on=on, # Target column for unique values
            ignore_nulls=ignore_nulls,
            # The accumulator is a set, to automatically handle uniqueness.
            # zero_factory returns an empty set.
            zero_factory=set,
        )

    def combine(self, current_accumulator: AggType, new: AggType) -> AggType:
        # Combines two sets of unique values (or items that will be converted to sets).
        # `current_accumulator` is typically a set from previous combines/blocks.
        # `new` is typically a list of unique items from a new block (from aggregate_block)
        # or another set from a parallel task.
        # The result is the union of the two sets.
        return self._to_set(current_accumulator) | self._to_set(new)

    def aggregate_block(self, block: Block) -> AggType:
        import pyarrow.compute as pac # Lazy import for pyarrow.compute

        # Access the target column from the block as a PyArrow Array.
        col = BlockAccessor.for_block(block).to_arrow().column(self._target_col_name)
        # Use pyarrow.compute.unique to get unique elements from the column in this block.
        # .to_pylist() converts the Arrow Array of unique values to a Python list.
        # This list will be converted to a set in the `combine` step or by null-safe wrappers.
        return pac.unique(col).to_pylist()

    @staticmethod
    def _to_set(x):
        # Helper to ensure an item is a set for the union operation in `combine`.
        if isinstance(x, set):
            return x
        elif isinstance(x, list):
            # If it's a list (e.g. from aggregate_block), convert to set.
            return set(x)
        else:
            # If it's a single item (e.g. after null_safe_zero_factory gave None, then combine got a single val),
            # put it in a set.
            return {x}
    
    # _finalize is not needed here. The accumulator (a set of unique items)
    # is implicitly converted to a list by Ray Data when results are presented,
    # which is the desired final format.


def _null_safe_zero_factory(zero_factory, ignore_nulls: bool):
    """NOTE: PLEASE READ CAREFULLY BEFORE CHANGING

    Null-safe zero factory is crucial for implementing proper aggregation
    protocol (monoid) w/o the need for additional containers.

    Main hurdle for implementing proper aggregation semantic is to be able to encode
    semantic of an "empty accumulator" and be able to tell it from the case when
    accumulator is actually holding null value:

        - Empty container can be overridden with any value
        - Container holding null can't be overridden if ignore_nulls=False

    However, it's possible for us to exploit asymmetry in cases of ignore_nulls being
    True or False:

        - Case of ignore_nulls=False entails that if there's any "null" in the sequence,
         aggregation is undefined and correspondingly expected to return null

        - Case of ignore_nulls=True in turn, entails that if aggregation returns "null"
        if and only if the sequence does NOT have any non-null value

    Therefore, we apply this difference in semantic to zero-factory to make sure that
    our aggregation protocol is adherent to that definition:

        - If ignore_nulls=True, zero-factory returns null, therefore encoding empty
        container
        - If ignore_nulls=False, couldn't return null as aggregation will incorrectly
        prioritize it, and instead it returns true zero value for the aggregation
        (ie 0 for count/sum, -inf for max, etc).
    """

    if ignore_nulls:

        def _safe_zero_factory(_):
            return None

    else:

        def _safe_zero_factory(_):
            return zero_factory()

    return _safe_zero_factory


def _null_safe_aggregate(
    aggregate: Callable[[Block], AggType],
    ignore_nulls: bool,
) -> Callable[[Block], Optional[AggType]]:
    def _safe_aggregate(block: Block) -> Optional[AggType]:
        result = aggregate(block)
        # NOTE: If `ignore_nulls=True`, aggregation will only be returning
        #       null if the block does NOT contain any non-null elements
        if is_null(result) and ignore_nulls:
            return None

        return result

    return _safe_aggregate


def _null_safe_finalize(
    finalize: Callable[[AggType], AggType]
) -> Callable[[Optional[AggType]], AggType]:
    def _safe_finalize(acc: Optional[AggType]) -> AggType:
        # If accumulator container is not null, finalize.
        # Otherwise, return as is.
        return acc if is_null(acc) else finalize(acc)

    return _safe_finalize


def _null_safe_combine(
    combine: Callable[[AggType, AggType], AggType], ignore_nulls: bool
) -> Callable[[Optional[AggType], Optional[AggType]], Optional[AggType]]:
    """Null-safe combination have to be an associative operation
    with an identity element (zero) or in other words implement a monoid.

    To achieve that in the presence of null values following semantic is
    established:

        - Case of ignore_nulls=True:
            - If current accumulator is null (ie empty), return new accumulator
            - If new accumulator is null (ie empty), return cur
            - Otherwise combine (current and new)

        - Case of ignore_nulls=False:
            - If new accumulator is null (ie has null in the sequence, b/c we're
            NOT ignoring nulls), return it
            - If current accumulator is null (ie had null in the prior sequence,
            b/c we're NOT ignoring nulls), return it
            - Otherwise combine (current and new)
    """

    if ignore_nulls:

        def _safe_combine(
            cur: Optional[AggType], new: Optional[AggType]
        ) -> Optional[AggType]:

            if is_null(cur):
                return new
            elif is_null(new):
                return cur
            else:
                return combine(cur, new)

    else:

        def _safe_combine(
            cur: Optional[AggType], new: Optional[AggType]
        ) -> Optional[AggType]:

            if is_null(new):
                return new
            elif is_null(cur):
                return cur
            else:
                return combine(cur, new)

    return _safe_combine
